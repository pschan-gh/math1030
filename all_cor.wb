@chapter{Technique of solving system of linear equations}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019

<h5 class="notkw">Reference.</h5>
<ol class="ltx_enumerate">
<li class="ltx_item">
Strang : Linear Algebra and Its Applications Section 1.3
</li>
<li class="ltx_item">
Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html Print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf Beezer, Ver 3.5 Sect SSLE (print version p7 - p14)
</li>

</ol>

@section{Introduction}

In this section we give examples of systems of linear equations and solve one example.

@eg
Solve the system of equations

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=3 \\
\displaystyle x_{1}-x_{2}&amp;\displaystyle=4
\end{align*}
@end

<b>Answer.</b>
Equation 1 $-$ 2 $\times$ equation 2

\begin{align*}
\displaystyle 5x_{2}&amp;\displaystyle=-5.
\end{align*}

So $x_{2}=-1$. Substitute $x_{2}=-1$ into equation 1:

\begin{align*}
\displaystyle 2x_{1}+3(-1)=3
\end{align*}

\begin{align*}
\displaystyle x_{1}=3.
\end{align*}

So $x_{1}=3$ and $x_{2}=-1$ is a solution. <b>Checking</b>:

\begin{align*}
\displaystyle 2(3)+3(-1)=3
\end{align*}

\begin{align*}
\displaystyle (3)-(-1)=4.
\end{align*}

In fact it is the unique solution.
$\square$

<b>Main goal</b>: One of the main goals of this course is to solve systems of linear equations with more variables and more equations.

@eg
\begin{align*}
\displaystyle x_{1}+2x_{2}+2x_{3}&amp;\displaystyle=4 \\
\displaystyle x_{1}+3x_{2}+3x_{3}&amp;\displaystyle=5 \\
\displaystyle 2x_{1}+6x_{2}+5x_{3}&amp;\displaystyle=6
\end{align*}
@end
@eg
@label{ex2}
\begin{align*}
\displaystyle x_{1}+2x_{2}+x_{4}&amp;\displaystyle=7 \\
\displaystyle x_{1}+x_{2}+x_{3}-x_{4}&amp;\displaystyle=3 \\
\displaystyle 3x_{1}+x_{2}+5x_{3}-7x_{4}&amp;\displaystyle=1
\end{align*}
@end

@section{System of linear equations of two unknowns}

You should all be very familiar with the procedure of solving a system equations of two unknowns.

@subsection{Substitution}

@eg
\begin{align*}
\displaystyle 3x+4y=2&amp;
\end{align*}

\begin{align*}
\displaystyle 4x+5y=3&amp;
\end{align*}

Use $\eqref{v2sube2}$, we can solve for $y$ in terms of $x$:

\begin{align*}
\displaystyle y=\frac{3}{5}-\frac{4}{5}x.&amp;
\end{align*}

Substitution this into $\eqref{v2sube1}$:

\begin{align*}
\displaystyle 3x+4(\frac{3}{5}-\frac{4}{5}x)=2
\end{align*}

\begin{align*}
\displaystyle \frac{12}{5}-\frac{x}{5}=2
\end{align*}

\begin{align*}
\displaystyle x=2.
\end{align*}

Substituting $x=2$ into ( @ref{v2sube3}), we can solve for $y$:

\begin{align*}
\displaystyle y=\frac{3}{5}-\frac{4}{5}\times 2=-1.
\end{align*}

So the solution is $x=2$, $y=-1$. <b>Remark</b>:
There are other ways to use substitution, for example

<ol class="ltx_enumerate">
<li class="ltx_item">
Solve $x$ by ( @ref{v2sube2}) in terms of $y$ and substitute it into ( @ref{v2sube1})
</li>
<li class="ltx_item">
Solve $y$ by ( @ref{v2sube1}) in terms of $x$ and substitute it into ( @ref{v2sube2})
</li>
<li class="ltx_item">
Solve $x$ by ( @ref{v2sube1}) in terms of $y$ and substitute it into ( @ref{v2sube2})
</li>

</ol>

But you <b>cannot</b> get the solution by

<ol class="ltx_enumerate">
<li class="ltx_item">
Solve $y$ by ( @ref{v2sube2}) in terms of $x$ and substitute it into ( @ref{v2sube2}) (No substitution back to the original equation)
</li>
<li class="ltx_item">
Solve $y$ by ( @ref{v2sube1}) in terms of $x$ and substitute it into ( @ref{v2sube1})
</li>
<li class="ltx_item">
Solve $x$ by ( @ref{v2sube2}) in terms of $y$ and substitute it into ( @ref{v2sube2})
</li>
<li class="ltx_item">
Solve $x$ by ( @ref{v2sube1}) in terms of $y$ and substitute it into ( @ref{v2sube1})
</li>

</ol>
@end

@subsection{Elimination}

@eg
Again, let’s solve the system of linear equations from the previous example.

\begin{align*}
\displaystyle 3x+4y=2&amp;
\end{align*}

\begin{align*}
\displaystyle 4x+5y=3&amp;
\end{align*}

Consider $\eqref{v2elime1}-\frac{3}{4}\eqref{v2elime2}$.

$3x$
+
$4y$
=
2

$-$)
$3x$
+
$\frac{15}{4}y$
=
$\frac{9}{4}$

$\frac{1}{4}y$
=
$-\frac{1}{4}$

Thus $y=-1$. Substituting it into ( @ref{v2elime1}):

\begin{align*}
\displaystyle 3x+4(-1)=2
\end{align*}

\begin{align*}
\displaystyle x=2.
\end{align*}

So we obtain the solution $x=2$, $y=-1$. <b>Remark</b>:

<ol class="ltx_enumerate">
<li class="ltx_item">
The number $\frac{3}{4}$ is so chosen such that the coefficient of $x$ is eliminated.
</li>
<li class="ltx_item">
At some point, we still need to use substitution to get the solution.
</li>

</ol>
@end

@section{System of linear equations of three unknowns}

<blockquote class="ltx_quote">

(Cold joke)
There were two men trying to decide what to do for a living. They
went to see a counselor, and he decided that they had good problem
solving skills.

He tried a test to narrow the area of specialty. He put each man in a
room with a stove, a table, and a pot of water on the table. He said
”Boil the water”. Both men moved the pot from the table to the stove
and turned on the burner to boil the water. Next, he put them into a
room with a stove, a table, and a pot of water on the floor. Again,
he said ”Boil the water”. The first man put the pot on the stove and
turned on the burner. The counselor told him to be an Engineer,
because he could solve each problem individually. The second man
moved the pot from the floor to the table, and then moved the pot from
the table to the stove and turned on the burner. The counselor told
him to be a mathematician because he <b>reduced the problem to a
previously solved problem</b>.

</blockquote>

@subsection{Substitution}

@eg
Solve the following system of linear equations

\begin{align*}
\displaystyle x+2y+2z=4&amp;
\end{align*}

\begin{align*}
\displaystyle x+3y+3z=5&amp;
\end{align*}

\begin{align*}
\displaystyle 2x+6y+5z=6&amp;
\end{align*}

Find $x$ in terms of $y,z$ by ( @ref{v3sube1}) :

\begin{align*}
\displaystyle x=4-2y-2z.&amp;
\end{align*}

Substituting ( @ref{v3sube4}) into ( @ref{v3sube2}), we obtain

\begin{align*}
\displaystyle (4-2y-2z)+3y+3z=5
\end{align*}

i.e.,

\begin{align*}
\displaystyle y+z=1.&amp;
\end{align*}

Substituting ( @ref{v3sube4}) into ( @ref{v3sube3}), we obtain

\begin{align*}
\displaystyle 2(4-2y-2z)+6y+5z=6,
\end{align*}

i.e.,

\begin{align*}
\displaystyle 2y+z=-2&amp;
\end{align*}

The equations are reduced to solving a linear system of equations with two unknowns:

\begin{align*}
\displaystyle \begin{cases}y+z=1\\
2y+z=-2\end{cases}
\end{align*}

Solve $y$ in terms of $z$ by ( @ref{v3sube5}):

\begin{align*}
\displaystyle y=1-z&amp;
\end{align*}

Then substitute $y=1-z$ into ( @ref{v3sube6}):

\begin{align*}
\displaystyle 2(1-z)+z=-2
\end{align*}

i.e.

\begin{align*}
\displaystyle z=4.
\end{align*}

By ( @ref{v3sube7})

\begin{align*}
\displaystyle y=1-z=-3.
\end{align*}

Substitute $y=-3$, $z=4$ into ( @ref{v3sube4}),

\begin{align*}
\displaystyle x=4-2y-2z=4-2\times(-3)-2\times 4=2.
\end{align*}

Hence $x=2,y=-3,z=4$ is a solution.
@end

@subsection{Elimination}

Using substitution all the way to solve linear equations is not the best way. Instead, we can use elimination to simplify the system of linear equations first.

@eg
We solve the following system by a sequence of equation operations.

\begin{align*}
\displaystyle x+2y+2z&amp;\displaystyle=4 \\
\displaystyle x+3y+3z&amp;\displaystyle=5 \\
\displaystyle 2x+6y+5z&amp;\displaystyle=6 \\
\\
\displaystyle x+2y+2z&amp;\displaystyle=4 \\
\displaystyle 0x+1y+1z&amp;\displaystyle=1 \\
\displaystyle 2x+6y+5z&amp;\displaystyle=6 \\
\\
\displaystyle x+2y+2z&amp;\displaystyle=4 \\
\displaystyle 0x+1y+1z&amp;\displaystyle=1 \\
\displaystyle 0x+2y+1z&amp;\displaystyle=-2 \\
\\
\displaystyle x+2y+2z&amp;\displaystyle=4 \\
\displaystyle 0x+1y+1z&amp;\displaystyle=1 \\
\displaystyle 0x+0y-1z&amp;\displaystyle=-4 \\
\\
\displaystyle x+2y+2z&amp;\displaystyle=4 \\
\displaystyle 0x+1y+1z&amp;\displaystyle=1 \\
\displaystyle 0x+0y+1z&amp;\displaystyle=4 \\
\\
\displaystyle x+2y+2z&amp;\displaystyle=4 \\
\displaystyle y+z&amp;\displaystyle=1 \\
\displaystyle z&amp;\displaystyle=4
\end{align*}

The third equation requires that $z=4$ to be true. Making this substitution into equation 2 we arrive at $y=-3$, and finally, substituting these values of $y$ and $z$ into the first equation, we find that $x=2$.

<b>Remark</b>: We can add several more eliminations to solve $x,y,z$ without substitution:

\begin{align*}
\displaystyle x+2y+2z&amp;\displaystyle=4 \\
\displaystyle 0x+1y+1z&amp;\displaystyle=1 \\
\displaystyle 0x+0y+1z&amp;\displaystyle=4 \\
\\
\displaystyle x+2y+0z&amp;\displaystyle=-4 \\
\displaystyle 0x+1y+0z&amp;\displaystyle=-3 \\
\displaystyle 0x+0y+1z&amp;\displaystyle=4 \\
\\
\displaystyle x+0y+0z&amp;\displaystyle=2 \\
\displaystyle 0x+1y+0z&amp;\displaystyle=-3 \\
\displaystyle 0x+0y+1z&amp;\displaystyle=4
\end{align*}

So $x=2,y=-3,z=4$ is a solution.
@end

@section{More examples}

@eg
\begin{align*}
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}-4x_{2}+x_{3}&amp;\displaystyle=0 \\
\displaystyle x_{1}+x_{2}-2x_{3}&amp;\displaystyle=-1 \\
\\
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\displaystyle x_{1}+x_{2}-2x_{3}&amp;\displaystyle=-1 \\
\\
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\\
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\displaystyle 0x_{1}+0x_{2}+0x_{3}&amp;\displaystyle=0 \\
\\
\displaystyle x_{1}+0x_{2}-\frac{7}{6}x_{3}&amp;\displaystyle=-\frac{2}{3} \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\displaystyle 0x_{1}+0x_{2}+0x_{3}&amp;\displaystyle=0
\end{align*}

We can express $x_{1},x_{2}$ in terms of $x_{3}$:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=-\frac{2}{3}+\frac{7}{6}x_{3} \\
\displaystyle x_{2}&amp;\displaystyle=-\frac{1}{3}+\frac{5}{6}x_{3}
\end{align*}

The solution set is

\begin{align*}
\displaystyle \{(-\frac{2}{3}+\frac{7}{6}a,-\frac{1}{3}+\frac{5}{6}a,a)\,|\,a\text{ real numbers}.\}
\end{align*}
@end
@eg
\begin{align*}
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}-4x_{2}+x_{3}&amp;\displaystyle=0 \\
\displaystyle x_{1}+x_{2}-2x_{3}&amp;\displaystyle=-2 \\
\\
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\displaystyle x_{1}+x_{2}-2x_{3}&amp;\displaystyle=-2 \\
\\
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-3 \\
\\
\displaystyle x_{1}-5x_{2}+3x_{3}&amp;\displaystyle=1 \\
\displaystyle 0x_{1}+6x_{2}-5x_{3}&amp;\displaystyle=-2 \\
\displaystyle 0x_{1}+0x_{2}+0x_{3}&amp;\displaystyle=-1
\end{align*}

The last equation, $0=-1$ has no solution. So the system of linear equations has no solution.
@end
@eg
\begin{align*}
\displaystyle x_{1}+2x_{2}+0x_{3}+x_{4}&amp;\displaystyle=7 \\
\displaystyle x_{1}+x_{2}+x_{3}-x_{4}&amp;\displaystyle=3 \\
\displaystyle 3x_{1}+x_{2}+5x_{3}-7x_{4}&amp;\displaystyle=1 \\
\\
\displaystyle x_{1}+2x_{2}+0x_{3}+x_{4}&amp;\displaystyle=7 \\
\displaystyle 0x_{1}-x_{2}+x_{3}-2x_{4}&amp;\displaystyle=-4 \\
\displaystyle 3x_{1}+x_{2}+5x_{3}-7x_{4}&amp;\displaystyle=1 \\
\\
\displaystyle x_{1}+2x_{2}+0x_{3}+x_{4}&amp;\displaystyle=7 \\
\displaystyle 0x_{1}-x_{2}+x_{3}-2x_{4}&amp;\displaystyle=-4 \\
\displaystyle 0x_{1}-5x_{2}+5x_{3}-10x_{4}&amp;\displaystyle=-20 \\
\\
\displaystyle x_{1}+2x_{2}+0x_{3}+x_{4}&amp;\displaystyle=7 \\
\displaystyle 0x_{1}-x_{2}+x_{3}-2x_{4}&amp;\displaystyle=-4 \\
\displaystyle 0x_{1}+0x_{2}+0x_{3}+0x_{4}&amp;\displaystyle=0 \\
\\
\displaystyle x_{1}+2x_{2}+0x_{3}+x_{4}&amp;\displaystyle=7 \\
\displaystyle 0x_{1}+x_{2}-x_{3}+2x_{4}&amp;\displaystyle=4 \\
\displaystyle 0x_{1}+0x_{2}+0x_{3}+0x_{4}&amp;\displaystyle=0 \\
\\
\displaystyle x_{1}+0x_{2}+2x_{3}-3x_{4}&amp;\displaystyle=-1 \\
\displaystyle 0x_{1}+x_{2}-x_{3}+2x_{4}&amp;\displaystyle=4 \\
\displaystyle 0x_{1}+0x_{2}+0x_{3}+0x_{4}&amp;\displaystyle=0 \\
\\
\displaystyle x_{1}+2x_{3}-3x_{4}&amp;\displaystyle=-1 \\
\displaystyle x_{2}-x_{3}+2x_{4}&amp;\displaystyle=4 \\
\displaystyle 0&amp;\displaystyle=0
\end{align*}

The last equation $0=0$ is always true, so we can ignore it and only consider the first two equations. We can analyze the second equation without consideration of the variable $x_{1}$. It would appear that there is considerable latitude in how we can choose $x_{2}$, $x_{3}$, $x_{4}$ and make this equation true. Let us choose $x_{3}$ and $x_{4}$ to be <b>anything</b> we please, say $x_{3}=a$ and $x_{4}=b$.

Now we can take these arbitrary values for $x_{3}$ and $x_{4}$, substitute them in equation 1,
to obtain

\begin{align*}
\displaystyle x_{1}+2a-3b=-1
\end{align*}

\begin{align*}
\displaystyle x_{1}=-1-2a+3b
\end{align*}

Similarly, equation 2 becomes

\begin{align*}
\displaystyle x_{2}-a+2b=4
\end{align*}

\begin{align*}
\displaystyle x_{2}=4+a-2b
\end{align*}

So our arbitrary choices of values for $x_{3}$ and $x_{4}$ ($a$ and $b$) translate into specific values of $x_{1}$ and $x_{2}$.
Now we can easily and quickly find many more (infinitely more). Suppose we choose $a=5$ and $b=-2$, then we compute

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=-1-2(5)+3(-2)=-17 \\
\displaystyle x_{2}&amp;\displaystyle=4+5-2(-2)=13
\end{align*}

and you can verify that $(x_{1},\,x_{2},\,x_{3},\,x_{4})=(-17,\,13,\,5,\,-2)$ makes all three equations true. The entire solution set is written as

\begin{align*}
\displaystyle \{(-1-2a+3b,\,4+a-2b,\,a,\,b)\,|\,a,b\text{ real numbers}\}.
\end{align*}
@end
@eg
Solve the following system of linear equations:

\begin{align*}
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 2x_{5}&amp;\displaystyle{}=&amp;\displaystyle 2 \\
\displaystyle x_{1}&amp;\displaystyle{}+&amp;\displaystyle 2x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 3x_{5}&amp;\displaystyle{}=&amp;\displaystyle 4 \\
\displaystyle-2x_{1}&amp;\displaystyle{}-&amp;\displaystyle x_{2}&amp;\displaystyle{}-&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 3x_{4}&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 3
\end{align*}

Swap equation 1 and equation 2:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle{}+&amp;\displaystyle 2x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 3x_{5}&amp;\displaystyle{}=&amp;\displaystyle 4 \\
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 2x_{5}&amp;\displaystyle{}=&amp;\displaystyle 2 \\
\displaystyle-2x_{1}&amp;\displaystyle{}-&amp;\displaystyle x_{2}&amp;\displaystyle{}-&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 3x_{4}&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 3
\end{align*}

2 times equation 1, and add it to equation 3:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle{}+&amp;\displaystyle 2x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 3x_{5}&amp;\displaystyle{}=&amp;\displaystyle 4 \\
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 2x_{5}&amp;\displaystyle{}=&amp;\displaystyle 2 \\
\displaystyle 3x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 7x_{4}&amp;\displaystyle{}+&amp;\displaystyle 7x_{5}&amp;\displaystyle{}=&amp;\displaystyle 11
\end{align*}

-3 times equation 2 and add it to equation 3:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle{}+&amp;\displaystyle 2x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 3x_{5}&amp;\displaystyle{}=&amp;\displaystyle 4 \\
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 2x_{5}&amp;\displaystyle{}=&amp;\displaystyle 2 \\
\displaystyle x_{4}&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 5
\end{align*}

Now the system of linear equations looks like an ”inverted stair”.
We can then solve the system of linear equations by substitution (<b>a better method well be given later</b>).
By the last equation:

\begin{align*}
\displaystyle x_{4}=5-x_{5}.
\end{align*}

Solve $x_{2}$ in terms of other variables by equation 2:

\begin{align*}
\displaystyle x_{2}&amp;\displaystyle=2-x_{3}-2x_{4}-2x_{5} \\
&amp;\displaystyle=2-x_{3}-2(5-x_{5})-2x_{5} \\
&amp;\displaystyle=-8-x_{3}
\end{align*}

Solve $x_{1}$ in terms of other variables by equation 1:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=4-2x_{2}-3x_{3}-2x_{4}-3x_{5} \\
&amp;\displaystyle=4-2(-8-x_{3})-3x_{3}-2(5-x_{5})-3x_{5} \\
&amp;\displaystyle=10-x_{3}-x_{5}.
\end{align*}

$x_{3},x_{5}$ can be taken as any values.
Set $x_{3}=a$, $x_{5}=b$, the solution set can be given by

\begin{align*}
\displaystyle \{(10-a-b,-8-a,a,5-b,b)\,|\,a,b\text{ real numbers}\}.
\end{align*}

<b>A better method</b> Instead of subsitution, we use more elimination:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle{}+&amp;\displaystyle 2x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 3x_{5}&amp;\displaystyle{}=&amp;\displaystyle 4 \\
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 2x_{5}&amp;\displaystyle{}=&amp;\displaystyle 2 \\
\displaystyle x_{4}&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 5
\end{align*}

-2 times equation 3 and add it to equation 2:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle{}+&amp;\displaystyle 2x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;\displaystyle 2x_{4}&amp;\displaystyle{}+&amp;\displaystyle 3x_{5}&amp;\displaystyle{}=&amp;\displaystyle 4 \\
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;&amp;&amp;\displaystyle{}=&amp;\displaystyle-8 \\
\displaystyle x_{4}&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 5
\end{align*}

-2 times equation 3 and add it to equation 1:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle{}+&amp;\displaystyle 2x_{2}&amp;\displaystyle{}+&amp;\displaystyle 3x_{3}&amp;\displaystyle{}+&amp;&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle-6 \\
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;&amp;&amp;\displaystyle{}=&amp;\displaystyle-8 \\
\displaystyle x_{4}&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 5
\end{align*}

-2 times equation 2 and add it to equation 1:

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle{}+&amp;&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;&amp;&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 10 \\
\displaystyle x_{2}&amp;\displaystyle{}+&amp;\displaystyle x_{3}&amp;&amp;&amp;\displaystyle{}=&amp;\displaystyle-8 \\
\displaystyle x_{4}&amp;\displaystyle{}+&amp;\displaystyle x_{5}&amp;\displaystyle{}=&amp;\displaystyle 5
\end{align*}

Notice the following:

<ol class="ltx_enumerate">
<li class="ltx_item">
The system of equations looks like an ”inverted” stairs.
</li>
<li class="ltx_item">
The leftmost variables in the equations are $x_{1}$, $x_{2}$ and $x_{4}$.
</li>
<li class="ltx_item">
Only the first equation has variable $x_{1}$.
</li>
<li class="ltx_item">
Only the second equation has variable $x_{2}$.
</li>
<li class="ltx_item">
Only the third equation has variable $x_{4}$.
</li>

</ol>

Move $x_{3},x_{5}$ to another side.

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=10-x_{3}-x_{5} \\
\displaystyle x_{2}&amp;\displaystyle=-8-x_{3} \\
\displaystyle x_{4}&amp;\displaystyle=5-x_{5}
\end{align*}

The right hand sides have $x_{3},x_{5}$ as variables only and $x_{3},x_{5}$ can be taken as any values.
Set $x_{3}=a$, $x_{5}=b$, the solution set can be given by

\begin{align*}
\displaystyle \{(10-a-b,-8-a,a,5-b,b)\,|\,a,b\text{ real numbers}\}.
\end{align*}
@end

<b>Observations</b>:

<ol class="ltx_enumerate">
<li class="ltx_item">
some systems of linear equations have exactly one solution.
</li>
<li class="ltx_item">
some systems of linear equations have no solution.
</li>
<li class="ltx_item">
some systems of linear equations have infinite many solutions. The solutions can be expressed in terms of 1 or 2 (or even more) variables.

</li>

</ol>

@chapter{Geometric interpretation of linear equations and vector spaces}

@section{Row Pictures: Intersections between lines and planes}

@subsection{2-dimensional}

@eg
@label{ex:1}
Solve the equations:

\begin{align*}
\displaystyle \begin{cases}2x-3y=-5,\\
x+y=5.\end{cases}
\end{align*}

There are two variables and we can draw these equations in a (2-dimensional) plane.
Each (linear) equation represented by a straight line.
For the line that corresponds to $2x-3y=-5$, it passes through the point $(0,5/3)$ (take $x=0$ and one gets $y=5/3$) and another the point $(-5/2,0)$. Hence it is determined.
Similarly, the second straight line is the one passes through the points $(0,5)$ and $(5.0)$.

Intersection between lines
Intersection between lines

The solution of the two equations corresponds to the unique intersection $(2,3)$
between the two straight lines.
@end

@eg
@label{ex:2}
The equations (obviously?) has no solution.

\begin{align*}
\displaystyle \begin{cases}2x-3y=-5,\\
2x-3y=0.\end{cases}
\end{align*}

Parallel lines
Parallel lines

This is because the corresponding lines are parallel to each other
and there is no intersections.
@end

@eg
@label{ex:3}
How about the equations:

\begin{align*}
\displaystyle \begin{cases}2x-3y=-5,\\
2x-3y=-5.\end{cases}
\end{align*}

There are infinite solutions:
for any real number $t$, $(x,y)=(t,\frac{2t+5}{3})$ is a solution.
The two lines coincide and their intersection is just the whole line.
@end

@subsection{3-dimensional}

intersection of planes
intersection of planes

@eg
Solve the equations:

\begin{align*}
\displaystyle \begin{cases}2x+3y+3z=3,\\
2x-y+0z=0.\end{cases}
\end{align*}

There are three variables and we work in 3-dimensional space.
Each (linear) equation corresponds to a plane.
For instance, the ones above correspond to the blue and red planes
in Figure  @ref{fig:3d}.
Their intersection is the thick straight line,
where the points on it can be expressed as

\begin{align*}
\displaystyle \{(\frac{3-3t}{8},\frac{3-3t}{4},t)\mid t\in\mathbf{R}\}.
\end{align*}
@end
@eg
The equations have no solution since the corresponding planes are parallel.

\begin{align*}
\displaystyle \begin{cases}2x-y+0z=2,\\
2x-y+0z=0.\end{cases}
\end{align*}
@end
@eg
@title{Singular Case}
Suppose that there are three (linear) equations with three variables
that corresponds to three distinguish planes.
If they do not share a unique intersection,
then either they share no intersections or they intersection is a line.
More precisely, there are four cases.

<ul class="ltx_itemize">
<li class="ltx_item">
All three planes are parallel to each other.
We shall illustrate this by the following picture.
</li>
<li class="ltx_item">
Only two planes are parallel to each other.
</li>
<li class="ltx_item">
The intersection of each pair of planes is a line and
three such lines are parallel to each other.
</li>
<li class="ltx_item">
Their intersection is a line.
</li>
<li class="ltx_item">
Their intersection is a point, e.g. the xy-plane, yz-plane and zx-plane intersect at $(0,0,0)$.
</li>

</ul>
@end
@remark
@title{Higher dimensions}
How does this row picture extend into $n$ dimensions? The $n$ equations will contain
$n$ unknowns. The first equation still determines a “plane”. It is no longer a two dimensional
plane in 3-space; somehow it has “dimension” $n-1$. It must be flat and
extremely thin within $n$-dimensional space, although it would look solid to us.

If time is the fourth dimension, then the plane t = 0 cuts through four-dimensional
space and produces the three-dimensional universe we live in (or rather, the universe as
it was at $t=0$). Another plane is $z=0$, which is also three-dimensional; it is the ordinary
$x-y$ plane taken over all time. Those three-dimensional planes will intersect! They share
the ordinary $x-y$ plane at $t=0$. We are down to two dimensions, and the next plane
leaves a line. Finally a fourth plane leaves a single point. It is the intersection point of 4
planes in 4 dimensions, and it solves the 4 underlying equations.

I will be in trouble if that example from relativity goes any further. The point is that
linear algebra can operate with any number of equations. The first equation produces an
$(n-1)$-dimensional plane in $n$ dimensions, The second plane intersects it (we hope) in
a smaller set of dimension $n-2$. Assuming all goes well, every new plane (every new
equation) reduces the dimension by one. At the end, when all $n$ planes are accounted
for, the intersection has dimension zero. It is a point, it lies on all the planes, and its
coordinates satisfy all $n$ equations. It is the solution!
@end

@section{Column Pictures: Vector point of view}

<b>Skipped. Can be discussed after introducing column vectors</b>

@eg
Back to the equations in Example  @ref{ex:1}, which can be written
in a single vector form equation:

\begin{align*}
\displaystyle x\begin{pmatrix}{2}\\
{1}\end{pmatrix}+y\begin{pmatrix}{-3}\\
{1}\end{pmatrix}=\begin{pmatrix}{-5}\\
{5}\end{pmatrix}=b.
\end{align*}

The problem is to find the combination of the column vectors on the left side that
produces the vector $b$ on the right side.
Note that the vector $b$ is in fact identified with the pint whose coordinates are $-5$ and $5$.
In this case, we look for the combination of $\begin{pmatrix}{2}\\
{1}\end{pmatrix}$ (red vector)
and $\begin{pmatrix}{-3}\\
{1}\end{pmatrix}$ (blue vector) to produce $\begin{pmatrix}{-5}\\
{5}\end{pmatrix}$ (black vector),
cf. Figure  @ref{fig:vec}

Combinations of vectors
Combinations of vectors
@end
@eg
In Example  @ref{ex:2} and Example  @ref{ex:3},
the corresponding equations are

\begin{align*}
\displaystyle x\begin{pmatrix}{2}\\
{2}\end{pmatrix}+y\begin{pmatrix}{-3}\\
{-3}\end{pmatrix}=\begin{pmatrix}{-5}\\
{0}\end{pmatrix}.
\end{align*}

and

\begin{align*}
\displaystyle x\begin{pmatrix}{2}\\
{2}\end{pmatrix}+y\begin{pmatrix}{-3}\\
{-3}\end{pmatrix}=\begin{pmatrix}{-5}\\
{-5}\end{pmatrix}.
\end{align*}

respectively.
The two vectors colinear.
When the target vector is not colinear with them, there is no solution (Example  @ref{ex:2})
and when the target vector is colinear with them, there are infinite solutions (Example  @ref{ex:3}).
@end
@eg
@label{fig:cube}
Let us check a ‘trivial’ 3-dimensional case.

\begin{align*}
\displaystyle x\begin{bmatrix}1\\
0\\
0\end{bmatrix}+y\begin{bmatrix}0\\
1\\
0\end{bmatrix}+z\begin{bmatrix}0\\
0\\
1\end{bmatrix}=\begin{bmatrix}3\\
5\\
4\end{bmatrix}.
\end{align*}

One sees that the solution is $x=3,y=4,z=5$,
which corresponds to the scaling of the red vectors so that
the sum of the rescaled vectors is the blue vector in Figure  @ref{fig:cube}.
More precisely, we have multiplication by scalars

\begin{align*}
\displaystyle 3\begin{bmatrix}1\\
0\\
0\end{bmatrix}=\begin{bmatrix}3\\
0\\
0\end{bmatrix},\quad 4\begin{bmatrix}0\\
1\\
0\end{bmatrix}=\begin{bmatrix}0\\
4\\
0\end{bmatrix},\quad 5\begin{bmatrix}0\\
0\\
1\end{bmatrix}=\begin{bmatrix}0\\
0\\
5\end{bmatrix},
\end{align*}

and the vector addition

\begin{align*}
\displaystyle \begin{bmatrix}3\\
0\\
0\end{bmatrix}+\begin{bmatrix}0\\
4\\
0\end{bmatrix}+\begin{bmatrix}0\\
0\\
5\end{bmatrix}=\begin{bmatrix}3\\
5\\
4\end{bmatrix}.
\end{align*}
@end
@eg
@label{fig:cube}
Let us see a non-trivial case.

\begin{align*}
\displaystyle x\begin{bmatrix}2\\
4\\
-2\end{bmatrix}+y\begin{bmatrix}1\\
-6\\
7\end{bmatrix}+z\begin{bmatrix}1\\
0\\
2\end{bmatrix}=\begin{bmatrix}5\\
-2\\
9\end{bmatrix}.
\end{align*}
@end
@eg
@title{Singular Case}
Suppose that there are three (linear) equations with three variables.
In the column picture, what it happens to the singular case.
So we have three vectors and try to combine them to produce vector $b$.

When the three column vectors are in the same plane $P$
any combination of them falls into $P$.
For instance, the vectors in the following equation are in the plane
$2x+3y+3z=0$.

\begin{align*}
\displaystyle x\begin{bmatrix}\frac{3}{2}\\
-1\\
0\end{bmatrix}+y\begin{bmatrix}0\\
1\\
-1\end{bmatrix}+z\begin{bmatrix}\frac{3}{2}\\
0\\
-1\end{bmatrix}=b.
\end{align*}

Then there are two cases:

<dl class="ltx_description">

<dt class="ltx_item"/>
<dd class="ltx_item">

When

\begin{align*}
\displaystyle b=b_{0}=\begin{bmatrix}0\\
1\\
1\end{bmatrix}
\end{align*}

is not in $P$, then there is no solution.
</dd>

<dt class="ltx_item"/>
<dd class="ltx_item">

When

\begin{align*}
\displaystyle b=b_{\infty}=\begin{bmatrix}3\\
-1\\
-1\end{bmatrix}
\end{align*}

is in $P$, it may have infinite solution.
</dd>

</dl>
@end
@remark

In general, if the $n$ planes have no point in common,
or infinitely many points, then the corresponding $n$
columns lie in the same plane.
@end

@chapter{System of linear equations}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019

The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html Print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf

<h5 class="notkw">Reference.</h5>
 Beezer, Ver 3.5 Sect SSLE (print version p7 - p14)

@section{Introduction}

@definition
@title{System of Linear Equations}
A <b>system of linear equations</b> is a collection of $m$ equations in the variable quantities $x_{1},\,x_{2},\,x_{3},\ldots,x_{n}$ of the form,

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\dots+a_{1n}x_{n}&amp;\displaystyle=b_{1} \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\dots+a_{2n}x_{n}&amp;\displaystyle=b_{2} \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\dots+a_{3n}x_{n}&amp;\displaystyle=b_{3} \\
&amp;\displaystyle\vdots \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+\dots+a_{mn}x_{n}&amp;\displaystyle=b_{m}
\end{align*}

where the values of $a_{ij}$, $b_{i}$ and $x_{j}$, $1\leq i\leq m$, $1\leq j\leq n$, are from the set of real numbers
@end

@definition
$(s_{1},s_{2},\ldots,s_{n})$ is a <b>solution</b> of a system of linear equations in $n$ variables if when we substitute $s_{1}$ for $x_{1}$, $s_{2}$ for $x_{2}$, $s_{3}$ for $x_{3}$, $\cdots$, $s_{n}$ for $x_{n}$, then for every equation of the system the left hand side will equal the right hand side, i.e. all equations are true simultaneously.

The <b>solution set</b> of a linear system of equations is the set which contains every solution to the system, and nothing more.
@end

@eg
The following system of linear equations

\begin{align*}
\displaystyle x_{1}+2x_{2}+x_{4}&amp;\displaystyle=7 \\
\displaystyle x_{1}+x_{2}+x_{3}-x_{4}&amp;\displaystyle=3 \\
\displaystyle 3x_{1}+x_{2}+5x_{3}-7x_{4}&amp;\displaystyle=1
\end{align*}

can be rewritten as

\begin{align*}
\displaystyle 1x_{1}+2x_{2}+0x_{3}+1x_{4}&amp;\displaystyle=7 \\
\displaystyle 1x_{1}+1x_{2}+1x_{3}-1x_{4}&amp;\displaystyle=3 \\
\displaystyle 3x_{1}+1x_{2}+5x_{3}-7x_{4}&amp;\displaystyle=1
\end{align*}

So it is a system of linear equations, with $n=4$ variables and $m=3$ equations. Also,

\begin{align*}
\displaystyle a_{11}&amp;\displaystyle=1&amp;\displaystyle a_{12}&amp;\displaystyle=2&amp;\displaystyle a_{13}&amp;\displaystyle=0&amp;\displaystyle a_{14}&amp;\displaystyle=1&amp;\displaystyle b_{1}&amp;\displaystyle=7 \\
\displaystyle a_{21}&amp;\displaystyle=1&amp;\displaystyle a_{22}&amp;\displaystyle=1&amp;\displaystyle a_{23}&amp;\displaystyle=1&amp;\displaystyle a_{24}&amp;\displaystyle=-1&amp;\displaystyle b_{2}&amp;\displaystyle=3 \\
\displaystyle a_{31}&amp;\displaystyle=3&amp;\displaystyle a_{32}&amp;\displaystyle=1&amp;\displaystyle a_{33}&amp;\displaystyle=5&amp;\displaystyle a_{34}&amp;\displaystyle=-7&amp;\displaystyle b_{3}&amp;\displaystyle=1
\end{align*}

One solution is given by $x_{1}=-2$, $x_{2}=4$, $x_{3}=2$, $x_{4}=1$. In fact, from the previous lecture this system of equations has infinitely many solutions. In fact the solution set is

\begin{align*}
\displaystyle \{(-1-2a+3b,\,4+a-2b,\,a,\,b)\,|\,a,b\text{ real numbers}\}.
\end{align*}
@end

@section{Possibilities for Solution Sets}

@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
The following system of linear equations has <b>only one</b> solution.

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=3 \\
\displaystyle x_{1}-x_{2}&amp;\displaystyle=4
\end{align*}

The solution set is $(x_{1},x_{2})=(3,-1)$
</li>
<li class="ltx_item">
The following system of linear equations has <b>infinite many</b> solutions.

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=3 \\
\displaystyle 4x_{1}+6x_{2}&amp;\displaystyle=6
\end{align*}

The solution set is $\{(x_{1},x_{2})=(t,\frac{3-2t}{3})\}$, where $t$ is any real number.
</li>
<li class="ltx_item">
The following system of linear equations has <b>no</b> solutions.

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=3 \\
\displaystyle 4x_{1}+6x_{2}&amp;\displaystyle=10
\end{align*}

The solution set is empty.
</li>

</ol>
@end
@thm

A system of linear equations can have (1) a unique solution or (2) infinitely many solutions
or (3) no solutions.
@end

<b>Remark</b>: For example it is impossible for a system of linear equation to have exactly $2$ solutions <b>Remark</b>: (can be ignored) For higher mathematics, the above theorem is not entirely true. The coefficients $a_{ij}$ in our course are real numbers. However, in higher mathematics, the coefficients can be something so called <b>finite field</b>.

@section{Equivalent Systems and Equation Operations}

@defn
@title{Equivalent Systems}
Two systems of linear equations are <b>equivalent</b> if their solution sets are equal.
@end

@defn
@title{Equation Operations}
@label{EO}
Given a system of linear equations, the following three operations will transform the system into a different one, and each operation is known as an
<b>equation operation</b>:

<ol class="ltx_enumerate">
<li class="ltx_item">
Swap the locations of two equations in the list of equations.
</li>
<li class="ltx_item">
Multiply each term of an equation by a nonzero quantity.
</li>
<li class="ltx_item">
Multiply each term of one equation by some quantity, and add these terms to a second equation, on both sides of the equality.

</li>

</ol>
@end

@eg
The following two systems equations are equivalent:

\begin{align*}
\displaystyle \begin{cases}2x_{1}+3x_{2}=3\\
x_{1}-x_{2}=4\end{cases}
\end{align*}

and

\begin{align*}
\displaystyle \begin{cases}5x_{2}=-5\\
x_{1}-x_{2}=4\end{cases}
\end{align*}

In fact, the second system of linear equations is obtained by applying operation 3 on equation 1 (namley, replace equation 1 with equation 1 - 2 $\times$ equation 2).
@end

@thm
@title{Equation Operations Preserve Solution Sets}
@label{EOPSS}
If we apply one of the three equation operations of Definition  @ref{EO} to a system of linear equations, then the original system and the transformed system are equivalent.
@end

@proof
@col
(Sketch) Let $S$ be a system of linear equations and $S^{\prime}$ be another system of linear equations obtained by applying equation operations on $S$.

Let $(x_{1},\ldots,x_{n})$ be a solution for $S$. Because $S^{\prime}$ is obtained by equation operations on $S$,
it is obvious that $(x_{1},\ldots,x_{n})$ is also a solution for $S^{\prime}$. Conversely, suppose that $(x_{1},\ldots,x_{n})$ is a solution for $S^{\prime}$.
Note that the reverse of the equation operations are also equation operations.
Reversing the equation operators on $S$, we can show that $S$ can be obtained by equation operations on $S^{\prime}$. So $(x_{1},\ldots,x_{n})$ is also a solution for $S$.
Hence $S$ and $S^{\prime}$ have the same solution set.

For details, See Beezer, Theorem EOPSS (Ver 3.5, print version p. 10).
∎
@end

@chapter{Matrices}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019

The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.htmlPrint version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf

<h5 class="notkw">Reference.</h5>

Beezer, Ver 3.5 Subsection MVNSE (print version p17 - p21) Strang, Sect 1.4

@section{Introduction}

<ul class="ltx_itemize">
<li class="ltx_item">
After solving a few systems of equations, you will recognize that it does not matter so much what we call our variables.
</li>
<li class="ltx_item">
A system in the variables $x_{1},\,x_{2},\,x_{3}$ would behave the same if we changed the names of the variables to $a,\,b,\,c$ and kept all the constants the same and in the same places.
</li>
<li class="ltx_item">
In this section, we will isolate the key bits of information about a system of equations into something called a <b>matrix</b>, and then use this matrix to systematically solve the equations. Along the way we will obtain one of our most important and useful computational tools.
</li>

</ul>

@section{Matrix and Vector Notation for Systems of Equations}

@defn
@title{Matrix}
@label{M}
An $m\times n$ <b>matrix</b> is a rectangular layout of numbers from real numbers having $m$ rows and $n$ columns.

<ul class="ltx_itemize">
<li class="ltx_item">
Many use large parentheses instead of brackets – the distinction is not important.
</li>
<li class="ltx_item">
Rows of a matrix will be referenced starting at the top and working down row 1 is at the top) and columns will be referenced starting from the left (i.e., column 1 is at the left).
</li>
<li class="ltx_item">
For a matrix $A$, the notation $\left[A\right]_{ij}$ will refer to the complex number in row $i$ and column $j$ of $A$.
</li>

</ul>
@end
@eg
\begin{align*}
\displaystyle B=\begin{bmatrix}-1&amp;2&amp;5&amp;3\\
1&amp;0&amp;-6&amp;1\\
-4&amp;2&amp;2&amp;-2\end{bmatrix}
\end{align*}

is a matrix with $m=3$ rows and $n=4$ columns. We can say that $\left[B\right]_{2,3}=-6$ while $\left[B\right]_{3,4}=-2$.
@end

When we do equation operations on system of equations, the names of the variables really are not very important. Use $x_{1}$, $x_{2}$, $x_{3}$, or $a$, $b$, $c$, or $x$, $y$, $z$, it really does not matter. In this subsection we will describe some notation that will make it easier to describe linear systems, solve the systems and describe the solution sets.

@defn
@title{Column Vector}
@label{CV}
<ul class="ltx_itemize">
<li class="ltx_item">
A <b>column vector</b> of <b>size</b> $m$ is an ordered list of $m$ numbers, which is written in order vertically, starting at the top and proceeding to the bottom. At times, we will refer to a column vector as simply a <b>vector</b>.
</li>
<li class="ltx_item">
Column vectors will be written in bold, usually with lower case Latin letter from the end of the alphabet such as $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$, $\mathbf{x}$, $\mathbf{y}$, $\mathbf{z}$.
</li>
<li class="ltx_item">
Some books like to write vectors with arrows, such as $\vec{u}$. Writing by hand, some like to put arrows on top of the symbol, or a tilde underneath the symbol, as in $\underset{\sim}{\textstyle u}$, or a line under the symbol, as $\underline{\textstyle u}$.
</li>
<li class="ltx_item">
To refer to the <b>entry</b> or <b>component</b> of vector $\mathbf{v}$ in location $i$ of the list, we write $\left[\mathbf{v}\right]_{i}$.
</li>

</ul>
@end
@defn
@title{Zero Column Vector}
@label{ZCV}
The <b>zero vector</b> of size $m$ is the column vector of size $m$ where each entry is the number zero,

\begin{align*}
\displaystyle \mathbf{0}=\begin{bmatrix}0\\
0\\
0\\
\vdots\\
0\end{bmatrix}
\end{align*}

or defined much more compactly, $\left[\mathbf{0}\right]_{i}=0$ for $1\leq i\leq m$.
@end

@section{Partition of matrices}

<b>Partition of matrices</b>: Sometimes we put horizontal lines or vertical lines to divide the matrix into different areas. It is same as the matrix without the lines.

@eg
The matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc}1&amp;2&amp;3&amp;4&amp;3.5\\
0&amp;-1&amp;1&amp;1.1&amp;1\\
3&amp;5.8&amp;1&amp;0&amp;-3\\
1&amp;8&amp;0&amp;0&amp;7\end{array}\right]
\end{align*}

is same as the following matrices:

\begin{align*}
\displaystyle \left[\begin{array}[]{cccc|c}1&amp;2&amp;3&amp;4&amp;3.5\\
0&amp;-1&amp;1&amp;1.1&amp;1\\
3&amp;5.8&amp;1&amp;0&amp;-3\\
1&amp;8&amp;0&amp;0&amp;7\end{array}\right],\,\,\,\left[\begin{array}[]{cc|cc|c}1&amp;2&amp;3&amp;4&amp;3.5\\
0&amp;-1&amp;1&amp;1.1&amp;1\\
3&amp;5.8&amp;1&amp;0&amp;-3\\
1&amp;8&amp;0&amp;0&amp;7\end{array}\right]
\end{align*}

,

\begin{align*}
\displaystyle \left[\begin{array}[]{c|c|c|c|c}1&amp;2&amp;3&amp;4&amp;3.5\\
0&amp;-1&amp;1&amp;1.1&amp;1\\
3&amp;5.8&amp;1&amp;0&amp;-3\\
1&amp;8&amp;0&amp;0&amp;7\end{array}\right],\,\,\,\left[\begin{array}[]{ccccc}1&amp;2&amp;3&amp;4&amp;3.5\\
0&amp;-1&amp;1&amp;1.1&amp;1\\
\hline 3&amp;5.8&amp;1&amp;0&amp;-3\\
1&amp;8&amp;0&amp;0&amp;7\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \left[\begin{array}[]{cc|ccc}1&amp;2&amp;3&amp;4&amp;3.5\\
0&amp;-1&amp;1&amp;1.1&amp;1\\
\hline 3&amp;5.8&amp;1&amp;0&amp;-3\\
1&amp;8&amp;0&amp;0&amp;7\end{array}\right],\,\,\,\left[\begin{array}[]{cc|ccc}1&amp;2&amp;3&amp;4&amp;3.5\\
0&amp;-1&amp;1&amp;1.1&amp;1\\
\hline 3&amp;5.8&amp;1&amp;0&amp;-3\\
\hline 1&amp;8&amp;0&amp;0&amp;7\end{array}\right]
\end{align*}
@end
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2\\
3&amp;4\\
5&amp;6\\
7&amp;8\end{bmatrix},\,\,\,\mathbf{u}=\begin{bmatrix}9\\
10\\
11\\
12\end{bmatrix}\,\,\,\mathbf{v}=\begin{bmatrix}13\\
14\\
15\\
16\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle [A|\mathbf{u}]=\left[\begin{array}[]{cc|c}1&amp;2&amp;9\\
3&amp;4&amp;10\\
5&amp;6&amp;11\\
6&amp;8&amp;12\end{array}\right]=\left[\begin{array}[]{ccc}1&amp;2&amp;9\\
3&amp;4&amp;10\\
5&amp;6&amp;11\\
6&amp;8&amp;12\end{array}\right],
\end{align*}

\begin{align*}
\displaystyle [A|\mathbf{u}|\mathbf{v}]=\left[\begin{array}[]{cc|c|c}1&amp;2&amp;9&amp;13\\
3&amp;4&amp;10&amp;14\\
5&amp;6&amp;11&amp;15\\
6&amp;8&amp;12&amp;15\end{array}\right]=\left[\begin{array}[]{cccc}1&amp;2&amp;9&amp;13\\
3&amp;4&amp;10&amp;14\\
5&amp;6&amp;11&amp;15\\
6&amp;8&amp;12&amp;15\end{array}\right],
\end{align*}
@end
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2\\
3&amp;4\end{bmatrix},\,\,B=\begin{bmatrix}5&amp;6&amp;7\\
8&amp;9&amp;10\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle C=\begin{bmatrix}11&amp;12\\
13&amp;14\\
15&amp;16\end{bmatrix},D=\begin{bmatrix}21&amp;22&amp;23\\
24&amp;25&amp;26\\
27&amp;28&amp;29\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle [A|B]=\left[\begin{array}[]{cc|ccc}1&amp;2&amp;5&amp;6&amp;7\\
3&amp;4&amp;8&amp;9&amp;10\end{array}\right]=\left[\begin{array}[]{ccccc}1&amp;2&amp;5&amp;6&amp;7\\
3&amp;4&amp;8&amp;9&amp;10\end{array}\right],
\end{align*}

\begin{align*}
\displaystyle \left[\begin{array}[]{c|c}A&amp;B\\
\hline C&amp;D\end{array}\right]=\left[\begin{array}[]{cc|ccc}1&amp;2&amp;5&amp;6&amp;7\\
3&amp;4&amp;8&amp;9&amp;10\\
\hline 11&amp;12&amp;21&amp;22&amp;23\\
13&amp;14&amp;24&amp;25&amp;26\\
15&amp;16&amp;27&amp;28&amp;29\end{array}\right]=\left[\begin{array}[]{ccccc}1&amp;2&amp;5&amp;6&amp;7\\
3&amp;4&amp;8&amp;9&amp;10\\
11&amp;12&amp;21&amp;22&amp;23\\
13&amp;14&amp;24&amp;25&amp;26\\
15&amp;16&amp;27&amp;28&amp;29\end{array}\right]
\end{align*}
@end

@section{Matrix Representations of Linear Systems}

@defn
@title{Coefficient Matrix}
@label{CM}
For a system of linear equations,

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\dots+a_{1n}x_{n}&amp;\displaystyle=b_{1} \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\dots+a_{2n}x_{n}&amp;\displaystyle=b_{2} \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\dots+a_{3n}x_{n}&amp;\displaystyle=b_{3} \\
\displaystyle\vdots&amp; \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+\dots+a_{mn}x_{n}&amp;\displaystyle=b_{m}
\end{align*}

The <b>coefficient matrix</b> is the $m\times n$ matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;\dots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;\dots&amp;a_{2n}\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;\dots&amp;a_{3n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;a_{m3}&amp;\dots&amp;a_{mn}\\
\end{bmatrix}
\end{align*}
@end

@defn
@title{Vector of Constants}
@label{VOC}
For a system of linear equations,

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\dots+a_{1n}x_{n}&amp;\displaystyle=b_{1} \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\dots+a_{2n}x_{n}&amp;\displaystyle=b_{2} \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\dots+a_{3n}x_{n}&amp;\displaystyle=b_{3} \\
\displaystyle\vdots&amp; \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+\dots+a_{mn}x_{n}&amp;\displaystyle=b_{m}
\end{align*}

the <b>vector of constants</b> is the column vector of size $m$

\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}b_{1}\\
b_{2}\\
b_{3}\\
\vdots\\
b_{m}\\
\end{bmatrix}
\end{align*}
@end

@defn
@title{Solution Vector}
@label{SOLV}
For a system of linear equations,

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\dots+a_{1n}x_{n}&amp;\displaystyle=b_{1} \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\dots+a_{2n}x_{n}&amp;\displaystyle=b_{2} \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\dots+a_{3n}x_{n}&amp;\displaystyle=b_{3} \\
\displaystyle\vdots&amp; \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+\dots+a_{mn}x_{n}&amp;\displaystyle=b_{m}
\end{align*}

the <b>solution vector</b> is the column vector of size $n$

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
\vdots\\
x_{n}\\
\end{bmatrix}
\end{align*}
@end

@defn
@title{Matrix Representation of a Linear System}
@label{MRLS}
If $A$ is the coefficient matrix of a system of linear equations and $\mathbf{b}$ is the vector of constants, then we will write $A\mathbf{x}=\mathbf{b}$ as a shorthand expression for the system of linear equations, which we will refer to as the <b>matrix representation</b> of the linear system.
@end

@defn
@title{Augmented Matrix}
@label{AM}
Suppose we have a system of $m$ equations in $n$ variables, with coefficient matrix $A$ and vector of constants $\mathbf{b}$. Then the <b>augmented matrix</b> of the system of equations is the $m\times(n+1)$ matrix whose first $n$ columns are the columns of $A$ and whose last column ($n+1$) is the column vector $\mathbf{b}$. This matrix will be written as $\left[A|\mathbf{b}\right]$.
@end

@eg
Notation for systems of linear equations

The system of linear equations

\begin{align*}
\displaystyle 2x_{1}+4x_{2}-3x_{3}+5x_{4}+x_{5}&amp;\displaystyle=9 \\
\displaystyle 3x_{1}+x_{2}+\quad\quad x_{4}-3x_{5}&amp;\displaystyle=0 \\
\displaystyle-2x_{1}+7x_{2}-5x_{3}+2x_{4}+2x_{5}&amp;\displaystyle=-3
\end{align*}

has coefficient matrix

\begin{align*}
\displaystyle A=\left[\begin{array}[]{ccccc}2&amp;4&amp;-3&amp;5&amp;1\\
3&amp;1&amp;0&amp;1&amp;-3\\
-2&amp;7&amp;-5&amp;2&amp;2\end{array}\right]
\end{align*}

and vector of constants

\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}9\\
0\\
-3\end{bmatrix}
\end{align*}

and so will be referenced as $A\mathbf{x}=\mathbf{b}$.
The augmented matrix is

\begin{align*}
\displaystyle \left[A|\mathbf{b}\right]=\left[\begin{array}[]{ccccc|c}2&amp;4&amp;-3&amp;5&amp;1&amp;9\\
3&amp;1&amp;0&amp;1&amp;-3&amp;0\\
-2&amp;7&amp;-5&amp;2&amp;2&amp;-3\end{array}\right]
\end{align*}
@end

@section{Row operations}

An augmented matrix can be used to represent a system of linear equations and release us from writing out all the variables.
We have seen how certain operations we can perform on equations will preserve their solutions .
The next two definitions and the following theorem carry over these ideas to augmented matrices.

@defn
@title{Row Operations}
@label{RO}
The following three operations will transform an $m\times n$ matrix into a different matrix of the same size, and each is known as a <b>row operation</b>.

<ol class="ltx_enumerate">
<li class="ltx_item">
Swap the locations of two rows.
</li>
<li class="ltx_item">
Multiply each entry of a single row by a nonzero quantity.
</li>
<li class="ltx_item">
Multiply each entry of one row by some quantity, and add these values to the entries in the same columns of a second row. Leave the first row the same after this operation, but replace the second row by the new values.
</li>

</ol>

We will use a symbolic shorthand to describe these row operations:

<ol class="ltx_enumerate">
<li class="ltx_item">
$R_{i}\leftrightarrow R_{j}$: Swap the location of rows $i$ and $j$.
</li>
<li class="ltx_item">
$\alpha R_{i}$: Multiply row $i$ by the nonzero scalar $\alpha$.
</li>
<li class="ltx_item">
$\alpha R_{i}+R_{j}$: Multiply row $i$ by the scalar $\alpha$ and add to row $j$.
</li>

</ol>
@end
@defn
@title{Row-Equivalent Matrices}
@label{REM}
Two matrices, $A$ and $B$, are <b>row-equivalent</b> if one can be obtained from the other by a sequence of row operations.
@end

<b>Remark</b> Notice that each of the three row operations is reversible, so we do not have to be careful about the distinction between $A$ is row-equivalent to $B$ and $B$ is row-equivalent to $A$.

@eg
The matrices

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;-1&amp;3&amp;4\\
5&amp;2&amp;-2&amp;3\\
1&amp;1&amp;0&amp;6\end{bmatrix}&amp;\displaystyle B=\begin{bmatrix}1&amp;1&amp;0&amp;6\\
3&amp;0&amp;-2&amp;-9\\
2&amp;-1&amp;3&amp;4\end{bmatrix}
\end{align*}

are row-equivalent as can be seen from

\begin{align*}
\displaystyle\begin{bmatrix}2&amp;-1&amp;3&amp;4\\
5&amp;2&amp;-2&amp;3\\
1&amp;1&amp;0&amp;6\end{bmatrix}\xrightarrow{R_{1}\leftrightarrow R_{3}}\begin{bmatrix}1&amp;1&amp;0&amp;6\\
5&amp;2&amp;-2&amp;3\\
2&amp;-1&amp;3&amp;4\end{bmatrix}&amp;\displaystyle\xrightarrow{-2R_{1}+R_{2}}\begin{bmatrix}1&amp;1&amp;0&amp;6\\
3&amp;0&amp;-2&amp;-9\\
2&amp;-1&amp;3&amp;4\end{bmatrix}
\end{align*}

We can also say that any pair of these three matrices are row-equivalent.
@end
@thm
@title{Row-Equivalent Matrices represent Equivalent Systems}
@label{REMES}
Suppose that $A$ and $B$ are row-equivalent augmented matrices. Then the systems of linear equations that they represent are equivalent systems.
@end
@proof
@col
See Beezer, Theorem REMES (Ver 3.5 print version p.20)
∎
@end

Our strategy for solving system of linear equations

<ol class="ltx_enumerate">
<li class="ltx_item">
Begin with a system of equations, represent the system by an augmented matrix.
</li>
<li class="ltx_item">
perform row operations (which will preserve solutions for the system) to get a ”simpler” augmented matrix
</li>
<li class="ltx_item">
convert back to a ”simpler” system of equations and then solve that system, knowing that its solutions are those of the original system.
</li>

</ol>

@eg
\begin{align*}
\displaystyle x_{1}+2x_{2}+2x_{3}&amp;\displaystyle=4 \\
\displaystyle x_{1}+3x_{2}+3x_{3}&amp;\displaystyle=5 \\
\displaystyle 2x_{1}+6x_{2}+5x_{3}&amp;\displaystyle=6
\end{align*}

Form the augmented matrix,

\begin{align*}
\displaystyle A=\left[\begin{array}[]{ccc|c}1&amp;2&amp;2&amp;4\\
1&amp;3&amp;3&amp;5\\
2&amp;6&amp;5&amp;6\end{array}\right]
\end{align*}

and apply row operations,

\begin{align*}
\displaystyle\xrightarrow{-1R_{1}+R_{2}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}1&amp;2&amp;2&amp;4\\
0&amp;1&amp;1&amp;1\\
2&amp;6&amp;5&amp;6\end{array}\right]\xrightarrow{-2R_{1}+R_{3}}\left[\begin{array}[]{ccc|c}1&amp;2&amp;2&amp;4\\
0&amp;1&amp;1&amp;1\\
0&amp;2&amp;1&amp;-2\end{array}\right] \\
\displaystyle\xrightarrow{-2R_{2}+R_{3}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}1&amp;2&amp;2&amp;4\\
0&amp;1&amp;1&amp;1\\
0&amp;0&amp;-1&amp;-4\end{array}\right]\xrightarrow{-1R_{3}}\left[\begin{array}[]{ccc|c}1&amp;2&amp;2&amp;4\\
0&amp;1&amp;1&amp;1\\
0&amp;0&amp;1&amp;4\end{array}\right]
\end{align*}

So the matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}1&amp;2&amp;2&amp;4\\
0&amp;1&amp;1&amp;1\\
0&amp;0&amp;1&amp;4\end{array}\right]
\end{align*}

is row equivalent to $A$. The system of equations below has the same solution set as the original system of equations.

\begin{align*}
\displaystyle x_{1}+2x_{2}+2x_{3}&amp;\displaystyle=4 \\
\displaystyle x_{2}+x_{3}&amp;\displaystyle=1 \\
\displaystyle x_{3}&amp;\displaystyle=4
\end{align*}

By the third equation requires that $x_{3}=4$ to be true. Making this substitution into equation 2 we arrive at $x_{2}=-3$, and finally, substituting these values of $x_{2}$ and $x_{3}$ into the first equation, we find that $x_{1}=2$.
@end

@chapter{More about matrices}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019

The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section MO, Section MM (print version p125 - p147)Strang, Sect 1.4 and Sect 1.6

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf

(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section MO (p52-56), all. Section MM (p57-60), all except T12 and T35.

@section{Matrix Equality, Addition, Scalar Multiplication}
@label{MEASM}
Recall $M_{mn}$ is the set of $m\times n$ matrices with real entries. Throughout the section, unless otherwise stated,

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;\cdots&amp;a_{mn}\end{bmatrix},\,\,B=\begin{bmatrix}b_{11}&amp;b_{12}&amp;\cdots&amp;b_{1n}\\
b_{21}&amp;b_{22}&amp;\cdots&amp;b_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
b_{m1}&amp;b_{m2}&amp;\cdots&amp;b_{mn}\end{bmatrix}
\end{align*}

@defn
@title{Matrix Equality}
The $m\times n$ matrices $A$ and $B$ are <b>equal</b>, written $A=B$ provided $\left[A\right]_{ij}=\left[B\right]_{ij}$ for all $1\leq i\leq m$, $1\leq j\leq n$, i.e.

\begin{align*}
\displaystyle a_{ij}=b_{ij}\text{ for all $i,j$}.
\end{align*}
@end
@defn
@title{Matrix Addition}
@label{MA}
Given $m\times n$ matrices $A$ and $B$, define the <b>sum</b> of $A$ and $B$ as an $m\times n$ matrix, written $A+B$, according to

\begin{align*}
\displaystyle\left[A+B\right]_{ij}&amp;\displaystyle=\left[A\right]_{ij}+\left[B\right]_{ij}&amp;\displaystyle 1\leq i\leq m,\,1\leq j\leq n,
\end{align*}

i.e.,

\begin{align*}
\displaystyle A+B=\begin{bmatrix}a_{11}+b_{11}&amp;a_{12}+b_{12}&amp;\cdots&amp;a_{1n}+b_{1n}\\
a_{21}+b_{21}&amp;a_{22}+b_{22}&amp;\cdots&amp;a_{2n}+b_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}+b_{m1}&amp;a_{m2}+b_{m2}&amp;\cdots&amp;a_{mn}+b_{mn}\end{bmatrix}
\end{align*}
@end
@eg
<b>Addition of two matrices in $M_{23}$</b>

If

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;-3&amp;4\\
1&amp;0&amp;-7\end{bmatrix}&amp;\displaystyle B=\begin{bmatrix}6&amp;2&amp;-4\\
3&amp;5&amp;2\end{bmatrix}
\end{align*}

then

\begin{align*}
\displaystyle A+B&amp;\displaystyle=\begin{bmatrix}2&amp;-3&amp;4\\
1&amp;0&amp;-7\end{bmatrix}+\begin{bmatrix}6&amp;2&amp;-4\\
3&amp;5&amp;2\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}2+6&amp;-3+2&amp;4+(-4)\\
1+3&amp;0+5&amp;-7+2\end{bmatrix}=\begin{bmatrix}8&amp;-1&amp;0\\
4&amp;5&amp;-5\end{bmatrix}
\end{align*}
@end
@defn
@title{Matrix Scalar Multiplication}
Given $m\times n$ matrix $A$
and a scalar $\alpha\in{\mathbb{R}}^{\hbox{}}$, the <b>scalar multiple</b> of $A$ by $\alpha$ is the $m\times n$ matrix, written $\alpha A$ and defined according to

\begin{align*}
\displaystyle\left[\alpha A\right]_{ij}&amp;\displaystyle=\alpha\left[A\right]_{ij}&amp;\displaystyle\quad 1\leq i\leq m,\,1\leq j\leq n,
\end{align*}

i.e.,

\begin{align*}
\displaystyle \alpha A=\begin{bmatrix}\alpha a_{11}&amp;\alpha a_{12}&amp;\cdots&amp;\alpha a_{1n}\\
\alpha a_{21}&amp;\alpha a_{22}&amp;\cdots&amp;\alpha a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
\alpha a_{m1}&amp;\alpha a_{m2}&amp;\cdots&amp;\alpha a_{mn}\end{bmatrix}.
\end{align*}
@end

Notice again that we have yet another kind of multiplication, and it is again written putting two symbols side-by-side. Computationally, scalar matrix multiplication is very easy.

@eg
<b>Scalar multiplication in $M_{32}$</b>

If

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;8\\
-3&amp;5\\
0&amp;1\end{bmatrix}
\end{align*}

and $\alpha=7$, then

\begin{align*}
\displaystyle \alpha A=7\begin{bmatrix}2&amp;8\\
-3&amp;5\\
0&amp;1\end{bmatrix}=\begin{bmatrix}7(2)&amp;7(8)\\
7(-3)&amp;7(5)\\
7(0)&amp;7(1)\end{bmatrix}=\begin{bmatrix}14&amp;56\\
-21&amp;35\\
0&amp;7\end{bmatrix}.
\end{align*}
@end
@defn
@title{Zero Matrix}
The $m\times n$ <b>zero matrix</b> is written as ${\cal O}={\cal O}_{m\times n}$ and defined by $\left[{\cal O}\right]_{ij}=0$, for all $1\leq i\leq m$, $1\leq j\leq n$, i.e.

\begin{align*}
\displaystyle {\cal O}={\cal O}_{m\times n}=\begin{bmatrix}0&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;0\end{bmatrix}.
\end{align*}
@end
@defn
@title{Additive Inverse}
The additive inverse of a matrix $A\in M_{mn}$, denoted by $-A$ is defined by $-A=(-1)A$, i.e.

\begin{align*}
\displaystyle -A=\begin{bmatrix}-a_{11}&amp;-a_{12}&amp;\cdots&amp;-a_{1n}\\
-a_{21}&amp;-a_{22}&amp;\cdots&amp;-a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a_{m1}&amp;-a_{m2}&amp;\cdots&amp;-a_{mn}\end{bmatrix}.
\end{align*}
@end

Below are some obvious properties satisfied by addition and scalar multiplication:

<ol class="ltx_enumerate">
<li class="ltx_item">
Commutativity, MatricesIf $A,\,B\in M_{mn}$, then $A+B=B+A$.
</li>
<li class="ltx_item">
Additive Associativity, MatricesIf $A,\,B,\,C\in M_{mn}$, then $A+\left(B+C\right)=\left(A+B\right)+C$.
</li>
<li class="ltx_item">
Zero Matrix, Matrices$A+{\cal O}=A$ for all $A\in M_{mn}$.
</li>
<li class="ltx_item">
Additive Inverses, Matrices$A+(-A)={\cal O}$.
</li>
<li class="ltx_item">
Scalar Multiplication Associativity, MatricesIf $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $A\in M_{mn}$, then $\alpha(\beta A)=(\alpha\beta)A$.
</li>
<li class="ltx_item">
Distributivity across Matrix Addition, MatricesIf $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $A,\,B\in M_{mn}$, then $\alpha(A+B)=\alpha A+\alpha B$.
</li>
<li class="ltx_item">
Distributivity across Scalar Addition, MatricesIf $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $A\in M_{mn}$, then
$(\alpha+\beta)A=\alpha A+\beta A$.
</li>
<li class="ltx_item">
One, Matrices
If $A\in M_{mn}$, then $1A=A$.
</li>

</ol>

As an example, we prove property  @ref{DSAM}, $(\alpha+\beta)A=\alpha A+\beta A$. We need to establish the equality of two matrices.

For any $i$ and $j$, $1\leq i\leq m$, $1\leq j\leq n$,

\begin{align*}
\displaystyle\left[(\alpha+\beta)A\right]_{ij}&amp;\displaystyle=(\alpha+\beta)\left[A\right]_{ij} \\
&amp;\displaystyle=\alpha\left[A\right]_{ij}+\beta\left[A\right]_{ij} \\
&amp;\displaystyle=\left[\alpha A\right]_{ij}+\left[\beta A\right]_{ij} \\
&amp;\displaystyle=\left[\alpha A+\beta A\right]_{ij}.
\end{align*}

Hence by the definition of equality of matrices, $(\alpha+\beta)A=\alpha A+\beta A$, as required.

@section{Transposes and Symmetric Matrices}
@label{TSM}
We describe one more common operation which can be performed on matrices. Informally, to transpose a matrix is to build a new matrix by swapping its rows and columns.

@defn
@title{Transpose of a Matrix}
@label{TM}
Given an $m\times n$ matrix $A$, its <b>transpose</b> is the $n\times m$ matrix $A^{t}$ given by

\begin{align*}
\displaystyle \left[A^{t}\right]_{ij}=\left[A\right]_{ji},\quad 1\leq i\leq n,\,1\leq j\leq m,
\end{align*}

i.e.

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;\dots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;\dots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;a_{n3}&amp;\dots&amp;a_{nm}\\
\end{bmatrix}^{t}=\begin{bmatrix}a_{11}&amp;a_{21}&amp;a_{31}&amp;\dots&amp;a_{m1}\\
a_{12}&amp;a_{22}&amp;a_{32}&amp;\dots&amp;a_{m2}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{1n}&amp;a_{2n}&amp;a_{3n}&amp;\dots&amp;a_{mn}\\
\end{bmatrix}.
\end{align*}
@end
@eg
<b>Transpose of a $3\times 4$ matrix</b>
Suppose that

\begin{align*}
\displaystyle D=\begin{bmatrix}3&amp;7&amp;2&amp;-3\\
-1&amp;4&amp;2&amp;8\\
0&amp;3&amp;-2&amp;5\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle D^{t}=\begin{bmatrix}3&amp;-1&amp;0\\
7&amp;4&amp;3\\
2&amp;2&amp;-2\\
-3&amp;8&amp;5\end{bmatrix}
\end{align*}
@end
@defn
@title{Symmetric Matrix}
@label{SYM}
A matrix $A$ is <b>symmetric</b> if $A=A^{t}$, i.e.

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\end{bmatrix}
\end{align*}

with

\begin{align*}
\displaystyle a_{ij}=a_{ji}\text{ for all $i,j$}.
\end{align*}
@end
@eg
<b>A symmetric $5\times 5$ matrix</b>The matrix

\begin{align*}
\displaystyle E=\begin{bmatrix}2&amp;3&amp;-9&amp;5&amp;7\\
3&amp;1&amp;6&amp;-2&amp;-3\\
-9&amp;6&amp;0&amp;-1&amp;9\\
5&amp;-2&amp;-1&amp;4&amp;-8\\
7&amp;-3&amp;9&amp;-8&amp;-3\end{bmatrix}
\end{align*}

is symmetric.
@end
@thm
@title{Symmetric Matrices are Square}
@label{SMS}
Suppose that $A$ is a symmetric matrix. Then $A$ is square.
@end
@proof
@col
Suppose $A$ is a $n\times m$ matrix. Then $A^{t}$ is a $m\times n$ matrix.
In order for $A$ and $A^{t}$ to be equal, they must have the same dimension. Hence $n=m$.
∎
@end

We finish this section with three easy theorems which illustrate the interplay of our three new operations, our new notation, and the techniques used to prove matrix equalities.

@thm
@title{Transpose and Matrix Addition}
Suppose that $A$ and $B$ are $m\times n$ matrices. Then $(A+B)^{t}=A^{t}+B^{t}$.
@end
@proof
@col
For $1\leq i\leq n$, $1\leq j\leq m$,

\begin{align*}
\displaystyle\left[(A+B)^{t}\right]_{ij}&amp;\displaystyle=\left[A+B\right]_{ji} \\
&amp;\displaystyle=\left[A\right]_{ji}+\left[B\right]_{ji} \\
&amp;\displaystyle=\left[A^{t}\right]_{ij}+\left[B^{t}\right]_{ij} \\
&amp;\displaystyle=\left[A^{t}+B^{t}\right]_{ij}
\end{align*}

Since the matrices $(A+B)^{t}$ and $A^{t}+B^{t}$ agree at each entry, they are equal.
∎
@end
@thm
@title{Transpose and Matrix Scalar Multiplication}
Suppose that $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $A$ is an $m\times n$ matrix. Then $(\alpha A)^{t}=\alpha A^{t}$.
@end
@proof
@col
For $1\leq i\leq m$, $1\leq j\leq n$,

\begin{align*}
\displaystyle\left[(\alpha A)^{t}\right]_{ji}&amp;\displaystyle=\left[\alpha A\right]_{ij} \\
&amp;\displaystyle=\alpha\left[A\right]_{ij} \\
&amp;\displaystyle=\alpha\left[A^{t}\right]_{ji} \\
&amp;\displaystyle=\left[\alpha A^{t}\right]_{ji}.
\end{align*}

Since the matrices $(\alpha A)^{t}$ and $\alpha A^{t}$ agree at each entry, they are equal.
∎
@end
@thm
@title{Transpose of a Transpose}
@label{TT}
Suppose that $A$ is an $m\times n$ matrix. Then $\left(A^{t}\right)^{t}=A$.
@end
@proof
@col
For $1\leq i\leq m$, $1\leq j\leq n$,

\begin{align*}
\displaystyle\left[\left(A^{t}\right)^{t}\right]_{ij}&amp;\displaystyle=\left[A^{t}\right]_{ji} \\
&amp;\displaystyle=\left[A\right]_{ij}.
\end{align*}

Since the matrices $\left(A^{t}\right)^{t}$ and $A$ agree at each entry, they are equal.
∎
@end

@section{Matrix-Vector Product}

@defn
@title{Matrix-Vector Product}
@label{MVP}
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$ and $\mathbf{u}$ is a vector of size $n$. Then the <b>matrix-vector product</b> of $A$ with $\mathbf{u}$ is the linear combination

\begin{align*}
\displaystyle A\mathbf{u}=\left[\mathbf{u}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{u}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{u}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{u}\right]_{n}\mathbf{A}_{n}
\end{align*}
@end

So, the matrix-vector product is yet another version of multiplication, at least in the sense that we have again overloaded juxtaposition of two symbols as our notation. Remember your objects, an $m\times n$ matrix times a vector of size $n$ will create a vector of size $m$. So if $A$ is (non-square) rectangular, then the size of the vector changes. With all the linear combinations we have performed so far, this computation should now seem second nature.

@eg
<b>A matrix times a vector</b>
Consider

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;4&amp;2&amp;3&amp;4\\
-3&amp;2&amp;0&amp;1&amp;-2\\
1&amp;6&amp;-3&amp;-1&amp;5\end{bmatrix}&amp;\displaystyle\mathbf{u}=\begin{bmatrix}2\\
1\\
-2\\
3\\
-1\end{bmatrix}
\end{align*}

Then

\begin{align*}
\displaystyle A\mathbf{u}=2\begin{bmatrix}1\\
-3\\
1\end{bmatrix}+1\begin{bmatrix}4\\
2\\
6\end{bmatrix}+(-2)\begin{bmatrix}2\\
0\\
-3\end{bmatrix}+3\begin{bmatrix}3\\
1\\
-1\end{bmatrix}+(-1)\begin{bmatrix}4\\
-2\\
5\end{bmatrix}=\begin{bmatrix}7\\
1\\
6\end{bmatrix}.
\end{align*}
@end

We can now represent systems of linear equations compactly with a matrix-vector product and column vector equality. This yields a very popular alternative to our unconventional $A\mathbf{x}=\mathbf{b}$ notation.

@thm
@title{Systems of Linear Equations as Matrix Multiplication}
@label{SLEMM}
The set of solutions to the linear system $A\mathbf{x}=\mathbf{b}$ equals the set of solutions for $\mathbf{x}$ in the vector equation $A\mathbf{x}=\mathbf{b}$.
@end
@proof
@col
\begin{align*}
\displaystyle\mathbf{x}\text{ is a solution to }A\mathbf{x}=\mathbf{b} \\
\displaystyle\iff\left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n}=\mathbf{b} \\
\displaystyle\iff\mathbf{x}\text{ is a solution to }A\mathbf{x}=\mathbf{b}
\end{align*}

∎
@end
@eg
<b>Matrix notation for systems of linear equations</b>

Consider the system of linear equations

\begin{align*}
\displaystyle 2x_{1}+4x_{2}-3x_{3}+5x_{4}+x_{5}&amp;\displaystyle=9 \\
\displaystyle 3x_{1}+x_{2}+x_{4}-3x_{5}&amp;\displaystyle=0 \\
\displaystyle-2x_{1}+7x_{2}-5x_{3}+2x_{4}+2x_{5}&amp;\displaystyle=-3
\end{align*}

has coefficient matrix and vector of constants

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;4&amp;-3&amp;5&amp;1\\
3&amp;1&amp;0&amp;1&amp;-3\\
-2&amp;7&amp;-5&amp;2&amp;2\end{bmatrix}\qquad\mathbf{b}=\begin{bmatrix}9\\
0\\
-3\end{bmatrix}
\end{align*}

and so will be described compactly by the vector equation $A\mathbf{x}=\mathbf{b}$.
@end

The matrix-vector product is a very natural computation. We have motivated it by its connections with systems of equations, but here is another example.

@thm
@title{Equal Matrices and Matrix-Vector Products}
@label{EMMVP}
Suppose that $A$ and $B$ are $m\times n$ matrices such that $A\mathbf{x}=B\mathbf{x}$ for every $\mathbf{x}\in{\mathbb{R}}^{n}$. Then $A=B$.
@end
@proof
@col
We are assuming $A\mathbf{x}=B\mathbf{x}$ for all $\mathbf{x}\in{\mathbb{R}}^{n}$, so we can employ this equality for any choice of the vector $\mathbf{x}$. However, we will limit our use of this equality to the standard unit vectors.
For $1\leq j\leq n$, we define <b>the standard unit vector</b> $\mathbf{e}_{j}$ the a column vector in ${\mathbb{R}}^{n}$ with the $j$-th entry $=1$ and other entries are zero.

\begin{align*}
\displaystyle\left[A\right]_{ij} \\
\displaystyle=0\left[A\right]_{i1}+\cdots+0\left[A\right]_{i,j-1}+1\left[A\right]_{ij}+0\left[A\right]_{i,j+1}+\cdots+0\left[A\right]_{in} \\
\displaystyle=\left[\mathbf{e}_{j}\right]_{1}\left[A\right]_{i1}+\left[\mathbf{e}_{j}\right]_{2}\left[A\right]_{i2}+\left[\mathbf{e}_{j}\right]_{3}\left[A\right]_{i3}+\cdots+\left[\mathbf{e}_{j}\right]_{n}\left[A\right]_{in} \\
\displaystyle=\left[A\right]_{i1}\left[\mathbf{e}_{j}\right]_{1}+\left[A\right]_{i2}\left[\mathbf{e}_{j}\right]_{2}+\left[A\right]_{i3}\left[\mathbf{e}_{j}\right]_{3}+\cdots+\left[A\right]_{in}\left[\mathbf{e}_{j}\right]_{n} \\
\displaystyle=\left[A\mathbf{e}_{j}\right]_{i} \\
\displaystyle=\left[B\mathbf{e}_{j}\right]_{i} \\
\displaystyle=\left[B\right]_{i1}\left[\mathbf{e}_{j}\right]_{1}+\left[B\right]_{i2}\left[\mathbf{e}_{j}\right]_{2}+\left[B\right]_{i3}\left[\mathbf{e}_{j}\right]_{3}+\cdots+\left[B\right]_{in}\left[\mathbf{e}_{j}\right]_{n} \\
\displaystyle=\left[\mathbf{e}_{j}\right]_{1}\left[B\right]_{i1}+\left[\mathbf{e}_{j}\right]_{2}\left[B\right]_{i2}+\left[\mathbf{e}_{j}\right]_{3}\left[B\right]_{i3}+\cdots+\left[\mathbf{e}_{j}\right]_{n}\left[B\right]_{in} \\
\displaystyle=0\left[B\right]_{i1}+\cdots+0\left[B\right]_{i,j-1}+1\left[B\right]_{ij}+0\left[B\right]_{i,j+1}+\cdots+0\left[B\right]_{in} \\
\displaystyle=\left[B\right]_{ij}
\end{align*}

So the matrices $A$ and $B$ are equal, as desired.
∎
@end

You might notice from studying the proof that the hypotheses of this theorem could be weakened i.e., made less restrictive). We need only suppose the equality of the matrix-vector products for the standard unit vectors or any other spanning set of ${\mathbb{R}}^{n}$. However, in practice, when we apply this theorem the stronger hypothesis will be in effect so this version of the theorem suffices for our purposes. (If we changed the statement of the theorem to have the less restrictive hypothesis, then we would call the theorem stronger.)

@section{Matrix Multiplication}

@defn
@title{Matrix Multiplication}
@label{NM}
Suppose $A$ is an $m\times n$ matrix and $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{p}$ are the columns of an $n\times p$ matrix $B$. Then the <b>matrix product</b> of $A$ with $B$ is the $m\times p$ matrix whose $i$th column is the matrix-vector product $A\mathbf{B}_{i}$. Symbolically,

\begin{align*}
\displaystyle AB=A\left[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}|\ldots|\mathbf{B}_{p}\right]=\left[A\mathbf{B}_{1}|A\mathbf{B}_{2}|A\mathbf{B}_{3}|\ldots|A\mathbf{B}_{p}\right].
\end{align*}
@end
@eg
Set

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;-1&amp;4&amp;6\\
0&amp;-4&amp;1&amp;2&amp;3\\
-5&amp;1&amp;2&amp;-3&amp;4\end{bmatrix}&amp;\displaystyle B=\begin{bmatrix}1&amp;6&amp;2&amp;1\\
-1&amp;4&amp;3&amp;2\\
1&amp;1&amp;2&amp;3\\
6&amp;4&amp;-1&amp;2\\
1&amp;-2&amp;3&amp;0\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle AB=\left[A\begin{bmatrix}1\\
-1\\
1\\
6\\
1\end{bmatrix}\left\lvert A\begin{bmatrix}6\\
4\\
1\\
4\\
-2\end{bmatrix}\right.\left\lvert A\begin{bmatrix}2\\
3\\
2\\
-1\\
3\end{bmatrix}\right.\left\lvert A\begin{bmatrix}1\\
2\\
3\\
2\\
0\end{bmatrix}\right.\right]=\begin{bmatrix}28&amp;17&amp;20&amp;10\\
20&amp;-13&amp;-3&amp;-1\\
-18&amp;-44&amp;12&amp;-3\end{bmatrix}.
\end{align*}
@end

Is this the definition of matrix multiplication you expected? Perhaps our previous operations for matrices caused you to think that we might multiply two matrices of the same size, entry-by-entry? Notice that our current definition uses matrices of different sizes (though the number of columns in the first must equal the number of rows in the second), and the result is of a third size.

Notice too that in the previous example we cannot even consider the product $BA$, since the sizes of the two matrices in this order are not compatible.

But it gets weirder than that. Many of your old ideas about multiplication will not apply to matrix multiplication, but some still will. So make no assumptions, and do not do anything until you have a theorem that says you can. Even if the sizes are right, matrix multiplication is not commutative – order matters.

@eg
<b>Matrix multiplication is not commutative</b>
Set

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;3\\
-1&amp;2\end{bmatrix}&amp;\displaystyle B=\begin{bmatrix}4&amp;0\\
5&amp;1\end{bmatrix}.
\end{align*}

So, we have two square ($2\times 2$) matrices. We find

\begin{align*}
\displaystyle AB=\begin{bmatrix}19&amp;3\\
6&amp;2\end{bmatrix}&amp;\displaystyle BA=\begin{bmatrix}4&amp;12\\
4&amp;17\end{bmatrix}
\end{align*}

and $AB\neq BA$. Not even close. It should not be hard for you to construct other pairs of matrices that do not commute (try a couple of $3\times 3$’s). Can you find a pair of non-identical matrices that do commute?
@end
@thm
@title{Entries of Matrix Products}
@label{EMP}
Suppose that $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix. Then for $1\leq i\leq m$, $1\leq j\leq p$, the individual entries of $AB$ are given by

\begin{align*}
\displaystyle\left[AB\right]_{ij}&amp;\displaystyle=\left[A\right]_{i1}\left[B\right]_{1j}+\left[A\right]_{i2}\left[B\right]_{2j}+\left[A\right]_{i3}\left[B\right]_{3j}+\cdots+\left[A\right]_{in}\left[B\right]_{nj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj}
\end{align*}

<b>Remark</b>: In most books, this is used as the definition of $AB$.
@end
@proof
@col
Let the vectors $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$ denote the columns of $A$ and let the vectors $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{p}$ denote the columns of $B$. Then for $1\leq i\leq m$, $1\leq j\leq p$,

\begin{align*}
\displaystyle\left[AB\right]_{ij}&amp;\displaystyle=\left[A\mathbf{B}_{j}\right]_{i} \\
&amp;\displaystyle=\left[\left[\mathbf{B}_{j}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{B}_{j}\right]_{2}\mathbf{A}_{2}+\cdots+\left[\mathbf{B}_{j}\right]_{n}\mathbf{A}_{n}\right]_{i} \\
&amp;\displaystyle=\left[\left[\mathbf{B}_{j}\right]_{1}\mathbf{A}_{1}\right]_{i}+\left[\left[\mathbf{B}_{j}\right]_{2}\mathbf{A}_{2}\right]_{i}+\cdots+\left[\left[\mathbf{B}_{j}\right]_{n}\mathbf{A}_{n}\right]_{i} \\
&amp;\displaystyle=\left[\mathbf{B}_{j}\right]_{1}\left[\mathbf{A}_{1}\right]_{i}+\left[\mathbf{B}_{j}\right]_{2}\left[\mathbf{A}_{2}\right]_{i}+\cdots+\left[\mathbf{B}_{j}\right]_{n}\left[\mathbf{A}_{n}\right]_{i} \\
&amp;\displaystyle=\left[B\right]_{1j}\left[A\right]_{i1}+\left[B\right]_{2j}\left[A\right]_{i2}+\cdots+\left[B\right]_{nj}\left[A\right]_{in} \\
&amp;\displaystyle=\left[A\right]_{i1}\left[B\right]_{1j}+\left[A\right]_{i2}\left[B\right]_{2j}+\cdots+\left[A\right]_{in}\left[B\right]_{nj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj}
\end{align*}

∎
@end
@eg
<b>Product of two matrices, entry-by-entry</b>Consider the matrices

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;-1&amp;4&amp;6\\
0&amp;-4&amp;1&amp;2&amp;3\\
-5&amp;1&amp;2&amp;-3&amp;4\end{bmatrix}&amp;\displaystyle B=\begin{bmatrix}1&amp;6&amp;2&amp;1\\
-1&amp;4&amp;3&amp;2\\
1&amp;1&amp;2&amp;3\\
6&amp;4&amp;-1&amp;2\\
1&amp;-2&amp;3&amp;0\end{bmatrix}
\end{align*}

Suppose we just wanted the entry of $AB$ in the second row, third column:

\begin{align*}
\displaystyle\left[AB\right]_{23}=&amp;\displaystyle\left[A\right]_{21}\left[B\right]_{13}+\left[A\right]_{22}\left[B\right]_{23}+\left[A\right]_{23}\left[B\right]_{33}+\left[A\right]_{24}\left[B\right]_{43}+\left[A\right]_{25}\left[B\right]_{53} \\
\displaystyle=&amp;\displaystyle(0)(2)+(-4)(3)+(1)(2)+(2)(-1)+(3)(3)=-3
\end{align*}

Notice how there are 5 terms in the sum, since 5 is the common dimension of the two matrices (column count for $A$, row count for $B$). In the conclusion of the above theorem, it would be the index $k$ that would run from 1 to 5 in this computation. Here is a bit more practice.

The entry of third row, first column:

\begin{align*}
\displaystyle\left[AB\right]_{31}=&amp;\displaystyle\left[A\right]_{31}\left[B\right]_{11}+\left[A\right]_{32}\left[B\right]_{21}+\left[A\right]_{33}\left[B\right]_{31}+\left[A\right]_{34}\left[B\right]_{41}+\left[A\right]_{35}\left[B\right]_{51} \\
\displaystyle=&amp;\displaystyle(-5)(1)+(1)(-1)+(2)(1)+(-3)(6)+(4)(1)=-18
\end{align*}

Try to compute all the other entries.
@end

<b>How to memorize the formula</b>: To find the $(i,j)$-th entry of $AB$. (1) Find the $i$-th row of $A$ (simply called the row below)(2) Find the $j$-th column of $B$ (simply called the column below)(3) sum up the product the corresponding entries of the row and the column, i.e.
(entry 1 of the row $\times$ entry 1 of the column) + (entry 2 of the row $\times$ entry 2 of the column) + $\cdots$

@eg
Find the $(3,2)$ entry of $AB$ in the previous example.The $3$-rd row of $A$ is $\begin{bmatrix}-5&amp;1&amp;2&amp;-3&amp;4\end{bmatrix}$ The $2$-nd column of $B$ is $\begin{bmatrix}6\\
4\\
1\\
4\\
-2\end{bmatrix}$. Let’s do the multiplication:

row
-5
1
2
-3
4

column
6
4
1
4
-2

product
-30
4
2
-12
-8

The sum is

\begin{align*}
\displaystyle -30+4+2-12-8=-44.
\end{align*}
@end

@section{Properties of Matrix Multiplication}

In this subsection, we collect properties of matrix multiplication and its interaction with
the zero matrix, the identity matrix, matrix addition, scalar matrix multiplication and
the transpose.

@thm
@title{Matrix Multiplication and the Zero Matrix}
@label{MMZM}
Suppose $A$ is an $m\times n$ matrix. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$A{\cal O}_{n\times p}={\cal O}_{m\times p}$
</li>
<li class="ltx_item">
${\cal O}_{p\times m}A={\cal O}_{p\times n}$
</li>

</ol>
@end
@proof
@col
We will prove (1) and leave (2) to you. Using the entry-by-entry definition of matrix multiplication, for $1\leq i\leq m$, $1\leq j\leq p$, we comoute

\begin{align*}
\displaystyle\left[A{\cal O}_{n\times p}\right]_{ij}&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[{\cal O}_{n\times p}\right]_{kj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}0 \\
&amp;\displaystyle=\sum_{k=1}^{n}0 \\
&amp;\displaystyle=0 \\
&amp;\displaystyle=\left[{\cal O}_{m\times p}\right]_{ij}
\end{align*}

So the matrices $A{\cal O}_{n\times p}$ and ${\cal O}_{m\times p}$ are equal.
∎
@end
@thm
@title{Matrix Multiplication and Identity Matrix}
@label{MMIM}
Suppose that $A$ is an $m\times n$ matrix. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$AI_{n}=A$

</li>
<li class="ltx_item">
$I_{m}A=A$
</li>

</ol>
@end
@proof
@col
Again, we will prove (1) and leave (2) to you. Using the entry-by-entry definition of matrix multiplication, for $1\leq i\leq m$, $1\leq j\leq n$, we compute

\begin{align*}
\displaystyle\left[AI_{n}\right]_{ij}=&amp;\displaystyle\sum_{k=1}^{n}\left[A\right]_{ik}\left[I_{n}\right]_{kj} \\
&amp;\displaystyle=\left[A\right]_{ij}\left[I_{n}\right]_{jj}+\sum_{\begin{subarray}{c}k=1\\
k\neq j\end{subarray}}^{n}\left[A\right]_{ik}\left[I_{n}\right]_{kj} \\
&amp;\displaystyle=\left[A\right]_{ij}(1)+\sum_{k=1,k\neq j}^{n}\left[A\right]_{ik}(0) \\
&amp;\displaystyle=\left[A\right]_{ij}+\sum_{k=1,k\neq j}^{n}0 \\
&amp;\displaystyle=\left[A\right]_{ij}.
\end{align*}

So the matrices $A$ and $AI_{n}$ are equal entrywise. By the definition of matrix equality, they are equal matrices.
∎
@end

It is the previous theorem that gives the identity matrix its name. It is a matrix that behaves with matrix multiplication like the scalar 1 does with scalar multiplication. To multiply by the identity matrix is to have no effect on the other matrix.

@thm
@title{Matrix Multiplication Distributes Across Addition}
@label{MMDAA}
Suppose that $A$ is an $m\times n$ matrix and $B$ and $C$ are $n\times p$ matrices and $D$ is a $p\times s$ matrix.
Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$A(B+C)=AB+AC$
</li>
<li class="ltx_item">
$(B+C)D=BD+CD$
</li>

</ol>
@end
@proof
@col
We will do (1), you do (2). Entry-by-entry, for $1\leq i\leq m$, $1\leq j\leq p$,

\begin{align*}
\displaystyle\left[A(B+C)\right]_{ij}&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B+C\right]_{kj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}(\left[B\right]_{kj}+\left[C\right]_{kj}) \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj}+\left[A\right]_{ik}\left[C\right]_{kj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj}+\sum_{k=1}^{n}\left[A\right]_{ik}\left[C\right]_{kj} \\
&amp;\displaystyle=\left[AB\right]_{ij}+\left[AC\right]_{ij} \\
&amp;\displaystyle=\left[AB+AC\right]_{ij}
\end{align*}

So the matrices $A(B+C)$ and $AB+AC$ are equal, entry-by-entry.
Hence by the definition of matrix equality, we can say they are equal matrices.
∎
@end
@thm
@title{Matrix Multiplication and Scalar Matrix Multiplication}
@label{MMSMM}
Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix. Let $\alpha$ be a scalar. Then $\alpha(AB)=(\alpha A)B=A(\alpha B)$.
@end
@proof
@col
These are equalities of matrices. We will do the first one, the second is similar and will be good practice for you.
For $1\leq i\leq m$, $1\leq j\leq p$,

\begin{align*}
\displaystyle\left[\alpha(AB)\right]_{ij}&amp;\displaystyle=\alpha\left[AB\right]_{ij} \\
&amp;\displaystyle=\alpha\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\alpha\left[A\right]_{ik}\left[B\right]_{kj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[\alpha A\right]_{ik}\left[B\right]_{kj} \\
&amp;\displaystyle=\left[(\alpha A)B\right]_{ij}
\end{align*}

So the matrices $\alpha(AB)$ and $(\alpha A)B$ are equal, entry-by-entry, and by the definition of matrix equality we can say they are equal matrices.
∎
@end
@thm
@title{Matrix Multiplication is Associative}
@label{MMA}
Suppose $A$ is an $m\times n$ matrix, $B$ is an $n\times p$ matrix and $D$ is a $p\times s$ matrix. Then $A(BD)=(AB)D$.
@end
@proof
@col
A matrix equality, so we will go entry-by-entry, no surprise there. For $1\leq i\leq m$, $1\leq j\leq s$,

\begin{align*}
\displaystyle\left[A(BD)\right]_{ij}&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[BD\right]_{kj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left(\sum_{\ell=1}^{p}\left[B\right]_{k\ell}\left[D\right]_{\ell j}\right) \\
&amp;\displaystyle=\sum_{k=1}^{n}\sum_{\ell=1}^{p}\left[A\right]_{ik}\left[B\right]_{k\ell}\left[D\right]_{\ell j} \\
\\
&amp;\displaystyle=\sum_{\ell=1}^{p}\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{k\ell}\left[D\right]_{\ell j} \\
\\
&amp;\displaystyle=\sum_{\ell=1}^{p}\left[D\right]_{\ell j}\left(\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{k\ell}\right) \\
&amp;\displaystyle=\sum_{\ell=1}^{p}\left[D\right]_{\ell j}\left[AB\right]_{i\ell} \\
&amp;\displaystyle=\sum_{\ell=1}^{p}\left[AB\right]_{i\ell}\left[D\right]_{\ell j} \\
&amp;\displaystyle=\left[(AB)D\right]_{ij}
\end{align*}

So the matrices $(AB)D$ and $A(BD)$ are equal, entry-by-entry, and by the definition of matrix equality we can say they are equal matrices.
∎
@end

The above result says matrix multiplication is associative; it means we do not have to be careful about how we parenthesize an expression with just several matrices multiplied together. So this is where we draw the line on explaining every last detail in a proof. We will frequently add, remove, or rearrange parentheses with no comment.

@thm
@title{Matrix Multiplication and Transposes}
@label{MMT}
Suppose that $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix. Then $(AB)^{t}=B^{t}A^{t}$.
@end
@proof
@col
This theorem may be surprising but if we check the sizes of the matrices involved, then maybe it will not seem so far-fetched. First, $AB$ has size $m\times p$, so its transpose has size $p\times m$. The product of $B^{t}$ with $A^{t}$ is a $p\times n$ matrix times an $n\times m$ matrix, also resulting in a $p\times m$ matrix. So at least our objects are compatible for equality (and would not be, in general, if we did not reverse the order of the matrix multiplication).

Here we go again, entry-by-entry. For $1\leq i\leq m$, $1\leq j\leq p$,

\begin{align*}
\displaystyle\left[(AB)^{t}\right]_{ji}=&amp;\displaystyle\left[AB\right]_{ij} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[B\right]_{kj}\left[A\right]_{ik} \\
&amp;\displaystyle=\sum_{k=1}^{n}\left[B^{t}\right]_{jk}\left[A^{t}\right]_{ki} \\
&amp;\displaystyle=\left[B^{t}A^{t}\right]_{ji}
\end{align*}

So the matrices $(AB)^{t}$ and $B^{t}A^{t}$ are equal, entry-by-entry, and by the definition of matrix equality we can say they are equal matrices.
∎
@end

@section{Row operations and matrix multiplication}

In this section, we will discuss the relation between elementary row operations and matrix multiplication.

@defn
@title{Row Operations}
@label{RO}
The following three operations, each of which is known as a <b>row operation</b>, will transform an $m\times n$ matrix into a different matrix of the same size.

<ol class="ltx_enumerate">
<li class="ltx_item">
Swap the locations of two rows.
</li>
<li class="ltx_item">
Multiply each entry of a single row by a nonzero quantity.
</li>
<li class="ltx_item">
Multiply each entry of one row by some quantity, and add these values to the entries in the same columns of a second row. Leave the first row the same after this operation, but replace the second row by the new values.
</li>

</ol>
@end
@thm

Let $A\in M_{mn}$. Let $B$ be a matrix obtained by applying one of the above row operations on $A$.
Let $J$ be a matrix obtained by applying the same row operation on $I_{m}$. Then

\begin{align*}
\displaystyle JA=B.
\end{align*}
@end
@proof
@col
Exercise.
∎
@end
@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;a_{14}&amp;a_{15}\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;a_{24}&amp;a_{25}\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;a_{34}&amp;a_{35}\\
a_{41}&amp;a_{42}&amp;a_{43}&amp;a_{44}&amp;a_{45}\\
\end{bmatrix}
\end{align*}

Consider the row operation $3R_{2}+R_{3}$.

\begin{align*}
\displaystyle A\xrightarrow{3R_{2}+R_{3}}B=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;a_{14}&amp;a_{15}\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;a_{24}&amp;a_{25}\\
3a_{21}+a_{31}&amp;3a_{22}+a_{32}&amp;3a_{23}+a_{33}&amp;3a_{24}+a_{34}&amp;3a_{25}+a_{35}\\
a_{41}&amp;a_{42}&amp;a_{43}&amp;a_{44}&amp;a_{45}\\
\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle I_{4}\xrightarrow{3R_{2}+R_{3}}J=\begin{bmatrix}1&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0\\
0&amp;3&amp;1&amp;0\\
0&amp;0&amp;0&amp;1\\
\end{bmatrix}
\end{align*}

Then

\begin{align*}
\displaystyle JA=B.
\end{align*}
@end

@chapter{Reduced Row Echelon Forms}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.htmlPrint version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf

<h5 class="notkw">Reference.</h5>

Beezer, Ver 3.5 Subsection RREF (print version p21 - p33) You can skip the proof of Thm REMEF on p.22 and Thm RREFU on p.24-27

<h5 class="notkw">Exercise.</h5>


<ol class="ltx_enumerate">
<li class="ltx_item">
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf
Section SSLE (p.1-6) C30-34, C50, M30, T20. Sect RREF (p.6-13) C10-19, C31-33, M40 Part 1, T10, T11, T12.
</li>

</ol>

@section{Reduced Row Echelon form}

<b>Terminology</b>: <b>Zero row</b>: a row consists of $0$. <b>Leftmost nonzero entry of a row</b>: the first nonzero entry of a row. <b>Index of the leftmost nonzero entry of a row</b>: The index of the first nonzero entry. <b>Notation</b>: Denote $\ell_{i}$ the index of leftmost nonzero entry of row $i$.

@eg
The underline entries are the leftmost nonzero entry for each row.

\begin{align*}
\displaystyle \begin{bmatrix}0&amp;\underline{1}&amp;1&amp;0&amp;2\\
0&amp;0&amp;0&amp;0&amp;\underline{1}\\
0&amp;0&amp;0&amp;\underline{1}&amp;3\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

The index of the leftmost nonzero entry of row 1 is $\ell_{1}=2$. The index of the leftmost nonzero entry of row 2 is $\ell_{2}=5$. The index of the leftmost nonzero entry of row 3 is $\ell_{3}=4$. row 4 is a zero row.
@end
@eg
The underline entries are the leftmost nonzero entry for each row.

\begin{align*}
\displaystyle \begin{bmatrix}\underline{2}&amp;0&amp;1&amp;2&amp;3&amp;4\\
0&amp;\underline{1}&amp;1&amp;-1&amp;0&amp;3\\
0&amp;0&amp;0&amp;0&amp;\underline{1}&amp;0\\
0&amp;\underline{-1}&amp;0&amp;0&amp;0&amp;1\\
\end{bmatrix}
\end{align*}

The index of the leftmost nonzero entry of row 1 is $\ell_{1}=1$. The index of the leftmost nonzero entry of row 2 is $\ell_{2}=2$. The index of the leftmost nonzero entry of row 3 is $\ell_{3}=5$. The index of the leftmost nonzero entry of row 3 is $\ell_{4}=2$.
@end

A matrix is said to be in <b>reduced row echelon form</b> if it looks like this
( $*$ means an arbitary number)

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots\\
0&amp;0&amp;\cdots&amp;1&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;1&amp;*&amp;\cdots\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
\end{bmatrix}
\end{align*}

<ol class="ltx_enumerate">
<li class="ltx_item">
It looks like an inverted staircase
</li>
<li class="ltx_item">
Each new step down gives a ”one”. Above it are zeros.
</li>
<li class="ltx_item">
The column that has a new step is call the <b>pivot column</b>.

</li>

</ol>

In below is the rigorous definition (you can read the examples first):

@defn
@title{Reduced Row-Echelon Form}
@label{RREF}
A matrix is in <b>reduced row-echelon form</b> if it meets all of the following conditions:

<ol class="ltx_enumerate">
<li class="ltx_item">
If there is a row where every entry is zero, then this row lies below any other row that contains a nonzero entry.
</li>
<li class="ltx_item">
The leftmost nonzero entry of a row is equal to 1.
</li>
<li class="ltx_item">
The leftmost nonzero entry of a row is the only nonzero entry in its column.
</li>
<li class="ltx_item">
If $i&lt;j$ and both row $i$ and row $j$ are not zero vectors, then $\ell_{i}&lt;\ell_{j}$, i.e.
$\ell_{1},\ell_{2},\ldots$ are in ascending order.
</li>

</ol>

A row of only zero entries is called a <b>zero row</b> and the leftmost nonzero entry of a nonzero row is a <b>leading 1</b>. A column containing a leading 1 will be called a <b>pivot column</b>. The number of nonzero rows will be denoted by $r$, which is also equal to the number of leading 1’s and the number of pivot columns.

The set of column indices for the pivot columns will be denoted by $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ where
$d_{1}&lt;d_{2}&lt;d_{3}&lt;\cdots&lt;d_{r}$,
while the columns that are not pivot columns will be denoted as $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$ where
$f_{1}&lt;f_{2}&lt;f_{3}&lt;\cdots&lt;f_{n-r}$.
@end
@eg
The matrix below are in reduced row echelon from

<ol class="ltx_enumerate">
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;3&amp;0&amp;0&amp;1&amp;0\\
0&amp;0&amp;1&amp;3&amp;4&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

Column 1, 3, 6 are pivot columns, $r=3$, $D=\{1,3,6\}$, $d_{1}=1,d_{2}=3,d_{3}=6$,
$F=\{2,4,5\}$, $f_{1}=2,f_{2}=4,f_{3}=5$.
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0&amp;5&amp;3&amp;0&amp;0&amp;5\\
0&amp;1&amp;3&amp;6&amp;0&amp;0&amp;6\\
0&amp;0&amp;0&amp;0&amp;1&amp;0&amp;7\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;3\end{bmatrix}
\end{align*}

Column 1, 2, 5, 6 are pivot columns, $r=4$, $D=\{1,2,5,6\}$, $d_{1}=1,d_{2}=2,d_{3}=5,d_{4}=6$,
$F=\{3,4,7\}$, $f_{1}=3,f_{2}=4,f_{3}=7$.
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;1\end{bmatrix}
\end{align*}

Column 1, 2, 3 are pivot columns, $r=3$, $D=\{1,2,3\}$, $d_{1}=1,d_{2}=2,d_{3}=3$, $F=\emptyset$ (an empty set).
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{bmatrix}0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;0&amp;9&amp;6\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;0&amp;8&amp;8\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;3&amp;4\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

Column 2, 5, 7 are pivot columns. Note that column 3 is <b>not</b> a pivot column. $r=3$,
$D=\{2,5,7\}$, $d_{1}=2,d_{2}=5,d_{3}=7$,
$F=\{1,3,4,6,8,9\}$, $f_{1}=1,f_{2}=3,f_{3}=4,f_{4}=6,f_{5}=8,f_{6}=9$.
</li>
<li class="ltx_item">
The matrix $C$ is in reduced row-echelon form.

\begin{align*}
\displaystyle C&amp;\displaystyle=\begin{bmatrix}1&amp;-3&amp;0&amp;6&amp;0&amp;0&amp;-5&amp;9\\
0&amp;0&amp;0&amp;0&amp;1&amp;0&amp;3&amp;-7\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;7&amp;3\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

This matrix has two zero rows and three pivot columns.
So $r=3$. Columns 1, 5, and 6 are the pivot columns,
so $D=\{1,\,5,\,6\}$, $d_{1}=1,d_{2}=5,d_{3}=6$,
$F=\{2,\,3,\,4,\,7,\,8\}$, $f_{1}=2,f_{2}=3,f_{3}=4,f_{4}=7,f_{5}=8$.
</li>

</ol>
@end
@eg
The following matrices are <b>not RREF</b>, explain why.

<ol class="ltx_enumerate">
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0&amp;1&amp;0&amp;1&amp;0\\
0&amp;1&amp;0&amp;1&amp;0&amp;2\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

It fails condition 1: row 3 is a zero row but row 4, which is under row 3, is not a zero row.
</li>
<li class="ltx_item">
The underline entries are the leftmost nonzero entry for each row.

\begin{align*}
\displaystyle \begin{bmatrix}\underline{1}&amp;0&amp;2&amp;0\\
0&amp;\underline{1}&amp;3&amp;0\\
0&amp;0&amp;0&amp;\underline{3}\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

It fails condition 2: the leftmost nonzero entry of row 3 is not 1.
</li>
<li class="ltx_item">
The underline entries are the leftmost nonzero entry for each row.

\begin{align*}
\displaystyle \begin{bmatrix}0&amp;\underline{1}&amp;0&amp;0&amp;1&amp;2\\
0&amp;0&amp;\underline{1}&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;\underline{1}&amp;3\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

It fails condition 3: For row 3, the column consists the left most nonzero entry (i.e. column 5) has more than $1$ nonzero entries.
</li>
<li class="ltx_item">
The underline entries are the leftmost nonzero entry for each row.

\begin{align*}
\displaystyle \begin{bmatrix}\underline{1}&amp;0&amp;0&amp;1&amp;2\\
0&amp;0&amp;\underline{1}&amp;0&amp;1\\
0&amp;\underline{1}&amp;0&amp;0&amp;3\\
0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\\
\end{bmatrix}
\end{align*}

It fails condition 4: The index of the leftmost nonzero entry of row 1 is $\ell_{1}=1$. The index of the leftmost nonzero entry of row 2 is $\ell_{2}=3$. The index of the leftmost nonzero entry of row 3 is $\ell_{2}=2$. $2&lt;3$ but $\ell_{2}&gt;\ell_{3}$.
</li>

</ol>
@end
@thm
@title{Row-Equivalent Matrix in Echelon Form}
Suppose $A$ is a matrix. Then there is a matrix $B$ so that

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ and $B$ are row-equivalent.
</li>
<li class="ltx_item">
$B$ is in reduced row-echelon form.
</li>

</ol>
@end
@proof
@col
<b>Skip the proof and go through the next example</b> Suppose that $A$ has $m$ rows and $n$ columns. We will describe a process for converting $A$ into $B$ via row operations. This procedure is known as <b>Gaussian elimination</b> or sometimes called <b>Gauss-Jordan elimination</b>. Tracing through this procedure will be easier if you recognize that $i$ refers to a row that is being converted, $j$ refers to a column that is being converted, and $r$ keeps track of the number of nonzero rows. Here we go.

<ol class="ltx_enumerate">
<li class="ltx_item">
Set $j=0$ and $r=0$.
</li>
<li class="ltx_item">
Increase $j$ by 1. If $j$ now equals $n+1$, then stop.
</li>
<li class="ltx_item">
Examine the entries of $A$ in column $j$ located in rows $r+1$ through $m$. If all of these entries are zero, then go to Step 2.
</li>
<li class="ltx_item">
Choose a row from rows $r+1$ through $m$ with a nonzero entry in column $j$. Let $i$ denote the index for this row.
</li>
<li class="ltx_item">
Increase $r$ by 1.
</li>
<li class="ltx_item">
Use the first row operation to swap rows $i$ and $r$.
</li>
<li class="ltx_item">
Use the second row operation to convert the entry in row $r$ and column $j$ to a 1.
</li>
<li class="ltx_item">
Use the third row operation with row $r$ to convert every other entry of column $j$ to zero.
</li>
<li class="ltx_item">
Go to Step 2.
</li>

</ol>

The result of this procedure is that the matrix $A$ is converted to a matrix in reduced row-echelon form, which we will refer to as $B$.
he matrix is only converted through row operations (Steps 6, 7, 8), so $A$ and $B$ are row-equivalent.
We need to now prove this claim by showing that the converted matrix has the requisite properties of Theorem  @ref{RREF}.
We will skip the proof for now. See Beezer, Ver 3.5 (print version p23).
∎
@end

We will now run through some examples of using these definitions and theorems to solve some systems of equations. From now on, when we have a matrix in reduced row-echelon form, we will mark the leading 1’s with a small box. This will help you count, and identify, the pivot columns. In your work, you can box ’em, circle ’em or write ’em in a different color – just identify ’em somehow.

@eg
Using the Gaussian elimination, find the RREF of

\begin{align*}
\displaystyle A=\begin{bmatrix}0&amp;0&amp;1&amp;1&amp;4\\
0&amp;0&amp;1&amp;1&amp;3\\
1&amp;1&amp;2&amp;4&amp;8\\
2&amp;2&amp;5&amp;9&amp;19\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
We first work on row 1. Set $r=1$.
</li>
<li class="ltx_item">
Consider column 1 (set $j=1$), find a nonzero entry (underline below) in the column.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}\underline{0}&amp;0&amp;1&amp;1&amp;4\\
\underline{0}&amp;0&amp;1&amp;1&amp;3\\
\underline{1}&amp;1&amp;2&amp;4&amp;8\\
\underline{2}&amp;2&amp;5&amp;9&amp;19\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Move the nonzero entry to row 1 by swapping rows $R_{1}\leftrightarrow R_{i}$.
</li>
<li class="ltx_item">
If the entry at row 1, column 1 is nonzero, you don’t have to swap rows. But you can consider swap it with entry $=1$ or $-1$.

</li>
<li class="ltx_item">
In this example, for column 1, 3rd entry and 4th entry are nonzero, so we can use $R_{1}\leftrightarrow R_{3}$ or $R_{1}\leftrightarrow R_{4}$.
</li>
<li class="ltx_item">
There is nothing wrong about $R_{1}\leftrightarrow R_{4}$ but <b>it is better to sway the row with entry equal to $1$ or $-1$.</b>
</li>
<li class="ltx_item">
So we use $R_{1}\leftrightarrow R_{3}$.
</li>

</ul>

\begin{align*}
\displaystyle \xrightarrow{R_{1}\leftrightarrow R_{3}}\begin{bmatrix}\boxed{1}&amp;1&amp;2&amp;4&amp;8\\
0&amp;0&amp;1&amp;1&amp;3\\
0&amp;0&amp;1&amp;1&amp;4\\
2&amp;2&amp;5&amp;9&amp;19\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
If the boxed number is $1$, we are good to go.
</li>
<li class="ltx_item">
If the boxed number is not equal to $1$, say it is $a$, use $\frac{1}{a}R_{1}$ to convert it to $1$.
</li>
<li class="ltx_item">
Then use the boxed number to eliminate the nonzero entries above and below it by $\alpha R_{1}+R_{i}$.
</li>
<li class="ltx_item">
In our example, the boxed number is $1$, so we don’t have to do anything. Use $-2R_{1}+R_{4}$ and to remove the nonzero entries below it and above it. Since we are at the first row, so there is nothing above it).

</li>

</ul>

\begin{align*}
\displaystyle \xrightarrow{-2R_{1}+R_{4}}\begin{bmatrix}\boxed{1}&amp;1&amp;2&amp;4&amp;8\\
0&amp;0&amp;1&amp;1&amp;3\\
0&amp;0&amp;1&amp;1&amp;4\\
0&amp;0&amp;1&amp;1&amp;3\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Ignore the row 1 and col 1.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*\\
*&amp;\underline{0}&amp;1&amp;1&amp;3\\
*&amp;\underline{0}&amp;1&amp;1&amp;4\\
*&amp;\underline{0}&amp;1&amp;1&amp;3\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Now work on row 2 ($r=2$) and column 2 ($j=2$).
</li>
<li class="ltx_item">
The entries of column 2 are underlined.
</li>
<li class="ltx_item">
None of them are nonzero, so we move to next column.

</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*\\
*&amp;0&amp;\underline{1}&amp;1&amp;3\\
*&amp;0&amp;\underline{1}&amp;1&amp;4\\
*&amp;0&amp;\underline{1}&amp;1&amp;3\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
consider column 3 (set $j=3$, $r$ is still $2$).
</li>
<li class="ltx_item">
Find a nonzero entry in column 1. All the entries are nonzero.
</li>
<li class="ltx_item">
We don’t need to so any swapping.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;1&amp;2&amp;4&amp;8\\
0&amp;0&amp;\boxed{1}&amp;1&amp;3\\
0&amp;0&amp;1&amp;1&amp;4\\
0&amp;0&amp;1&amp;1&amp;3\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Then use the boxed number to eliminate the nonzero entries above and below it by $\alpha R_{2}+R_{i}$.
</li>

</ul>

\begin{align*}
\displaystyle \xrightarrow{-2R_{2}+R_{1},-1R_{2}+R_{3},-1R_{2}+R_{4}}\begin{bmatrix}1&amp;1&amp;0&amp;2&amp;2\\
0&amp;0&amp;\boxed{1}&amp;1&amp;3\\
0&amp;0&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Because $r=2$, $j=3$, ignore the first two rows and the 3 columns

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;\underline{0}&amp;1\\
*&amp;*&amp;*&amp;\underline{0}&amp;0\end{bmatrix}
\end{align*}

</li>
<li class="ltx_item">
All the underlined entries are zeros, so we move to the next row.

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;0&amp;\underline{1}\\
*&amp;*&amp;*&amp;0&amp;\underline{0}\end{bmatrix}
\end{align*}

</li>
<li class="ltx_item">
We can then use the boxed number to eliminate all the nonzero entries
above it and below it and get the RREF.

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;1&amp;0&amp;2&amp;2\\
0&amp;0&amp;1&amp;1&amp;3\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{-2R_{3}+R_{1},-3R_{3}+R_{2}}\begin{bmatrix}1&amp;1&amp;0&amp;2&amp;0\\
0&amp;0&amp;1&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

</li>

</ul>
@end
@eg
Using the Gaussian elimination, find the RREF of

\begin{align*}
\displaystyle A=\begin{bmatrix}0&amp;0&amp;2&amp;2&amp;6&amp;2&amp;3\\
2&amp;4&amp;1&amp;3&amp;7&amp;3&amp;-1\\
1&amp;2&amp;2&amp;3&amp;8&amp;2&amp;1\\
1&amp;2&amp;-1&amp;0&amp;-1&amp;2&amp;-1\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
We first work on row 1. Set $r=1$.
</li>
<li class="ltx_item">
Consider column 1 (set $j=1$), find a nonzero entry (underline below) in the column.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}\underline{0}&amp;0&amp;2&amp;2&amp;6&amp;2&amp;3\\
\underline{2}&amp;4&amp;1&amp;3&amp;7&amp;3&amp;-1\\
\underline{1}&amp;2&amp;2&amp;3&amp;8&amp;2&amp;1\\
\underline{1}&amp;2&amp;-1&amp;0&amp;-1&amp;2&amp;-1\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Move the nonzero entry to row 1 by swapping rows $R_{1}\leftrightarrow R_{i}$.
</li>
<li class="ltx_item">
If the entry at row 1, column 1 is nonzero, you don’t have to swap rows. But you can consider swap it with entry $=1$ or $-1$.

</li>
<li class="ltx_item">
In this example, for column 1, 2nd entry, 3rd entry and 4th entry are nonzeros, so we can use $R_{1}\leftrightarrow R_{2}$, $R_{1}\leftrightarrow R_{3}$ or $R_{1}\leftrightarrow R_{4}$.
</li>
<li class="ltx_item">
There is nothing wrong about $R_{1}\leftrightarrow R_{2}$ but <b>it is better to swap with the row with entry equal to $1$ or $-1$.</b>
</li>
<li class="ltx_item">
So we use $R_{1}\leftrightarrow R_{3}$.
</li>

</ul>

\begin{align*}
\displaystyle \xrightarrow{R_{1}\leftrightarrow R_{3}}\begin{bmatrix}\boxed{1}&amp;2&amp;2&amp;3&amp;8&amp;2&amp;1\\
2&amp;4&amp;1&amp;3&amp;7&amp;3&amp;-1\\
0&amp;0&amp;2&amp;2&amp;6&amp;2&amp;3\\
1&amp;2&amp;-1&amp;0&amp;-1&amp;2&amp;-1\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
If the boxed number is $1$, we are good to go.
</li>
<li class="ltx_item">
If the boxed number is not equal to $1$, say it is $a$, use $\frac{1}{a}R_{1}$ to convert it to $1$.
</li>
<li class="ltx_item">
Then use the boxed number to eliminate the nonzero entries above and below it by $\alpha R_{1}+R_{i}$.
</li>
<li class="ltx_item">
In our example, the boxed number is $1$, so we don’t have to do anything. Use $-2R_{1}+R_{2}$ and $-1R_{1}+R_{4}$ to remove the nonzero entries below it and above it. Since we are at the first row, so there is nothing above it.
</li>

</ul>

\begin{align*}
\displaystyle \xrightarrow{-2R_{1}+R_{2},-1R_{1}+R_{4}}\begin{bmatrix}1&amp;2&amp;2&amp;3&amp;8&amp;2&amp;1\\
0&amp;0&amp;-3&amp;-3&amp;-9&amp;-1&amp;-3\\
0&amp;0&amp;2&amp;2&amp;6&amp;2&amp;3\\
0&amp;0&amp;-3&amp;-3&amp;-9&amp;0&amp;-2\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Ignore the row 1 and col 1.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;\underline{0}&amp;-3&amp;-3&amp;-9&amp;-1&amp;-3\\
*&amp;\underline{0}&amp;2&amp;2&amp;6&amp;2&amp;3\\
*&amp;\underline{0}&amp;-3&amp;-3&amp;-9&amp;0&amp;-2\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Now work on row 2 ($r=2$) and column 2 ($j=2$).
</li>
<li class="ltx_item">
The entries of column 2 are underlined.
</li>
<li class="ltx_item">
None of them are nonzero, so we move to next column.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;0&amp;\underline{-3}&amp;-3&amp;-9&amp;-1&amp;-3\\
*&amp;0&amp;\underline{2}&amp;2&amp;6&amp;2&amp;3\\
*&amp;0&amp;\underline{-3}&amp;-3&amp;-9&amp;0&amp;-2\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
consider column 3 (set $j=3$, $r$ is still $2$).
</li>
<li class="ltx_item">
Find a nonzero entry in column 3. All the entries are nonzeros.
</li>
<li class="ltx_item">
There is no entry equal to $1$ or $-1$.
</li>
<li class="ltx_item">
We don’t need to so any swapping.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;0&amp;\boxed{-3}&amp;-3&amp;-9&amp;-1&amp;-3\\
*&amp;0&amp;2&amp;2&amp;6&amp;2&amp;3\\
*&amp;0&amp;-3&amp;-3&amp;-9&amp;0&amp;-2\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Turn the boxed number into $1$ by $-\frac{1}{3}R_{2}$.
</li>

</ul>

\begin{align*}
\displaystyle \xrightarrow{-\frac{1}{3}R_{2}}\begin{bmatrix}1&amp;2&amp;2&amp;3&amp;8&amp;2&amp;1\\
0&amp;0&amp;\boxed{1}&amp;1&amp;3&amp;\frac{1}{3}&amp;1\\
0&amp;0&amp;2&amp;2&amp;6&amp;2&amp;3\\
0&amp;0&amp;-3&amp;-3&amp;-9&amp;0&amp;-2\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
We then use the boxed number to remove the nonzero entries about it and below it.
</li>

</ul>

\begin{align*}
\displaystyle \xrightarrow{-2R_{2}+R_{1},-2R_{2}+R_{3},3R_{2}+R_{4}}\begin{bmatrix}1&amp;2&amp;0&amp;1&amp;2&amp;\frac{4}{3}&amp;-1\\
0&amp;0&amp;1&amp;1&amp;3&amp;\frac{1}{3}&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;\frac{4}{3}&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Because $r=2$, $j=3$. Ignore the first 2 rows and the first 3 columns.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;\underline{0}&amp;0&amp;\frac{4}{3}&amp;1\\
*&amp;*&amp;*&amp;\underline{0}&amp;0&amp;1&amp;1\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Increase both $r$ and $j$ by $1$, i.e. $r=3$, $j=4$.
</li>
<li class="ltx_item">
Consider column 4. All the entries in column 4 (underlined) are zeros.
</li>
<li class="ltx_item">
We move to the next column, set $j=5$.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;0&amp;\underline{0}&amp;\frac{4}{3}&amp;1\\
*&amp;*&amp;*&amp;0&amp;\underline{0}&amp;1&amp;1\\
\end{bmatrix}
\end{align*}

<ul class="ltx_itemize">
<li class="ltx_item">
Again, all the entries in column 5 (underlined) are zeros.
</li>
<li class="ltx_item">
We move to the next column, set $j=6$.
</li>

</ul>

\begin{align*}
\displaystyle \begin{bmatrix}*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;*&amp;*&amp;*&amp;*\\
*&amp;*&amp;*&amp;0&amp;0&amp;\underline{\frac{4}{3}}&amp;1\\
*&amp;*&amp;*&amp;0&amp;0&amp;\underline{1}&amp;1\\
\end{bmatrix}
\end{align*}

In below we continue the process without detail explanations:

\begin{align*}
\displaystyle \xrightarrow{R_{3}\leftrightarrow R_{4}}\begin{bmatrix}1&amp;2&amp;0&amp;1&amp;2&amp;\frac{4}{3}&amp;-1\\
0&amp;0&amp;1&amp;1&amp;3&amp;\frac{1}{3}&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;\frac{4}{3}&amp;1\\
\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{-\frac{4}{3}R_{3}+R_{1},-\frac{1}{3}R_{3}+R_{2},-\frac{4}{3}R_{3}+R_{4}}\begin{bmatrix}1&amp;2&amp;0&amp;1&amp;2&amp;0&amp;-\frac{7}{3}\\
0&amp;0&amp;1&amp;1&amp;3&amp;0&amp;\frac{2}{3}\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;-\frac{1}{3}\\
\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{-3R_{4}}\begin{bmatrix}1&amp;2&amp;0&amp;1&amp;2&amp;0&amp;-\frac{7}{3}\\
0&amp;0&amp;1&amp;1&amp;3&amp;0&amp;\frac{2}{3}\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1\\
\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{\frac{7}{3}R_{4}+R_{1},-\frac{2}{3}R_{4}+R_{2},-1R_{4}+R_{3}}B=\begin{bmatrix}1&amp;2&amp;0&amp;1&amp;2&amp;0&amp;0\\
0&amp;0&amp;1&amp;1&amp;3&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1\\
\end{bmatrix}
\end{align*}

$B$ is the reduced row echelon form of $A$, denoted by $A\xrightarrow{\text{RREF}}B$.
@end
@thm
@title{Reduced Row-Echelon Form is Unique}
@label{RREFU}
Suppose that $A$ is an $m\times n$ matrix and that $B$ and $C$ are $m\times n$ matrices that are row-equivalent to $A$ and in reduced row-echelon form. Then $B=C$.
@end
@proof
@col
See Beezer, Ver 3.5 (print version p24). We will prove it later. You can skip the proof for now.
∎
@end
@eg
Find the solutions to the following system of equations,

\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}

First, form the augmented matrix, is

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}-7&amp;-6&amp;-12&amp;-33\\
5&amp;5&amp;7&amp;24\\
1&amp;0&amp;4&amp;5\\
\end{array}\right]
\end{align*}

and work to reduced row-echelon form, first with $j=1$,

\begin{align*}
\displaystyle\xrightarrow{R_{1}\leftrightarrow R_{3}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}1&amp;0&amp;4&amp;5\\
5&amp;5&amp;7&amp;24\\
-7&amp;-6&amp;-12&amp;-33\end{array}\right]\xrightarrow{-5R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;4&amp;5\\
0&amp;5&amp;-13&amp;-1\\
-7&amp;-6&amp;-12&amp;-33\end{array}\right] \\
\displaystyle\xrightarrow{7R_{1}+R_{3}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;4&amp;5\\
0&amp;5&amp;-13&amp;-1\\
0&amp;-6&amp;16&amp;2\end{array}\right] \\
\\
\displaystyle\xrightarrow{\frac{1}{5}R_{2}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;4&amp;5\\
0&amp;1&amp;\frac{-13}{5}&amp;\frac{-1}{5}\\
0&amp;-6&amp;16&amp;2\end{array}\right]\xrightarrow{6R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;4&amp;5\\
0&amp;\boxed{1}&amp;\frac{-13}{5}&amp;\frac{-1}{5}\\
0&amp;0&amp;\frac{2}{5}&amp;\frac{4}{5}\end{array}\right] \\
\\
\displaystyle\xrightarrow{\frac{5}{2}R_{3}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;4&amp;5\\
0&amp;\boxed{1}&amp;\frac{-13}{5}&amp;\frac{-1}{5}\\
0&amp;0&amp;1&amp;2\end{array}\right]\xrightarrow{\frac{13}{5}R_{3}+R_{2}}\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;4&amp;5\\
0&amp;\boxed{1}&amp;0&amp;5\\
0&amp;0&amp;1&amp;2\end{array}\right] \\
\displaystyle\xrightarrow{-4R_{3}+R_{1}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;-3\\
0&amp;\boxed{1}&amp;0&amp;5\\
0&amp;0&amp;\boxed{1}&amp;2\end{array}\right]
\end{align*}

This is now the augmented matrix of a very simple system of equations, namely $x_{1}=-3$, $x_{2}=5$, $x_{3}=2$, which has an obvious solution. Furthermore, we can see that this is the <b>only</b> solution to this system, so we have determined the entire solution set,

\begin{align*}
\displaystyle S&amp;\displaystyle=\{\begin{bmatrix}-3\\
5\\
2\end{bmatrix}\}
\end{align*}
@end
@eg
Let us find the solutions to the following system of equations,

\begin{align*}
\displaystyle x_{1}-x_{2}+2x_{3}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}+x_{2}+x_{3}&amp;\displaystyle=8 \\
\displaystyle x_{1}+x_{2}&amp;\displaystyle=5
\end{align*}

First, form the augmented matrix,

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}1&amp;-1&amp;2&amp;1\\
2&amp;1&amp;1&amp;8\\
1&amp;1&amp;0&amp;5\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle\xrightarrow{-2R_{1}+R_{2}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}1&amp;-1&amp;2&amp;1\\
0&amp;3&amp;-3&amp;6\\
1&amp;1&amp;0&amp;5\end{array}\right]\xrightarrow{-1R_{1}+R_{3}}\left[\begin{array}[]{ccc|c}\boxed{1}&amp;-1&amp;2&amp;1\\
0&amp;3&amp;-3&amp;6\\
0&amp;2&amp;-2&amp;4\end{array}\right] \\
\\
\displaystyle\xrightarrow{\frac{1}{3}R_{2}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}\boxed{1}&amp;-1&amp;2&amp;1\\
0&amp;1&amp;-1&amp;2\\
0&amp;2&amp;-2&amp;4\end{array}\right]\xrightarrow{1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;1&amp;3\\
0&amp;1&amp;-1&amp;2\\
0&amp;2&amp;-2&amp;4\end{array}\right] \\
\displaystyle\xrightarrow{-2R_{2}+R_{3}}&amp;\displaystyle\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;1&amp;3\\
0&amp;\boxed{1}&amp;-1&amp;2\\
0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

The system of equations represented by this augmented matrix needs to be considered a bit differently than the previous case.
First, the last row of the matrix is the equation $0=0$, which is always true, so it imposes no restrictions on our possible solutions and therefore we can safely ignore it as we analyze the other two equations. These equations are,

\begin{align*}
\displaystyle x_{1}+x_{3}&amp;\displaystyle=3 \\
\displaystyle x_{2}-x_{3}&amp;\displaystyle=2.
\end{align*}

While this system is fairly easy to solve, it also appears to have a multitude of solutions.
For example, choose $x_{3}=1$ and see that then $x_{1}=2$ and $x_{2}=3$ will together form a solution.
Or choose $x_{3}=0$, and then discover that $x_{1}=3$ and $x_{2}=2$ lead to a solution.
Try it yourself: pick any value of $x_{3}$ you please, and figure out what $x_{1}$ and $x_{2}$ should be to make the first and second equations (respectively) true. We’ll wait while you do that. Because of this behavior, we say that $x_{3}$ is a <b>free</b> or <b>independent</b> variable. But why do we vary $x_{3}$ and not some other variable? For now, notice that the third column of the augmented matrix is not a pivot column. With this idea, we can rearrange the two equations, solving each for the variable whose index is the same as the column index of a pivot column.

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=3-x_{3} \\
\displaystyle x_{2}&amp;\displaystyle=2+x_{3}
\end{align*}

To write the set of solution vectors in set notation, we have

\begin{align*}
\displaystyle S&amp;\displaystyle=\left\{\left.\begin{bmatrix}3-x_{3}\\
2+x_{3}\\
x_{3}\end{bmatrix}\,\right|\,x_{3}\in\mathbf{R}\right\}
\end{align*}

We will learn more in the next lecture about systems with infinitely many solutions and how to express their solution sets.
@end
@eg
Let us find the solutions to the following system of equations,

\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&amp;\displaystyle=2 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&amp;\displaystyle=3 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&amp;\displaystyle=2
\end{align*}

First, form the augmented matrix,

\begin{align*}
\displaystyle \left[\begin{array}[]{cccc|c}2&amp;1&amp;7&amp;-7&amp;2\\
-3&amp;4&amp;-5&amp;-6&amp;3\\
1&amp;1&amp;4&amp;-5&amp;2\\
\end{array}\right]
\end{align*}

and work to reduced row-echelon form, first with $j=1$,

\begin{align*}
\displaystyle\xrightarrow{R_{1}\leftrightarrow R_{3}}&amp;\displaystyle\left[\begin{array}[]{cccc|c}1&amp;1&amp;4&amp;-5&amp;2\\
-3&amp;4&amp;-5&amp;-6&amp;3\\
2&amp;1&amp;7&amp;-7&amp;2\end{array}\right]\xrightarrow{3R_{1}+R_{2}}\left[\begin{array}[]{cccc|c}1&amp;1&amp;4&amp;-5&amp;2\\
0&amp;7&amp;7&amp;-21&amp;9\\
2&amp;1&amp;7&amp;-7&amp;2\end{array}\right] \\
\displaystyle\xrightarrow{-2R_{1}+R_{3}}&amp;\displaystyle\left[\begin{array}[]{cccc|c}\boxed{1}&amp;1&amp;4&amp;-5&amp;2\\
0&amp;7&amp;7&amp;-21&amp;9\\
0&amp;-1&amp;-1&amp;3&amp;-2\end{array}\right] \\
\\
\displaystyle\xrightarrow{R_{2}\leftrightarrow R_{3}}&amp;\displaystyle\left[\begin{array}[]{cccc|c}\boxed{1}&amp;1&amp;4&amp;-5&amp;2\\
0&amp;-1&amp;-1&amp;3&amp;-2\\
0&amp;7&amp;7&amp;-21&amp;9\end{array}\right]\xrightarrow{-1R_{2}}\left[\begin{array}[]{cccc|c}\boxed{1}&amp;1&amp;4&amp;-5&amp;2\\
0&amp;1&amp;1&amp;-3&amp;2\\
0&amp;7&amp;7&amp;-21&amp;9\end{array}\right] \\
\displaystyle\xrightarrow{-1R_{2}+R_{1}}&amp;\displaystyle\left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;1&amp;1&amp;-3&amp;2\\
0&amp;7&amp;7&amp;-21&amp;9\end{array}\right]\xrightarrow{-7R_{2}+R_{3}}\left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;2\\
0&amp;0&amp;0&amp;0&amp;-5\end{array}\right] \\
\\
\displaystyle\xrightarrow{-\frac{1}{5}R_{3}}&amp;\displaystyle\left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;2\\
0&amp;0&amp;0&amp;0&amp;1\end{array}\right]\xrightarrow{-2R_{3}+R_{2}}\left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}\end{array}\right]
\end{align*}

The third equation will read $0=1$. This is patently false, all the time. No choice of values for our variables will ever make it true. We are done. Since we cannot even make the last equation true, we have no hope of making all of the equations simultaneously true. So this system has <b>no solutions</b>, and its solution set is the empty set, $\emptyset=\{\ \}$

Notice that we could have reached this conclusion sooner. After performing the row operation
$-7R_{2}+R_{3}$, we can see that the third equation reads $0=-5$, a false statement. Since the system represented by this matrix has no solutions, none of the systems represented has any solutions. However, for this example, we have chosen to bring the matrix all the way to reduced row-echelon form as practice.
@end

The above three examples
illustrate the full range of possibilities for a system of linear equations –
no solutions, one solution, or infinitely many solutions.
In the next lecture we will examine these three scenarios more closely.

@chapter{Type of solution sets}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Subsection TTS (print version p35 - p41)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf
Section TSS (p.13 - 18) C21-28, T11, T20, T40, T41.

@section{Introduction}

We will now be more careful about analyzing the reduced row-echelon form derived from the augmented matrix of a system of linear equations.
In particular, we will see how to systematically handle the situation when we have infinitely many solutions to a system, and we will prove that every system of linear equations has either zero, one or infinitely many solutions.
With these tools, we will be able to routinely solve any linear system.

@section{Consistent Systems}
@label{CS}
A system of linear equations is <b>consistent</b>
if it has at least one solution. Otherwise, the system is called <b>inconsistent</b>.

@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
The system of linear equations

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=3 \\
\displaystyle x_{1}-x_{2}&amp;\displaystyle=4
\end{align*}

is consistent because it has solution $(x_{1},x_{2})=(1,-3)$.
</li>
<li class="ltx_item">
The system of linear equations

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=3 \\
\displaystyle 4x_{1}+6x_{2}&amp;\displaystyle=6
\end{align*}

is consistent because has infinite many solutions: $\{(t,\frac{3-2t}{3})\,|\,t\text{ real number}\}$.
</li>
<li class="ltx_item">
The system of linear equations

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=3 \\
\displaystyle 4x_{1}+6x_{2}&amp;\displaystyle=10
\end{align*}

is inconsistent because it has no solution.
</li>

</ol>
@end

<b>Notation Recall</b>: Let $A$ be a reduced row echelon form.

<ol class="ltx_enumerate">
<li class="ltx_item">
The number of non-zero rows is called the <b>rank</b> of $A$ and is denoted by $r$.
</li>
<li class="ltx_item">
The set of the column indexes for the pivot columns is denoted by

\begin{align*}
\displaystyle D=\{(d_{1},d_{2},d_{3},\ldots,d_{r})\,|\,d_{1}&lt;d_{2}&lt;d_{3}&lt;\cdots&lt;d_{r}.\}
\end{align*}

</li>
<li class="ltx_item">
The set of column indexes that are not pivot columns is denoted by

\begin{align*}
\displaystyle F=\{(f_{1},f_{2},f_{3},\ldots,f_{n-r})\,|\,f_{1}&lt;f_{2}&lt;f_{3}&lt;\cdots&lt;f_{n-r}.\}
\end{align*}

</li>

</ol>

@eg
@label{RREFN}
Reduced row-echelon form notation

For the $5\times 9$ matrix

\begin{align*}
\displaystyle B&amp;\displaystyle=\begin{bmatrix}\boxed{1}&amp;5&amp;0&amp;0&amp;2&amp;8&amp;0&amp;5&amp;-1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;4&amp;7&amp;0&amp;2&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;3&amp;9&amp;0&amp;3&amp;-6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;4&amp;2\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

in reduced row-echelon form we have

\begin{align*}
\displaystyle r&amp;\displaystyle=4 \\
\displaystyle d_{1}&amp;\displaystyle=1&amp;\displaystyle d_{2}&amp;\displaystyle=3&amp;\displaystyle d_{3}&amp;\displaystyle=4&amp;\displaystyle d_{4}&amp;\displaystyle=7 \\
\displaystyle f_{1}&amp;\displaystyle=2&amp;\displaystyle f_{2}&amp;\displaystyle=5&amp;\displaystyle f_{3}&amp;\displaystyle=6&amp;\displaystyle f_{4}&amp;\displaystyle=8&amp;\displaystyle f_{5}&amp;\displaystyle=9
\end{align*}

Notice that the sets

\begin{align*}
\displaystyle D&amp;\displaystyle=\{d_{1},\,d_{2},\,d_{3},\,d_{4}\}=\{1,\,3,\,4,\,7\}&amp;\displaystyle F=\{f_{1},\,f_{2},\,f_{3},\,f_{4},\,f_{5}\}=\{2,\,5,\,6,\,8,\,9\}
\end{align*}

have nothing in common and together account for all of the columns of $B$.
@end
@eg
@label{ISSI}
Describing infinite solution sets of the following system of linear equation
with $m=4$ equations in $n=7$ variables.

\begin{align*}
\displaystyle x_{1}+4x_{2}-x_{4}+7x_{6}-9x_{7}&amp;\displaystyle=3 \\
\displaystyle 2x_{1}+8x_{2}-x_{3}+3x_{4}+9x_{5}-13x_{6}+7x_{7}&amp;\displaystyle=9 \\
\displaystyle 2x_{3}-3x_{4}-4x_{5}+12x_{6}-8x_{7}&amp;\displaystyle=1 \\
\displaystyle-x_{1}-4x_{2}+2x_{3}+4x_{4}+8x_{5}-31x_{6}+37x_{7}&amp;\displaystyle=4
\end{align*}

This system has a $4\times 8$ augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccccc|c}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9&amp;3\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7&amp;9\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8&amp;1\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37&amp;4\end{array}\right]
\end{align*}

The matrix is row-equivalent to the following matrix reduced row-echelon form (<h5 class="notkw">Exercise.</h5>
: check this)

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccccc|c}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3&amp;4\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5&amp;2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

So we find that $r=3$ and

\begin{align*}
\displaystyle D&amp;\displaystyle=\{d_{1},\,d_{2},\,d_{3}\}=\{1,\,3,\,4\}&amp;\displaystyle F&amp;\displaystyle=\{f_{1},\,f_{2},\,f_{3},\,f_{4},\,f_{5}\}=\{2,\,5,\,6,\,7,\,8\}
\end{align*}

Let $i$ denote any one of the $r=3$ nonzero rows. Then the index $d_{i}$ is a pivot column. It will be easy in this case to use the equation represented by row $i$ to write an expression for the variable $x_{d_{i}}$. It will be a linear function of the variables $x_{f_{1}},\,x_{f_{2}},\,x_{f_{3}},\,x_{f_{4}}$

\begin{align*}
\displaystyle(i=1)&amp;\displaystyle x_{d_{1}}&amp;\displaystyle=x_{1}=4-4x_{2}-2x_{5}-x_{6}+3x_{7} \\
\displaystyle(i=2)&amp;\displaystyle x_{d_{2}}&amp;\displaystyle=x_{3}=2-x_{5}+3x_{6}-5x_{7} \\
\displaystyle(i=3)&amp;\displaystyle x_{d_{3}}&amp;\displaystyle=x_{4}=1-2x_{5}+6x_{6}-6x_{7}
\end{align*}

Each element of the set $F=\{f_{1},\,f_{2},\,f_{3},\,f_{4},\,f_{5}\}=\{2,\,5,\,6,\,7,\,8\}$ is the index of a variable, except for $f_{5}=8$. We refer to $x_{f_{1}}=x_{2}$, $x_{f_{2}}=x_{5}$, $x_{f_{3}}=x_{6}$ and $x_{f_{4}}=x_{7}$ as <b>free</b> (or <b>independent</b>) variables since they are allowed to assume any possible combination of values that we can imagine and we can continue on to build a solution to the system by solving individual equations for the values of the other (<b>dependent</b>) variables.

Each element of the set $D=\{d_{1},\,d_{2},\,d_{3}\}=\{1,\,3,\,4\}$ is the index of a variable. We refer to the variables $x_{d_{1}}=x_{1}$, $x_{d_{2}}=x_{3}$ and $x_{d_{3}}=x_{4}$ as <b>dependent</b> variables since they depend on the independent variables. More precisely, for each possible choice of values for the independent variables we get exactly one set of values for the dependent variables that combine to form a solution of the system.

To express the solutions as a set, we write

\begin{align*}
\displaystyle \left\{\left.\begin{bmatrix}4-4x_{2}-2x_{5}-x_{6}+3x_{7}\\
x_{2}\\
2-x_{5}+3x_{6}-5x_{7}\\
1-2x_{5}+6x_{6}-6x_{7}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}\,\right|\,x_{2},\,x_{5},\,x_{6},\,x_{7}\text{ real numbers}\right\}&amp;
\end{align*}

The condition that $x_{2},\,x_{5},\,x_{6},\,x_{7}$ are real numbers is how we specify that the variables $x_{2},\,x_{5},\,x_{6},\,x_{7}$ are <b>free</b> to assume any possible values.

This systematic approach to solving a system of equations will allow us to create a precise description of the solution set for any consistent system once we have found the reduced row-echelon form of the augmented matrix. It will work just as well when the set of free variables is empty and we get just a single solution.
@end
@defn
@title{Independent and Dependent Variables}
@label{IDV}
Suppose $A$ is the augmented matrix of a consistent system of linear equations and $B$ is a row-equivalent matrix in reduced row-echelon form.
Suppose $j$ is the index of a pivot column of $B$. Then the variable $x_{j}$ is <b>dependent</b>.
A variable that is not dependent is called <b>independent</b> or <b>free</b>.
@end
@eg
@label{FDV}
<b>Free and dependent variables</b>

Consider the system of five equations in five variables,

\begin{align*}
\displaystyle x_{1}-x_{2}-2x_{3}+x_{4}+11x_{5}&amp;\displaystyle=13 \\
\displaystyle x_{1}-x_{2}+x_{3}+x_{4}+5x_{5}&amp;\displaystyle=16 \\
\displaystyle 2x_{1}-2x_{2}+x_{4}+10x_{5}&amp;\displaystyle=21 \\
\displaystyle 2x_{1}-2x_{2}-x_{3}+3x_{4}+20x_{5}&amp;\displaystyle=38 \\
\displaystyle 2x_{1}-2x_{2}+x_{3}+x_{4}+8x_{5}&amp;\displaystyle=22
\end{align*}

whose augmented matrix row-reduces to

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}\boxed{1}&amp;-1&amp;0&amp;0&amp;3&amp;6\\
0&amp;0&amp;\boxed{1}&amp;0&amp;-2&amp;1\\
0&amp;0&amp;0&amp;\boxed{1}&amp;4&amp;9\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\end{array}\right]&amp;
\end{align*}

Columns 1, 3 and 4 are pivot columns, so $D=\{1,\,3,\,4\}$. From this we know that the variables $x_{1}$, $x_{3}$ and $x_{4}$ will be dependent variables, and each of the $r=3$ nonzero rows of the row-reduced matrix will yield an expression for one of these three variables. The set $F$ is all the remaining column indices, $F=\{2,\,5,\,6\}$. The column index $6$ in $F$ means that the final column is not a pivot column, and thus the system is consistent (see the next theorem). The remaining indices in $F$ indicate free variables, so $x_{2}$ and $x_{5}$ (the remaining variables) are our free variables. The resulting three equations that describe our solution set are then,

\begin{align*}
\displaystyle(x_{d_{1}}=x_{1})&amp;\displaystyle x_{1}&amp;\displaystyle=6+x_{2}-3x_{5} \\
\displaystyle(x_{d_{2}}=x_{3})&amp;\displaystyle x_{3}&amp;\displaystyle=1+2x_{5} \\
\displaystyle(x_{d_{3}}=x_{4})&amp;\displaystyle x_{4}&amp;\displaystyle=9-4x_{5}
\end{align*}

Make sure you understand where these three equations came from, and notice how the location of the pivot columns determined the variables on the left-hand side of each equation. We can compactly describe the solution set as,

\begin{align*}
\displaystyle S=\left\{\left.\begin{bmatrix}6+x_{2}-3x_{5}\\
x_{2}\\
1+2x_{5}\\
9-4x_{5}\\
x_{5}\end{bmatrix}\,\right|\,x_{2},\,x_{5}\text{ real numbers}\right\}&amp;
\end{align*}

Notice how we express the freedom for $x_{2}$ and $x_{5}$: $x_{2},\,x_{5}\text{ real numbers}$.
@end
@thm
@title{Recognizing Consistency of a Linear System}
@label{RCLS}
Suppose $A$ is the augmented matrix of a system of linear equations with $n$ variables. Suppose also that $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ nonzero rows.

<ul class="ltx_itemize">
<li class="ltx_item">
Then the system of equations is <b>inconsistent</b> if and only if column $n+1$ (i.e., the last column) of $B$ is a pivot column.
</li>
<li class="ltx_item">
Equivalently a system is <b>consistent</b> if and only if column $n+1$ is not a pivot column of $B$.
</li>
<li class="ltx_item">
Another way of expressing the theorem is to say the last the system of equations is <b>inconsistent</b> if and only if the last <b>non-zero row</b> is not $(0,0,\ldots,0,1)$.
</li>

</ul>
@end
@proof
@col
(sketch, for details, see Beezer, Ver 3.5 print version p.38, proof of theorem RCLS, you can skip the proof in the textbook)

If the last column vector of $B$ is a pivot column, then $B$ is in the form of

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;*&amp;0\\
0&amp;0&amp;\cdots&amp;1&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;*&amp;0\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;1&amp;*&amp;\cdots&amp;*&amp;0\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;*&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;1\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
\end{bmatrix}
\end{align*}

For the system of linear equations with the above augmented matrix, the $r+1$-st equation (i.e. the last non-zero equation) is

\begin{align*}
\displaystyle 0=1.
\end{align*}

So the system of linear equations has no solution.

Conversely, if the last column vector is not a pivot column vector, then $B$ is in the form of

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;*\\
0&amp;0&amp;\cdots&amp;1&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;*\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;1&amp;*&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;*\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;*&amp;\cdots&amp;*\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;1&amp;*&amp;\cdots&amp;*\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
\end{bmatrix}
\end{align*}

For the system of equations with the above augmented matrix,
we can move the variables corresponding to the non-pivot columns (i.e., $x_{f_{1}},x_{f_{2}},\ldots$) to the right hand side of the equations and therefore solve the equations.
Hence it is consistent.
Note that $x_{f_{1}},x_{f_{2}},\ldots$ are free variables.
∎
@end
@eg
Determine if the following system of linear equation is consistent.

\begin{align*}
\displaystyle x_{1}+x_{2}+2x_{3}+3x_{4}+2x_{5}+5x_{6}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}+2x_{2}+3x_{3}-x_{4}&amp;\displaystyle=1 \\
\displaystyle 3x_{1}+3x_{2}+5x_{3}+x_{4}+x_{5}-2x_{6}&amp;\displaystyle=3 \\
\displaystyle x_{4}+x_{5}+7x_{6}&amp;\displaystyle=0
\end{align*}

The augmented matrix is

\begin{align*}
\displaystyle \left[\begin{array}[]{cccccc|c}1&amp;1&amp;2&amp;3&amp;2&amp;5&amp;1\\
2&amp;2&amp;3&amp;-1&amp;0&amp;0&amp;1\\
3&amp;3&amp;5&amp;1&amp;1&amp;-2&amp;3\\
0&amp;0&amp;0&amp;1&amp;1&amp;7&amp;-1\\
\end{array}\right]
\end{align*}

The reduced row echelon form is

\begin{align*}
\displaystyle \left[\begin{array}[]{cccccc|c}1&amp;1&amp;0&amp;0&amp;5&amp;62&amp;0\\
0&amp;0&amp;1&amp;0&amp;-3&amp;-39&amp;0\\
0&amp;0&amp;0&amp;1&amp;1&amp;7&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1\\
\end{array}\right]
\end{align*}

The last column is a pivot column. So the system is inconsistent.
@end
@eg
Determine if the following system of linear equation is consistent.

\begin{align*}
\displaystyle x_{1}+x_{2}+2x_{3}+3x_{4}+2x_{5}+5x_{6}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}+2x_{2}+3x_{3}-x_{4}&amp;\displaystyle=1 \\
\displaystyle 3x_{1}+3x_{2}+5x_{3}+x_{4}+x_{5}-2x_{6}&amp;\displaystyle=3 \\
\displaystyle x_{4}+x_{5}+7x_{6}&amp;\displaystyle=-1
\end{align*}

The augmented matrix is

\begin{align*}
\displaystyle \left[\begin{array}[]{cccccc|c}1&amp;1&amp;2&amp;3&amp;2&amp;5&amp;1\\
2&amp;2&amp;3&amp;-1&amp;0&amp;0&amp;1\\
3&amp;3&amp;5&amp;1&amp;1&amp;-2&amp;3\\
0&amp;0&amp;0&amp;1&amp;1&amp;7&amp;-1\\
\end{array}\right]
\end{align*}

The reduced row echelon form is

\begin{align*}
\displaystyle \left[\begin{array}[]{cccccc|c}1&amp;1&amp;0&amp;0&amp;5&amp;62&amp;-12\\
0&amp;0&amp;1&amp;0&amp;-3&amp;-39&amp;8\\
0&amp;0&amp;0&amp;1&amp;1&amp;7&amp;-1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
\end{array}\right]
\end{align*}

The last column is not a pivot column. So the system is consistent.
@end
@thm
@title{Consistent Systems, $r$ and $n$}
@label{CSRN}
Suppose $A$ is the augmented matrix of a consistent system of linear equations with $n$ variables. Suppose also that $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Then $r\leq n$. If $r=n$, then the system has a unique solution, and if $r&lt;n$, then the system has infinitely many solutions.
@end
@proof
@col
This theorem contains three implications that we must establish.
Notice first that $B$ has $n+1$ columns, so there can be at most $n+1$ pivot columns, i.e., $r\leq n+1$.
If $r=n+1$, then every column of $B$ is a pivot column, and in particular, the last column is a pivot column.
So the previous theorem tells us that the system is inconsistent, contrary to our hypothesis. We are left with $r\leq n$.

When $r=n$, we find $n-r=0$ free variables (i.e., $F=\{n+1\}$) and the only solution is given by setting the $n$ variables to the the first $n$ entries of column $n+1$ of $B$.

When $r&lt;n$, we have $n-r&gt;0$ free variables. Choose one free variable and set all the other free variables to zero. Now, set the chosen free variable to any fixed value. It is possible to then determine the values of the dependent variables to create a solution to the system. By setting the chosen free variable to different values, in this manner we can create infinitely many solutions.
∎
@end

@section{Free variables}
@label{FV}
The next theorem simply states a conclusion from the final paragraph of the previous proof, allowing us to state explicitly the number of free variables for a consistent system.

@thm
@title{Free Variables for Consistent Systems}
@label{FVCS}
Suppose $A$ is the augmented matrix of a consistent system of linear equations with $n$ variables. Suppose also that $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ rows that are not completely zeros. Then the solution set can be described with $n-r$ free variables.
@end
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
System of linear equations with $n=3$, $m=3$.

\begin{align*}
\displaystyle x_{1}-x_{2}+2x_{3}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}+x_{2}+x_{3}&amp;\displaystyle=8 \\
\displaystyle x_{1}+x_{2}&amp;\displaystyle=5
\end{align*}

Augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}1&amp;-1&amp;2&amp;1\\
2&amp;1&amp;1&amp;8\\
1&amp;1&amp;0&amp;5\end{array}\right]
\end{align*}

The reduced row echelon form of the augmented matrix.

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;1&amp;3\\
0&amp;\boxed{1}&amp;-1&amp;2\\
0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

The last column is not a pivot column. So the system of linear equations is consistent. $r=2$, there are $3-2$ free variables. In fact $D=\{1,2\}$, $F=\{3\}$. $x_{1},x_{2}$ are dependent variables, $x_{3}$ is a free variables.

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=3-x_{3} \\
\displaystyle x_{2}&amp;\displaystyle=2+x_{3}
\end{align*}

</li>
<li class="ltx_item">
System of linear equations with $n=3,m=3$.

\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}

Augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}-7&amp;-6&amp;-12&amp;-33\\
5&amp;5&amp;7&amp;24\\
1&amp;0&amp;4&amp;5\end{array}\right]
\end{align*}

The reduced row echelon form of the augmented matrix.

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;-3\\
0&amp;\boxed{1}&amp;0&amp;5\\
0&amp;0&amp;\boxed{1}&amp;2\end{array}\right]
\end{align*}

The last column is not a pivot column. So the system of linear equations is consistent. $r=3$, there are $3-3=0$ free variables. So the solution is unique. In fact

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=-3 \\
\displaystyle x_{2}&amp;\displaystyle=5 \\
\displaystyle x_{3}&amp;\displaystyle=2
\end{align*}

</li>
<li class="ltx_item">
System of linear equations with $n=2$, $m=5$.

\begin{align*}
\displaystyle 2x_{1}+3x_{2}&amp;\displaystyle=6 \\
\displaystyle-x_{1}+4x_{2}&amp;\displaystyle=-14 \\
\displaystyle 3x_{1}+10x_{2}&amp;\displaystyle=-2 \\
\displaystyle 3x_{1}-x_{2}&amp;\displaystyle=20 \\
\displaystyle 6x_{1}+9x_{2}&amp;\displaystyle=18
\end{align*}

Augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{cc|c}2&amp;3&amp;6\\
-1&amp;4&amp;-14\\
3&amp;10&amp;-2\\
3&amp;-1&amp;20\\
6&amp;9&amp;18\end{array}\right]
\end{align*}

The reduced row echelon form of the augmented matrix.

\begin{align*}
\displaystyle \left[\begin{array}[]{cc|c}\boxed{1}&amp;0&amp;6\\
0&amp;\boxed{1}&amp;-2\\
0&amp;0&amp;0\\
0&amp;0&amp;0\\
0&amp;0&amp;0\end{array}\right]
\end{align*}

The last column is not a pivot column. So the system of linear equations is consistent. $r=3$, there are $2-2=0$ free variables. So the solution is unique. In fact

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=6 \\
\displaystyle x_{2}&amp;\displaystyle=-2
\end{align*}

</li>
<li class="ltx_item">
System of linear equations with $n=4,m=3$.

\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&amp;\displaystyle=2 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&amp;\displaystyle=3 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&amp;\displaystyle=2
\end{align*}

Augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{cccc|c}2&amp;1&amp;7&amp;-7&amp;2\\
-3&amp;4&amp;-5&amp;-6&amp;3\\
1&amp;1&amp;4&amp;-5&amp;2\end{array}\right]
\end{align*}

The reduced row echelon form of the augmented matrix.

\begin{align*}
\displaystyle \left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}\end{array}\right]
\end{align*}

The last column is a pivot column. Hence the system of linear equations is inconsistent. It has no solution.
</li>

</ol>
@end
@thm
@title{Possible Solution Sets for Linear Systems}
@label{PSSLS}
A system of linear equations has no solutions, a unique solution or infinitely many solutions.
@end
@proof
@col
If the system is inconsistent, that it has no solutions.Suppose the system is consistent. If it has $0$ free variable, it has a unique solution.
If it has $\geq 1$ free variables, it has infinite many solutions.
∎
@end
@thm
@title{Consistent, More Variables than Equations, Infinite solutions}
@label{CMVEI}
Suppose a consistent system of linear equations has $m$ equations in $n$ variables. If $n&gt;m$, then the system has infinitely many solutions.
@end
@proof
@col
Suppose that the augmented matrix of the system of equations is row-equivalent to $B$, a matrix in reduced row-echelon form with $r$ nonzero rows.
Because $B$ has $m$ rows in total, the number of nonzero rows is less than or equal to $m$. In other words, $r\leq m$.
Follow this with the hypothesis that $n&gt;m$ and we find that the system has a solution set described by at least one free variable because

\begin{align*}
\displaystyle n-r\geq n-m&gt;0.
\end{align*}

A consistent system with free variables will have an infinite number of solutions, as given by Theorem  @ref{CSRN}.
∎
@end
@eg
\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&amp;\displaystyle=8 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&amp;\displaystyle=-12 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&amp;\displaystyle=4
\end{align*}

Because $n=4$, $m=3$ and $n&gt;m$. Hence it is consistent and has infinitely many solutions by the above theorem.
@end

These theorems give us the procedures and implications that allow us to completely solve any system of linear equations. The main computational tool is using row operations to convert an augmented matrix into reduced row-echelon form. Here is a broad outline of how we would instruct a computer to solve a system of linear equations.

<b>Steps of solving system of linear equations</b>

<ol class="ltx_enumerate">
<li class="ltx_item">
Represent a system of linear equations in $n$ variables by an augmented matrix.
</li>
<li class="ltx_item">
Convert the matrix to a row-equivalent matrix in reduced row-echelon form using the procedure given in lecture 3 Theorem 2.
Identify the location of the pivot columns, and the rank $r$.
</li>
<li class="ltx_item">
If column $n+1$ is a pivot column, then the system is inconsistent.
</li>
<li class="ltx_item">
If column $n+1$ is not a pivot column, there are two possibilities:

<ol class="ltx_enumerate">
<li class="ltx_item">
$r=n$ and the solution is unique. It can be read off directly from the entries in rows 1 through $n$ of column $n+1$.
</li>
<li class="ltx_item">
$r&lt;n$ and there are infinitely many solutions. we can describe the solution sets by the free variables.
</li>

</ol>
</li>

</ol>

@chapter{Homogeneous Systems of Equations and non singular matrices}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section HSE (print version p44 - p50)Section NM (print version p51 - p56)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf

<ol class="ltx_enumerate">
<li class="ltx_item">
Section HSE (ex p.18-23) C21-C23, C25-C27, C30-C31, M50-M52, T10-T12, T20
</li>
<li class="ltx_item">
Section NM (ex p.23-27) C30-C33, C50, M30, M51-M52, T10, T12, T30, T31, T90.
</li>

</ol>

@section{Solutions of Homogeneous Systems}
@label{SHS}
@defn
@title{Homogeneous System}
@label{HS}
A system of linear equations, $A\mathbf{x}=\mathbf{b}$ is <b>homogeneous</b>
if the vector of constants is the zero vector, in other words, if $\mathbf{b}=\mathbf{0}$, i.e.

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}&amp;\displaystyle=0 \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}&amp;\displaystyle=0 \\
\displaystyle\vdots&amp; \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}&amp;\displaystyle=0
\end{align*}
@end
@defn
@title{Homogenous System corresponding to system of linear equation}
The <b>homogenous system</b> corresponding to $A\mathbf{x}=\mathbf{b}$:

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\dots+a_{1n}x_{n}&amp;\displaystyle=b_{1} \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\dots+a_{2n}x_{n}&amp;\displaystyle=b_{2} \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\dots+a_{3n}x_{n}&amp;\displaystyle=b_{3} \\
\displaystyle\vdots&amp; \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+\dots+a_{mn}x_{n}&amp;\displaystyle=b_{m}
\end{align*}

is $A\mathbf{x}=\mathbf{0}$:

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\dots+a_{1n}x_{n}&amp;\displaystyle=0 \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\dots+a_{2n}x_{n}&amp;\displaystyle=0 \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\dots+a_{3n}x_{n}&amp;\displaystyle=0 \\
\displaystyle\vdots&amp; \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+\dots+a_{mn}x_{n}&amp;\displaystyle=0
\end{align*}
@end
@eg
The following is a homogenous system of linear equations:

\begin{align*}
\displaystyle x_{1}-2x_{2}+3x_{3}-4x_{4}&amp;\displaystyle=0 \\
\displaystyle x_{2}-x_{4}&amp;\displaystyle=0 \\
\displaystyle x_{1}+3x_{2}-5x_{3}+5x_{4}&amp;\displaystyle=0
\end{align*}

It is the homogenous system of linear equations corresponding to

\begin{align*}
\displaystyle x_{1}-2x_{2}+3x_{3}-4x_{4}&amp;\displaystyle=1 \\
\displaystyle x_{2}-x_{4}&amp;\displaystyle=2 \\
\displaystyle x_{1}+3x_{2}-5x_{3}+5x_{4}&amp;\displaystyle=3
\end{align*}
@end
@thm
@title{Homogeneous Systems are Consistent}
@label{HSC}
Suppose that a system of linear equations is homogeneous.
Then the system is consistent. In fact $\mathbf{0}$ is a solution, i.e $x_{1}=x_{2}=\cdots=x_{n}=0$ is a solution. Such solution is called a <b>trivial solution</b>.
@end
@proof
@col
Set each variable of the system to zero. The left hand side of the all equations are zero, which are equal to the right hand side.
∎
@end
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=0 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=0 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=0
\end{align*}

The reduced row echelon form of the augmented matrix is

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0\end{array}\right]
\end{align*}

It has $n-r=3-3=0$ free variable. Hence it has only one solution.
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle x_{1}-x_{2}+2x_{3}&amp;\displaystyle=0 \\
\displaystyle 2x_{1}+x_{2}+x_{3}&amp;\displaystyle=0 \\
\displaystyle x_{1}+x_{2}&amp;\displaystyle=0
\end{align*}

The reduced row echelon form of the augmented matrix is

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;1&amp;0\\
0&amp;\boxed{1}&amp;-1&amp;0\\
0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

The system is consistent. It has $n-r=3-2=1$ free variable. The solution set is

\begin{align*}
\displaystyle S=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}\,\right|\,x_{1}=-x_{3},\,x_{2}=x_{3}\right\}=\left\{\left.\begin{bmatrix}-x_{3}\\
x_{3}\\
x_{3}\end{bmatrix}\,\right|\,x_{3}\text{ real number}\right\}
\end{align*}

Geometrically, these are points in three dimensions that lie on a line through the origin.
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&amp;\displaystyle=0 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&amp;\displaystyle=0 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&amp;\displaystyle=0
\end{align*}

The reduced row echelon form of the augmented matrix is

\begin{align*}
\displaystyle \left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

The system is consistent. It has $n-r=4-2=2$ free variables. The solution set is

\begin{align*}
\displaystyle S&amp;\displaystyle=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{1}=-3x_{3}+2x_{4},\,x_{2}=-x_{3}+3x_{4}\right\} \\
&amp;\displaystyle=\left\{\left.\begin{bmatrix}-3x_{3}+2x_{4}\\
-x_{3}+3x_{4}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{3},\,x_{4}\text{ real numbers}\right\}
\end{align*}

</li>

</ol>
@end

Notice that when we do row operations on the augmented matrix of a homogeneous system of linear equations the last column of the matrix is all zeros. Any one of the three allowable row operations will convert zeros to zeros and thus, the final column of the matrix in reduced row-echelon form will also be all zeros. So in this case, we may be as likely to reference only the coefficient matrix and presume that we remember that the final column begins with zeros, and after any number of row operations is still zero.

@thm
@label{HMVEI}
Suppose that a homogeneous system of linear equations has $m$ equations and $n$ variables with $n&gt;m$.
Then the system has infinitely many solutions.
@end
@proof
@col
The system is homogeneous, by theorem  @ref{HSC} it is consistent.
Then the hypothesis that $n&gt;m$, together with lecture 4 theorem 6, gives infinitely many solutions.
∎
@end

If $n=m$, then we can have a unique solution or infinitely many solutions (see the above examples).

@section{Null Space of a Matrix}
@label{NSM}
@defn
The <b>null space</b> of a matrix $A$, denoted by ${\mathcal{N}}\!\left(A\right)$, is the set of all the vectors that are solutions to the homogeneous system $A\mathbf{x}=\mathbf{0}$.
That is, if

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;\dots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;\dots&amp;a_{2n}\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;\dots&amp;a_{3n}\\
\vdots&amp;\\
a_{m1}&amp;a_{m2}&amp;a_{m3}&amp;\dots&amp;a_{mn}\\
\end{bmatrix}
\end{align*}

then ${\mathcal{N}}\!\left(A\right)$ is the solution set of

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\dots+a_{1n}x_{n}&amp;\displaystyle=0 \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\dots+a_{2n}x_{n}&amp;\displaystyle=0 \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\dots+a_{3n}x_{n}&amp;\displaystyle=0 \\
\displaystyle\vdots&amp; \\
\displaystyle a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+\dots+a_{mn}x_{n}&amp;\displaystyle=0
\end{align*}
@end

@eg
Suppose

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix}
\end{align*}

Then

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}3\\
0\\
-5\\
-6\\
0\\
0\\
1\end{bmatrix}\qquad\qquad\mathbf{y}=\begin{bmatrix}-4\\
1\\
-3\\
-2\\
1\\
1\\
1\end{bmatrix}
\end{align*}

are in ${\mathcal{N}}\!\left(A\right)$ as $A\mathbf{x}=\mathbf{0}$, $A\mathbf{y}=\mathbf{0}$.
However, the vector

\begin{align*}
\displaystyle \mathbf{z}=\begin{bmatrix}1\\
0\\
0\\
0\\
0\\
0\\
2\end{bmatrix}
\end{align*}

is not in ${\mathcal{N}}\!\left(A\right)$ as

\begin{align*}
\displaystyle A\mathbf{z}=\begin{bmatrix}-17\\
16\\
-16\\
73\end{bmatrix}\neq\mathbf{0}.
\end{align*}
@end

@eg
@label{CNS1}
Let us compute the null space of

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;-1&amp;7&amp;-3&amp;-8\\
1&amp;0&amp;2&amp;4&amp;9\\
2&amp;2&amp;-2&amp;-1&amp;8\end{bmatrix}
\end{align*}

which we write as ${\mathcal{N}}\!\left(A\right)$. Translating Definition  @ref{NSM},
we simply desire to solve the homogeneous system $A\mathbf{x}=\mathbf{0}$. So we row-reduce the augmented matrix to obtain

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}\boxed{1}&amp;0&amp;2&amp;0&amp;1&amp;0\\
0&amp;\boxed{1}&amp;-3&amp;0&amp;4&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;0\end{array}\right]
\end{align*}

The variables (of the homogeneous system) $x_{3}$ and $x_{5}$ are free (since columns 1, 2 and 4 are pivot columns), so we arrange the equations represented by the matrix in reduced row-echelon form to

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=-2x_{3}-x_{5} \\
\displaystyle x_{2}&amp;\displaystyle=3x_{3}-4x_{5} \\
\displaystyle x_{4}&amp;\displaystyle=-2x_{5}
\end{align*}

So we can write the infinite solution set as sets using column vectors,

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}-2x_{3}-x_{5}\\
3x_{3}-4x_{5}\\
x_{3}\\
-2x_{5}\\
x_{5}\end{bmatrix}\,\right|\,x_{3},\,x_{5}\text{ real numbers}\right\}.
\end{align*}
@end

@eg
@label{CNS2}
Let us compute the null space of

\begin{align*}
\displaystyle C=\begin{bmatrix}-4&amp;6&amp;1\\
-1&amp;4&amp;1\\
5&amp;6&amp;7\\
4&amp;7&amp;1\end{bmatrix}
\end{align*}

which we write as ${\mathcal{N}}\!\left(C\right)$. Translating definition  @ref{NSM}, we simply desire to solve the homogeneous system $C\mathbf{x}=\mathbf{0}$. So we row-reduce the augmented matrix to obtain

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0\\
0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

There are no free variables in the homogeneous system represented by the row-reduced matrix, so there is only the trivial solution, the zero vector, $\mathbf{0}$. So we can write the (trivial) solution set as

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(C\right)=\{\mathbf{0}\}=\{\begin{bmatrix}0\\
0\\
0\end{bmatrix}\}.
\end{align*}
@end

@section{Augmented matrix vs coefficient matrix}

The augmented matrix for the homogenous of system of linear equation $A\mathbf{x}=\mathbf{0}$ is $[A|\mathbf{0}]$.
Any row operators on $[A|\mathbf{0}]$ will not change the last zero columns.
If

\begin{align*}
\displaystyle A\xrightarrow{\text{row operations}}B
\end{align*}

then

\begin{align*}
\displaystyle [A|\mathbf{0}]\xrightarrow{\text{same row operations}}[B|\mathbf{0}].
\end{align*}

Therefore, for the homogenous system of linear equations, we can replace the augmented matrix by the coefficient matrix. Just remember there is actually a zero column as the last column. For example:

\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&amp;\displaystyle=0 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&amp;\displaystyle=0 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&amp;\displaystyle=0
\end{align*}

We can start with the matrix coefficient

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;1&amp;6&amp;-7\\
-3&amp;4&amp;-5&amp;-6\\
1&amp;1&amp;4&amp;-5\end{bmatrix}
\end{align*}

The RREF is

\begin{align*}
\displaystyle \left[\begin{array}[]{cccc}\boxed{1}&amp;0&amp;3&amp;-2\\
0&amp;\boxed{1}&amp;1&amp;-3\\
0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

The corresponding augmented matrix is

\begin{align*}
\displaystyle \left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{array}\right]
\end{align*}

The system is consistent. It has $n-r=4-2=2$ free variables. The solution set is

\begin{align*}
\displaystyle S&amp;\displaystyle=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{1}=-3x_{3}+2x_{4},\,x_{2}=-x_{3}+3x_{4}\right\} \\
&amp;\displaystyle=\left\{\left.\begin{bmatrix}-3x_{3}+2x_{4}\\
-x_{3}+3x_{4}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{3},\,x_{4}\text{ real numbers}\right\}
\end{align*}

@section{Nonsingular Matrices}

In this section we specialize further and consider matrices with equal numbers of rows and columns,
which when considered as coefficient matrices lead to systems with equal numbers of equations and variables.

@defn
@title{Square matrix}
@label{SQM}
A matrix with $m$ rows and $n$ columns is <b>square</b> if $m=n$.
In this case, we say the matrix has <b>size</b> $n$.
To emphasize the situation when a matrix is not square, we will call it <b>rectangular</b>.
@end
@defn
@title{Nonsingular Matrix}
@label{NM}
Suppose $A$ is a square matrix.
Suppose further that the solution set to the homogeneous linear system of equations $A\mathbf{x}=\mathbf{0}$ is $\{\mathbf{0}\}$, in other words, the system has only the trivial solution.
Then we say that $A$ is a <b>nonsingular</b> matrix. Otherwise we say $A$ is a <b>singular</b> matrix.
@end
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;-1&amp;2\\
2&amp;1&amp;1\\
1&amp;1&amp;0\end{bmatrix}.
\end{align*}

By Example 2 Part 2, the system of linear equations $A\mathbf{x}=\mathbf{0}$ has nontrivial solutions.
Hence $A$ is singular.
</li>
<li class="ltx_item">
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}.
\end{align*}

By example 2 part 1, the system of linear equations $A\mathbf{x}=\mathbf{0}$ has only trivial solutions.
So it is nonsingular.
</li>

</ol>
@end

Recall

@defn
@title{Identity Matrix}
@label{IM}
The $m\times m$ <b>identity matrix</b>, $I_{m}$, is defined by

\begin{align*}
\displaystyle\left[I_{m}\right]_{ij}&amp;\displaystyle=\begin{cases}1&amp;i=j\\
0&amp;i\neq j\end{cases}\quad\quad 1\leq i,\,j\leq m
\end{align*}

i.e.

\begin{align*}
\displaystyle I_{m}=\begin{bmatrix}1&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;1&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;1&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;1\end{bmatrix}.
\end{align*}
@end
@eg
The $4\times 4$ identity matrix is

\begin{align*}
\displaystyle I_{4}=\begin{bmatrix}1&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0\\
0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;1\end{bmatrix}.
\end{align*}
@end

Notice that an identity matrix is square, and in reduced row-echelon form. Also, every column is a pivot column, and every possible pivot column appears once.

@thm
@title{Nonsingular Matrices Row Reduce to the Identity matrix}
@label{NMRRI}
Suppose that $A$ is a square matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Then $A$ is nonsingular if and only if $B$ is the identity matrix.
@end
@proof
@col
($\Leftarrow$) Suppose $B$ is the identity matrix. When the augmented matrix $\left[A|\mathbf{0}\right]$ is row-reduced, the result is $\left[B|\mathbf{0}\right]=\left[I_{n}|\mathbf{0}\right]$. The number of nonzero rows is equal to the number of variables in the linear system of equations $A\mathbf{x}=\mathbf{0}$, so $n=r$ and has $n-r=0$ free variables.
Thus, the homogeneous system $A\mathbf{x}=\mathbf{0}$ has just one solution, which must be the trivial solution.
This is exactly the definition of a nonsingular matrix.

($\Rightarrow$) If $A$ is nonsingular, then the homogeneous system $A\mathbf{x}=\mathbf{0}$ has a unique solution, and has no free variables in the description of the solution set. The homogeneous system is consistent,
by Lecture 4 Theorem 4, the homogenous system has $n-r$ free variables. Thus, $n-r=0$, and so $n=r$. So $B$ has $n$ pivot columns among its total of $n$ columns. This is enough to force $B$ to be the $n\times n$ identity matrix $I_{n}$ (why?).
∎
@end
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;-1&amp;2\\
2&amp;1&amp;1\\
1&amp;1&amp;0\end{bmatrix}
\end{align*}

is row equivalent to the reduced row echelon form

\begin{align*}
\displaystyle B=\begin{bmatrix}1&amp;0&amp;1\\
0&amp;1&amp;-1\\
0&amp;0&amp;0\end{bmatrix}.
\end{align*}

Since $B$ is not the $3\times 3$ identity matrix, the above theorem tells us that $A$ is a singular matrix.
@end
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}.
\end{align*}

It is row-equivalent to the reduced row echelon form

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;1\end{bmatrix}
\end{align*}

Since $B$ is the $3\times 3$ identity matrix, $A$ is a nonsingular matrix by the above theorem.
@end

@section{Null Space of a Nonsingular Matrix}
@label{NSNM}
@thm
@title{Nonsingular Matrices have Trivial Null Spaces}
@label{NMTNS}
Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if the null space of $A$ is the set containing only the zero vector, i.e., ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
@end

@proof
@col
The null space of a square matrix, $A$,
is equal to the set of solutions to the homogeneous system, $A\mathbf{x}=\mathbf{0}$. A matrix is nonsingular if and only if the set of solutions to the homogeneous system, $A\mathbf{x}=\mathbf{0}$, has only a trivial solution. These two observations may be chained together to construct the two proofs necessary for each half of this theorem.
∎
@end

@thm
@title{Nonsingular Matrices and Unique Solutions}
@label{NMUS}
Suppose that $A$ is a square matrix. $A$ is a nonsingular matrix if and only if the system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every choice of the constant vector $\mathbf{b}$.
@end

@proof
@col
($\Rightarrow$) The hypothesis for this half of the proof is that the system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every choice of the constant vector $\mathbf{b}$. We will make a very specific choice for $\mathbf{b}$: $\mathbf{b}=\mathbf{0}$. Then we know that the system $A\mathbf{x}=\mathbf{0}$ has a unique solution. But this is precisely the definition of what it means for $A$ to be nonsingular

($\Leftarrow$) We assume that $A$ is nonsingular of size $n\times n$, so we know there is a sequence of row operations that will convert $A$ into the identity matrix $I_{n}$ (Theorem  @ref{NMRRI}). Form the augmented matrix $A^{\prime}=\left[A|\mathbf{b}\right]$ and apply this same sequence of row operations to $A^{\prime}$. The result will be the matrix $B^{\prime}=\left[I_{n}|\mathbf{c}\right]$, which is in reduced row-echelon form with $r=n$. Then the augmented matrix $B^{\prime}$ represents the (extremely simple) system of equations $x_{i}=\left[\mathbf{c}\right]_{i}$, $1\leq i\leq n$. The vector $\mathbf{c}$ is clearly a solution, so the system is consistent. With a consistent system, we use Lecture 4 Theorem 4 to count free variables. We find that there are $n-r=n-n=0$ free variables, and so we therefore know that the solution is unique.
∎
@end

@thm
@title{Nonsingular Matrix Equivalences}
@label{NME1}
Suppose that $A$ is a square matrix. The following are equivalent.

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is nonsingular.
</li>
<li class="ltx_item">
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item">
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
</li>
<li class="ltx_item">
The linear system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>

</ol>
@end

@proof
@col
The statement that $A$ is nonsingular is equivalent to each of the subsequent statements by, in turn,
theorem  @ref{NMRRI},  @ref{NMTNS},  @ref{NMUS}.
So the statement of this theorem is just a convenient way to organize all these results.
∎
@end

@section{Particular Solutions, Homogeneous Solutions}
@label{PSHS}
The next theorem tells us that in order to find all of the solutions to a linear system of equations, it is sufficient to find just one solution, and then find all of the solutions to the corresponding homogeneous system. This explains part of our interest in the null space, the set of all solutions to a homogeneous system.

@thm
@title{Particular Solution Plus Homogeneous Solutions}
@label{PSPHS}
Suppose that $\mathbf{w}$ is one solution to the linear system of equations $A\mathbf{x}=\mathbf{b}$. Then $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$ if and only if $\mathbf{y}=\mathbf{w}+\mathbf{z}$ for some vector $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, i.e.

<ol class="ltx_enumerate">
<li class="ltx_item">
If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$, then $\mathbf{y}-\mathbf{w}\in{\mathcal{N}}\!\left(A\right)$
</li>
<li class="ltx_item">
If $\mathbf{z}{\mathcal{N}}\!\left(A\right)$, then $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$
</li>

</ol>

In other words, there is a one-to-one correspondence between

\begin{align*}
\displaystyle \text{solution set of $A\mathbf{x}=\mathbf{b}$}\longleftrightarrow{\mathcal{N}}\!\left(A\right),
\end{align*}

through

\begin{align*}
\displaystyle \mathbf{y}\rightarrow\mathbf{y}-\mathbf{w},
\end{align*}

\begin{align*}
\displaystyle \mathbf{w}+\mathbf{z}\leftarrow\mathbf{z}.
\end{align*}
@end
@proof
@col
Because $\mathbf{w}$ is one solution to the linear system of equations $A\mathbf{x}=\mathbf{b}$, $A\mathbf{w}=\mathbf{b}$.

<ol class="ltx_enumerate">
<li class="ltx_item">
If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$, then $A\mathbf{y}=\mathbf{b}$.
Hence $A(\mathbf{y}-\mathbf{w})=A\mathbf{y}-A\mathbf{w}=\mathbf{b}-\mathbf{b}=\mathbf{0}$.
So $\mathbf{y}-\mathbf{y}\in{\mathcal{N}}\!\left(A\right)$.
</li>
<li class="ltx_item">
Suppose $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, $A\mathbf{z}=\mathbf{0}$.
So $A(\mathbf{w}+\mathbf{z})=A\mathbf{w}+A\mathbf{z}=\mathbf{b}+\mathbf{0}=\mathbf{b}$. Hence $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$.

</li>

</ol>

∎
@end
@eg
\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&amp;\displaystyle=8 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&amp;\displaystyle=-12 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&amp;\displaystyle=4
\end{align*}

is a consistent system of equations with a nontrivial null space. Let $A$ denote the coefficient matrix of this system. The write-up for this system begins with three solutions,

\begin{align*}
\displaystyle\mathbf{y}_{1}=\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}&amp;\displaystyle\mathbf{y}_{2}=\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{y}_{3}=\begin{bmatrix}7\\
8\\
1\\
3\end{bmatrix}
\end{align*}

Let $\mathbf{w}=\mathbf{y}_{1}$. Then from the theorem,

\begin{align*}
\displaystyle \mathbf{y}_{2}-\mathbf{w}=\begin{bmatrix}4\\
-1\\
-2\\
-1\end{bmatrix},\,\,\mathbf{y}_{3}-\mathbf{w}=\begin{bmatrix}7\\
7\\
-1\\
2\end{bmatrix}
\end{align*}

are in ${\mathcal{N}}\!\left(A\right)$ (check!).

We can show that

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left\{\left.x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}
\end{align*}

By the theorem, the solution set is

\begin{align*}
\displaystyle \mathbf{w}+{\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}
\end{align*}
@end

@chapter{Vector space and subspace}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section VO (print version p57 - p63)Subsection VS, EVS (print version p197-203)
Strang, Section 2.1

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection VO (p.28-31) All questions.C10-C15, T05-T07, T13, T17, T18, T30-T32 Section VS (p.75-77) Replace $\mathbb{C}$ (the set of complex numbers) by $\mathbb{R}$ (the set of real numbers) M11, M12, M13, M14, M15, M20.

@section{Vectors}

<b>Notation</b>: ${\mathbb{R}}^{\hbox{}}$ is the set of real numbers. If $X$ is a set, $x\in X$ means $x$ is an element of the set $X$.

@defn
@title{Vector Space of Column Vectors}
@label{VSCV}
The vector space ${\mathbb{R}}^{m}$ is the set of all column vectors of size $m$ with entries from the set of real numbers, ${\mathbb{R}}^{\hbox{}}$. ${\mathbb{R}}^{m}$ is also called the <b>Euclidean $m$-space</b>.
@end
@defn
@title{Column Vector Equality}
@label{CVE}
Suppose that $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$. Then $\mathbf{u}$ and $\mathbf{v}$ are <b>equal</b>, written $\mathbf{u}=\mathbf{v}$ if

\begin{align*}
\displaystyle\left[\mathbf{u}\right]_{i}&amp;\displaystyle=\left[\mathbf{v}\right]_{i}&amp;\displaystyle 1\leq i\leq m
\end{align*}

That is,

\begin{align*}
\displaystyle \begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}=\begin{bmatrix}v_{1}\\
v_{2}\\
\vdots\\
v_{m}\end{bmatrix}
\end{align*}

if

\begin{align*}
\displaystyle u_{i}&amp;\displaystyle=v_{i}&amp;\displaystyle 1\leq i\leq m.
\end{align*}
@end
@eg
The system of linear equations

\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}

can be rewritten as

\begin{align*}
\displaystyle \begin{bmatrix}-7x_{1}-6x_{2}-12x_{3}\\
5x_{1}+5x_{2}+7x_{3}\\
x_{1}+4x_{3}\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*}
@end
@defn
@title{Column Vector Addition}
@label{CVA}
Suppose that $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$. The <b>sum</b> of $\mathbf{u}$ and $\mathbf{v}$ is the vector $\mathbf{u}+\mathbf{v}$ defined by

\begin{align*}
\displaystyle\left[\mathbf{u}+\mathbf{v}\right]_{i}&amp;\displaystyle=\left[\mathbf{u}\right]_{i}+\left[\mathbf{v}\right]_{i}&amp;\displaystyle 1\leq i\leq m.
\end{align*}

That is

\begin{align*}
\displaystyle \begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}+\begin{bmatrix}v_{1}\\
v_{2}\\
\vdots\\
v_{m}\end{bmatrix}=\begin{bmatrix}u_{1}+v_{1}\\
u_{2}+v_{2}\\
\vdots\\
u_{m}+v_{m}\end{bmatrix}.
\end{align*}
@end
@eg
Addition of two vectors in ${\mathbb{R}}^{4}$

If

\begin{align*}
\displaystyle\mathbf{u}=\begin{bmatrix}2\\
-3\\
4\\
2\end{bmatrix}&amp;\displaystyle\mathbf{v}=\begin{bmatrix}-1\\
5\\
2\\
-7\end{bmatrix}
\end{align*}

then

\begin{align*}
\displaystyle \mathbf{u}+\mathbf{v}=\begin{bmatrix}2\\
-3\\
4\\
2\end{bmatrix}+\begin{bmatrix}-1\\
5\\
2\\
-7\end{bmatrix}=\begin{bmatrix}2+(-1)\\
-3+5\\
4+2\\
2+(-7)\end{bmatrix}=\begin{bmatrix}1\\
2\\
6\\
-5\end{bmatrix}
\end{align*}
@end
@defn
@title{Column Vector Scalar Multiplication}
@label{CVSM}
Suppose $\mathbf{u}\in{\mathbb{R}}^{m}$ and $\alpha\in{\mathbb{R}}^{\hbox{}}$, then the <b>scalar multiple</b> of $\mathbf{u}$ by $\alpha$ is the vector $\alpha\mathbf{u}$ defined by

\begin{align*}
\displaystyle\left[\alpha\mathbf{u}\right]_{i}&amp;\displaystyle=\alpha\left[\mathbf{u}\right]_{i}&amp;\displaystyle 1\leq i\leq m.
\end{align*}

That is

\begin{align*}
\displaystyle \alpha\begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}=\begin{bmatrix}\alpha u_{1}\\
\alpha u_{2}\\
\vdots\\
\alpha u_{m}\end{bmatrix}.
\end{align*}
@end
@eg
If

\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}3\\
1\\
-2\\
4\\
-1\end{bmatrix}
\end{align*}

and $\alpha=6$, then

\begin{align*}
\displaystyle \alpha\mathbf{u}=6\begin{bmatrix}3\\
1\\
-2\\
4\\
-1\end{bmatrix}=\begin{bmatrix}6(3)\\
6(1)\\
6(-2)\\
6(4)\\
6(-1)\end{bmatrix}=\begin{bmatrix}18\\
6\\
-12\\
24\\
-6\end{bmatrix}.
\end{align*}
@end
@eg
The system of linear equations

\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}

can be written as

\begin{align*}
\displaystyle x_{1}\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+x_{2}\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*}
@end

@section{Vector Space Properties}
@label{VSP}
<b>Warning</b>: Read the statements of Theorem 5 and skip the rest of this section <b>unless you are/going to be </b> a math major. The material skipped will not appear in the tests and the final exam.

With definitions of vector addition and scalar multiplication we can state, and prove, several properties of each operation, and some properties that involve their interplay. We now collect ten of them here for later reference.

@thm
@label{VSPCV}
Vector Space Properties of Column Vectors

Suppose that ${\mathbb{R}}^{m}$ is the set of column vectors of size $m$ with addition and scalar multiplication as defined in Definition  @ref{CVA} and Definition  @ref{CVSM}.
Then

<ol class="ltx_enumerate">
<li class="ltx_item">
<b>ACC</b> Additive Closure, Column VectorsIf $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\mathbf{v}\in{\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
<b>SCC</b>
Scalar Closure, Column VectorsIf $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then $\alpha\mathbf{u}\in{\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
<b>CC</b>
Commutativity, Column VectorsIf $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$.
</li>
<li class="ltx_item">
<b>AAC</b>
Additive Associativity, Column VectorsIf $\mathbf{u},\,\mathbf{v},\,\mathbf{w}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\left(\mathbf{v}+\mathbf{w}\right)=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}$.
</li>
<li class="ltx_item">
<b>ZC</b>
Zero Vector, Column VectorsThere is a vector, $\mathbf{0}$, called the <b>zero vector</b>, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}\in{\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
<b>AIC</b>
Additive Inverses, Column VectorsIf $\mathbf{u}\in{\mathbb{R}}^{m}$, then there exists a vector $\mathbf{-u}\in{\mathbb{R}}^{m}$ so that $\mathbf{u}+(\mathbf{-u})=\mathbf{0}$.
</li>
<li class="ltx_item">
<b>SMAC</b>
Scalar Multiplication Associativity, Column VectorsIf $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then $\alpha(\beta\mathbf{u})=(\alpha\beta)\mathbf{u}$.
</li>
<li class="ltx_item">
<b>DVAC</b>
Distributivity across Vector Addition, Column VectorsIf $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\alpha(\mathbf{u}+\mathbf{v})=\alpha\mathbf{u}+\alpha\mathbf{v}$.
</li>
<li class="ltx_item">
<b>DSAC</b>
Distributivity across Scalar Addition, Column VectorsIf $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then
$(\alpha+\beta)\mathbf{u}=\alpha\mathbf{u}+\beta\mathbf{u}$.
</li>
<li class="ltx_item">
<b>OC</b>
One, Column VectorsIf $\mathbf{u}\in{\mathbb{R}}^{m}$, then $1\mathbf{u}=\mathbf{u}$.
</li>

</ol>
@end
@proof
@col
While some of these properties seem very obvious, they all require proof. However, the proofs are not very interesting, and border on tedious.
We will prove one version of distributivity very carefully, and you can test your proof-building skills on some of the others. We need to establish an equality, so we will do so by beginning with one side of the equality, apply various definitions and theorems (listed to the right of each step) to massage the expression from the left into the expression on the right. Here we go with a proof of Property  @ref{DSAC} (DSAC)

For $1\leq i\leq m$,

\begin{align*}
\displaystyle\left[(\alpha+\beta)\mathbf{u}\right]_{i}&amp;\displaystyle=(\alpha+\beta)\left[\mathbf{u}\right]_{i}&amp;efinition \\
&amp;\displaystyle=\alpha\left[\mathbf{u}\right]_{i}+\beta\left[\mathbf{u}\right]_{i} \\
&amp;\displaystyle=\left[\alpha\mathbf{u}\right]_{i}+\left[\beta\mathbf{u}\right]_{i}&amp;efinition \\
&amp;\displaystyle=\left[\alpha\mathbf{u}+\beta\mathbf{u}\right]_{i}&amp;efinition
\end{align*}

Since the individual components of the vectors $(\alpha+\beta)\mathbf{u}$ and $\alpha\mathbf{u}+\beta\mathbf{u}$ are equal for all $i$, $1\leq i\leq m$, Definition  @ref{CVE} tells us the vectors are equal.
∎
@end

Many of the conclusions of our theorems can be characterized as <b>identities</b>, especially when we are establishing basic properties of operations such as those in this section. Most of the properties listed in Theorem  @ref{VSPCV} are examples. So some advice about the style we use for proving identities is appropriate right now.

Be careful with the notion of the vector $\mathbf{-u}$. This is a vector that we add to $\mathbf{u}$ so that the result is the particular vector $\mathbf{0}$. This is basically a property of vector addition. It happens that we can compute $\mathbf{-u}$ using the other operation, scalar multiplication. We can prove this directly by writing that

\begin{align*}
\displaystyle \left[\mathbf{-u}\right]_{i}=-\left[\mathbf{u}\right]_{i}=(-1)\left[\mathbf{u}\right]_{i}=\left[(-1)\mathbf{u}\right]_{i}
\end{align*}

We will see later how to derive this property as a <b>consequence</b> of several of the ten properties listed in Theorem  @ref{VSPCV}.

Similarly, we will often write something you would immediately recognize as <b>vector subtraction</b>. This could be placed on a firm theoretical foundation – as you can do yourself with exercise T30.

A final note. Theorem  @ref{VSPCV} Property  @ref{AAC} (AAC) implies that we do not have to be careful about how we <b>parenthesize</b> the addition of vectors. In other words, there is nothing to be gained by writing
$\left(\mathbf{u}+\mathbf{v}\right)+\left(\mathbf{w}+\left(\mathbf{x}+\mathbf{y}\right)\right)$
rather than
$\mathbf{u}+\mathbf{v}+\mathbf{w}+\mathbf{x}+\mathbf{y}$, since we get the same result no matter which order we choose to perform the four additions. So we will not be careful about using parentheses this way.

@section{Vector space}

<b>For math major only. Non-math major can skip the rest of this section. The material will not appear in the midterms or final</b> In this section we will give an abstract definition of vector space. <b>Why do we need the abstract defintion?</b>
A lot of different algebraic objects (e.g. polynomials, matrices, sequences, functions) share similar properties
with the set of column vectors. We can use
the common properties to derive similar results.
we therefore don’t need to reproof and restate the results. <b>One stone, kill many birds</b>.

@defn
@label{VS}
Suppose that $V$ is a set upon which we have defined two operations: (1) <b>vector addition</b>, which combines two elements of $V$ and is denoted by $+$, and (2) <b>scalar multiplication</b>, which combines a real number with an element of $V$ and is denoted by juxtaposition. Then $V$, along with the two operations, is a <b>vector space</b> over ${\mathbb{R}}^{\hbox{}}$ if the following ten properties hold.

<ol class="ltx_enumerate">
<li class="ltx_item">
<b>AC</b> Additive Closure If $\mathbf{u},\,\mathbf{v}\in V$, then $\mathbf{u}+\mathbf{v}\in V$.
</li>
<li class="ltx_item">
<b>SC</b> Scalar Closure  If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then $\alpha\mathbf{u}\in V$.
</li>
<li class="ltx_item">
<b>C</b> Commutativity  If $\mathbf{u},\,\mathbf{v}\in V$, then $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$.
</li>
<li class="ltx_item">
<b>AA</b> Additive Associativity  If $\mathbf{u},\,\mathbf{v},\,\mathbf{w}\in V$, then $\mathbf{u}+\left(\mathbf{v}+\mathbf{w}\right)=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}$.

</li>
<li class="ltx_item">
<b>Z</b> Zero Vector There is a vector, $\mathbf{0}$, called the <b>zero vector</b>, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}\in V$.
</li>
<li class="ltx_item">
<b>AI</b> Additive Inverses If $\mathbf{u}\in V$, then there exists a vector $\mathbf{-u}\in V$ so that $\mathbf{u}+(\mathbf{-u})=\mathbf{0}$.
</li>
<li class="ltx_item">
<b>SMA</b> Scalar Multiplication Associativity If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then $\alpha(\beta\mathbf{u})=(\alpha\beta)\mathbf{u}$.
</li>
<li class="ltx_item">
<b>DVA</b> Distributivity across Vector Addition If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u},\,\mathbf{v}\in V$, then $\alpha(\mathbf{u}+\mathbf{v})=\alpha\mathbf{u}+\alpha\mathbf{v}$.
</li>
<li class="ltx_item">
<b>DSA</b> Distributivity across Scalar Addition  If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then
$(\alpha+\beta)\mathbf{u}=\alpha\mathbf{u}+\beta\mathbf{u}$.
</li>
<li class="ltx_item">
<b>O</b> One  If $\mathbf{u}\in V$, then $1\mathbf{u}=\mathbf{u}$.
</li>

</ol>

The objects in $V$ are called <b>vectors</b>, no matter what else they might really be, simply by virtue of being elements of a vector space.
@end

<b>Example 1</b> <b>column vector space</b> The set of column vector ${\mathbb{R}}^{n}$ is a vector space.

<b>Example 2</b> <b>row vector space</b> The set of row vector ($1\times n$ matrices), is a vector space with the following operations:

<ul class="ltx_itemize">
<li class="ltx_item">
Vector addition: $[a_{1}\,a_{2}\,\ldots\,a_{n}]+[a_{1}\,b_{2}\,\ldots\,b_{n}]=[a_{1}+b_{1}\,a_{2}+b_{2}\,\ldots\,a_{n}+b_{n}]$
</li>
<li class="ltx_item">
Scalar multiplication $\alpha[a_{1}\,a_{2}\,\ldots,a_{n}]=[\alpha a_{1}\,\alpha a_{2},\ldots,\alpha a_{n}]$
</li>

</ul>

<b>Example 3</b> <b>Matrices</b> The set of $m\times n$ matrices, denoted by $M_{mn}$, is a vector space with the following operations:

<ul class="ltx_itemize">
<li class="ltx_item">
Vector addition:

\begin{align*}
\displaystyle \begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;\cdots&amp;a_{mn}\end{bmatrix}+\begin{bmatrix}b_{11}&amp;b_{12}&amp;\cdots&amp;b_{1n}\\
b_{21}&amp;b_{22}&amp;\cdots&amp;b_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
b_{m1}&amp;b_{m2}&amp;\cdots&amp;b_{mn}\end{bmatrix}=\begin{bmatrix}a_{11}+b_{11}&amp;a_{12}+b_{12}&amp;\cdots&amp;a_{1n}+b_{1n}\\
a_{21}+b_{21}&amp;a_{22}+b_{22}&amp;\cdots&amp;a_{2n}+b_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}+b_{m1}&amp;a_{m2}+b_{m2}&amp;\cdots&amp;a_{mn}+b_{mn}\end{bmatrix}
\end{align*}

</li>
<li class="ltx_item">
Scalar multiplication

\begin{align*}
\displaystyle \alpha\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;\cdots&amp;a_{mn}\end{bmatrix}=\begin{bmatrix}\alpha a_{11}&amp;\alpha a_{12}&amp;\cdots&amp;\alpha a_{1n}\\
\alpha a_{21}&amp;\alpha a_{22}&amp;\cdots&amp;\alpha a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
\alpha a_{m1}&amp;\alpha a_{m2}&amp;\cdots&amp;\alpha a_{mn}\end{bmatrix}
\end{align*}

</li>

</ul>

Property Z: The zero vector is

\begin{align*}
\displaystyle \begin{bmatrix}0&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;0\end{bmatrix}
\end{align*}

Property AI:The inverse of

\begin{align*}
\displaystyle \begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;\cdots&amp;a_{mn}\end{bmatrix}
\end{align*}

is

\begin{align*}
\displaystyle \begin{bmatrix}-a_{11}&amp;-a_{12}&amp;\cdots&amp;-a_{1n}\\
-a_{21}&amp;-a_{22}&amp;\cdots&amp;-a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a_{m1}&amp;-a_{m2}&amp;\cdots&amp;-a_{mn}\end{bmatrix}
\end{align*}

You can try proving all other properties.

<b>Example 4</b> <b>The vector space of polynomials, $P_{n}$</b>The set of all polynomials of degree $n$ or less in the variable $x$ with coefficients from ${\mathbb{R}}^{\hbox{}}$,
denoted by $P_{n}$ is a vector space.

<ul class="ltx_itemize">
<li class="ltx_item">
Vector Addition:

\begin{align*}
\displaystyle (a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n})+(b_{0}+b_{1}x+b_{2}x^{2}+\cdots+b_{n}x^{n})
\end{align*}

\begin{align*}
\displaystyle =(a_{0}+b_{0})+(a_{1}+b_{1})x+(a_{2}+b_{2})x^{2}+\cdots+(a_{n}+b_{n})x^{n}
\end{align*}

</li>
<li class="ltx_item">
Scalar Multiplication:

\begin{align*}
\displaystyle \alpha(a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n})=(\alpha a_{0})+(\alpha a_{1})x+(\alpha a_{2})x^{2}+\cdots+(\alpha a_{n})x^{n}
\end{align*}

</li>

</ul>

This set, with these operations, will fulfill the ten properties, though we will not work all the details here. However, we will make a few comments and prove one of the properties. First, the zero vector (property Z)
is what you might expect, and you can check that it has the required property.

\begin{align*}
\displaystyle \mathbf{0}=0+0x+0x^{2}+\cdots+0x^{n}
\end{align*}

The additive inverse (Property AI) is also no surprise, though consider how we have chosen to write it.

\begin{align*}
\displaystyle -\left(a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}\right)=(-a_{0})+(-a_{1})x+(-a_{2})x^{2}+\cdots+(-a_{n})x^{n}
\end{align*}

Now let us prove the associativity of vector addition (Property AA). This is a bit tedious, though necessary. Throughout, the plus sign ($+$) does triple-duty. You might ask yourself what each plus sign represents as you work through this proof.

\begin{align*}
\displaystyle\mathbf{u}+&amp;\displaystyle(\mathbf{v}+\mathbf{w}) \\
&amp;\displaystyle=(a_{0}+a_{1}x+\cdots+a_{n}x^{n})+\left((b_{0}+b_{1}x+\cdots+b_{n}x^{n})+(c_{0}+c_{1}x+\cdots+c_{n}x^{n})\right) \\
&amp;\displaystyle=(a_{0}+a_{1}x+\cdots+a_{n}x^{n})+((b_{0}+c_{0})+(b_{1}+c_{1})x+\cdots+(b_{n}+c_{n})x^{n}) \\
&amp;\displaystyle=(a_{0}+(b_{0}+c_{0}))+(a_{1}+(b_{1}+c_{1}))x+\cdots+(a_{n}+(b_{n}+c_{n}))x^{n} \\
&amp;\displaystyle=((a_{0}+b_{0})+c_{0})+((a_{1}+b_{1})+c_{1})x+\cdots+((a_{n}+b_{n})+c_{n})x^{n} \\
&amp;\displaystyle=((a_{0}+b_{0})+(a_{1}+b_{1})x+\cdots+(a_{n}+b_{n})x^{n})+(c_{0}+c_{1}x+\cdots+c_{n}x^{n}) \\
&amp;\displaystyle=\left((a_{0}+a_{1}x+\cdots+a_{n}x^{n})+(b_{0}+b_{1}x+\cdots+b_{n}x^{n})\right)+(c_{0}+c_{1}x+\cdots+c_{n}x^{n}) \\
&amp;\displaystyle=(\mathbf{u}+\mathbf{v})+\mathbf{w}
\end{align*}

You might try proving all the other properties.

<b>Example 5</b> <b>The vector space of functions</b> Let $F$ be the set of functions for ${\mathbb{R}}^{\hbox{}}$ to ${\mathbb{R}}^{\hbox{}}$
Equality: $f=g$ if and only if $f(x)=g(x)$ for all $x\in{\mathbb{R}}^{\hbox{}}$.

<ul class="ltx_itemize">
<li class="ltx_item">
Vector Addition: $f+g$ is the function with outputs defined by $(f+g)(x)=f(x)+g(x)$.
</li>
<li class="ltx_item">
Scalar Multiplication: $\alpha f$ is the function with outputs defined by $(\alpha f)(x)=\alpha f(x)$.
</li>

</ul>

The zero vector is a function $z$ whose definition is $z(x)=0$ for every input $x\in{\mathbb{R}}^{\hbox{}}$.Try proving all the other properties.

<b>Example 6</b> <b>The crazy vector space</b>
Let $C=\left\{\left.(x_{1},\,x_{2})\,\right|\,x_{1},\,x_{2}\in{\mathbb{R}}^{\hbox{}}\right\}$.

<ol class="ltx_enumerate">
<li class="ltx_item">
Vector Addition: $(x_{1},\,x_{2})+(y_{1},\,y_{2})=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1)$.
</li>
<li class="ltx_item">
Scalar Multiplication: $\alpha(x_{1},\,x_{2})=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)$.
</li>

</ol>

I am free to define my set and my operations any way I please. They may not look natural, or even useful, but we will now verify that they provide us with another example of a vector space. We will check all it satisfies all the definition of vector spaces.

<ul class="ltx_itemize">
<li class="ltx_item">
Property AC, SCThe result of each operation is a pair of complex numbers, so these two closure properties are fulfilled.
</li>
<li class="ltx_item">
Property C

\begin{align*}
\displaystyle\mathbf{u}+\mathbf{v}&amp;\displaystyle=(x_{1},\,x_{2})+(y_{1},\,y_{2})=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1) \\
&amp;\displaystyle=(y_{1}+x_{1}+1,\,y_{2}+x_{2}+1)=(y_{1},\,y_{2})+(x_{1},\,x_{2}) \\
&amp;\displaystyle=\mathbf{v}+\mathbf{u}
\end{align*}

</li>
<li class="ltx_item">
Property AA

\begin{align*}
\displaystyle\mathbf{u}+(\mathbf{v}+\mathbf{w})&amp;\displaystyle=(x_{1},\,x_{2})+\left((y_{1},\,y_{2})+(z_{1},\,z_{2})\right) \\
&amp;\displaystyle=(x_{1},\,x_{2})+(y_{1}+z_{1}+1,\,y_{2}+z_{2}+1) \\
&amp;\displaystyle=(x_{1}+(y_{1}+z_{1}+1)+1,\,x_{2}+(y_{2}+z_{2}+1)+1) \\
&amp;\displaystyle=(x_{1}+y_{1}+z_{1}+2,\,x_{2}+y_{2}+z_{2}+2) \\
&amp;\displaystyle=((x_{1}+y_{1}+1)+z_{1}+1,\,(x_{2}+y_{2}+1)+z_{2}+1) \\
&amp;\displaystyle=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1)+(z_{1},\,z_{2}) \\
&amp;\displaystyle=\left((x_{1},\,x_{2})+(y_{1},\,y_{2})\right)+(z_{1},\,z_{2}) \\
&amp;\displaystyle=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}
\end{align*}

</li>
<li class="ltx_item">
Property Z

The zero vector is $\mathbf{0}=(-1,\,-1)$ (<b>not</b> $(0,0)$)

\begin{align*}
\displaystyle \mathbf{u}+\mathbf{0}=(x_{1},\,x_{2})+(-1,\,-1)=(x_{1}+(-1)+1,\,x_{2}+(-1)+1)=(x_{1},\,x_{2})=\mathbf{u}
\end{align*}

</li>
<li class="ltx_item">
Property AI For each vector, $\mathbf{u}$, we must locate an additive inverse, $\mathbf{-u}$. Here it is, $-(x_{1},\,x_{2})=(-x_{1}-2,\,-x_{2}-2)$. As odd as it may look, I hope you are withholding judgment. Check:

\begin{align*}
\displaystyle\mathbf{u}+(\mathbf{-u})&amp;\displaystyle=(x_{1},\,x_{2})+(-x_{1}-2,\,-x_{2}-2) \\
&amp;\displaystyle=(x_{1}+(-x_{1}-2)+1,\,-x_{2}+(x_{2}-2)+1)=(-1,\,-1)=\mathbf{0}
\end{align*}

</li>
<li class="ltx_item">
Property SMA

\begin{align*}
\displaystyle\alpha(\beta\mathbf{u})&amp;\displaystyle=\alpha(\beta(x_{1},\,x_{2})) \\
&amp;\displaystyle=\alpha(\beta x_{1}+\beta-1,\,\beta x_{2}+\beta-1) \\
&amp;\displaystyle=(\alpha(\beta x_{1}+\beta-1)+\alpha-1,\,\alpha(\beta x_{2}+\beta-1)+\alpha-1) \\
&amp;\displaystyle=((\alpha\beta x_{1}+\alpha\beta-\alpha)+\alpha-1,\,(\alpha\beta x_{2}+\alpha\beta-\alpha)+\alpha-1) \\
&amp;\displaystyle=(\alpha\beta x_{1}+\alpha\beta-1,\,\alpha\beta x_{2}+\alpha\beta-1) \\
&amp;\displaystyle=(\alpha\beta)(x_{1},\,x_{2}) \\
&amp;\displaystyle=(\alpha\beta)\mathbf{u}
\end{align*}

</li>
<li class="ltx_item">
Property DVA If you have hung on so far, here is where it gets even wilder. In the next two properties we mix and mash the two operations.

\begin{align*}
\displaystyle\alpha(\mathbf{u}&amp;\displaystyle+\mathbf{v}) \\
&amp;\displaystyle=\alpha\left((x_{1},\,x_{2})+(y_{1},\,y_{2})\right) \\
&amp;\displaystyle=\alpha(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1) \\
&amp;\displaystyle=(\alpha(x_{1}+y_{1}+1)+\alpha-1,\,\alpha(x_{2}+y_{2}+1)+\alpha-1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha y_{1}+\alpha+\alpha-1,\,\alpha x_{2}+\alpha
y_{2}+\alpha+\alpha-1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1+\alpha y_{1}+\alpha-1+1,\,\alpha x_{2}+\alpha-1+\alpha y_{2}+\alpha-1+1) \\
&amp;\displaystyle=((\alpha x_{1}+\alpha-1)+(\alpha y_{1}+\alpha-1)+1,\,(\alpha x_{2}+\alpha-1)+(\alpha y_{2}+\alpha-1)+1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)+(\alpha y_{1}+\alpha-1,\,\alpha y_{2}+\alpha-1) \\
&amp;\displaystyle=\alpha(x_{1},\,x_{2})+\alpha(y_{1},\,y_{2}) \\
&amp;\displaystyle=\alpha\mathbf{u}+\alpha\mathbf{v}
\end{align*}

</li>
<li class="ltx_item">
Property DSA

\begin{align*}
\displaystyle(\alpha&amp;\displaystyle+\beta)\mathbf{u} \\
&amp;\displaystyle=(\alpha+\beta)(x_{1},\,x_{2}) \\
&amp;\displaystyle=((\alpha+\beta)x_{1}+(\alpha+\beta)-1,\,(\alpha+\beta)x_{2}+(\alpha+\beta)-1) \\
&amp;\displaystyle=(\alpha x_{1}+\beta x_{1}+\alpha+\beta-1,\,\alpha x_{2}+\beta x_{2}+\alpha+\beta-1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1+\beta x_{1}+\beta-1+1,\,\alpha x_{2}+\alpha-1+\beta x_{2}+\beta-1+1) \\
&amp;\displaystyle=((\alpha x_{1}+\alpha-1)+(\beta x_{1}+\beta-1)+1,\,(\alpha x_{2}+\alpha-1)+(\beta x_{2}+\beta-1)+1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)+(\beta x_{1}+\beta-1,\,\beta x_{2}+\beta-1) \\
&amp;\displaystyle=\alpha(x_{1},\,x_{2})+\beta(x_{1},\,x_{2}) \\
&amp;\displaystyle=\alpha\mathbf{u}+\beta\mathbf{u}
\end{align*}

</li>
<li class="ltx_item">
Property OAfter all that, this one is easy, but no less pleasing.

\begin{align*}
\displaystyle 1\mathbf{u}=1(x_{1},\,x_{2})=(x_{1}+1-1,\,x_{2}+1-1)=(x_{1},\,x_{2})=\mathbf{u}
\end{align*}

</li>

</ul>

That is it, $C$ is a vector space, as crazy as that may seem.

Notice that in the case of the zero vector and additive inverses, we only had to propose possibilities and then verify that they were the correct choices. You might try to discover how you would arrive at these choices, though you should understand why the process of discovering them is not a necessary component of the proof itself.

@section{Basic properties of vector spaces}

<b>For math major only. Non-math major can skip the rest of this section. The material will not appear in the midterms or final</b>

@thm
@title{Cancellation Law for Vector Addition}
@label{cancell}
if $\mathbf{v}$, $\mathbf{u}$ and $\mathbf{w}$ are vectors in a vector space $V$ such that

\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{u}+\mathbf{w},
\end{align*}

then $\mathbf{v}=\mathbf{u}$
@end
@proof
@col
By Property AI, there exists a vector $-\mathbf{w}$ such that $\mathbf{w}+(-\mathbf{w})=\mathbf{0}$.
Thus

\begin{align*}
\displaystyle (\mathbf{v}+\mathbf{w})+(-\mathbf{w})=(\mathbf{u}+\mathbf{w})+(-\mathbf{w})
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}+(\mathbf{w}+(-\mathbf{w}))=\mathbf{u}+(\mathbf{w}+(-\mathbf{w}))\hskip 28.452756pt\text{Property AA}
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}+\mathbf{0}=\mathbf{u}+\mathbf{0}\hskip 28.452756pt\text{Property AI}
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}=\mathbf{u}\hskip 28.452756pt\text{Property Z}.
\end{align*}

∎
@end
@thm
@title{Uniqueness of the zero vector}
Let $V$ be a vector space.
The vector $\mathbf{0}$ described in Property Z is unique.
@end
@proof
@col
Suppose both $\mathbf{0}_{1}$ and $\mathbf{0}_{2}$ satisfy the property described in Property Z.
Let $\mathbf{w}$ be an element in $V$.

\begin{align*}
\displaystyle \mathbf{0}_{1}+\mathbf{w}=\mathbf{w}=\mathbf{0}_{2}+\mathbf{w}\hskip 28.452756pt\text{Property Z}
\end{align*}

\begin{align*}
\displaystyle \mathbf{0}_{1}=\mathbf{0}_{2}\hskip 28.452756pt\text{by the previous theorem}
\end{align*}

∎
@end
@thm
@title{Uniqueness of the additive inverse}
@label{invunique}
Let $V$ be a vector space and $\mathbf{v},\mathbf{u},\mathbf{w}$ are vectors of $V$. If
both $\mathbf{v}$ and $\mathbf{u}$ satisfies

\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{0},
\end{align*}

\begin{align*}
\displaystyle \mathbf{u}+\mathbf{w}=\mathbf{0},
\end{align*}

i.e., both $\mathbf{u}$ and $\mathbf{v}$ are additive inverse of $\mathbf{w}$ in Property AI, then

\begin{align*}
\displaystyle \mathbf{v}=\mathbf{u}.
\end{align*}

This shows that the additive inverse is unique.
@end
@proof
@col
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{0}=\mathbf{u}+\mathbf{w}.
\end{align*}

By Theorem  @ref{cancell},

\begin{align*}
\displaystyle \mathbf{v}=\mathbf{u}.
\end{align*}

∎
@end
@thm

Let $V$ be a vector space, $\alpha$ a real number, $\mathbf{v}$ a vector in $V$. We have the following statement.

<ol class="ltx_enumerate">
<li class="ltx_item">
$0\mathbf{v}=\mathbf{0}$ .
</li>
<li class="ltx_item">
$a\mathbf{0}=\mathbf{0}$.
</li>
<li class="ltx_item">
$(-\alpha)\mathbf{v}=-(\alpha\mathbf{v})=\alpha(-\mathbf{v})$.
</li>

</ol>
@end
@proof
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
\begin{align*}
\displaystyle 0\mathbf{v}+0\mathbf{v}=(0+0)\mathbf{v}\hskip 28.452756pt\text{Property DSA}
\end{align*}

\begin{align*}
\displaystyle 0\mathbf{v}+0\mathbf{v}=0\mathbf{v}=\mathbf{0}+0\mathbf{v}\hskip 28.452756pt\text{Property Z}
\end{align*}

\begin{align*}
\displaystyle 0\mathbf{v}=\mathbf{0}\hskip 28.452756pt\text{by Theorem \ref{cancell}, $\mathbf{w}=0\mathbf{v}$}
\end{align*}

</li>
<li class="ltx_item">
\begin{align*}
\displaystyle\alpha\mathbf{0}+a\mathbf{0}&amp;\displaystyle=\alpha(\mathbf{0}+\mathbf{0})\hskip 28.452756pt\text{Property DVA} \\
&amp;\displaystyle=\alpha\mathbf{0}\hskip 28.452756pt\text{Property Z} \\
&amp;\displaystyle=\mathbf{0}+\alpha\mathbf{0}\hskip 28.452756pt\text{Property Z}
\end{align*}

By Theorem  @ref{cancell}.

\begin{align*}
\displaystyle \alpha\mathbf{0}=\mathbf{0}
\end{align*}

</li>
<li class="ltx_item">
\begin{align*}
\displaystyle\alpha\mathbf{v}+(-\alpha)\mathbf{v}&amp;\displaystyle=(\alpha+(-\alpha))\mathbf{v}\hskip 28.452756pt\text{Property DSA}. \\
&amp;\displaystyle=0\mathbf{v} \\
&amp;\displaystyle=\mathbf{0}\hskip 28.452756pt\text{item 1}
\end{align*}

By Property AI and the uniqueness of the additive inverse (Theorem  @ref{invunique}),

\begin{align*}
\displaystyle (-\alpha)\mathbf{v}=-\alpha\mathbf{v}.
\end{align*}

Next

\begin{align*}
\displaystyle\alpha\mathbf{v}+\alpha(-\mathbf{v})&amp;\displaystyle=\alpha(\mathbf{v}+(-\mathbf{v}))\hskip 28.452756pt\text{PropertyDVA}. \\
&amp;\displaystyle=\alpha\mathbf{0}\hskip 28.452756pt\text{Property AI} \\
&amp;\displaystyle=\mathbf{0}\hskip 28.452756pt\text{item 2}
\end{align*}

By Property AI and the uniqueness of the additive inverse (Theorem  @ref{invunique}),

\begin{align*}
\displaystyle \alpha(-\mathbf{v})=-\alpha\mathbf{v}.
\end{align*}

</li>

</ol>

∎
@end

@section{Subspaces}

@defn
@label{subdef}
Let $V$ be vector space.
A subset $W$ of $V$ is said to be a <b>subspace</b> of $V$ if

<ol class="ltx_enumerate">
<li class="ltx_item">
$W$ is nonempty.
</li>
<li class="ltx_item">
For $\mathbf{v},\mathbf{w}\in W$, then $\mathbf{v}+\mathbf{w}\in W$.
</li>
<li class="ltx_item">
For $\alpha\in\mathbf{R}$, $\mathbf{v}\in W$, then $\alpha\mathbf{v}\in W$.
</li>

</ol>
@end

We will prove several theorem first before we give examples.

@proposition
@label{0}
Let $V$ be a vector spaceand $W$ a subspace of $V$.
Then $\mathbf{0}$
is in $W$.
@end
@proof
@col
By Definition  @ref{subdef}, Condition 1, $W$ is nonempty. Let $\mathbf{w}\in W$.By Definition  @ref{subdef}, Condition 3, with $\alpha=-1$, $(-1)\mathbf{w}=-\mathbf{w}\in W$. By Definition  @ref{subdef}, Condition 2, with $\mathbf{v}=-\mathbf{w}$, then $\mathbf{v}+\mathbf{w}\in W$.
But $\mathbf{v}+\mathbf{w}=(-\mathbf{w})+\mathbf{w}=\mathbf{0}$. So $\mathbf{0}\in W$.
∎
@end
@thm
@label{subdef2}
Let $V$ be a vector space
and $W$ a subset of $V$,
then $W$ is a subspace if and only if

<ol class="ltx_enumerate">
<li class="ltx_item">
$W$ is nonempty.
</li>
<li class="ltx_item">
For any $\alpha\in\mathbf{R}$, $\mathbf{v},\mathbf{w}\in W$, $\alpha\mathbf{v}+\mathbf{w}\in W$.
</li>

</ol>
@end
@proof
@col
($\Rightarrow$) By Definition  @ref{subdef}, Condition 1, $W$ is nonempty. Next, for $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W$.
By Definition  @ref{subdef}, Condition 3, $\alpha\mathbf{v}\in W$.
By Definition  @ref{subdef}, Condition 2, $\alpha\mathbf{v}+\mathbf{w}\in W$.

($\Leftarrow$) By Condition 1, Definition  @ref{subdef}, Condition 1 is true.Because $W$ is nonempty, let $\mathbf{x}\in W$. Let $\mathbf{v}=\mathbf{w}=\mathbf{x}$ and $\alpha=-1$.
Then by condition 2, $(-1)\mathbf{w}+\mathbf{w}=\mathbf{0}\in W$. Now we want to check Definition  @ref{subdef} Condition 2, suppose $\mathbf{v},\mathbf{w}\in W$.
In condition 2, let $\alpha=1$, then $\mathbf{v}+\mathbf{w}=\alpha\mathbf{v}+\mathbf{w}\in W$. Next we want to check Definition  @ref{subdef} Condition 3, suppose $\mathbf{v}\in W$, $\alpha\in{\mathbb{R}}^{\hbox{}}$.
Let $\mathbf{w}=\mathbf{0}$, then $\alpha\mathbf{v}=\alpha\mathbf{v}+\mathbf{w}\in W$.
∎
@end

@section{Examples}

To verify if $W$ is a subspace, we can either use Definition  @ref{subdef} or Theorem  @ref{subdef2}.
Usually we use the equivalent definition given in Theorem  @ref{subdef2} because it involves only one condition.

@eg
$V={\mathbb{R}}^{m}$, $W=\{\mathbf{0}\}$. $W$ consists of one element. It is called the <b>zero subspace</b> of $V$. Check that it is a subspace: $W$ is nonempty.
For any $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W$, $\mathbf{v}=\mathbf{w}=\mathbf{0}$,
$\alpha\mathbf{v}+\mathbf{w}=\mathbf{0}\in W$. Thus by Theorem  @ref{subdef2}, $W$ is a subspace.
@end
@eg
$V={\mathbb{R}}^{m}$, $W=V$. Then obviously $W$ is a subspace.
@end
@eg
$V={\mathbb{R}}^{m}$, $W=\{\mathbf{v}\in V\,|\,[\mathbf{v}]_{1}=0\}$. Check that $W$ is a subspace: First all all, $\mathbf{0}\in W$, so $W$ is nonempty.
For any $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W$. Then $[\mathbf{v}]_{1}=[\mathbf{w}]_{1}=0$.
So $[\alpha\mathbf{v}+\mathbf{w}]_{1}=\alpha[\mathbf{v}]_{1}+[\mathbf{w}]_{1}=0$.
Hence $\alpha\mathbf{v}+\mathbf{w}\in W$. Thus by Theorem  @ref{subdef2}, $W$ is a subspace.
@end
@eg
$V={\mathbb{R}}^{3}$, $W=\left\{\left.\begin{bmatrix}x\\
y\\
z\end{bmatrix}\in{\mathbb{R}}^{3}\,\right|\,x+2y+3z=0\right\}$.Check that $W$ is a subspace: Obviously $\mathbf{0}\in W$, so $W$ is nonempty. For any $\alpha\in{\mathbb{R}}^{\hbox{}}$,
$\mathbf{v}=\begin{bmatrix}v_{1}\\
v_{2}\\
v_{3}\end{bmatrix}$,
$\mathbf{w}=\begin{bmatrix}w_{1}\\
w_{2}\\
w_{3}\end{bmatrix}\in W$.
Then

\begin{align*}
\displaystyle v_{1}+2v_{2}+3v_{3}=0,
\end{align*}

\begin{align*}
\displaystyle w_{1}+2w_{2}+3w_{3}=0.
\end{align*}

So

\begin{align*}
\displaystyle 0=\alpha(v_{1}+2v_{2}+3v_{3})+(w_{1}+2w_{2}+3w_{3})=(\alpha v_{1}+w_{1})+2(\alpha v_{2}+w_{2})+3(\alpha v_{3}+w_{3}).
\end{align*}

Hence

\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}=\begin{bmatrix}\alpha v_{1}+w_{1}\\
\alpha v_{2}+w_{2}\\
\alpha v_{3}+w_{3}\end{bmatrix}\in W.
\end{align*}

Thus by Theorem  @ref{subdef2}, $W$ is a subspace.
@end
@thm

Let $A\in M_{mn}$, then $W={\mathcal{N}}\!\left(A\right)$ is a subspace of ${\mathbb{R}}^{n}$.
@end
@proof
@col
Because $\mathbf{0}\in{\mathcal{N}}\!\left(A\right)$, so $W$ is nonempty. For $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W$.
Then

\begin{align*}
\displaystyle A\mathbf{v}=\mathbf{0},\qquad A\mathbf{w}=\mathbf{0}.
\end{align*}

Then

\begin{align*}
\displaystyle A(\alpha\mathbf{v}+\mathbf{w})=\alpha A\mathbf{v}+A\mathbf{w}=\alpha\mathbf{0}+\mathbf{0}=\mathbf{0}.
\end{align*}

So

\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}\in{\mathcal{N}}\!\left(A\right).
\end{align*}

Thus by Theorem  @ref{subdef2}, $W={\mathcal{N}}\!\left(A\right)$ is a subspace.
∎
@end
@thm

<b>Skipped for now, until you learn the definition of span</b> Let $S=\{\mathbf{u}_{1},\ldots,\mathbf{u}_{k}\}\subseteq V={\mathbb{R}}^{m}$.
Then $\left&lt;S\right&gt;$ is a subspace of $V$.
@end
@proof
@col
Obviously $\left&lt;S\right&gt;$ is nonempty. For $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W=\left&lt;S\right&gt;$.
Then there exists $\alpha_{1},\ldots,\alpha_{k},\beta_{1},\ldots,\beta_{k}$ such that

\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{k}\mathbf{u}_{k},
\end{align*}

\begin{align*}
\displaystyle \mathbf{w}=\beta_{1}\mathbf{u}_{1}+\cdots+\beta_{k}\mathbf{u}_{k}.
\end{align*}

Then

\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}=(\alpha\alpha_{1}+\beta_{1})\mathbf{u}_{1}+\cdots+(\alpha\alpha_{k}+\beta_{k})\mathbf{u}_{k}
\end{align*}

is in $\left&lt;S\right&gt;$.
Thus by Theorem  @ref{subdef2}, $W$ is a subspace.
∎
@end
@corollary
<b>Skipped for now, until you learn the definition of column space</b> Let $A\in M_{mn}$, then $\mathcal{C}\!\left(A\right)$ is a subspace of ${\mathbb{R}}^{m}$.
@end
@proof
@col
$\mathcal{C}\!\left(A\right)=\left&lt;\{\mathbf{A}_{1},\ldots,\mathbf{A}_{n}\}\right&gt;$. So by the previous theorem, $\mathcal{C}\!\left(A\right)$ is a subspace of ${\mathbb{R}}^{m}$. <b>Alternate proof</b>: Suppose For $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W=\mathcal{C}\!\left(A\right)$.
Recall

\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\{A\mathbf{x}\,|\,x\in{\mathbb{R}}^{m}\}.
\end{align*}

Then there exist $\mathbf{x},\mathbf{y}$ such that $A\mathbf{x}=\mathbf{v}$, $A\mathbf{y}=\mathbf{w}$.

\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}=\alpha A\mathbf{x}+A\mathbf{y}=A(\alpha\mathbf{x}+\mathbf{y})\in\mathcal{C}\!\left(A\right).
\end{align*}

Thus by Theorem  @ref{subdef2}, $W$ is a subspace.
∎
@end
@eg
<b>for math major only</b> Let $S_{n}$ be the set of symmetric matrices of $M_{nn}$.
Then $S_{n}$ is a subspace of $M_{nn}$. Check that $W=S_{n}$ is a subspace: Because ${\cal O}\in W$, so $W$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $A,B\in W$. Then $A^{t}=A$, $B^{t}=B$.

\begin{align*}
\displaystyle (\alpha A+B)^{t}=\alpha A^{t}+B^{t}=\alpha A+B.
\end{align*}

Thus $\alpha A+B\in S_{n}$. Hence $S_{n}$ is a subspace by Theorem  @ref{subdef2}.
@end
@eg
<b>for math major only</b> Let

\begin{align*}
\displaystyle F=\{f(x)\in P_{n}\,|\,f(1)=0\}.
\end{align*}

Then $F$ is a subspace of $P_{n}$ Because $0\in E$, so $E$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $f(x),g(x)\in E$. Then $f(1)=g(1)=0$.
Let $h=\alpha f+g$. Then

\begin{align*}
\displaystyle h(1)=\alpha f(1)+g(1)=\alpha 0+0=0.
\end{align*}

So $h\in F$. Hence $F$ is a subspace by Theorem  @ref{subdef2}.
@end
@eg
<b>for math major only</b> Let

\begin{align*}
\displaystyle E=\{f(x)\in P_{n}\,|\,f(x)=f(-x)\}.
\end{align*}

Then $E$ is a subspace of $P_{n}$: Because $0\in E$, so $E$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $f(x),g(x)\in E$. Then $f(x)=f(-x)$, $g(x)=g(-x)$.
Let $h=\alpha f+g$. Then

\begin{align*}
\displaystyle h(-x)=\alpha f(-x)+g(-x)=\alpha f(x)+g(x)=h(x).
\end{align*}

So $h\in E$. Hence $E$ is a subspace.
@end

@section{Non examples}

To show that $W$ is <b>not</b> a subspace of $V$, it suffices to show that it violates Definition  @ref{subdef} condition 1 or condition 2.
This can be done by finding counter examples to either condition. Usually before checking those conditions, we quickly check if $\mathbf{0}_{V}\in W$
(see Proposition  @ref{0}).

@eg
$V={\mathbb{R}}^{m}$, $W=\{\mathbf{v}\in V\,|\,[\mathbf{v}]_{1}=1\}$. <b>Method 1</b> Obviously $\mathbf{0}$ is not in $W$. So by proposition  @ref{0}, $W$ is not a subspace. <b>Method 2</b> For Suppose $\mathbf{v},\mathbf{w}\in W$.
Then $[\mathbf{v}+\mathbf{w}]_{1}=[\mathbf{v}]_{1}+[\mathbf{w}]_{1}=2$.
So $\mathbf{v}+\mathbf{w}\not\in W$. So $W$ violates Definition  @ref{subdef} condition 1 and hence not a subspace.
@end
@eg
$V={\mathbb{R}}^{m}$, $W=\{\mathbf{v}\in V\,|\,\sum_{i=1}^{n}[\mathbf{v}]_{i}=1\}$. <b>Method 1</b> (the easiest method) Obviously $\mathbf{0}$ is not in $W$. So by Proposition  @ref{0}, $W$ is not a subspace.<b>Method 2</b> We will find an explicit counter example, let

\begin{align*}
\displaystyle \mathbf{v}=\mathbf{w}=\begin{bmatrix}1\\
0\\
\vdots\\
0\end{bmatrix}.
\end{align*}

Then both $\mathbf{v}$ and $\mathbf{w}$ are in $W$.

\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\begin{bmatrix}2\\
0\\
\vdots\\
0\end{bmatrix}.
\end{align*}

Obvious $\mathbf{v}+\mathbf{w}\notin W$.
Therefore $W$ violates Definition  @ref{subdef} condition 1 and hence not a subspace.
@end
@eg
$V=\mathbf{R}^{n}$, $W=\{\mathbf{v}\in V\,|\,[\mathbf{v}]_{1}\geq 0\}$. Let $\alpha=-1$ and

\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}1\\
0\end{bmatrix}.
\end{align*}

Then $[\alpha\mathbf{v}]_{1}=\alpha[\mathbf{v}]_{1}=\alpha=-1&lt;0$.
So $\alpha\mathbf{v}\notin W$.
Thus $W$ violates Theorem  @ref{subdef} condition 3 and hence not a subspace.
@end
@eg
$V={\mathbb{R}}^{2}$, $W=\{\begin{bmatrix}v_{1}\\
v_{2}\end{bmatrix}\in V\,|\,v_{1}v_{2}\geq 0\}$. Let $\mathbf{v}=\begin{bmatrix}1\\
0\end{bmatrix}$ , $\mathbf{w}=\begin{bmatrix}0\\
-1\end{bmatrix}\in W$.
$\mathbf{v}+\mathbf{w}=\begin{bmatrix}1\\
-1\end{bmatrix}$. Because
$1\times(-1)=-1&lt;0$. So $\mathbf{v}+\mathbf{w}\notin W$.
Thus $W$ violates Definition  @ref{subdef} condition 2 and hence not a subspace. In fact, we can show that $W$ satisfies Definition  @ref{subdef} condition 3 but fails condition 2.
@end
@eg
<b>for math major only</b>
Let $V=P_{n}$. Let $G$ be the set of polynomial with degree exactly equal to $n$. Let $f(x)=x^{n}$, $g(x)=-x^{n}+1$. Both $f$ and $g$ have degree exactly equal to $n$.
But

\begin{align*}
\displaystyle f(x)+g(x)=1
\end{align*}

is a polynomial with degree $0$. So $f+g$ is not in $G$. Thus $W$ violates Definition  @ref{subdef} condition 2 and hence not a subspace.
@end

@chapter{Linear Combinations}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section LC (print version p65 - p81)Strang: Section 2.3

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection LC (p.32-33) C40, C41, M10, M11

@section{Linear Combinations}
@label{LC}
@defn
@title{Linear Combination of Column Vectors}
@label{LCCV}
Given $n$ vectors $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}$ in ${\mathbb{R}}^{m}$ and $n$ scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\ldots,\,\alpha_{n}$,
their <b>linear combination</b> is the vector

\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}
\end{align*}

in $\mathbb{R}^{m}$.
@end
@eg
@label{TLC}
Two linear combinations in ${\mathbb{R}}^{6}$

Suppose that

\begin{align*}
\displaystyle\alpha_{1}=1&amp;\displaystyle\alpha_{2}=-4&amp;\displaystyle\alpha_{3}=2&amp;\displaystyle\alpha_{4}=-1
\end{align*}

and

\begin{align*}
\displaystyle\mathbf{u}_{1}&amp;\displaystyle=\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}&amp;\displaystyle\mathbf{u}_{2}&amp;\displaystyle=\begin{bmatrix}6\\
3\\
0\\
-2\\
1\\
4\end{bmatrix}&amp;\displaystyle\mathbf{u}_{3}&amp;\displaystyle=\begin{bmatrix}-5\\
2\\
1\\
1\\
-3\\
0\end{bmatrix}&amp;\displaystyle\mathbf{u}_{4}&amp;\displaystyle=\begin{bmatrix}3\\
2\\
-5\\
7\\
1\\
3\end{bmatrix}.
\end{align*}

The resulting linear combination is

\begin{align*}
\displaystyle\alpha_{1}\mathbf{u_{1}}+\alpha_{2}\mathbf{u_{2}}+\alpha_{3}\mathbf{u_{3}}+\alpha_{4}\mathbf{u_{4}}&amp;\displaystyle=(1)\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}+(-4)\begin{bmatrix}6\\
3\\
0\\
-2\\
1\\
4\end{bmatrix}+(2)\begin{bmatrix}-5\\
2\\
1\\
1\\
-3\\
0\end{bmatrix}+(-1)\begin{bmatrix}3\\
2\\
-5\\
7\\
1\\
3\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}+\begin{bmatrix}-24\\
-12\\
0\\
8\\
-4\\
-16\end{bmatrix}+\begin{bmatrix}-10\\
4\\
2\\
2\\
-6\\
0\end{bmatrix}+\begin{bmatrix}-3\\
-2\\
5\\
-7\\
-1\\
-3\end{bmatrix}=\begin{bmatrix}-35\\
-6\\
4\\
4\\
-9\\
-10\end{bmatrix}
\end{align*}

A different linear combination, but with the same set of vectors, can be formed with different scalars. Taking

\begin{align*}
\displaystyle\beta_{1}=3&amp;\displaystyle\beta_{2}=0&amp;\displaystyle\beta_{3}=5&amp;\displaystyle\beta_{4}=-1,
\end{align*}

we can form the linear combination

\begin{align*}
\displaystyle\beta_{1}\mathbf{u_{1}}+\beta_{2}\mathbf{u_{2}}+\beta_{3}\mathbf{u_{3}}+\beta_{4}\mathbf{u_{4}}&amp;\displaystyle=(3)\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}+(0)\begin{bmatrix}6\\
3\\
0\\
-2\\
1\\
4\end{bmatrix}+(5)\begin{bmatrix}-5\\
2\\
1\\
1\\
-3\\
0\end{bmatrix}+(-1)\begin{bmatrix}3\\
2\\
-5\\
7\\
1\\
3\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}6\\
12\\
-9\\
3\\
6\\
27\end{bmatrix}+\begin{bmatrix}0\\
0\\
0\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}-25\\
10\\
5\\
5\\
-15\\
0\end{bmatrix}+\begin{bmatrix}-3\\
-2\\
5\\
-7\\
-1\\
-3\end{bmatrix}=\begin{bmatrix}-22\\
20\\
1\\
1\\
-10\\
24\end{bmatrix}.
\end{align*}

Notice how we could keep our set of vectors fixed but use a different set of scalars to construct different vectors. You might build a few new linear combinations of $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\mathbf{u}_{4}$ right now. We will be right here when you get back. What vectors were you able to create? Do you think you could create the following vector $\mathbf{w}$ with a <b>suitable</b> choice of four scalars?

\begin{align*}
\displaystyle \mathbf{w}=\begin{bmatrix}13\\
15\\
5\\
-17\\
2\\
25\end{bmatrix}
\end{align*}

Do you think you could create any possible vector from ${\mathbb{R}}^{6}$ by choosing the proper scalars? These last two questions are fundamental, and time spent considering them now will prove beneficial later.
@end
@eg
The system of linear equation

\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}

or equivalently

\begin{align*}
\displaystyle \begin{bmatrix}-7x_{1}-6x_{2}-12x_{3}\\
5x_{1}+5x_{2}+7x_{3}\\
x_{1}+4x_{3}\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix},
\end{align*}

can be rewritten as

\begin{align*}
\displaystyle \begin{bmatrix}-7x_{1}\\
5x_{1}\\
x_{1}\end{bmatrix}+\begin{bmatrix}-6x_{2}\\
5x_{2}\\
0x_{2}\end{bmatrix}+\begin{bmatrix}-12x_{3}\\
7x_{3}\\
4x_{3}\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}
\end{align*}

or

\begin{align*}
\displaystyle x_{1}\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+x_{2}\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*}

The solution is

\begin{align*}
\displaystyle x_{1}=-3&amp;\displaystyle x_{2}=5&amp;\displaystyle x_{3}=2.
\end{align*}

So, in the context of this example, we can express the fact that these values of the variables are a solution by writing a linear combination:

\begin{align*}
\displaystyle (-3)\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+(5)\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+(2)\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}
\end{align*}

Furthermore, these are the only three scalars that will accomplish this equality, since they come from a unique solution.

Notice how the three vectors in this example are the columns of the coefficient matrix of the system of equations. This is our first hint of the important interplay between the vectors that form the columns of a matrix, and the matrix itself.
@end
@eg
The system of linear equations

\begin{align*}
\displaystyle x_{1}-x_{2}+2x_{3}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}+x_{2}+x_{3}&amp;\displaystyle=8 \\
\displaystyle x_{1}+x_{2}&amp;\displaystyle=5
\end{align*}

can be written as

\begin{align*}
\displaystyle \begin{bmatrix}x_{1}-x_{2}+2x_{3}\\
2x_{1}+x_{2}+x_{3}\\
x_{1}+x_{2}\end{bmatrix}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}.
\end{align*}

Now bust up the linear expressions on the left, first using vector addition,

\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
2x_{1}\\
x_{1}\end{bmatrix}+\begin{bmatrix}-x_{2}\\
x_{2}\\
x_{2}\end{bmatrix}+\begin{bmatrix}2x_{3}\\
x_{3}\\
0x_{3}\end{bmatrix}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}.
\end{align*}

Rewrite each of these vectors as a scalar multiple of a fixed vector, where the scalar is one of the unknown variables, converting the left-hand side into a linear combination:

\begin{align*}
\displaystyle x_{1}\begin{bmatrix}1\\
2\\
1\end{bmatrix}+x_{2}\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+x_{3}\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}.
\end{align*}

Row-reducing the augmented matrix for the system leads to the conclusion that the system is consistent and has free variables, hence infinitely many solutions. So for example, the two solutions

\begin{align*}
\displaystyle x_{1}=2&amp;\displaystyle x_{2}=3&amp;\displaystyle x_{3}=1 \\
\displaystyle x_{1}=3&amp;\displaystyle x_{2}=2&amp;\displaystyle x_{3}=0
\end{align*}

can be used together to say that

\begin{align*}
\displaystyle (2)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(3)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}=(3)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(2)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(0)\begin{bmatrix}2\\
1\\
0\end{bmatrix}.
\end{align*}

Ignore the middle of this equation, and move all the terms to the left-hand side:

\begin{align*}
\displaystyle (2)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(3)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
0\end{bmatrix}+(-3)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(-2)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(-0)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\end{bmatrix}.
\end{align*}

Regrouping gives

\begin{align*}
\displaystyle (-1)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(1)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\end{bmatrix}.
\end{align*}

Notice that the three vectors on the left hand side are the columns of the coefficient matrix for the system of equations. This equality says that there is a linear combination of those columns that equals the vector of all zeros. Give it some thought, but this says that

\begin{align*}
\displaystyle x_{1}=-1&amp;\displaystyle x_{2}=1&amp;\displaystyle x_{3}=1
\end{align*}

is a nontrivial solution to the homogeneous system of equations with the coefficient matrix of the original system. In particular, this demonstrates that this coefficient matrix is singular.
@end
@thm
@title{Solutions to Linear Systems are Linear Combinations}
@label{SLSLC}
Denote the columns of the $m\times n$ matrix $A$ as vectors $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$. Then $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution to the linear system of equations $A\mathbf{x}=\mathbf{b}$ if and only if $\mathbf{b}$ is equal to the linear combination of the columns of $A$ formed with the entries of $\mathbf{x}$,

\begin{align*}
\displaystyle \left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n}=\mathbf{b}
\end{align*}
@end
@proof
@col
If $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution of $A\mathbf{x}=\mathbf{b}$, then

\begin{align*}
\displaystyle \mathbf{b}=A\mathbf{x}=\left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n}.
\end{align*}

Hence $\mathbf{b}$ is a linear combination of the columns of $A$.

Conversely, if $\mathbf{b}$ is a linear combinations of the columns of $A$, say

\begin{align*}
\displaystyle \mathbf{b}=\left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n},
\end{align*}

then

\begin{align*}
\displaystyle \mathbf{b}=A\mathbf{x}.
\end{align*}

So $\mathbf{x}$ is a solution of $A\mathbf{x}=\mathbf{b}$.
∎
@end

<b>Computational question</b>: Determine if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{n}$. That is, determine whether or not the system of linear equations:

\begin{align*}
\displaystyle x_{1}\mathbf{v}_{1}+\cdots+x_{n}\mathbf{v}_{n}=\mathbf{u}
\end{align*}

has a solution. The augmented matrix is

\begin{align*}
\displaystyle [\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}|\mathbf{u}].
\end{align*}

Then we can solve the system of linear questions by the technique you learned.

@eg
Let

\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}1\\
1\\
3\end{bmatrix},\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
3\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}-1\\
1\\
0\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}2\\
1\\
3\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}-1\\
0\\
1\end{bmatrix}.
\end{align*}

<ol class="ltx_enumerate">
<li class="ltx_item">
Determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$.
If yes, find the linear combination.
</li>
<li class="ltx_item">
Determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4}\}$.
If yes, find the linear combination.
</li>

</ol>

<b>Answer</b>:

<ol class="ltx_enumerate">
<li class="ltx_item">
To determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$,
we need to solve

\begin{align*}
\displaystyle x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3}=\mathbf{u},
\end{align*}

i.e.

\begin{align*}
\displaystyle x_{1}-x_{2}+2x_{3}&amp;\displaystyle=1 \\
\displaystyle 2x_{1}+x+2+x_{3}&amp;\displaystyle=1 \\
\displaystyle 3x_{1}+3x_{3}&amp;\displaystyle=3.
\end{align*}

The augmented matrix is

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}1&amp;-1&amp;2&amp;1\\
2&amp;1&amp;1&amp;1\\
3&amp;0&amp;3&amp;3\end{array}\right]\xrightarrow{\operatorname{RREF}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;1&amp;0\\
0&amp;1&amp;-1&amp;0\\
0&amp;0&amp;0&amp;1\\
\end{array}\right].
\end{align*}

Because the last column of the RREF is a pivot column, the system of linear equations is not solvable.
Hence $\mathbf{u}$ is not a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$.
</li>
<li class="ltx_item">
To determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4}\}$,
we need to solve

\begin{align*}
\displaystyle x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3}+x_{4}\mathbf{v}_{4}=\mathbf{u}.
\end{align*}

The augmented matrix is

\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{v}_{4}|\mathbf{u}]=\left[\begin{array}[]{cccc|c}1&amp;-1&amp;2&amp;-1&amp;1\\
2&amp;1&amp;1&amp;0&amp;1\\
3&amp;0&amp;3&amp;1&amp;3\\
\end{array}\right].
\end{align*}

Using the standard method, we find one solution (there are infinitely many)

\begin{align*}
\displaystyle x_{1}=\frac{5}{6},x_{2}=-\frac{2}{3},x_{3}=0,x_{4}=\frac{1}{2},
\end{align*}

i.e.

\begin{align*}
\displaystyle \mathbf{u}=\frac{5}{6}\mathbf{v}_{1}-\frac{2}{3}\mathbf{v}_{2}+\frac{1}{2}\mathbf{v}_{4}.
\end{align*}

</li>

</ol>
@end

@section{Vector Form of Solution Sets}
@label{VFSS}
@eg
Consider the linear system

\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&amp;\displaystyle=8 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&amp;\displaystyle=-12 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&amp;\displaystyle=4.
\end{align*}

Row-reducing the augmented matrix yields

\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;3&amp;-2&amp;4\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

from which we see that there are $r=2$ pivot columns. Also, $D=\{1,\,2\}$, so that the dependent variables are $x_{1}$ and $x_{2}$, and $F=\{3,\,4,\,5\}$, so that the free variables are $x_{3}$ and $x_{4}$. We will express a generic solution for the system by two slightly different methods, though both arrive at the same conclusion.

Rearranging each equation represented in the row-reduced form of the augmented matrix by solving for the dependent variable in each row yields the vector equality,

\begin{align*}
\displaystyle\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}&amp;\displaystyle=\begin{bmatrix}4-3x_{3}+2x_{4}\\
-x_{3}+3x_{4}\\
x_{3}\\
x_{4}\end{bmatrix}. \\
\\
&amp;\displaystyle=\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}-3x_{3}\\
-x_{3}\\
x_{3}\\
0\end{bmatrix}+\begin{bmatrix}2x_{4}\\
3x_{4}\\
0\\
x_{4}\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}.
\end{align*}

We will develop the same linear combination a bit quicker, using three steps. While the method above is instructive, the method below will be our preferred approach.

Step 1. Write the vector of variables as a fixed vector plus a linear combination of $n-r$ vectors, using the free variables as the scalars:

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
\\
\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
\\
\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
\\
\end{bmatrix}
\end{align*}

Step 2. Use 0’s and 1’s to ensure equality for the entries of the vectors with indices in $F$ (corresponding to the free variables):

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
0\\
1\end{bmatrix}.
\end{align*}

Step 3. For each dependent variable, use the augmented matrix to formulate an equation expressing the dependent variable as a constant plus a linear combination of the free variables. Convert this equation into entries of the vectors that ensure equality for each dependent variable, one at a time.

\begin{align*}
\displaystyle x_{1}=4-3x_{3}+2x_{4}&amp;\displaystyle\Rightarrow&amp;\displaystyle\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}4\\
\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
\\
0\\
1\end{bmatrix} \\
\displaystyle x_{2}=0-1x_{3}+3x_{4}&amp;\displaystyle\Rightarrow&amp;\displaystyle\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}
\end{align*}

While this form is useful for quickly creating solutions, it is even better because it tells us exactly what every solution looks like. We know the solution set is infinite, which is pretty big, but now we can say that a solution is some multiple of $\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}$ plus a multiple of $\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}$ plus the fixed vector $\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}$. Period. So it only takes us three vectors to describe the entire infinite solution set, provided we also agree on how to combine the three vectors into a linear combination.
@end

@eg
Consider a linear system of $m=5$ equations in $n=7$ variables, having the augmented matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;1&amp;-1&amp;-2&amp;2&amp;1&amp;5&amp;21\\
1&amp;1&amp;-3&amp;1&amp;1&amp;1&amp;2&amp;-5\\
1&amp;2&amp;-8&amp;5&amp;1&amp;1&amp;-6&amp;-15\\
3&amp;3&amp;-9&amp;3&amp;6&amp;5&amp;2&amp;-24\\
-2&amp;-1&amp;1&amp;2&amp;1&amp;1&amp;-9&amp;-30\end{bmatrix}.
\end{align*}

Row-reducing we obtain the matrix

\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;2&amp;-3&amp;0&amp;0&amp;9&amp;15\\
0&amp;\boxed{1}&amp;-5&amp;4&amp;0&amp;0&amp;-8&amp;-10\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;-6&amp;11\\
0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;7&amp;-21\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

and we see that there are $r=4$ pivot columns. Also, $D=\{1,\,2,\,5,\,6\}$ so the dependent variables are $x_{1},\,x_{2},\,x_{5},$ and $x_{6}$. Similarly, $F=\{3,\,4,\,7,\,8\}$ and the $n-r=3$ free variables are $x_{3},\,x_{4}$ and $x_{7}$. We will express a generic solution for the system by two different methods: both a decomposition and a construction.

Rearranging each equation represented in the row-reduced echelon form of the augmented matrix by solving for the dependent variable in each row yields the vector equality

\begin{align*}
\displaystyle\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}&amp;\displaystyle=\begin{bmatrix}15-2x_{3}+3x_{4}-9x_{7}\\
-10+5x_{3}-4x_{4}+8x_{7}\\
x_{3}\\
x_{4}\\
11+6x_{7}\\
-21-7x_{7}\\
x_{7}\end{bmatrix}. \\
\\
&amp;\displaystyle=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+\begin{bmatrix}-2x_{3}\\
5x_{3}\\
x_{3}\\
0\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}3x_{4}\\
-4x_{4}\\
0\\
x_{4}\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}-9x_{7}\\
8x_{7}\\
0\\
0\\
6x_{7}\\
-7x_{7}\\
x_{7}\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}.
\end{align*}

We will now develop the same linear combination a bit quicker, using three steps. While the method above is instructive, the method below will be our preferred approach.

Step 1. Write the vector of variables as a fixed vector, plus a linear combination of $n-r$ vectors, using the free variables as the scalars:

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}+x_{7}\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}
\end{align*}

Step 2. Use 0’s and 1’s to ensure equality for the entries of the vectors with indices in $F$ (corresponding to the free variables):

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}\\
\\
0\\
0\\
\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
1\\
0\\
\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
0\\
1\\
\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}\\
\\
0\\
0\\
\\
\\
1\end{bmatrix}
\end{align*}

Step 3. For each dependent variable, use the augmented matrix to formulate an equation expressing the dependent variable as a constant plus multiples of the free variables. Convert this equation into entries of the vectors that ensure equality for each dependent variable, one at a time.

\begin{align*}
\displaystyle x_{1}&amp;\displaystyle=15-2x_{3}+3x_{4}-9x_{7}\ \Rightarrow \\
&amp;\displaystyle\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
\\
0\\
0\\
\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
\\
1\\
0\\
\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
\\
0\\
1\\
\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
\\
0\\
0\\
\\
\\
1\end{bmatrix} \\
\displaystyle x_{2}&amp;\displaystyle=-10+5x_{3}-4x_{4}+8x_{7}\ \Rightarrow \\
&amp;\displaystyle\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
\\
\\
1\end{bmatrix} \\
\displaystyle x_{5}&amp;\displaystyle=11+6x_{7}\ \Rightarrow \\
&amp;\displaystyle\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
\\
1\end{bmatrix} \\
\displaystyle x_{6}&amp;\displaystyle=-21-7x_{7}\ \Rightarrow \\
&amp;\displaystyle\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}
\end{align*}

This final form of a typical solution is especially pleasing and useful. For example, we can build solutions quickly by choosing values for our free variables, and then compute a linear combination. For example

\begin{align*}
\displaystyle x_{3}&amp;\displaystyle=2,\,x_{4}=-4,\,x_{7}=3\quad\quad\Rightarrow \\
\displaystyle\mathbf{x}&amp;\displaystyle=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+(2)\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+(-4)\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+(3)\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}=\begin{bmatrix}-28\\
40\\
2\\
-4\\
29\\
-42\\
3\end{bmatrix}
\end{align*}

or perhaps,

\begin{align*}
\displaystyle x_{3}&amp;\displaystyle=5,\,x_{4}=2,\,x_{7}=1\quad\quad\Rightarrow \\
\displaystyle\mathbf{x}&amp;\displaystyle=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+(5)\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+(2)\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+(1)\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}=\begin{bmatrix}2\\
15\\
5\\
2\\
17\\
-28\\
1\end{bmatrix}
\end{align*}

or even,

\begin{align*}
\displaystyle x_{3}&amp;\displaystyle=0,\,x_{4}=0,\,x_{7}=0\quad\quad\Rightarrow \\
\displaystyle\mathbf{x}&amp;\displaystyle=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+(0)\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+(0)\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+(0)\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}.
\end{align*}

So we can compactly express all of the solutions to this linear system with just 4 fixed vectors, provided we agree how to combine them in a linear combinations to create solution vectors.

Suppose you were told that the vector $\mathbf{w}$ below was a solution to this system of equations. Could you turn the problem around and write $\mathbf{w}$ as a linear combination of the four vectors $\mathbf{c}$, $\mathbf{u}_{1}$, $\mathbf{u}_{2}$, $\mathbf{u}_{3}$?

\begin{align*}
\displaystyle\mathbf{w}&amp;\displaystyle=\begin{bmatrix}100\\
-75\\
7\\
9\\
-37\\
35\\
-8\end{bmatrix}&amp;\displaystyle\mathbf{c}&amp;\displaystyle=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}&amp;\displaystyle\mathbf{u}_{1}&amp;\displaystyle=\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{u}_{2}&amp;\displaystyle=\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{u}_{3}&amp;\displaystyle=\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}
\end{align*}
@end

@thm
@title{Vector Form of Solutions to Linear Systems}
@label{VFSLS}
Suppose that $\left[A|\mathbf{b}\right]$ is the augmented matrix for a consistent linear system $A\mathbf{x}=\mathbf{b}$ of $m$ equations in $n$ variables.
Let $B$ be a row-equivalent $m\times(n+1)$ matrix in reduced row-echelon form. Suppose that $B$ has $r$ pivot columns, with indices $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$, while the $n-r$ non-pivot columns have indices in $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r},\,n+1\}$. Define vectors $\mathbf{c}$, $\mathbf{u}_{j}$, $1\leq j\leq n-r$ of size $n$ by

\begin{align*}
\displaystyle\left[\mathbf{c}\right]_{i}&amp;\displaystyle=\begin{cases}0&amp;\text{if $i\in F$}\\
\left[B\right]_{k,n+1}&amp;\text{if $i\in D$, $i=d_{k}$}\end{cases} \\
\displaystyle\left[\mathbf{u}_{j}\right]_{i}&amp;\displaystyle=\begin{cases}1&amp;\text{if $i\in F$, $i=f_{j}$}\\
0&amp;\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&amp;\text{if $i\in D$, $i=d_{k}$}\end{cases}.
\end{align*}

Then the set of solutions to the system of equations $A\mathbf{x}=\mathbf{b}$ is

\begin{align*}
\displaystyle S=\left\{\left.\mathbf{c}+\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n-r}\mathbf{u}_{n-r}\,\right|\,\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\ldots,\,\alpha_{n-r}\in{\mathbb{R}}^{\hbox{}}.\right\}
\end{align*}
@end

@proof
@col
<b>You can skip this proof for now, as long as you understand the examples</b> First, the equation $A\mathbf{x}=\mathbf{b}$ is equivalent to the linear system of equations that has the matrix $B$ as its augmented matrix.
So we need only show that $S$ is the solution set for the system with $B$ as its augmented matrix. The conclusion of this theorem is that the solution set is equal to the set $S$.

We begin by showing that every element of $S$ is indeed a solution to the system. Let $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\ldots,\,\alpha_{n-r}$ be one choice of the scalars used to describe elements of $S$. So an arbitrary element of $S$, which we will consider as a proposed solution, is

\begin{align*}
\displaystyle \mathbf{x}=\mathbf{c}+\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n-r}\mathbf{u}_{n-r}.
\end{align*}

When $r+1\leq\ell\leq m$, row $\ell$ of the matrix $B$ is a zero row, so the equation represented by that row is always true, no matter which solution vector we propose. So concentrate on rows representing equations $1\leq\ell\leq r$. We evaluate equation $\ell$ of the system represented by $B$ with the proposed solution vector $\mathbf{x}$ and refer to the value of the left-hand side of the equation as $\beta_{\ell}$:

\begin{align*}
\displaystyle \beta_{\ell}=\left[B\right]_{\ell 1}\left[\mathbf{x}\right]_{1}+\left[B\right]_{\ell 2}\left[\mathbf{x}\right]_{2}+\left[B\right]_{\ell 3}\left[\mathbf{x}\right]_{3}+\cdots+\left[B\right]_{\ell n}\left[\mathbf{x}\right]_{n}
\end{align*}

Since $\left[B\right]_{\ell d_{i}}=0$ for all $1\leq i\leq r$, except that $\left[B\right]_{\ell d_{\ell}}=1$, we see that $\beta_{\ell}$ simplifies to

\begin{align*}
\displaystyle \beta_{\ell}=\left[\mathbf{x}\right]_{d_{\ell}}+\left[B\right]_{\ell f_{1}}\left[\mathbf{x}\right]_{f_{1}}+\left[B\right]_{\ell f_{2}}\left[\mathbf{x}\right]_{f_{2}}+\left[B\right]_{\ell f_{3}}\left[\mathbf{x}\right]_{f_{3}}+\cdots+\left[B\right]_{\ell f_{n-r}}\left[\mathbf{x}\right]_{f_{n-r}}.
\end{align*}

Notice that for $1\leq i\leq n-r$

\begin{align*}
\displaystyle\left[\mathbf{x}\right]_{f_{i}}&amp;\displaystyle=\left[\mathbf{c}\right]_{f_{i}}+\alpha_{1}\left[\mathbf{u}_{1}\right]_{f_{i}}+\alpha_{2}\left[\mathbf{u}_{2}\right]_{f_{i}}+\cdots+\alpha_{i}\left[\mathbf{u}_{i}\right]_{f_{i}}+\cdots+\alpha_{n-r}\left[\mathbf{u_{n-r}}\right]_{f_{i}} \\
&amp;\displaystyle=0+\alpha_{1}(0)+\alpha_{2}(0)+\cdots+\alpha_{i}(1)+\cdots+\alpha_{n-r}(0) \\
&amp;\displaystyle=\alpha_{i}.
\end{align*}

So $\beta_{\ell}$ simplifies further, and we expand the first term

\begin{align*}
\displaystyle\beta_{\ell}&amp;\displaystyle=\left[\mathbf{x}\right]_{d_{\ell}}+\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&amp;\displaystyle=\left[\mathbf{c}+\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n-r}\mathbf{u}_{n-r}\right]_{d_{\ell}}+ \\
&amp;\displaystyle\quad\quad\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&amp;\displaystyle=\left[\mathbf{c}\right]_{d_{\ell}}+\alpha_{1}\left[\mathbf{u}_{1}\right]_{d_{\ell}}+\alpha_{2}\left[\mathbf{u}_{2}\right]_{d_{\ell}}+\alpha_{3}\left[\mathbf{u}_{3}\right]_{d_{\ell}}+\cdots+\alpha_{n-r}\left[\mathbf{u_{n-r}}\right]_{d_{\ell}}+ \\
&amp;\displaystyle\quad\quad\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&amp;\displaystyle=\left[B\right]_{\ell,{n+1}}+ \\
&amp;\displaystyle\quad\quad\alpha_{1}(-\left[B\right]_{\ell f_{1}})+\alpha_{2}(-\left[B\right]_{\ell f_{2}})+\alpha_{3}(-\left[B\right]_{\ell f_{3}})+\cdots+\alpha_{n-r}(-\left[B\right]_{\ell f_{n-r}})+ \\
&amp;\displaystyle\quad\quad\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&amp;\displaystyle=\left[B\right]_{\ell,{n+1}}.
\end{align*}

So $\beta_{\ell}$ began as the left-hand side of equation $\ell$ of the system represented by $B$ and we now know it equals $\left[B\right]_{\ell,{n+1}}$, the constant term for equation $\ell$ of this system. So the arbitrarily chosen vector from $S$ makes every equation of the system true, and therefore is a solution to the system. So all the elements of $S$ are solutions to the system.

For the second half of the proof, assume that $\mathbf{x}$ is a solution vector for the system having $B$ as its augmented matrix. For convenience and clarity, denote the entries of $\mathbf{x}$ by $x_{i}$. In other words, $x_{i}=\left[\mathbf{x}\right]_{i}$. We desire to show that this solution vector is also an element of the set $S$. Begin with the observation that the entries of a solution vector make equation $\ell$ of the system true for all $1\leq\ell\leq m$:

\begin{align*}
\displaystyle \left[B\right]_{\ell,1}x_{1}+\left[B\right]_{\ell,2}x_{2}+\left[B\right]_{\ell,3}x_{3}+\cdots+\left[B\right]_{\ell,n}x_{n}=\left[B\right]_{\ell,n+1}
\end{align*}

When $\ell\leq r$, the pivot columns of $B$ have zero entries in row $\ell$ with the exception of column $d_{\ell}$, which will contain a $1$. So for $1\leq\ell\leq r$, equation $\ell$ simplifies to

\begin{align*}
\displaystyle 1x_{d_{\ell}}+\left[B\right]_{\ell,f_{1}}x_{f_{1}}+\left[B\right]_{\ell,f_{2}}x_{f_{2}}+\left[B\right]_{\ell,f_{3}}x_{f_{3}}+\cdots+\left[B\right]_{\ell,f_{n-r}}x_{f_{n-r}}=\left[B\right]_{\ell,n+1}.
\end{align*}

This allows us to write,

\begin{align*}
\displaystyle\left[\mathbf{x}\right]_{d_{\ell}}&amp;\displaystyle=x_{d_{\ell}} \\
&amp;\displaystyle=\left[B\right]_{\ell,n+1}-\left[B\right]_{\ell,f_{1}}x_{f_{1}}-\left[B\right]_{\ell,f_{2}}x_{f_{2}}-\left[B\right]_{\ell,f_{3}}x_{f_{3}}-\cdots-\left[B\right]_{\ell,f_{n-r}}x_{f_{n-r}} \\
&amp;\displaystyle=\left[\mathbf{c}\right]_{d_{\ell}}+x_{f_{1}}\left[\mathbf{u}_{1}\right]_{d_{\ell}}+x_{f_{2}}\left[\mathbf{u}_{2}\right]_{d_{\ell}}+x_{f_{3}}\left[\mathbf{u}_{3}\right]_{d_{\ell}}+\cdots+x_{f_{n-r}}\left[\mathbf{u}_{n-r}\right]_{d_{\ell}} \\
&amp;\displaystyle=\left[\mathbf{c}+x_{f_{1}}\mathbf{u}_{1}+x_{f_{2}}\mathbf{u}_{2}+x_{f_{3}}\mathbf{u}_{3}+\cdots+x_{f_{n-r}}\mathbf{u}_{n-r}\right]_{d_{\ell}}.
\end{align*}

This tells us that the entries of the solution vector $\mathbf{x}$ corresponding to dependent variables (indices in $D$) are equal to those of a vector in the set $S$. We still need to check the other entries of the solution vector $\mathbf{x}$ corresponding to the free variables (indices in $F$) to see if they are equal to the entries of the same vector in the set $S$. To this end, suppose $i\in F$ and $i=f_{j}$. Then

\begin{align*}
\displaystyle\left[\mathbf{x}\right]_{i}&amp;\displaystyle=x_{i}=x_{f_{j}} \\
&amp;\displaystyle=0+0x_{f_{1}}+0x_{f_{2}}+0x_{f_{3}}+\cdots+0x_{f_{j-1}}+1x_{f_{j}}+0x_{f_{j+1}}+\cdots+0x_{f_{n-r}} \\
&amp;\displaystyle=\left[\mathbf{c}\right]_{i}+x_{f_{1}}\left[\mathbf{u}_{1}\right]_{i}+x_{f_{2}}\left[\mathbf{u}_{2}\right]_{i}+x_{f_{3}}\left[\mathbf{u}_{3}\right]_{i}+\cdots+x_{f_{j}}\left[\mathbf{u}_{j}\right]_{i}+\cdots+x_{f_{n-r}}\left[\mathbf{u}_{n-r}\right]_{i} \\
&amp;\displaystyle=\left[\mathbf{c}+x_{f_{1}}\mathbf{u}_{1}+x_{f_{2}}\mathbf{u}_{2}+\cdots+x_{f_{n-r}}\mathbf{u}_{n-r}\right]_{i}.
\end{align*}

So entries of
$\mathbf{x}$ and $\mathbf{c}+x_{f_{1}}\mathbf{u}_{1}+x_{f_{2}}\mathbf{u}_{2}+\cdots+x_{f_{n-r}}\mathbf{u}_{n-r}$
are equal and therefore they are equal vectors. Since $x_{f_{1}},\,x_{f_{2}},\,x_{f_{3}},\,\ldots,\,x_{f_{n-r}}$ are scalars, this shows us that $\mathbf{x}$ qualifies for membership in $S$. So the set $S$ contains all of the solutions to the system.
∎
@end

@chapter{Spanning Sets}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section SS (print version p83 - p94)Strang, Sect 2.3

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection SS (p.34-40) C40-45, C50, C60, M10, M11, M12. (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions) T10, T20, T21, T22. Strang, Sect 2.3.

In this section we will provide an extremely compact way to describe an infinite set of vectors, making use of linear combinations. This will give us a convenient way to describe the solution set of a linear system, the null space of a matrix, and many other sets of vectors.

@section{Span of a Set of Vectors}
@label{SSV}
We saw that the solution set of a homogeneous system can be described as all possible linear combinations of two particular sets of vectors. This is a useful way to construct or describe infinite sets of vectors, so we encapsulate the idea in a definition.

@defn
@title{Span of a Set of Column Vectors}
@label{SSCV}
Given a set of vectors

\begin{align*}
\displaystyle S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\},
\end{align*}

their <b>span</b>, $\left&lt;S\right&gt;$, is the set of all linear combinations of $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}$. Symbolically,

\begin{align*}
\displaystyle\left&lt;S\right&gt;&amp;\displaystyle=\left\{\left.\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}\,\right|\,\alpha_{i}\in{\mathbb{R}}^{\hbox{}},\,1\leq i\leq n\right\} \\
&amp;\displaystyle=\left\{\left.\sum_{i=1}^{n}\alpha_{i}\mathbf{u}_{i}\,\right|\,\alpha_{i}\in{\mathbb{R}}^{\hbox{}},\,1\leq i\leq n\right\}
\end{align*}
@end
@thm

Let $S=\{\mathbf{u}_{1},\ldots,\mathbf{u}_{k}\}\subseteq V={\mathbb{R}}^{m}$.
Then $\left&lt;S\right&gt;$ is a subspace of $V$.
@end
@proof
@col
Obviously $\left&lt;S\right&gt;$ is nonempty. Let $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W=\left&lt;S\right&gt;$.
Then there exists $\alpha_{1},\ldots,\alpha_{k},\beta_{1},\ldots,\beta_{k}$ such that

\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{k}\mathbf{u}_{k},
\end{align*}

\begin{align*}
\displaystyle \mathbf{w}=\beta_{1}\mathbf{u}_{1}+\cdots+\beta_{k}\mathbf{u}_{k}.
\end{align*}

Then

\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}=(\alpha\alpha_{1}+\beta_{1})\mathbf{u}_{1}+\cdots+(\alpha\alpha_{k}+\beta_{k})\mathbf{u}_{k}
\end{align*}

is in $\left&lt;S\right&gt;$.
Thus by Lecture 9 Theorem 13, $W$ is a subspace.
∎
@end

<b>Main Questions</b>:

<ol class="ltx_enumerate">
<li class="ltx_item">
Determine if a vector $\mathbf{v}$ is an element of $\left&lt;S\right&gt;$.
</li>
<li class="ltx_item">
Describe the set $\left&lt;S\right&gt;$.
</li>
<li class="ltx_item">
Is $\left&lt;S\right&gt;$ equal to ${\mathbb{R}}^{m}$?
</li>

</ol>

@eg
@label{ABS}
Consider the set of 5 vectors, $S$, from ${\mathbb{R}}^{4}$

\begin{align*}
\displaystyle S=\{\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix},\,\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix},\,\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix},\,\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}.\}
\end{align*}

Consider the infinite set of vectors $\left&lt;S\right&gt;$ formed by all linear combinations of the elements of $S$. Here are four vectors which we definitely know are elements of $\left&lt;S\right&gt;$:

\begin{align*}
\displaystyle \mathbf{w}=(2)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(-1)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(2)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(3)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}-4\\
2\\
28\\
10\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \mathbf{x}=(5)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(-6)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(-3)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(4)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(2)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}-26\\
-6\\
2\\
34\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \mathbf{y}=(1)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(0)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(1)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(0)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(1)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}7\\
4\\
17\\
-4\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \mathbf{z}=(0)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(0)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(0)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(0)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(0)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\\
0\end{bmatrix}.
\end{align*}

<b>Fundamental question:</b> Determine if a given vector is an element of the set or not. Let us learn more about $\left&lt;S\right&gt;$ by investigating which vectors are elements of the set, and which are not.

First, is $\mathbf{u}=\begin{bmatrix}-15\\
-6\\
19\\
5\end{bmatrix}$ an element of $\left&lt;S\right&gt;$? We are asking if there are scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$ such that

\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+\alpha_{2}\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+\alpha_{3}\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+\alpha_{4}\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+\alpha_{5}\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\mathbf{u}=\begin{bmatrix}-15\\
-6\\
19\\
5\end{bmatrix}.
\end{align*}

Searching for such scalars is equivalent to finding a solution to the linear system of equations with augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&amp;2&amp;7&amp;1&amp;-1&amp;-15\\
1&amp;1&amp;3&amp;1&amp;0&amp;-6\\
3&amp;2&amp;5&amp;-1&amp;9&amp;19\\
1&amp;-1&amp;-5&amp;2&amp;0&amp;5\end{array}\right].
\end{align*}

This matrix row-reduces to

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}\boxed{1}&amp;0&amp;-1&amp;0&amp;3&amp;10\\
0&amp;\boxed{1}&amp;4&amp;0&amp;-1&amp;-9\\
0&amp;0&amp;0&amp;\boxed{1}&amp;-2&amp;-7\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\end{array}\right].
\end{align*}

At this point, we see that the system is consistent, so we know there is a solution for the five scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$. This is enough evidence for us to say that $\mathbf{u}\in\left&lt;S\right&gt;$. Moreover, we can compute an actual solution, say

\begin{align*}
\displaystyle\alpha_{1}&amp;\displaystyle=2&amp;\displaystyle\alpha_{2}&amp;\displaystyle=1&amp;\displaystyle\alpha_{3}&amp;\displaystyle=-2&amp;\displaystyle\alpha_{4}&amp;\displaystyle=-3&amp;\displaystyle\alpha_{5}&amp;\displaystyle=2.
\end{align*}

This particular solution allows us to write

\begin{align*}
\displaystyle (2)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(-2)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(-3)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(2)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\mathbf{u}=\begin{bmatrix}-15\\
-6\\
19\\
5\end{bmatrix}
\end{align*}

making it even more obvious that $\mathbf{u}\in\left&lt;S\right&gt;$.

Let us do it again. Is $\mathbf{v}=\begin{bmatrix}3\\
1\\
2\\
-1\end{bmatrix}$ an element of $\left&lt;S\right&gt;$? We are asking if there are scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$ such that

\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+\alpha_{2}\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+\alpha_{3}\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+\alpha_{4}\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+\alpha_{5}\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\mathbf{v}=\begin{bmatrix}3\\
1\\
2\\
-1\end{bmatrix}
\end{align*}

Again, this is equivalent to finding a solution to the linear system of equations with augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&amp;2&amp;7&amp;1&amp;-1&amp;3\\
1&amp;1&amp;3&amp;1&amp;0&amp;1\\
3&amp;2&amp;5&amp;-1&amp;9&amp;2\\
1&amp;-1&amp;-5&amp;2&amp;0&amp;-1\end{array}\right].
\end{align*}

This matrix row-reduces to

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}\boxed{1}&amp;0&amp;-1&amp;0&amp;3&amp;0\\
0&amp;\boxed{1}&amp;4&amp;0&amp;-1&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;-2&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}\end{array}\right].
\end{align*}

At this point, we see that the system is inconsistent, so we know there is no solution for the five scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$. This is enough evidence for us to say that $\mathbf{v}\not\in\left&lt;S\right&gt;$. End of story.
@end

From the above, we have the following theorem:

@thm

Suppose that $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}$ are in $\mathbf{R}^{m}$.
Let $A$ be the $m\times n$ matrix whose $i$-th column is $\mathbf{u}_{i}$.
Then $\mathbf{v}\in\left&lt;S\right&gt;$ is and only if $A\mathbf{x}=\mathbf{v}$ is consistent.
@end

<b>Computational Technique</b>: Given $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ in ${\mathbb{R}}^{m}$, determine if

\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{m}\end{bmatrix}\in\left&lt;S\right&gt;.
\end{align*}

<b>Answer</b>:

<ol class="ltx_enumerate">
<li class="ltx_item">
To determine if $\mathbf{v}\in\left&lt;S\right&gt;$, we need to find $\alpha_{1},\ldots,\alpha_{n}$ such that

\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{v}.
\end{align*}

</li>
<li class="ltx_item">
This is equivalent to solving the system of linear equations $A\mathbf{x}=\mathbf{v}$,
where $A$ is the $m\times n$ matrix whose $i$-th column is $\mathbf{u}_{i}$.
</li>
<li class="ltx_item">
Row-reduce the augmented matrix $[A|\mathbf{v}]$ to a RREF $B$.

<ol class="ltx_enumerate">
<li class="ltx_item">
If the last column of $B$ is a pivot column, then the system is inconsistent and $\mathbf{v}\notin\left&lt;S\right&gt;$.
</li>
<li class="ltx_item">
If the last column of $B$ is not a pivot column, then the system is consistent and $\mathbf{v}\in\left&lt;S\right&gt;$.
</li>

</ol>
</li>

</ol>

@eg
Following the previous example, determine if $\mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{4}\end{bmatrix}\in\left&lt;S\right&gt;$.

Applying Gauss-Jordan elimination to the augmented matrix

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&amp;2&amp;7&amp;1&amp;-1&amp;v_{1}\\
1&amp;1&amp;3&amp;1&amp;0&amp;v_{2}\\
3&amp;2&amp;5&amp;-1&amp;9&amp;v_{3}\\
1&amp;-1&amp;-5&amp;2&amp;0&amp;v_{4}\end{array}\right],
\end{align*}

we obtain

\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&amp;0&amp;-1&amp;0&amp;3&amp;-3v_{1}+5v_{2}-v_{4}\\
0&amp;1&amp;4&amp;0&amp;-1&amp;v_{1}-v_{2}\\
0&amp;0&amp;0&amp;1&amp;-2&amp;2v_{1}-3v_{2}+v_{4}\\
0&amp;0&amp;0&amp;0&amp;0&amp;9v_{1}-16v_{2}+v_{3}+4v_{4}\\
\end{array}\right].
\end{align*}

If $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$, then the last column is not a pivot column and the above is a RREF. In this case, $\mathbf{v}\in\left&lt;S\right&gt;$. If instead $9v_{1}-16v_{2}+v_{3}+4v_{4}\neq 0$, then the above not a RREF. Hence the corresponding system of linear equations is inconsistent
and thus $\mathbf{v}\not\in\left&lt;S\right&gt;$.

We therefore conclude that $\mathbf{v}\in\left&lt;S\right&gt;$ if and only if $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$.
@end
@eg
@label{SCAA}
Begin with the finite set of three vectors of size $3$

\begin{align*}
\displaystyle S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3}\}=\left\{\begin{bmatrix}1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
1\end{bmatrix},\,\begin{bmatrix}2\\
1\\
0\end{bmatrix}\right\}
\end{align*}

and consider the infinite set $\left&lt;S\right&gt;$.

First, as an example, note that

\begin{align*}
\displaystyle \mathbf{v}=(5)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(-3)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(7)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}22\\
14\\
2\end{bmatrix}
\end{align*}

is in $\left&lt;S\right&gt;$, since it is a linear combination of $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3}$. There is nothing magical about the scalars $\alpha_{1}=5,\,\alpha_{2}=-3,\,\alpha_{3}=7$; they can be chosen arbitrarily. So repeat this part of the example yourself, using different values of $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$. What happens if you choose all three scalars to be zero?

So we know how to quickly construct sample elements of the set $\left&lt;S\right&gt;$. A slightly different question arises when you are handed a vector of the correct size and asked if it is an element of $\left&lt;S\right&gt;$. For example, is $\mathbf{w}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}$ in $\left&lt;S\right&gt;$? More succinctly, $\mathbf{w}\in\left&lt;S\right&gt;$?

To answer this question, we will look for scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ such that

\begin{align*}
\displaystyle\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}&amp;\displaystyle=\mathbf{w}.
\end{align*}

This is equivalent to solving the system of linear equations

\begin{align*}
\displaystyle\alpha_{1}-\alpha_{2}+2\alpha_{3}&amp;\displaystyle=1 \\
\displaystyle 2\alpha_{1}+\alpha_{2}+\alpha_{3}&amp;\displaystyle=8 \\
\displaystyle\alpha_{1}+\alpha_{2}&amp;\displaystyle=5.
\end{align*}

Building the augmented matrix for this linear system, and row-reducing, gives

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;1&amp;3\\
0&amp;\boxed{1}&amp;-1&amp;2\\
0&amp;0&amp;0&amp;0\end{array}\right].
\end{align*}

This system has infinitely many solutions (there is a free variable in $x_{3}$), but all we need is one solution vector. The solution,

\begin{align*}
\displaystyle\alpha_{1}&amp;\displaystyle=2&amp;\displaystyle\alpha_{2}&amp;\displaystyle=3&amp;\displaystyle\alpha_{3}&amp;\displaystyle=1
\end{align*}

tells us that

\begin{align*}
\displaystyle (2)\mathbf{u}_{1}+(3)\mathbf{u}_{2}+(1)\mathbf{u}_{3}=\mathbf{w}.
\end{align*}

So we are convinced that $\mathbf{w}$ really is in $\left&lt;S\right&gt;$. Notice that there is an infinite number of ways to answer this question affirmatively. We could choose a different solution, this time choosing the free variable to be zero,

\begin{align*}
\displaystyle\alpha_{1}&amp;\displaystyle=3&amp;\displaystyle\alpha_{2}&amp;\displaystyle=2&amp;\displaystyle\alpha_{3}&amp;\displaystyle=0,
\end{align*}

showing that

\begin{align*}
\displaystyle (3)\mathbf{u}_{1}+(2)\mathbf{u}_{2}+(0)\mathbf{u}_{3}=\mathbf{w}.
\end{align*}

Verifying the arithmetic in this second solution will make it obvious that $\mathbf{w}$ is in this span. And of course, we now realize that there are an <b>infinite</b> number of ways to realize $\mathbf{w}$ as element of $\left&lt;S\right&gt;$.

Let us ask the same type of question again, but this time with $\mathbf{y}=\begin{bmatrix}2\\
4\\
3\end{bmatrix}$, i.e., is $\mathbf{y}\in\left&lt;S\right&gt;$?

So we will look for scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ such that

\begin{align*}
\displaystyle\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}&amp;\displaystyle=\mathbf{y}.
\end{align*}

This is equivalent to finding solutions to the system of equations

\begin{align*}
\displaystyle\alpha_{1}-\alpha_{2}+2\alpha_{3}&amp;\displaystyle=2 \\
\displaystyle 2\alpha_{1}+\alpha_{2}+\alpha_{3}&amp;\displaystyle=4 \\
\displaystyle\alpha_{1}+\alpha_{2}&amp;\displaystyle=3,
\end{align*}

Building the augmented matrix for this linear system and row-reducing gives

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;1&amp;0\\
0&amp;\boxed{1}&amp;-1&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}\end{array}\right].
\end{align*}

This system is inconsistent because the last column is a pivot column.
So there are no scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ that will create a linear combination of $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3}$ that equals $\mathbf{y}$. More precisely, $\mathbf{y}\not\in\left&lt;S\right&gt;$.

There are three things to observe in this example.

<ol class="ltx_enumerate">
<li class="ltx_item">
It is easy to construct vectors in $\left&lt;S\right&gt;$.
</li>
<li class="ltx_item">
It is possible that some vectors are in $\left&lt;S\right&gt;$ (such as $\mathbf{w}$), while others are not (such as $\mathbf{y}$).
</li>
<li class="ltx_item">
Deciding if a given vector is in $\left&lt;S\right&gt;$ leads to a linear system of equations and asking if the system is consistent.
</li>

</ol>
@end
@eg
Let

\begin{align*}
\displaystyle R=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3}\}=\left\{\begin{bmatrix}-7\\
5\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
5\\
0\end{bmatrix},\,\begin{bmatrix}-12\\
7\\
4\end{bmatrix}\right\}
\end{align*}

and consider the infinite set $\left&lt;R\right&gt;$.

First, as an example, note that

\begin{align*}
\displaystyle \mathbf{x}=(2)\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+(4)\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+(-3)\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-2\\
9\\
-10\end{bmatrix}
\end{align*}

is in $\left&lt;R\right&gt;$, since it is a linear combination of $\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3}$. In other words, $\mathbf{x}\in\left&lt;R\right&gt;$. Try some different values of $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ yourself, and see what vectors you can create as elements of $\left&lt;R\right&gt;$.

Now ask if a given vector is an element of $\left&lt;R\right&gt;$. For example, is $\mathbf{z}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}$ in $\left&lt;R\right&gt;$? Is $\mathbf{z}\in\left&lt;R\right&gt;$?

To answer this question, we will look for scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ so that

\begin{align*}
\displaystyle\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\mathbf{v}_{3}&amp;\displaystyle=\mathbf{z}.
\end{align*}

This is equivalent to finding solutions to the following system of linear equations:

\begin{align*}
\displaystyle-7\alpha_{1}-6\alpha_{2}-12\alpha_{3}&amp;\displaystyle=-33 \\
\displaystyle 5\alpha_{1}+5\alpha_{2}+7\alpha_{3}&amp;\displaystyle=24 \\
\displaystyle\alpha_{1}+4\alpha_{3}&amp;\displaystyle=5.
\end{align*}

Building the augmented matrix for this linear system and row-reducing gives

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;-3\\
0&amp;\boxed{1}&amp;0&amp;5\\
0&amp;0&amp;\boxed{1}&amp;2\end{array}\right].
\end{align*}

This system has a unique solution,

\begin{align*}
\displaystyle\alpha_{1}=-3&amp;\displaystyle\alpha_{2}=5&amp;\displaystyle\alpha_{3}=2
\end{align*}

telling us that

\begin{align*}
\displaystyle (-3)\mathbf{v}_{1}+(5)\mathbf{v}_{2}+(2)\mathbf{v}_{3}=\mathbf{z}.
\end{align*}

So we are convinced that $\mathbf{z}$ really is in $\left&lt;R\right&gt;$. Notice that in this case we have only one way to answer the question affirmatively, since the solution is unique.

Let us ask about another vector. Say, is $\mathbf{x}=\begin{bmatrix}-7\\
8\\
-3\end{bmatrix}$ in $\left&lt;R\right&gt;$? Is $\mathbf{x}\in\left&lt;R\right&gt;$?

We desire scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ so that

\begin{align*}
\displaystyle\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\mathbf{v}_{3}&amp;\displaystyle=\mathbf{x}
\end{align*}

This is equivalent to finding the solutions to the system of equations

\begin{align*}
\displaystyle-7\alpha_{1}-6\alpha_{2}-12\alpha_{3}&amp;\displaystyle=-7 \\
\displaystyle 5\alpha_{1}+5\alpha_{2}+7\alpha_{3}&amp;\displaystyle=8 \\
\displaystyle\alpha_{1}+4\alpha_{3}&amp;\displaystyle=-3.
\end{align*}

Building the augmented matrix for this linear system and row-reducing gives

\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;1\\
0&amp;\boxed{1}&amp;0&amp;2\\
0&amp;0&amp;\boxed{1}&amp;-1\end{array}\right].
\end{align*}

This system has a unique solution,

\begin{align*}
\displaystyle\alpha_{1}=1&amp;\displaystyle\alpha_{2}=2&amp;\displaystyle\alpha_{3}=-1
\end{align*}

telling us that

\begin{align*}
\displaystyle (1)\mathbf{v}_{1}+(2)\mathbf{v}_{2}+(-1)\mathbf{v}_{3}=\mathbf{x}.
\end{align*}

So we are convinced that $\mathbf{x}$ really is in $\left&lt;R\right&gt;$. Notice that in this case we again have only one way to answer the question affirmatively since the solution is again unique.

We could continue to test other vectors for membership in $\left&lt;R\right&gt;$, but there is no point. A question about membership in $\left&lt;R\right&gt;$ inevitably leads to a system of three equations in the three variables $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ with a coefficient matrix whose columns are the vectors $\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3}$. This particular coefficient matrix is nonsingular, so by Lecture 5, Theorem 12, item 4, the system is guaranteed to have a solution. (This solution is unique, but that is not critical here.) So no matter which vector we might have chosen for $\mathbf{z}$, we are certain to discover that it was an element of $\left&lt;R\right&gt;$.

<b>Conclusion</b>: Every vector of size 3 is in $\left&lt;R\right&gt;$, or $\left&lt;R\right&gt;={\mathbb{R}}^{3}$.
@end

The above inspires the following result:

@thm

Given $m$ vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{m}\}$ in ${\mathbb{R}}^{m}$, let $A$ be the $m\times m$ square matrix whose $i$-column is $\mathbf{u}_{i}$. Then $A$ is non-singular if and only if $\left&lt;S\right&gt;={\mathbb{R}}^{m}$.
@end
@proof
@col
($\Rightarrow$) If $A$ is non-singular, then for every $\mathbf{b}\in{\mathbb{R}}^{m}$ the equation $A\mathbf{x}=\mathbf{b}$ is consistent
by Lecture 8 Theorem 12 Item 4. Hence $\mathbf{b}\in\left&lt;S\right&gt;$. So ${\mathbb{R}}^{m}=\left&lt;S\right&gt;$. ($\Leftarrow$) This part is difficult; we will only sketch the idea.
Let the RREF of $A$ be $B$. Suppose that $A$ is singular. Then $B\neq I_{m}$ and the last row of $B$ is a zero row. So there exists $\mathbf{b}\in{\mathbb{R}}^{m}$ such that $B\mathbf{x}=\mathbf{b}$ is inconsistent.
(e.g. $\mathbf{b}=\begin{bmatrix}0\\
\vdots\\
0\\
1\end{bmatrix}$).
Hence there exists $\mathbf{c}\in{\mathbb{R}}^{m}$ such that $A\mathbf{x}=\mathbf{c}$ is inconsistent (why?).
Thus $\mathbf{c}\notin\left&lt;S\right&gt;$. So $\left&lt;S\right&gt;\neq{\mathbb{R}}^{m}$. This completes the proof.
∎
@end

@section{Spanning Sets of Null Spaces}

The following theorem describe how to express a null space as $\left&lt;S\right&gt;$.

@thm
@title{Spanning Sets for Null Spaces}
@label{SSNS}
Suppose that $A$ is an $m\times n$ matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Suppose that $B$ has $r$ pivot columns, with indices given by $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$, while the $n-r$ non-pivot columns have indices $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$,

\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&amp;\text{if $i\in F$, $i=f_{j}$}\\
0&amp;\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&amp;\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}

Then the null space of $A$ is given by

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left&lt;\left\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\right\}\right&gt;
\end{align*}
@end
@proof
@col
The can be seen by moving the free variables to another side. For details. See Beezer p88. Don’t memorize this theorem. Instead, study the examples below.
∎
@end
@eg
<b>Spanning set of a null space</b>

Find a set of vectors, $S$, so that the null space of the matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;3&amp;3&amp;-1&amp;-5\\
2&amp;5&amp;7&amp;1&amp;1\\
1&amp;1&amp;5&amp;1&amp;5\\
-1&amp;-4&amp;-2&amp;0&amp;4\end{bmatrix}
\end{align*}

is the span of $S$, that is, $\left&lt;S\right&gt;={\mathcal{N}}\!\left(A\right)$.

The null space of $A$ is the set of all solutions to the homogeneous system $A\mathbf{x}=\mathbf{0}$.
Begin by row-reducing $A$. The result is

\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;6&amp;0&amp;4\\
0&amp;\boxed{1}&amp;-1&amp;0&amp;-2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;3\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

We have $D=\{1,\,2,\,4\}$ and $F=\{3,\,5\}$. Hence $x_{3}$ and $x_{5}$ are free variables and we can interpret each nonzero row as an expression for the dependent variables $x_{1}$, $x_{2}$, $x_{4}$ (respectively) in the free variables $x_{3}$ and $x_{5}$. With this we can write the vector form of a solution vector as

\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\end{bmatrix}=\begin{bmatrix}-6x_{3}-4x_{5}\\
x_{3}+2x_{5}\\
x_{3}\\
-3x_{5}\\
x_{5}\end{bmatrix}=x_{3}\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix}+x_{5}\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}.
\end{align*}

Then, in the notation of the above theorem, we have

\begin{align*}
\displaystyle\mathbf{z}_{1}&amp;\displaystyle=\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{2}&amp;\displaystyle=\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}
\end{align*}

and

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left&lt;\{\mathbf{z}_{1},\,\mathbf{z}_{2}\}\right&gt;=\left&lt;\{\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix},\,\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}\}\right&gt;.
\end{align*}
@end
@eg
<b>Null space directly as a span</b>Consider the matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;1&amp;5&amp;1&amp;5&amp;1\\
1&amp;1&amp;3&amp;1&amp;6&amp;-1\\
-1&amp;1&amp;-1&amp;0&amp;4&amp;-3\\
-3&amp;2&amp;-4&amp;-4&amp;-7&amp;0\\
3&amp;-1&amp;5&amp;2&amp;2&amp;3\end{bmatrix}.
\end{align*}

Row-reducing $A$ gives the matrix

\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;2&amp;0&amp;-1&amp;2\\
0&amp;\boxed{1}&amp;1&amp;0&amp;3&amp;-1\\
0&amp;0&amp;0&amp;\boxed{1}&amp;4&amp;-2\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

First, the non-pivot columns have indices $F=\{3,\,5,\,6\}$, so we will construct the $n-r=6-3=3$ vectors with a pattern of zeros and ones dictated by the indices in $F$. This is the realization of the first two lines of the three-case definition of the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$.

\begin{align*}
\displaystyle\mathbf{z}_{1}&amp;\displaystyle=\begin{bmatrix}\\
\\
1\\
\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{2}&amp;\displaystyle=\begin{bmatrix}\\
\\
0\\
\\
1\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{3}&amp;\displaystyle=\begin{bmatrix}\\
\\
0\\
\\
0\\
1\end{bmatrix}.
\end{align*}

Each of these vectors arises due to the presence of a non-pivot column. The remaining entries of each vector are the entries of the non-pivot column, negated, and distributed into the empty slots in order (these slots have indices in the set $D$, so also refer to pivot columns). This is the realization of the third line of the three-case definition of the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$.

\begin{align*}
\displaystyle\mathbf{z}_{1}&amp;\displaystyle=\begin{bmatrix}-2\\
-1\\
1\\
0\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{2}&amp;\displaystyle=\begin{bmatrix}1\\
-3\\
0\\
-4\\
1\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{3}&amp;\displaystyle=\begin{bmatrix}-2\\
1\\
0\\
2\\
0\\
1\end{bmatrix}.
\end{align*}

So we have

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left&lt;\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3}\}\right&gt;=\left&lt;\{\begin{bmatrix}-2\\
-1\\
1\\
0\\
0\\
0\end{bmatrix},\,\begin{bmatrix}1\\
-3\\
0\\
-4\\
1\\
0\end{bmatrix},\,\begin{bmatrix}-2\\
1\\
0\\
2\\
0\\
1\end{bmatrix}\}\right&gt;.
\end{align*}
@end

@section{Span by using few vectors}

@eg
<b>Span of the columns</b>Begin with the following set of four vectors of size $3$:

\begin{align*}
\displaystyle T=\{\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3},\,\mathbf{w}_{4}\}=\{\begin{bmatrix}2\\
-3\\
1\end{bmatrix},\,\begin{bmatrix}1\\
4\\
1\end{bmatrix},\,\begin{bmatrix}7\\
-5\\
4\end{bmatrix},\,\begin{bmatrix}-7\\
-6\\
-5\end{bmatrix}\}.
\end{align*}

Let

\begin{align*}
\displaystyle D=\begin{bmatrix}2&amp;1&amp;7&amp;-7\\
-3&amp;4&amp;-5&amp;-6\\
1&amp;1&amp;4&amp;-5\end{bmatrix}
\end{align*}

and consider the infinite set $W=\left&lt;T\right&gt;$. Check that the vector

\begin{align*}
\displaystyle \mathbf{z}_{2}=\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}
\end{align*}

is a solution to the homogeneous system $D\mathbf{x}=\mathbf{0}$

We can write the linear combination,

\begin{align*}
\displaystyle 2\mathbf{w}_{1}+3\mathbf{w}_{2}+0\mathbf{w}_{3}+1\mathbf{w}_{4}=\mathbf{0}
\end{align*}

which we can solve for $\mathbf{w}_{4}$ as

\begin{align*}
\displaystyle \mathbf{w}_{4}=(-2)\mathbf{w}_{1}+(-3)\mathbf{w}_{2}.
\end{align*}

This equation says that whenever we encounter the vector $\mathbf{w}_{4}$, we can replace it with a specific linear combination of the vectors $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$. So using $\mathbf{w}_{4}$ in the set $T$, along with $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$, is excessive. An example of what we mean here can be illustrated by the computation,

\begin{align*}
\displaystyle 5\mathbf{w}_{1}&amp;\displaystyle+(-4)\mathbf{w}_{2}+6\mathbf{w}_{3}+(-3)\mathbf{w}_{4} \\
&amp;\displaystyle=5\mathbf{w}_{1}+(-4)\mathbf{w}_{2}+6\mathbf{w}_{3}+(-3)\left((-2)\mathbf{w}_{1}+(-3)\mathbf{w}_{2}\right) \\
&amp;\displaystyle=5\mathbf{w}_{1}+(-4)\mathbf{w}_{2}+6\mathbf{w}_{3}+\left(6\mathbf{w}_{1}+9\mathbf{w}_{2}\right) \\
&amp;\displaystyle=11\mathbf{w}_{1}+5\mathbf{w}_{2}+6\mathbf{w}_{3}.
\end{align*}

So, what began as a linear combination of the vectors $\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3},\,\mathbf{w}_{4}$ has been reduced to a linear combination of the vectors $\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3}$. So

\begin{align*}
\displaystyle W=\left&lt;\left\{\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3}\right\}\right&gt;,
\end{align*}

and the span of our set of vectors, $W$, has not changed, but we have described it by the span of a set of three vectors, rather than four. Furthermore, we can achieve yet another, similar, reduction.

Check that the vector

\begin{align*}
\displaystyle \mathbf{z}_{1}=\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}
\end{align*}

is a solution to the homogeneous system $D\mathbf{x}=\mathbf{0}$.
We can write the linear combination,

\begin{align*}
\displaystyle (-3)\mathbf{w}_{1}+(-1)\mathbf{w}_{2}+1\mathbf{w}_{3}=\mathbf{0}
\end{align*}

which we can solve for $\mathbf{w}_{3}$ as

\begin{align*}
\displaystyle \mathbf{w}_{3}=3\mathbf{w}_{1}+1\mathbf{w}_{2}.
\end{align*}

This equation says that whenever we encounter the vector $\mathbf{w}_{3}$, we can replace it with a specific linear combination of the vectors $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$. So, as before, the vector $\mathbf{w}_{3}$ is not needed in the description of $W$, provided we have $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$ available. In particular, a careful proof would show that

\begin{align*}
\displaystyle W=\left&lt;\left\{\mathbf{w}_{1},\,\mathbf{w}_{2}\right\}\right&gt;
\end{align*}

So $W$ began life as the span of a set of four vectors. We have now shown (utilizing solutions to a homogeneous system) that $W$ can also be described as the span of a set of just two vectors. Convince yourself that we cannot go any further. In other words, it is not possible to dismiss either $\mathbf{w}_{1}$ or $\mathbf{w}_{2}$ in a similar fashion and winnow the set down to just one vector.

What was it about the original set of four vectors that allowed us to declare certain vectors as surplus? And just which vectors were we able to dismiss? And why did we have to stop once we had two vectors remaining? The answers to these questions motivate <b>linear independence</b>, our next section and next definition, and so are worth considering carefully now.
@end

@chapter{Linear independence}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The notes may be updated later. Check online for the latest version. <b>Last updated</b>: August 19, 2019The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section LI (print version p95 - p104)Strang, Section 2.3

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection LI (p.40-48) (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions)
C20-25, C30-33, C60, M20, M21, M50, M51, T10-13, T15, T20, T50. Strang, Section 2.3

<b>Linear independence</b> is one of the most fundamental conceptual ideas in linear algebra, along with the notion of a span. So this lecture, and the subsequent, will explore this new idea.

@section{Linearly Independent Sets of Vectors}
@label{LISV}
@defn
@title{Relation of Linear Dependence for Column Vectors}
@label{RLDCV}
Given a set of vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$, an equality of the form

\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{0}
\end{align*}

is a <b>relation of linear dependence</b> on $S$. If this equality is formed in a trivial fashion, i.e., $\alpha_{i}=0$, $1\leq i\leq n$, then we say that it is the <b>trivial relation of linear dependence</b> on $S$.
@end
@defn
@title{Linear Independence of Column Vectors}
@label{LICV}
The set of vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ is <b>linearly dependent</b> if there is a relation of linear dependence on $S$ that is not trivial. In the case where the only relation of linear dependence on $S$ is the trivial one, then $S$ is a <b>linearly independent</b> set of vectors.
@end
@eg
@label{LDS}
<b>Linearly dependent set in ${\mathbb{R}}^{5}$</b>
Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$:

\begin{align*}
\displaystyle S=\{\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix},\}
\end{align*}

To determine linear independence, we first form an arbitrary relation of linear dependence,

\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+\alpha_{2}\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+\alpha_{3}\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+\alpha_{4}\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}=\mathbf{0}
\end{align*}

We know that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$ is a solution to this equation, but that is of no interest whatsoever. That is always the case, no matter what four vectors we might have chosen. We are curious to know if there are other, nontrivial, solutions. Row-reducing the associated matrix gives

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;1&amp;2&amp;-6\\
-1&amp;2&amp;1&amp;7\\
3&amp;-1&amp;-3&amp;-1\\
1&amp;5&amp;6&amp;0\\
2&amp;2&amp;1&amp;1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-2\\
0&amp;\boxed{1}&amp;0&amp;4\\
0&amp;0&amp;\boxed{1}&amp;-3\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

We could solve the corresponding homogeneous system completely, but for this example all we need is one nontrivial solution. Setting the lone free variable to any nonzero value, such as $x_{4}=1$, yields the nontrivial solution

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}2\\
-4\\
3\\
1\end{bmatrix}.
\end{align*}

Completing our application of Lecture 7 Theorem 2, we have

\begin{align*}
\displaystyle 2\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+(-4)\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+3\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+1\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}=\mathbf{0}.
\end{align*}

This is a relation of linear dependence on $S$ that is not trivial, so we conclude that $S$ is linearly dependent.
@end
@eg
@label{LIS}
<b>Linearly independent set in ${\mathbb{R}}^{5}$</b>

Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$:

\begin{align*}
\displaystyle T=\{\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
7\\
-1\\
1\\
1\end{bmatrix}\}.
\end{align*}

To determine linear independence we first form an arbitrary relation of linear dependence,

\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+\alpha_{2}\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+\alpha_{3}\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+\alpha_{4}\begin{bmatrix}-6\\
7\\
-1\\
1\\
1\end{bmatrix}=\mathbf{0}.
\end{align*}

We know that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$ is a solution to this equation, but that is of no interest whatsoever. Row-reducing the associated matrix gives

\begin{align*}
\displaystyle B=\begin{bmatrix}2&amp;1&amp;2&amp;-6\\
-1&amp;2&amp;1&amp;7\\
3&amp;-1&amp;-3&amp;-1\\
1&amp;5&amp;6&amp;1\\
2&amp;2&amp;1&amp;1\end{bmatrix}&amp;\displaystyle\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

From the form of this matrix, we see that there are no free variables. Hence the associated homogeneous linear system has only the trivial solution. So we now know that there is but one way to combine the four vectors of $T$ into a relation of linear dependence, and that this one way is the easy and obvious way. Hence, the set $T$ is linearly independent.
@end

The above examples relied on solving a homogeneous system of equations to determine linear independence. We can codify this process in a time-saving theorem.

@thm
@title{Linearly Independent Vectors and Homogeneous Systems}
@label{LIVHS}
Suppose that $S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\}\subseteq{\mathbb{R}}^{m}$ is a set of vectors and that $A$ is the $m\times n$ matrix whose columns are the vectors in $S$. Then $S$ is a linearly independent set if and only if the homogeneous system $A\mathbf{x}=\mathbf{0}$ has a unique solution.
@end
@proof
@col
($\Leftarrow$) Suppose that $A\mathbf{x}=\mathbf{0}$ has a unique solution. Since it is a homogeneous system, this solution must be the trivial solution, $\mathbf{x}=\mathbf{0}$. This means that the only relation of linear dependence on $S$ is the trivial one. So $S$ is linearly independent.

($\Rightarrow$) We will prove the contrapositive. Suppose that $A\mathbf{x}=\mathbf{0}$ does not have a unique solution. Since it is a homogeneous system, it is consistent. And so must have infinitely many solutions (Lecture 10 Theorem 13). One of these infinitely many solutions must be nontrivial (in fact, almost all of them are); choose one. This nontrivial solution will give a nontrivial relation of linear dependence on $S$. We therefore conclude that $S$ is a linearly dependent set.
∎
@end

Since the above theorem is an equivalence, we can use it to determine the linear independence or dependence of any set of column vectors, just by creating a matrix and analyzing its row-reduced echelon form. Let us illustrate this with two more examples.

@eg
@label{LIHS}
<b>Linearly independent, homogeneous system</b>  Is the set of vectors

\begin{align*}
\displaystyle S=\{\begin{bmatrix}2\\
-1\\
3\\
4\\
2\end{bmatrix},\,\begin{bmatrix}6\\
2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}4\\
3\\
-4\\
5\\
1\end{bmatrix}\}
\end{align*}

linearly independent or linearly dependent?
@end

<b>Answer.</b>
The above theorem suggests that we study the matrix $A$ whose columns are the vectors in $S$. Specifically, we are interested in the size of the solution set of the homogeneous system $A\mathbf{x}=\mathbf{0}$. Row-reducing $A$ gives

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;6&amp;4\\
-1&amp;2&amp;3\\
3&amp;-1&amp;-4\\
4&amp;3&amp;5\\
2&amp;4&amp;1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0\\
0&amp;0&amp;\boxed{1}\\
0&amp;0&amp;0\\
0&amp;0&amp;0\end{bmatrix}.
\end{align*}

We have $r=3$, so there are $n-r=3-3=0$ free variables. Hence $A\mathbf{x}=\mathbf{0}$ has a unique solution. By the above theorem, the set $S$ is linearly independent.
$\square$

@eg
@label{LDHS}
<b>Linearly dependent, homogeneous system</b>
Is the set of vectors

\begin{align*}
\displaystyle S=\{\begin{bmatrix}2\\
-1\\
3\\
4\\
2\end{bmatrix},\,\begin{bmatrix}6\\
2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}4\\
3\\
-4\\
-1\\
2\end{bmatrix}\}
\end{align*}

linearly independent or linearly dependent?
@end

<b>Answer.</b>
Theorem  @ref{LIVHS} suggests that we study the matrix $A$ whose columns are the vectors in $S$. Specifically, we are interested in the size of the solution set of the homogeneous system $A\mathbf{x}=\mathbf{0}$. Row-reducing $A$ gives

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;6&amp;4\\
-1&amp;2&amp;3\\
3&amp;-1&amp;-4\\
4&amp;3&amp;-1\\
2&amp;4&amp;2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;-1\\
0&amp;\boxed{1}&amp;1\\
0&amp;0&amp;0\\
0&amp;0&amp;0\\
0&amp;0&amp;0\end{bmatrix}.
\end{align*}

We have $r=2$, so there are $n-r=3-2=1$ free variables. Hence $A\mathbf{x}=\mathbf{0}$ has infinitely many solutions. By
Theorem  @ref{LIVHS}, the set $S$ is linearly dependent.
$\square$

As an equivalence, Theorem  @ref{LIVHS} gives us a straightforward way to determine if a set of vectors is linearly independent or dependent.

Review the previous two examples. They are very similar, differing only in the last two slots of the third vector. This resulted in slightly different matrices when row-reduced, and different values of $r$, the number of nonzero rows. Notice, too, that we are less interested in the actual solution set, and more interested in its form or size. These observations allow us to make a slight improvement on Theorem  @ref{LIVHS}.

@thm
@title{Linearly Independent Vectors, $r$ and $n$}
@label{LIVRN}
Suppose that

\begin{align*}
\displaystyle S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\}\subseteq{\mathbb{R}}^{m}
\end{align*}

is a set of vectors and that $A$ is the $m\times n$ matrix whose columns are the vectors in $S$. Let $B$ be a matrix in reduced row-echelon form that is row-equivalent to $A$ and let $r$ denote the number of pivot columns in $B$. Then $S$ is linearly independent if and only if $n=r$.
@end
@proof
@col
Theorem  @ref{LIVHS} says the linear independence of $S$ is equivalent to the homogeneous linear system $A\mathbf{x}=\mathbf{0}$ having a unique solution. Since the zero vector is a solution of $A\mathbf{x}=\mathbf{0}$,
$A\mathbf{x}=\mathbf{0}$ is consistent. We can therefore can apply Lecture 7 Theorem 3 to see that the solution is unique exactly when $n=r$.
∎
@end

Here is an example of the most straightforward way to determine if a set of column vectors is linearly independent or linearly dependent. While this method can be quick and easy, do not forget the logical progression from the definition of linear independence through homogeneous system of equations which makes it possible.

@eg
<b>Linearly dependent, $r$ and $n$</b> Is the set of vectors

\begin{align*}
\displaystyle S=\{\begin{bmatrix}2\\
-1\\
3\\
1\\
0\\
3\end{bmatrix},\,\begin{bmatrix}9\\
-6\\
-2\\
3\\
2\\
1\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
0\\
0\\
1\end{bmatrix},\,\begin{bmatrix}-3\\
1\\
4\\
2\\
1\\
2\end{bmatrix},\,\begin{bmatrix}6\\
-2\\
1\\
4\\
3\\
2\end{bmatrix}\}
\end{align*}

linearly independent or linearly dependent?
@end

<b>Answer.</b>
Theorem  @ref{LIVRN} suggests that we take the vectors of $S$ as the columns of a matrix and then analyze its reduced row-echelon form:

\begin{align*}
\displaystyle \begin{bmatrix}2&amp;9&amp;1&amp;-3&amp;6\\
-1&amp;-6&amp;1&amp;1&amp;-2\\
3&amp;-2&amp;1&amp;4&amp;1\\
1&amp;3&amp;0&amp;2&amp;4\\
0&amp;2&amp;0&amp;1&amp;3\\
3&amp;1&amp;1&amp;2&amp;2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;-1\\
0&amp;\boxed{1}&amp;0&amp;0&amp;1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;1\\
0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

Now we need only compute that
$r=4&lt;5=n$
to recognize, via Theorem  @ref{LIVRN}, that $S$ is a linearly dependent set. Boom!
$\square$

@eg
<b>Large linearly dependent set in ${\mathbb{R}}^{4}$</b>

Consider the set of $n=9$ vectors from ${\mathbb{R}}^{4}$,

\begin{align*}
\displaystyle R=\{\begin{bmatrix}-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}7\\
1\\
-3\\
6\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}0\\
4\\
2\\
9\end{bmatrix},\,\begin{bmatrix}5\\
-2\\
4\\
3\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-6\\
4\end{bmatrix},\,\begin{bmatrix}3\\
0\\
-3\\
1\end{bmatrix},\,\begin{bmatrix}1\\
1\\
5\\
3\end{bmatrix},\,\begin{bmatrix}-6\\
-1\\
1\\
1\end{bmatrix}\}.
\end{align*}

To employ Theorem  @ref{LIVHS}, we form a $4\times 9$ matrix $C$ whose columns are the vectors in $R$:

\begin{align*}
\displaystyle C=\begin{bmatrix}-1&amp;7&amp;1&amp;0&amp;5&amp;2&amp;3&amp;1&amp;-6\\
3&amp;1&amp;2&amp;4&amp;-2&amp;1&amp;0&amp;1&amp;-1\\
1&amp;-3&amp;-1&amp;2&amp;4&amp;-6&amp;-3&amp;5&amp;1\\
2&amp;6&amp;-2&amp;9&amp;3&amp;4&amp;1&amp;3&amp;1\end{bmatrix}.
\end{align*}

To determine if the homogeneous system $C\mathbf{x}=\mathbf{0}$ has a unique solution or not, we would normally row-reduce this matrix. But in this particular example, we can do better. Lecture 7 Theorem 6 tells us that since the system is homogeneous with $n=9$ variables in $m=4$ equations, and $n&gt;m$, there are infinitely many solutions. Since there is not a unique solution, Theorem  @ref{LIVHS} says the set $R$ is linearly dependent.
@end

The following theorem generalizes the previous example.

@thm
@title{More Vectors than Size implies Linear Dependence}
@label{MVSLD}
Suppose that $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}\subseteq{\mathbb{R}}^{m}$ and $n&gt;m$. Then $S$ is a linearly dependent set.
@end
@proof
@col
Form the $m\times n$ matrix $A$ whose columns are $\mathbf{u}_{i}$, $1\leq i\leq n$. Consider the homogeneous system $A\mathbf{x}=\mathbf{0}$. By Lecture 8 Theorem 4 this system has infinitely many solutions. Since the system does not have a unique solution, Theorem  @ref{LIVHS} says the columns of $A$ form a linearly dependent set, as desired.
∎
@end

@section{Linear Independence and Nonsingular Matrices}
@label{LINM}
We will now specialize to sets of $n$ vectors in ${\mathbb{R}}^{n}$. This will put Theorem  @ref{MVSLD} off-limits, while Theorem  @ref{LIVHS} will involve square matrices.

@eg
<b>Linearly dependent columns</b> Do the columns of the matrix

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;-1&amp;2\\
2&amp;1&amp;1\\
1&amp;1&amp;0\end{bmatrix}
\end{align*}

form a linearly independent or dependent set?
@end

<b>Answer.</b>
We can show that $A$ is singular.
According to the definition of nonsingular matrices, the homogeneous system $A\mathbf{x}=\mathbf{0}$ has infinitely many solutions. So, by Theorem  @ref{LIVHS}, the columns of $A$ form a linearly dependent set.
$\square$

@eg
<b>Linearly independent columns</b>
Do the columns of this matrix

\begin{align*}
\displaystyle B=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}
\end{align*}

form a linearly independent or dependent set?
@end

<b>Answer.</b>
We can show that $B$ is nonsingular. According to the definition of nonsingular matrices, the homogeneous system $A\mathbf{x}=\mathbf{0}$ has a unique solution. So, by Theorem  @ref{LIVHS}, the columns of $B$ form a linearly independent set.
$\square$

That the previous two examples have opposite properties for the columns of their coefficient matrices is no accident. Here is the theorem, and then we will update our equivalences for nonsingular matrices.

@thm
@title{Nonsingular Matrices have Linearly Independent Columns}
@label{NMLIC}
Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if the columns of $A$ form a linearly independent set.
@end
@proof
@col
This is a proof where we can chain together equivalences, rather than proving the two halves separately.

\begin{align*}
A$ nonsingula&amp;\displaystyle\iff\text{$A\mathbf{x}=\mathbf{0}$ has a unique solution} \\
&amp;\displaystyle\iff\text{columns of $A$ are linearly independent}
\end{align*}

∎
@end

Here is the update to Lecture 8 Theorem 12.

@thm
@title{Nonsingular Matrix Equivalences, Round 2}
@label{NME2}
Suppose that $A$ is a square matrix. The following are equivalent.

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is nonsingular.
</li>
<li class="ltx_item">
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item">
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
</li>
<li class="ltx_item">
The linear system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>
<li class="ltx_item">
The columns of $A$ form a linearly independent set.

</li>

</ol>
@end
@proof
@col
The above theorem is yet another equivalence for a nonsingular matrix, so we can add it to the list in Lecture 8 Theorem 12.
∎
@end

@section{Null Spaces, Spans, Linear Independence}

In this section, we will find a linearly independent set that spans a null space.
In Lecture 10 Section 2, we proved Lecture 10 Theorem 4, showing that a particular set of $n-r$ vectors that could be used to span the null space of a matrix.

@eg
@label{LINSB}
<b>Linear independence of null space basis</b>
Suppose that we are interested in the null space of a $3\times 7$ matrix $A$ which row-reduces to

\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;-2&amp;4&amp;0&amp;3&amp;9\\
0&amp;\boxed{1}&amp;5&amp;6&amp;0&amp;7&amp;1\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;8&amp;-5\end{bmatrix}.
\end{align*}

Then $F=\{3,\,4,\,6,\,7\}$ is the set of indices for our four free variables that would be used in a description of the solution set for the homogeneous system $A\mathbf{x}=\mathbf{0}$. Applying Lecture 8 Theorem 4, we can begin to construct a set of four vectors whose span is the null space of $A$, a set of vectors we will refer to as $T$.

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left&lt;T\right&gt;=\left&lt;\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\mathbf{z}_{4}\}\right&gt;=\left&lt;\{\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix}\}\right&gt;
\end{align*}

So far, we have constructed as much of these individual vectors as we can, based just on the knowledge of the contents of the set $F$. This has allowed us to determine the entries in slots 3, 4, 6 and 7, while we have left slots 1, 2 and 5 blank. Without doing any more, let us ask if $T$ is linearly independent? Begin with a relation of linear dependence on $T$, and see what we can learn about the scalars.

\begin{align*}
\displaystyle\mathbf{0}&amp;\displaystyle=\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\alpha_{4}\mathbf{z}_{4} \\
\displaystyle\begin{bmatrix}0\\
0\\
0\\
0\\
0\\
0\\
0\end{bmatrix}&amp;\displaystyle=\alpha_{1}\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix}+\alpha_{2}\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix}+\alpha_{3}\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix}+\alpha_{4}\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}\\
\\
\alpha_{1}\\
0\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
\alpha_{2}\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
\alpha_{3}\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
\alpha_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
\alpha_{1}\\
\alpha_{2}\\
\\
\alpha_{3}\\
\alpha_{4}\end{bmatrix}
\end{align*}

Applying the equalities of vectors, we see that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$. So the only relation of linear dependence on the set $T$ is the trivial one. By the definition of linear independence, the set $T$ is linearly independent. The important feature of this example is how the <b>pattern of zeros and ones</b> in the four vectors led to the conclusion of linear independence.
@end

The proof of theorem  @ref{BNS} is quite straightforward, and relies on the <b>pattern of zeros and ones</b> that arise in the vectors $\mathbf{z}_{i}$, $1\leq i\leq n-r$, in the entries that arise with the locations of the non-pivot columns.

@thm
@title{Basis for Null Spaces}
@label{BNS}
Suppose that $A$ is an $m\times n$ matrix, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ and $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$ be the sets of column indices of $B$ which are and are not, respectively, pivot columns. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$ as

\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&amp;\text{if $i\in F$, $i=f_{j}$}\\
0&amp;\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&amp;\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}

(In fact $\mathbf{z}_{j}$ corresponding to the solution $x_{f_{j}}=1$ and $x_{f_{k}}=0$ for $k\neq j$.) Define the set $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
${\mathcal{N}}\!\left(A\right)=\left&lt;S\right&gt;$.
</li>
<li class="ltx_item">
$S$ is a linearly independent set.
</li>

</ol>
@end
@proof
@col
Study the above example. You can skip the proof for now. Notice first that the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$, are the same as the $n-r$ vectors defined in Lecture 10 Theorem 4. Also, the hypotheses of Lecture 10 Theorem 4 are the same as the hypotheses of the theorem we are currently proving. So Lecture 8 Theorem 4 tells us that ${\mathcal{N}}\!\left(A\right)=\left&lt;S\right&gt;$. That was the easy half, but the second part is not much harder. What is new here is the claim that $S$ is a linearly independent set.

To prove the linear independence of a set, we need to start with a relation of linear dependence and somehow conclude that the scalars involved must all be zero, i.e., that the relation of linear dependence is trivial. So, we start with and equation of the form

\begin{align*}
\displaystyle \alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}=\mathbf{0}.
\end{align*}

For each $j$, $1\leq j\leq n-r$, consider the equality of the individual entries of the vectors on both sides of this equality in position $f_{j}$:

\begin{align*}
\displaystyle 0&amp;\displaystyle=\left[\mathbf{0}\right]_{f_{j}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}\right]_{f_{j}}+\left[\alpha_{2}\mathbf{z}_{2}\right]_{f_{j}}+\left[\alpha_{3}\mathbf{z}_{3}\right]_{f_{j}}+\cdots+\left[\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&amp;\displaystyle=\alpha_{1}\left[\mathbf{z}_{1}\right]_{f_{j}}+\alpha_{2}\left[\mathbf{z}_{2}\right]_{f_{j}}+\alpha_{3}\left[\mathbf{z}_{3}\right]_{f_{j}}+\cdots+ \\
&amp;\displaystyle\quad\quad\alpha_{j-1}\left[\mathbf{z}_{j-1}\right]_{f_{j}}+\alpha_{j}\left[\mathbf{z}_{j}\right]_{f_{j}}+\alpha_{j+1}\left[\mathbf{z}_{j+1}\right]_{f_{j}}+\cdots+ \\
&amp;\displaystyle\quad\quad\alpha_{n-r}\left[\mathbf{z}_{n-r}\right]_{f_{j}} \\
&amp;\displaystyle=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+ \\
&amp;\displaystyle\quad\quad\alpha_{j-1}(0)+\alpha_{j}(1)+\alpha_{j+1}(0)+\cdots+\alpha_{n-r}(0)&amp;definition of $\mathbf{z}_{j}$ \\
&amp;\displaystyle=\alpha_{j}
\end{align*}

So for all $j$, $1\leq j\leq n-r$, we have $\alpha_{j}=0$. Hence, the only relation of linear dependence on $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$ is the trivial one. By the definition of linear independence, the set is linearly independent, as desired.
∎
@end
@eg
Find the null space of the matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}-2&amp;-1&amp;-2&amp;-4&amp;4\\
-6&amp;-5&amp;-4&amp;-4&amp;6\\
10&amp;7&amp;7&amp;10&amp;-13\\
-7&amp;-5&amp;-6&amp;-9&amp;10\\
-4&amp;-3&amp;-4&amp;-6&amp;6\\
\end{bmatrix}.
\end{align*}
@end

<b>Answer.</b>
The RREF of $A$ is

\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;1&amp;-2\\
0&amp;\boxed{1}&amp;0&amp;-2&amp;2\\
0&amp;0&amp;\boxed{1}&amp;2&amp;-1\\
0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

The free variables are $x_{4}$ and $x_{5}$.Setting $x_{4}=1$ and $x_{5}=0$ gives

\begin{align*}
\displaystyle \mathbf{z}_{1}=\begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix}.
\end{align*}

Setting instead $x_{4}=0$ and $x_{5}=1$ gives

\begin{align*}
\displaystyle \mathbf{z}_{2}=\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}.
\end{align*}

Hence

\begin{align*}
\displaystyle {\mathcal{N}}\!\left(L\right)=\left&lt;\begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix},\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}\right&gt;.
\end{align*}

$\square$

@chapter{Linear Dependence and Span}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section LDS (print version p105 - p113)Strang, Sect 2.3

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection LI (p.48-51) (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions)
C20, C40, C50, C51, C52, C55, C70, M10, T40.Strang, Sect 2.3

@section{Linearly Dependent Sets and Spans}

If we use a linearly dependent set to construct a span, then we can always create the same infinite set by starting with a set that is one vector smaller in size. We will illustrate this behavior in Example  @ref{RSC5}. However, this will not be possible if we build a span from a linearly independent set. So, in a certain sense, using a linearly independent set to formulate a span is the best possible way – there are no any extra vectors being used to build up all the necessary linear combinations. OK, here is the theorem, and then the example.

@thm
@title{Dependency in Linearly Dependent Sets}
@label{DLDS}
Suppose that $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ is a set of vectors. Then $S$ is a linearly dependent set if and only if there is an index $t$, $1\leq t\leq n$, such that $\mathbf{u_{t}}$ is a linear combination of the vectors $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{t-1},\,\mathbf{u}_{t+1},\,\ldots,\,\mathbf{u}_{n}$.
@end
@proof
@col
($\Rightarrow$) Suppose that $S$ is linearly dependent. Then there exists a nontrivial relation of linear dependence (Lecture 12 Definition 1). That is, there are scalars, $\alpha_{i}$, $1\leq i\leq n$, not all of which are zero, such that

\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{0}.
\end{align*}

Suppose that $\alpha_{t}$ is nonzero. Then,

\begin{align*}
\displaystyle\mathbf{u}_{t}&amp;\displaystyle=\frac{-1}{\alpha_{t}}\left(-\alpha_{t}\mathbf{u}_{t}\right) \\
&amp;\displaystyle=\frac{-1}{\alpha_{t}}\left(\alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{t-1}\mathbf{u}_{t-1}+\alpha_{t+1}\mathbf{u}_{t+1}+\cdots+\alpha_{n}\mathbf{u}_{n}\right) \\
&amp;\displaystyle=\frac{-\alpha_{1}}{\alpha_{t}}\mathbf{u}_{1}+\cdots+\frac{-\alpha_{t-1}}{\alpha_{t}}\mathbf{u}_{t-1}+\frac{-\alpha_{t+1}}{\alpha_{t}}\mathbf{u}_{t+1}+\cdots+\frac{-\alpha_{n}}{\alpha_{t}}\mathbf{u}_{n}.
\end{align*}

Since $\frac{\alpha_{i}}{\alpha_{t}}$ is again a scalar, we have expressed $\mathbf{u}_{t}$ as a linear combination of the other elements of $S$.

($\Leftarrow$) Assume that the vector $\mathbf{u}_{t}$ is a linear combination of the other vectors in $S$. Write such a linear combination as

\begin{align*}
\displaystyle\mathbf{u_{t}}&amp;\displaystyle=\beta_{1}\mathbf{u}_{1}+\beta_{2}\mathbf{u}_{2}+\cdots+\beta_{t-1}\mathbf{u}_{t-1}+\beta_{t+1}\mathbf{u}_{t+1}+\cdots+\beta_{n}\mathbf{u}_{n}.
\end{align*}

Then we have

\begin{align*}
\displaystyle\beta_{1}\mathbf{u}_{1}&amp;\displaystyle+\cdots+\beta_{t-1}\mathbf{u}_{t-1}+(-1)\mathbf{u}_{t}+\beta_{t+1}\mathbf{u}_{t+1}+\cdots+\beta_{n}\mathbf{u}_{n} \\
&amp;\displaystyle=\mathbf{u}_{t}+(-1)\mathbf{u}_{t} \\
&amp;\displaystyle=\left(1+\left(-1\right)\right)\mathbf{u}_{t} \\
&amp;\displaystyle=0\mathbf{u}_{t} \\
&amp;\displaystyle=\mathbf{0}.
\end{align*}

So the scalars $\beta_{1},\,\beta_{2},\,\beta_{3},\,\ldots,\,\beta_{t-1},\,\beta_{t}=-1,\beta_{t+1},\,\,\ldots,\,\beta_{n}$ provide a nontrivial relation of linear dependence of the vectors in $S$, thus establishing that $S$ is a linearly dependent set.
∎
@end

This theorem can be used, sometimes repeatedly, to whittle down the size of a set of vectors used in a span construction.
In the next example we will detail some of the subtleties.

@eg
@label{RSC5}
<b>Reducing a span in ${\mathbb{R}}^{5}$</b>Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$,

\begin{align*}
\displaystyle R=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\mathbf{v}_{4}\}=\{\begin{bmatrix}1\\
2\\
-1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}0\\
-7\\
6\\
-11\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
1\\
2\\
1\\
6\end{bmatrix}\}.\\

\end{align*}

Define $V=\left&lt;R\right&gt;$.

We form a $5\times 4$ matrix, $D$, and row-reduce it to understand the solutions to the homogeneous system $D\mathbf{x}=\mathbf{0}$:

\begin{align*}
\displaystyle D=\begin{bmatrix}1&amp;2&amp;0&amp;4\\
2&amp;1&amp;-7&amp;1\\
-1&amp;3&amp;6&amp;2\\
3&amp;1&amp;-11&amp;1\\
2&amp;2&amp;-2&amp;6\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;4\\
0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;1\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

We can find infinitely many solutions to the system $D\mathbf{x}=\mathbf{0}$, most of which are nontrivial. Choose any nontrivial solution to build a nontrivial relation of linear dependence on $R$. Let us begin with $x_{4}=1$, to find the solution

\begin{align*}
\displaystyle \begin{bmatrix}-4\\
0\\
-1\\
1\end{bmatrix}.
\end{align*}

The corresponding relation of linear dependence is

\begin{align*}
\displaystyle (-4)\mathbf{v}_{1}+0\mathbf{v}_{2}+(-1)\mathbf{v}_{3}+1\mathbf{v}_{4}=\mathbf{0}.
\end{align*}

The theorem above guarantees that we can solve this relation of linear dependence for some vector in $R$, but the choice of which one is up to us. Notice however that $\mathbf{v}_{2}$ has a zero coefficient. In this case, we cannot choose to solve for $\mathbf{v}_{2}$. Maybe some other relation of linear dependence would produce a nonzero coefficient for $\mathbf{v}_{2}$ if we just had to solve for this vector. Unfortunately, this example has been engineered to always produce a zero coefficient here, as you can see from solving the homogeneous system. Every solution has $x_{2}=0$!

OK, if we are convinced that we cannot solve for $\mathbf{v}_{2}$, let us instead solve for $\mathbf{v}_{3}$:

\begin{align*}
\displaystyle \mathbf{v}_{3}=(-4)\mathbf{v}_{1}+0\mathbf{v}_{2}+1\mathbf{v}_{4}=(-4)\mathbf{v}_{1}+1\mathbf{v}_{4}
\end{align*}

We claim that this particular equation will allow us to write

\begin{align*}
\displaystyle V=\left&lt;R\right&gt;=\left&lt;\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\mathbf{v}_{4}\}\right&gt;=\left&lt;\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{4}\}\right&gt;,
\end{align*}

in essence declaring $\mathbf{v}_{3}$ as surplus for the task of building $V$ as a span of $R$. This claim is an equality of two sets. Let $R^{\prime}=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{4}\}$ and $V^{\prime}=\left&lt;R^{\prime}\right&gt;$. We want to show that $V=V^{\prime}$.

First show that $V^{\prime}\subseteq V$. Since every vector of $R^{\prime}$ is in $R$, any vector we can construct in $V^{\prime}$ as a linear combination of vectors from $R^{\prime}$ can also be constructed as a vector in $V$ by the same linear combination of the same vectors in $R$. That was easy, now turn it around.

Next show that $V\subseteq V^{\prime}$. Choose any $\mathbf{v}$ from $V$. So there are scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4}$ such that

\begin{align*}
\displaystyle\mathbf{v}&amp;\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\mathbf{v}_{3}+\alpha_{4}\mathbf{v}_{4} \\
&amp;\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\left((-4)\mathbf{v}_{1}+1\mathbf{v}_{4}\right)+\alpha_{4}\mathbf{v}_{4} \\
&amp;\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\left((-4\alpha_{3})\mathbf{v}_{1}+\alpha_{3}\mathbf{v}_{4}\right)+\alpha_{4}\mathbf{v}_{4} \\
&amp;\displaystyle=\left(\alpha_{1}-4\alpha_{3}\right)\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\left(\alpha_{3}+\alpha_{4}\right)\mathbf{v}_{4}.
\end{align*}

This equation says that $\mathbf{v}$ can be written as a linear combination of the vectors in $R^{\prime}$ and hence qualifies for membership in $V^{\prime}$. So $V\subseteq V^{\prime}$ and we have established that $V=V^{\prime}$.

If $R^{\prime}$ was also linearly dependent (in fact, it is not), we could reduce the set $R^{\prime}$ even further. Notice that we could have chosen to eliminate any one of $\mathbf{v}_{1}$, $\mathbf{v}_{3}$ or $\mathbf{v}_{4}$, but somehow $\mathbf{v}_{2}$ is essential to the creation of $V$ since it cannot be replaced by any linear combination of $\mathbf{v}_{1}$, $\mathbf{v}_{3}$ or $\mathbf{v}_{4}$.
@end

@section{Casting Out Vectors}
@label{COV}
In Example 1 we used four vectors to create a span. With a relation of linear dependence in hand, we were able to toss out one of these four vectors and create the same span from a subset of just three of the original set of four vectors. We did have to take some care as to just which vector we tossed out. In the next example, we will be more methodical about just how we choose to eliminate vectors from a linearly dependent set while preserving a span.

@eg
<b>Casting out vectors</b> <b>Attention</b>: Important example. Read carefully. We begin with a set $S$ containing seven vectors in ${\mathbb{R}}^{4}$:

\begin{align*}
\displaystyle S=\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\,\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\,\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\,\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\,\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\,\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\}.
\end{align*}

Define $W=\left&lt;S\right&gt;$.

The set $S$ is obviously linearly dependent, since we have $n=7$ vectors from ${\mathbb{R}}^{4}$. So we can slim down $S$ some and create $W$ as the span of a smaller set of vectors.

As a device for identifying relations of linear dependence among the vectors of $S$, we place the seven vectors of $S$ into a matrix $A$ as columns:

\begin{align*}
\displaystyle A=\left[\mathbf{A}_{1}|\mathbf{A}_{2}|\mathbf{A}_{3}|\ldots|\mathbf{A}_{7}\right]=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix}.
\end{align*}

A nontrivial solution to $A\mathbf{x}=\mathbf{0}$ will give us a nontrivial relation of linear dependence
on the columns of $A$ (which are the elements of the set $S$). The row-reduced form of $A$ is the matrix

\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

So we can easily create solutions to the homogeneous system $A\mathbf{x}=\mathbf{0}$ using the free variables $x_{2},\,x_{5},\,x_{6},\,x_{7}$. Any such solution will provide a relation of linear dependence on the columns of $B$. These solutions will allow us to solve for one column vector as a linear combination the others, in the spirit of Theorem  @ref{DLDS}, and remove that vector from the set. We will set about forming these linear combinations methodically.

Set the free variable $x_{2}=1$, and set the other free variables to zero. Then a solution to $A\mathbf{x}=\mathbf{0}$ is

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}-4\\
1\\
0\\
0\\
0\\
0\\
0\end{bmatrix}.
\end{align*}

This can be used to create the linear combination

\begin{align*}
\displaystyle (-4)\mathbf{A}_{1}+1\mathbf{A}_{2}+0\mathbf{A}_{3}+0\mathbf{A}_{4}+0\mathbf{A}_{5}+0\mathbf{A}_{6}+0\mathbf{A}_{7}=\mathbf{0}
\end{align*}

which can then be arranged and solved for $\mathbf{A}_{2}$. This results in $\mathbf{A}_{2}$ being written as a linear combination of $\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4}\}$,

\begin{align*}
\displaystyle \mathbf{A}_{2}=4\mathbf{A}_{1}+0\mathbf{A}_{3}+0\mathbf{A}_{4}
\end{align*}

This means that $\mathbf{A}_{2}$ is surplus, and we can create $W$ just as well with a smaller set with this vector removed,

\begin{align*}
\displaystyle W=\left&lt;\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4},\,\mathbf{A}_{5},\,\mathbf{A}_{6},\,\mathbf{A}_{7}\}\right&gt;.
\end{align*}

Technically, this set equality for $W$ requires a proof, in the spirit of Example 1, but we will bypass this requirement here, and in the next few paragraphs.

Now, set the free variable $x_{5}=1$, and set the other free variables to zero. Then a solution to $B\mathbf{x}=\mathbf{0}$ is

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}-2\\
0\\
-1\\
-2\\
1\\
0\\
0\end{bmatrix}
\end{align*}

which can be used to create the linear combination

\begin{align*}
\displaystyle (-2)\mathbf{A}_{1}+0\mathbf{A}_{2}+(-1)\mathbf{A}_{3}+(-2)\mathbf{A}_{4}+1\mathbf{A}_{5}+0\mathbf{A}_{6}+0\mathbf{A}_{7}=\mathbf{0}.
\end{align*}

This can be arranged and solved for $\mathbf{A}_{5}$, resulting in $\mathbf{A}_{5}$ being expressed as a linear combination of $\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4}\}$,

\begin{align*}
\displaystyle \mathbf{A}_{5}=2\mathbf{A}_{1}+1\mathbf{A}_{3}+2\mathbf{A}_{4}.
\end{align*}

This means that $\mathbf{A}_{5}$ is surplus, and we can create $W$ just as well with a smaller set with this vector removed,

\begin{align*}
\displaystyle W=\left&lt;\left\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4},\,\mathbf{A}_{6},\,\mathbf{A}_{7}\right\}\right&gt;.
\end{align*}

Do it again; set the free variable $x_{6}=1$, and set the other free variables to zero. Then a solution to $B\mathbf{x}=\mathbf{0}$ is

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}-1\\
0\\
3\\
6\\
0\\
1\\
0\end{bmatrix}
\end{align*}

which can be used to create the linear combination

\begin{align*}
\displaystyle (-1)\mathbf{A}_{1}+0\mathbf{A}_{2}+3\mathbf{A}_{3}+6\mathbf{A}_{4}+0\mathbf{A}_{5}+1\mathbf{A}_{6}+0\mathbf{A}_{7}=\mathbf{0}.
\end{align*}

This can be arranged and solved for $\mathbf{A}_{6}$, resulting in $\mathbf{A}_{6}$ expressed as a linear combination of $\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4}\}$,

\begin{align*}
\displaystyle \mathbf{A}_{6}=1\mathbf{A}_{1}+(-3)\mathbf{A}_{3}+(-6)\mathbf{A}_{4}
\end{align*}

This means that $\mathbf{A}_{6}$ is surplus, and we can create $W$ just as well with a smaller set with this vector removed,

\begin{align*}
\displaystyle W=\left&lt;\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4},\,\mathbf{A}_{7}\}\right&gt;.
\end{align*}

Set the free variable $x_{7}=1$, and set the other free variables to zero. Then a solution to $B\mathbf{x}=\mathbf{0}$ is

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}3\\
0\\
-5\\
-6\\
0\\
0\\
1\end{bmatrix}
\end{align*}

which can be used to create the linear combination

\begin{align*}
\displaystyle 3\mathbf{A}_{1}+0\mathbf{A}_{2}+(-5)\mathbf{A}_{3}+(-6)\mathbf{A}_{4}+0\mathbf{A}_{5}+0\mathbf{A}_{6}+1\mathbf{A}_{7}=\mathbf{0}.
\end{align*}

This can be arranged and solved for $\mathbf{A}_{7}$, resulting in $\mathbf{A}_{7}$ expressed as a linear combination of $\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4}\}$,

\begin{align*}
\displaystyle \mathbf{A}_{7}=(-3)\mathbf{A}_{1}+5\mathbf{A}_{3}+6\mathbf{A}_{4}
\end{align*}

This means that $\mathbf{A}_{7}$ is surplus, and we can create $W$ just as well with a smaller set with this vector removed,

\begin{align*}
\displaystyle W=\left&lt;\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4}\}\right&gt;.
\end{align*}

You might think we could keep this up, but we have run out of free variables. And not coincidentally; the set $\{\mathbf{A}_{1},\,\mathbf{A}_{3},\,\mathbf{A}_{4}\}$ is linearly independent (check this!). It should be clear how each free variable was used to eliminate the a column from the set used to span the column space, as this will be the essence of the proof of the next theorem.

<b>Important</b>: The above shows that

<ol class="ltx_enumerate">
<li class="ltx_item">
The columns of $A$ corresponding to the pivot columns of the RREF $B$ form a linear independent set. The pivot column index of $B$ is $D=\{1,3,4\}$. So $\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\}$ is a linearly independent sets.
</li>
<li class="ltx_item">
All the other columns of $A$ can be written as linear combinations of $\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}$. In fact, the relation can be written explicitly. First, from the RREF $B$ we have

\begin{align*}
\displaystyle\mathbf{B}_{2}&amp;\displaystyle=4\mathbf{B}_{1} \\
\displaystyle\mathbf{B}_{5}&amp;\displaystyle=2\mathbf{B}_{1}+\mathbf{B}_{3}+2\mathbf{B}_{4} \\
\displaystyle\mathbf{B}_{6}&amp;\displaystyle=\mathbf{B}_{1}-3\mathbf{B}_{3}+6\mathbf{B}_{4} \\
\displaystyle\mathbf{B}_{7}&amp;\displaystyle=-3\mathbf{B}_{1}+5\mathbf{B}_{3}+6\mathbf{B}_{4}.
\end{align*}

Correspondingly we have

\begin{align*}
\displaystyle\mathbf{A}_{2}&amp;\displaystyle=4\mathbf{A}_{1} \\
\displaystyle\mathbf{A}_{5}&amp;\displaystyle=2\mathbf{A}_{1}+\mathbf{A}_{3}+2\mathbf{A}_{4} \\
\displaystyle\mathbf{A}_{6}&amp;\displaystyle=\mathbf{A}_{1}-3\mathbf{A}_{3}+6\mathbf{A}_{4} \\
\displaystyle\mathbf{A}_{7}&amp;\displaystyle=-3\mathbf{A}_{1}+5\mathbf{A}_{3}+6\mathbf{A}_{4}.
\end{align*}

</li>

</ol>
@end

The above important example motivates the following fundamental theorem.

@thm
@title{Basis of a Span}
@label{BS}
Suppose that $S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\}$ is a set of column vectors. Define $W=\left&lt;S\right&gt;$ and let $A$ be the matrix whose columns are the vectors from $S$. Let $B$ be the reduced row-echelon form of $A$, with $D=\{{d}_{1},\,{d}_{2},\,{d}_{3},\,\ldots,\,{d}_{r}\}$ the set of indices for the pivot columns of $B$. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$T=\{\mathbf{v}_{d_{1}},\,\mathbf{v}_{d_{2}},\,\mathbf{v}_{d_{3}},\,\ldots\,\mathbf{v}_{d_{r}}\}$ is a linearly independent set.
</li>
<li class="ltx_item">
$W=\left&lt;T\right&gt;$.
</li>

</ol>
@end
@proof
@col
<b>Try to understand the example and skip the proof for now</b> To prove that $T$ is linearly independent, begin with a relation of linear dependence on $T$,

\begin{align*}
\displaystyle \mathbf{0}=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\alpha_{3}\mathbf{v}_{d_{3}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}.
\end{align*}

We will try to conclude that the only possibility for the scalars $\alpha_{i}$ is that they are all zero.
Denote the non-pivot columns of $B$ by $F=\{{f}_{1},\,{f}_{2},\,{f}_{3},\,\ldots,\,{f}_{n-r}\}$. Then we can preserve the equality by adding a big fat zero to the linear combination:

\begin{align*}
\displaystyle \mathbf{0}=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\alpha_{3}\mathbf{v}_{d_{3}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+0\mathbf{v}_{f_{1}}+0\mathbf{v}_{f_{2}}+0\mathbf{v}_{f_{3}}+\ldots+0\mathbf{v}_{f_{n-r}}.
\end{align*}

The scalars in this linear combination (suitably reordered) are a solution to the homogeneous system $A\mathbf{x}=\mathbf{0}$. Notice that this is the solution obtained by setting each free variable to zero. In the case of a homogeneous system, we see that if all of the free variables are set to zero, then the resulting solution vector is trivial (all zeros). So it must be that $\alpha_{i}=0$, $1\leq i\leq r$. This implies, by the definition of linear independence, that $T$ is a linearly independent set.

The second conclusion of this theorem is an equality of sets. Since $T$ is a subset of $S$, any linear combination of elements of the set $T$ can also be viewed as a linear combination of elements of the set $S$. So $\left&lt;T\right&gt;\subseteq\left&lt;S\right&gt;=W$. It remains to prove that $W=\left&lt;S\right&gt;\subseteq\left&lt;T\right&gt;$.

For each $k$, $1\leq k\leq n-r$, form a solution $\mathbf{x}$ to $A\mathbf{x}=\mathbf{0}$ by setting the free variables as follows:

\begin{align*}
\displaystyle x_{f_{1}}&amp;\displaystyle=0&amp;\displaystyle x_{f_{2}}&amp;\displaystyle=0&amp;\displaystyle x_{f_{3}}&amp;\displaystyle=0&amp;\displaystyle\ldots&amp;\displaystyle x_{f_{k}}&amp;\displaystyle=1&amp;\displaystyle\ldots&amp;\displaystyle x_{f_{n-r}}&amp;\displaystyle=0.
\end{align*}

The remainder of this solution vector is given by

\begin{align*}
\displaystyle x_{d_{1}}&amp;\displaystyle=-\left[B\right]_{1,f_{k}}&amp;\displaystyle x_{d_{2}}&amp;\displaystyle=-\left[B\right]_{2,f_{k}}&amp;\displaystyle x_{d_{3}}&amp;\displaystyle=-\left[B\right]_{3,f_{k}}&amp;\displaystyle\ldots&amp;\displaystyle x_{d_{r}}&amp;\displaystyle=-\left[B\right]_{r,f_{k}}.
\end{align*}

From this solution, we obtain a relation of linear dependence on the columns of $A$,

\begin{align*}
\displaystyle -\left[B\right]_{1,f_{k}}\mathbf{v}_{d_{1}}-\left[B\right]_{2,f_{k}}\mathbf{v}_{d_{2}}-\left[B\right]_{3,f_{k}}\mathbf{v}_{d_{3}}-\ldots-\left[B\right]_{r,f_{k}}\mathbf{v}_{d_{r}}+1\mathbf{v}_{f_{k}}=\mathbf{0},
\end{align*}

which can be arranged to the equality

\begin{align*}
\displaystyle \mathbf{v}_{f_{k}}=\left[B\right]_{1,f_{k}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{k}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{k}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{k}}\mathbf{v}_{d_{r}}.
\end{align*}

Now, suppose we take an arbitrary element $\mathbf{w}$ of $W=\left&lt;S\right&gt;$ and write it as a linear combination of the elements of $S$, but with the terms organized according to the indices in $D$ and $F$:

\begin{align*}
\displaystyle\mathbf{w}&amp;\displaystyle=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+\beta_{1}\mathbf{v}_{f_{1}}+\beta_{2}\mathbf{v}_{f_{2}}+\ldots+\beta_{n-r}\mathbf{v}_{f_{n-r}}
\end{align*}

From the above, we can replace each $\mathbf{v}_{f_{j}}$ by a linear combination of the $\mathbf{v}_{d_{i}}$:

\begin{align*}
\displaystyle\mathbf{w}&amp;\displaystyle=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+ \\
&amp;\displaystyle\beta_{1}\left(\left[B\right]_{1,f_{1}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{1}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{1}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{1}}\mathbf{v}_{d_{r}}\right)+ \\
&amp;\displaystyle\beta_{2}\left(\left[B\right]_{1,f_{2}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{2}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{2}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{2}}\mathbf{v}_{d_{r}}\right)+ \\
&amp;\displaystyle\quad\quad\vdots \\
&amp;\displaystyle\beta_{n-r}\left(\left[B\right]_{1,f_{n-r}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{n-r}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{n-r}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{n-r}}\mathbf{v}_{d_{r}}\right) \\
\displaystyle=&amp;\displaystyle\ \left(\alpha_{1}+\beta_{1}\left[B\right]_{1,f_{1}}+\beta_{2}\left[B\right]_{1,f_{2}}+\beta_{3}\left[B\right]_{1,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{1,f_{n-r}}\right)\mathbf{v}_{d_{1}}+ \\
&amp;\displaystyle\left(\alpha_{2}+\beta_{1}\left[B\right]_{2,f_{1}}+\beta_{2}\left[B\right]_{2,f_{2}}+\beta_{3}\left[B\right]_{2,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{2,f_{n-r}}\right)\mathbf{v}_{d_{2}}+ \\
&amp;\displaystyle\quad\quad\vdots \\
&amp;\displaystyle\left(\alpha_{r}+\beta_{1}\left[B\right]_{r,f_{1}}+\beta_{2}\left[B\right]_{r,f_{2}}+\beta_{3}\left[B\right]_{r,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{r,f_{n-r}}\right)\mathbf{v}_{d_{r}}.
\end{align*}

This mess expresses the vector $\mathbf{w}$ as a linear combination of the vectors in

\begin{align*}
\displaystyle T=\{\mathbf{v}_{d_{1}},\,\mathbf{v}_{d_{2}},\,\mathbf{v}_{d_{3}},\,\ldots\,\mathbf{v}_{d_{r}}\},
\end{align*}

thus saying that $\mathbf{w}\in\left&lt;T\right&gt;$. Therefore, $W=\left&lt;S\right&gt;\subseteq\left&lt;T\right&gt;$.
∎
@end

In Example 2, we tossed-out vectors one at a time. But in each instance, we rewrote the offending vector as a linear combination of those vectors with the column indices of the pivot columns of the reduced row-echelon form of the matrix of columns. In the proof of Theorem  @ref{BS}, we accomplish this reduction in one big step. In Example 2 we arrived at a linearly independent set at exactly the same moment that we ran out of free variables to exploit. This was not a coincidence; it is the substance of our conclusion of linear independence in Theorem  @ref{BS}.

Here is a straightforward application of Theorem  @ref{BS}.

@eg
<b>Reducing a span in ${\mathbb{R}}^{4}$</b>Begin with a set of five vectors in ${\mathbb{R}}^{4}$,

\begin{align*}
\displaystyle S=\{\begin{bmatrix}1\\
1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}2\\
2\\
4\\
2\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}7\\
1\\
-1\\
4\end{bmatrix},\,\begin{bmatrix}0\\
2\\
5\\
1\end{bmatrix}\}
\end{align*}

and let $W=\left&lt;S\right&gt;$. To arrive at a (smaller) linearly independent set, follow the procedure described in Theorem  @ref{BS}. Place the vectors from $S$ into a matrix as columns, and row-reduce:

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;2&amp;2&amp;7&amp;0\\
1&amp;2&amp;0&amp;1&amp;2\\
2&amp;4&amp;-1&amp;-1&amp;5\\
1&amp;2&amp;1&amp;4&amp;1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;2&amp;0&amp;1&amp;2\\
0&amp;0&amp;\boxed{1}&amp;3&amp;-1\\
0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

Columns 1 and 3 are the pivot columns ($D=\{1,\,3\}$). So the set

\begin{align*}
\displaystyle T=\{\begin{bmatrix}1\\
1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix}\}
\end{align*}

is linearly independent and $\left&lt;T\right&gt;=\left&lt;S\right&gt;=W$. Boom!

Since the reduced row-echelon form of a matrix is unique, the procedure of Theorem  @ref{BS} leads us to a unique set $T$. However, there is a wide variety of possibilities for sets $T$ that are linearly independent and which can be employed in a span to create $W$. Without proof, we list two other possibilities:

\begin{align*}
\displaystyle T^{\prime}&amp;\displaystyle=\{\begin{bmatrix}2\\
2\\
4\\
2\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix}\} \\
\displaystyle T^{*}&amp;\displaystyle=\{\begin{bmatrix}3\\
1\\
1\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
3\\
0\end{bmatrix}\}.
\end{align*}

Can you prove that $T^{\prime}$ and $T^{*}$ are linearly independent sets and that $W=\left&lt;S\right&gt;=\left&lt;T^{\prime}\right&gt;=\left&lt;T^{*}\right&gt;$?
@end
@eg
<b>Reworking elements of a span</b>

Begin with a set of five vectors in ${\mathbb{R}}^{4}$

\begin{align*}
\displaystyle R=\{\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix},\,\begin{bmatrix}-8\\
-1\\
-9\\
-4\end{bmatrix},\,\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}-10\\
-1\\
-1\\
4\end{bmatrix}\}.
\end{align*}

It is easy to create elements of $X=\left&lt;R\right&gt;$ – we will create one at random,

\begin{align*}
\displaystyle \mathbf{y}=6\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix}+(-7)\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix}+1\begin{bmatrix}-8\\
-1\\
-9\\
-4\end{bmatrix}+6\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}+2\begin{bmatrix}-10\\
-1\\
-1\\
4\end{bmatrix}=\begin{bmatrix}9\\
2\\
1\\
-3\end{bmatrix}.
\end{align*}

We know we can replace $R$ by a smaller set, that will create the same span. Here goes:

\begin{align*}
\displaystyle \begin{bmatrix}2&amp;-1&amp;-8&amp;3&amp;-10\\
1&amp;1&amp;-1&amp;1&amp;-1\\
3&amp;0&amp;-9&amp;-1&amp;-1\\
2&amp;1&amp;-4&amp;-2&amp;4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;-3&amp;0&amp;-1\\
0&amp;\boxed{1}&amp;2&amp;0&amp;2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;-2\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

So, if we collect the first, second and fourth vectors from $R$,

\begin{align*}
\displaystyle P=\{\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix},\,\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}\}
\end{align*}

then $P$ is linearly independent and $\left&lt;P\right&gt;=\left&lt;R\right&gt;=X$ by Theorem  @ref{BS}. Since we built $\mathbf{y}$ as an element of $\left&lt;R\right&gt;$ it must also be an element of $\left&lt;P\right&gt;$. Can we write $\mathbf{y}$ as a linear combination of just the three vectors in $P$? The answer is, of course, yes. But let us compute an explicit linear combination just for fun. We can get such a linear combination by solving a system of equations with the column vectors of $R$ as the columns of a coefficient matrix, and $\mathbf{y}$ as the vector of constants.

We employ an augmented matrix to solve this system:

\begin{align*}
\displaystyle \begin{bmatrix}2&amp;-1&amp;3&amp;9\\
1&amp;1&amp;1&amp;2\\
3&amp;0&amp;-1&amp;1\\
2&amp;1&amp;-2&amp;-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;1\\
0&amp;\boxed{1}&amp;0&amp;-1\\
0&amp;0&amp;\boxed{1}&amp;2\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

So we see, as expected, that

\begin{align*}
\displaystyle 1\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix}+(-1)\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix}+2\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}=\begin{bmatrix}9\\
2\\
1\\
-3\end{bmatrix}=\mathbf{y}.
\end{align*}

A key feature of this example is that the linear combination that expresses $\mathbf{y}$ as a linear combination of the vectors in $P$ is unique. This is a consequence of the linear independence of $P$. The linearly independent set $P$ is smaller than $R$, but still just (barely) big enough to create elements of the set $X=\left&lt;R\right&gt;$. There are many, many ways to write $\mathbf{y}$ as a linear combination of the five vectors in $R$ (the appropriate system of equations to verify this claim yields two free variables in the description of the solution set), yet there is precisely one way to write $\mathbf{y}$ as a linear combination of the three vectors in $P$.
@end

@section{Uniqueness of RREF}

<b>Math Major only. You can skip this section. Similar concept appears in the classworks. </b>

@eg
<b>Entries of RREF $B$ gives relationship of columns of $A$</b> Let

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;1&amp;8&amp;1&amp;17\\
1&amp;2&amp;2&amp;13&amp;3&amp;37\\
1&amp;2&amp;0&amp;3&amp;-2&amp;-10\\
\end{bmatrix}.
\end{align*}

Then $A$ can be row reduced to

\begin{align*}
\displaystyle B=\begin{bmatrix}1&amp;2&amp;0&amp;3&amp;0&amp;4\\
0&amp;0&amp;1&amp;5&amp;0&amp;6\\
0&amp;0&amp;0&amp;0&amp;1&amp;7\end{bmatrix}.
\end{align*}

Let $\mathbf{A}_{i}$ (resp. $\mathbf{B}_{i}$) be the $i$-th column of $A$ (resp. $B$) for $i=1,\ldots,6$.
By the equivalence of system of linear equation $A\mathbf{x}=\mathbf{0}$ and $B\mathbf{x}=\mathbf{0}$, we have

\begin{align*}
\displaystyle x_{1}\mathbf{A}_{1}+x_{2}\mathbf{A}_{2}+\cdots+x_{6}\mathbf{A}_{6}=\mathbf{0}&amp;
\end{align*}

if and only if

\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}+x_{2}\mathbf{B}_{2}+\cdots+x_{6}\mathbf{B}_{6}=\mathbf{0}.&amp;
\end{align*}

<b>Step 1</b> First of all, if $(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},0,0,0,0,0)$ is a solution of ( @ref{B}), then

\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}=\mathbf{0}.
\end{align*}

So $x_{1}$ is zero. This is equivalent to

\begin{align*}
\displaystyle x_{1}\mathbf{A}_{1}=\mathbf{0}.
\end{align*}

It has only the trivial solution, i.e. $\{\mathbf{A}_{1}\}$ is linearly independent. Hence $d_{1}=1$ is a pivot column.

<b>Step 2</b>. Let’s move to $x_{2}$. Suppose that
$(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},x_{2},0,0,0,0)$.
Then

\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}+x_{2}\mathbf{B}_{1}=\mathbf{0}
\end{align*}

has nontrivial solution. Say $(x_{1},x_{2})=(-2,1)$.
These can also be seen as

\begin{align*}
\displaystyle -2\mathbf{A}_{1}+\mathbf{A}_{2}=\mathbf{0}
\end{align*}

or equivalently

\begin{align*}
\displaystyle \mathbf{A}_{2}=2\mathbf{A}_{1}.
\end{align*}

<b>Step 3</b>. Consider $x_{3}$. Let $(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},0,x_{3},0,0,0)$. Then

\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}+x_{3}\mathbf{B}_{3}=\mathbf{0}
\end{align*}

has only trivial solution. Equivalently $\{\mathbf{A}_{1},\mathbf{A}_{3}\}$ is linearly independent.Column $3$ of $B$ is a pivot column.

<b>Step 4</b>
Consder

\begin{align*}
\displaystyle \mathbf{B}_{4}=3\mathbf{B}_{1}+5\mathbf{B}_{3},
\end{align*}

or equivalently

\begin{align*}
\displaystyle \mathbf{A}_{4}=3\mathbf{A}_{1}+5\mathbf{A}_{3}.
\end{align*}

The relation of columns of $A$ gives the entries of the column 4 of $B$.

<b>Step 5</b>
$\mathbf{B}_{5}$ is not in span of $\mathbf{B}_{1}$ and $\mathbf{B}_{3}$. Equivalently $\mathbf{A}_{5}$ is not in span of $\mathbf{A}_{1}$ and $\mathbf{A}_{3}$. Column $5$ of $B$ is a pivot column.

<b>Step 6</b>
Consider

\begin{align*}
\displaystyle \mathbf{B}_{6}=4\mathbf{B}_{1}+6\mathbf{B}_{3}+7\mathbf{B}_{5}.
\end{align*}

Equivalently

\begin{align*}
\displaystyle \mathbf{A}_{6}=4\mathbf{A}_{1}+6\mathbf{A}_{3}+7\mathbf{A}_{5}.
\end{align*}

The relation of columns of $A$ gives the entries of the column 6 of $B$.
@end
@eg
<b>Relationship of columns of $A$ determine entries of $B$</b> Row reduce

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;1&amp;3&amp;1&amp;0&amp;0&amp;4\\
2&amp;1&amp;5&amp;1&amp;1&amp;2&amp;7\\
1&amp;-1&amp;1&amp;2&amp;1&amp;-3&amp;10\\
1&amp;3&amp;5&amp;1&amp;-1&amp;1&amp;1\\
\end{bmatrix}
\end{align*}

to a RREF $B$ by the above technique. Let $\mathbf{A}_{i}$ (resp. $\mathbf{B}_{i}$) be the $i$-th column of $A$ (resp. $B$) for $i=1,\ldots,7$. <b>Step 1</b> $\mathbf{A}_{1}$ is nonzero column. So the index $d_{1}=1$ corresponds to a pivot column. We have

\begin{align*}
\displaystyle \mathbf{B}_{1}=\begin{bmatrix}1\\
0\\
0\\
0\end{bmatrix}.
\end{align*}

<b>Step 2</b> $\mathbf{A}_{2}$ is not in $\left&lt;\{\mathbf{A}_{d_{1}}\}\right&gt;$. So the index $d_{2}=2$ corresponds to a pivot column. We have

\begin{align*}
\displaystyle \mathbf{B}_{2}=\begin{bmatrix}0\\
1\\
0\\
0\end{bmatrix}.
\end{align*}

<b>Step 3</b>
Consider

\begin{align*}
\displaystyle \mathbf{A}_{3}=2\mathbf{A}_{d_{1}}+\mathbf{A}_{d_{2}}.
\end{align*}

So we have

\begin{align*}
\displaystyle \mathbf{B}_{3}=2\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}=\begin{bmatrix}2\\
1\\
0\\
0\end{bmatrix}.
\end{align*}

<b>Step 4</b>
$\mathbf{A}_{4}$ is not in $\left&lt;\{\mathbf{A}_{d_{1}},\mathbf{A}_{d_{2}}\}\right&gt;$.
So the index $d_{3}=4$ corresponds to a pivot column. We have

\begin{align*}
\displaystyle \mathbf{B}_{4}=\begin{bmatrix}0\\
0\\
1\\
0\end{bmatrix}.
\end{align*}

<b>Step 5</b>
$\mathbf{A}_{5}$ is not in $\left&lt;\{\mathbf{A}_{d_{1}},\mathbf{A}_{d_{3}},\mathbf{A}_{d_{3}}\}\right&gt;$.
So the index $d_{4}=5$ corresponds to a pivot column. We have

\begin{align*}
\displaystyle \mathbf{B}_{5}=\begin{bmatrix}0\\
0\\
0\\
1\end{bmatrix}.
\end{align*}

<b>Step 6</b>
Consider

\begin{align*}
\displaystyle \mathbf{A}_{6}=\mathbf{A}_{d_{1}}+\mathbf{A}_{d_{2}}-2\mathbf{A}_{d_{3}}+\mathbf{A}_{d_{4}}.
\end{align*}

So, we have

\begin{align*}
\displaystyle \mathbf{B}_{6}=\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}-2\mathbf{B}_{d_{3}}+\mathbf{B}_{d_{4}}=\begin{bmatrix}1\\
1\\
-2\\
1\end{bmatrix}
\end{align*}

<b>Step 7</b>
Consider

\begin{align*}
\displaystyle \mathbf{A}_{7}=2\mathbf{A}_{d_{1}}-\mathbf{A}_{d_{2}}+3\mathbf{A}_{d_{3}}+\mathbf{A}_{d_{4}}.
\end{align*}

So, we have

\begin{align*}
\displaystyle \mathbf{B}_{7}=\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}-2\mathbf{B}_{d_{3}}+\mathbf{B}_{d_{4}}=\begin{bmatrix}2\\
-1\\
3\\
1\end{bmatrix}
\end{align*}

Hence the RREF of $A$ is

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0&amp;2&amp;0&amp;0&amp;1&amp;2\\
0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;-1\\
0&amp;0&amp;0&amp;1&amp;0&amp;-2&amp;3\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1\\
\end{bmatrix}.
\end{align*}

<b>Important remark</b>: from the above computation, the entries of $B$ are uniquely determined by $A$.
So the RREF $B$ is unique.
@end

@chapter{Column and Row Space}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section CRS (print version p167-178)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section CRS p.66-71 C20, C30-C35, M10, M20, M21, T40, T41, T45.

@section{Column Spaces and Systems of Equations}

@defn
@title{Column Space of a Matrix}
@label{CSM}
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$. Then the <b>column space</b> of $A$, written $\mathcal{C}\!\left(A\right)$, is the subset of ${\mathbb{R}}^{m}$ containing all linear combinations of the columns of $A$,

\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\left&lt;\{\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}\}\right&gt;
\end{align*}
@end
@thm
@title{Column Spaces and Consistent Systems}
@label{CSCS}
Suppose $A$ is an $m\times n$ matrix and $\mathbf{b}$ is a vector of size $m$.
Then $\mathbf{b}\in\mathcal{C}\!\left(A\right)$ if and only if $A\mathbf{x}=\mathbf{b}$ is consistent.
@end
@proof
@col
($\Rightarrow$) Suppose $\mathbf{b}\in\mathcal{C}\!\left(A\right)$. Then we can write $\mathbf{b}$ as some linear combination of the columns of $A$. Then by Lecture 7 Theorem 2 we can use the scalars from this linear combination to form a solution to $A\mathbf{x}=\mathbf{b}$, so this system is consistent.

($\Leftarrow$) If $A\mathbf{x}=\mathbf{b}$ is consistent, there is a solution that may be used with Lecture 7 Theorem 2 to write $\mathbf{b}$ as a linear combination of the columns of $A$. This qualifies $\mathbf{b}$ for membership in $\mathcal{C}\!\left(A\right)$.
∎
@end

This theorem tells us that asking if the system $A\mathbf{x}=\mathbf{b}$ is consistent is exactly the same question as asking if $\mathbf{b}$ is in the column space of $A$. Or equivalently, it tells us that the column space of the matrix $A$ is precisely those vectors of constants, $\mathbf{b}$, that can be paired with $A$ to create a system of linear equations $A\mathbf{x}=\mathbf{b}$ that is consistent.

We can form the chain of equivalences

\begin{align*}
\displaystyle\mathbf{b}\in\mathcal{C}\!\left(A\right)\iff A\mathbf{x}=\mathbf{b}\text{ is consistent}\iff A\mathbf{x}=\mathbf{b}\text{ for some }\mathbf{x}
\end{align*}

Thus, an alternative (and popular) definition of the column space of an $m\times n$ matrix $A$ is

\begin{align*}
\displaystyle\mathcal{C}\!\left(A\right)&amp;\displaystyle=\left\{\left.\mathbf{y}\in{\mathbb{R}}^{m}\,\right|\,\mathbf{y}=A\mathbf{x}\text{ for some }\mathbf{x}\in{\mathbb{R}}^{n}\right\}=\left\{\left.A\mathbf{x}\,\right|\,\mathbf{x}\in{\mathbb{R}}^{n}\right\}\subseteq{\mathbb{R}}^{m}
\end{align*}

@eg
Consider the column space of the $3\times 4$ matrix $A$,

\begin{align*}
\displaystyle A=\begin{bmatrix}3&amp;2&amp;1&amp;-4\\
-1&amp;1&amp;-2&amp;3\\
2&amp;-4&amp;6&amp;-8\end{bmatrix}
\end{align*}

Show that $\mathbf{v}=\begin{bmatrix}18\\
-6\\
12\end{bmatrix}$ is in the column space of $A$, $\mathbf{v}\in\mathcal{C}\!\left(A\right)$.
The above theorem says that we need to check the consistency of $A\mathbf{x}=v$.
From the augmented matrix and row-reduce,

\begin{align*}
\displaystyle \begin{bmatrix}3&amp;2&amp;1&amp;-4&amp;18\\
-1&amp;1&amp;-2&amp;3&amp;-6\\
2&amp;-4&amp;6&amp;-8&amp;12\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;1&amp;-2&amp;6\\
0&amp;\boxed{1}&amp;-1&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

Since the last column is not a pivot column, so the system is consistent and hence $v\in\mathcal{C}\!\left(A\right)$.
In fact, we have

\begin{align*}
\displaystyle \mathbf{v}=6\mathbf{A}_{1}.
\end{align*}

Next we show that $\mathbf{w}=\begin{bmatrix}2\\
1\\
-3\end{bmatrix}$ is not in the column space of $A$, $\mathbf{w}\not\in\mathcal{C}\!\left(A\right)$.
The above theorem says that we need to check the consistency of $A\mathbf{x}=v$.
From the augmented matrix and row-reduce,

\begin{align*}
\displaystyle \begin{bmatrix}3&amp;2&amp;1&amp;-4&amp;2\\
-1&amp;1&amp;-2&amp;3&amp;1\\
2&amp;-4&amp;6&amp;-8&amp;-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;1&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;-1&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}\end{bmatrix}
\end{align*}

Since the final column is a pivot column, the system is inconsistent and therefore $\mathbf{w}\not\in\mathcal{C}\!\left(A\right)$.
@end

The next two examples illustrate the main idea of describing $\mathcal{C}\!\left(A\right)$.

@eg
<b>Describe $\mathcal{C}\!\left(A\right)$ as a null space</b>Let

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;7&amp;1&amp;-1\\
1&amp;1&amp;3&amp;1&amp;0\\
3&amp;2&amp;5&amp;-1&amp;9\\
1&amp;-1&amp;-5&amp;2&amp;0\end{bmatrix}.
\end{align*}

Find $\mathcal{C}\!\left(A\right)$. Let’s determine if $\mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{4}\end{bmatrix}\in\left&lt;S\right&gt;$.

Applying Gauss-Jordan elimination to the augmented matrix

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;2&amp;7&amp;1&amp;-1&amp;v_{1}\\
1&amp;1&amp;3&amp;1&amp;0&amp;v_{2}\\
3&amp;2&amp;5&amp;-1&amp;9&amp;v_{3}\\
1&amp;-1&amp;-5&amp;2&amp;0&amp;v_{4}\end{bmatrix},
\end{align*}

we obtain

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0&amp;-1&amp;0&amp;3&amp;-3v_{1}+5v_{2}-v_{4}\\
0&amp;1&amp;4&amp;0&amp;-1&amp;v_{1}-v_{2}\\
0&amp;0&amp;0&amp;1&amp;-2&amp;2v_{1}-3v_{2}+v_{4}\\
0&amp;0&amp;0&amp;0&amp;0&amp;9v_{1}-16v_{2}+v_{3}+4v_{4}\\
\end{bmatrix}
\end{align*}

If $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$, the above is a RREF. The last column is not a pivot columns. So $\mathbf{v}\in\left&lt;S\right&gt;$. If $9v_{1}-16v_{2}+v_{3}+4v_{4}\neq 0$, the equation corresponding to the last row is

\begin{align*}
\displaystyle 9v_{1}-16v_{2}+v_{3}+4v_{4}=0.
\end{align*}

So the corresponding system of linear equations is inconsistent. So $\mathbf{v}\in\left&lt;S\right&gt;$. Hence $\mathbf{v}\in\left&lt;S\right&gt;$ if and only if $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$. Therefore

\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)={\mathcal{N}}\!\left([9\,\,-16\,\,1\,\,4]\right).
\end{align*}
@end
@eg
<b>Describe $\mathcal{C}\!\left(A\right)$ by basis</b> Let

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix},
\end{align*}

find $\mathcal{C}\!\left(A\right)$.

\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}B=\begin{bmatrix}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

The indexes of the pivot columns are $D=\{1,3,4\}$.
Hence $\mathcal{C}\!\left(A\right)=\left&lt;A\right&gt;=\left&lt;\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\}\right&gt;$.
@end

@section{Column Space Spanned by Original Columns}
@label{CSSOC}
So we have a foolproof, automated procedure for determining membership in $\mathcal{C}\!\left(A\right)$. While this works just fine a vector at a time, we would like to have a more useful description of the set $\mathcal{C}\!\left(A\right)$ as a whole. The next example will preview the first of two fundamental results about the column space of a matrix.

@eg
Consider the $5\times 7$ matrix $A$,

\begin{align*}
\displaystyle \begin{bmatrix}2&amp;4&amp;1&amp;-1&amp;1&amp;4&amp;4\\
1&amp;2&amp;1&amp;0&amp;2&amp;4&amp;7\\
0&amp;0&amp;1&amp;4&amp;1&amp;8&amp;7\\
1&amp;2&amp;-1&amp;2&amp;1&amp;9&amp;6\\
-2&amp;-4&amp;1&amp;3&amp;-1&amp;-2&amp;-2\end{bmatrix}
\end{align*}

The column space of $A$ is

\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\left&lt;\{\begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
2\\
0\\
2\\
-4\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix},\,\begin{bmatrix}4\\
4\\
8\\
9\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
7\\
7\\
6\\
-2\end{bmatrix}\}\right&gt;
\end{align*}

While this is a concise description of an infinite set, we might be able to describe the span with fewer than seven vectors. Now we row-reduce,

\begin{align*}
\displaystyle \begin{bmatrix}2&amp;4&amp;1&amp;-1&amp;1&amp;4&amp;4\\
1&amp;2&amp;1&amp;0&amp;2&amp;4&amp;7\\
0&amp;0&amp;1&amp;4&amp;1&amp;8&amp;7\\
1&amp;2&amp;-1&amp;2&amp;1&amp;9&amp;6\\
-2&amp;-4&amp;1&amp;3&amp;-1&amp;-2&amp;-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;2&amp;0&amp;0&amp;0&amp;3&amp;1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;-1&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;2&amp;1\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;1&amp;3\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

The pivot columns are $D=\{1,\,3,\,4,\,5\}$, so we can create the set

\begin{align*}
\displaystyle T=\{\begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix}\}
\end{align*}

and know that $\mathcal{C}\!\left(A\right)=\left&lt;T\right&gt;$ and $T$ is a linearly independent set of columns from the set of columns of $A$.
@end
@thm
@title{Basis of the Column Space}
@label{BCS}
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ be the set of indices for the pivot columns of $B$.
Let $T=\{\mathbf{A}_{d_{1}},\,\mathbf{A}_{d_{2}},\,\mathbf{A}_{d_{3}},\,\ldots,\,\mathbf{A}_{d_{r}}\}$. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$T$ is a linearly independent set.
</li>
<li class="ltx_item">
$\mathcal{C}\!\left(A\right)=\left&lt;T\right&gt;$.
</li>

</ol>
@end

@section{Column Space of a Nonsingular Matrix}

@thm
@title{Column Space of a Nonsingular Matrix}
Suppose $A$ is a square matrix of size $n$. Then $A$ is nonsingular if and only if $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$.
@end

@proof
@col
See Lecture 11 Thm 4.

∎
@end

@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}0&amp;1&amp;2&amp;3\\
-1&amp;1&amp;2&amp;1\\
0&amp;1&amp;0&amp;2\\
1&amp;1&amp;1&amp;4\\
\end{bmatrix}.
\end{align*}

We can show that $A$ is nonsingular as $A\xrightarrow{\text{RREF}}I_{4}$.
So $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{4}$.
@end

@section{Row Space of a Matrix}

@defn
@title{Row Space of a Matrix}
Suppose $A$ is an $m\times n$ matrix. Then the <b>row space</b> of $A$, $\mathcal{R}\!\left(A\right)$, is the column space of $A^{t}$, i.e., $\mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)$.
@end

Informally, the row space is the set of all linear combinations of the rows of $A$. However, we write the rows as column vectors, thus the necessity of using the transpose to make the rows into columns. Additionally, with the row space defined in terms of the column space, all of the previous results of this section can be applied to row spaces.

Notice that if $A$ is a rectangular $m\times n$ matrix, then $\mathcal{C}\!\left(A\right)\subseteq{\mathbb{R}}^{m}$, while $\mathcal{R}\!\left(A\right)\subseteq{\mathbb{R}}^{n}$ and the two sets are not comparable since they do not even hold objects of the same type. However, when $A$ is square of size $n$, both $\mathcal{C}\!\left(A\right)$ and $\mathcal{R}\!\left(A\right)$ are subsets of ${\mathbb{R}}^{n}$, though usually the sets will not be equal.

@eg
Find $\mathcal{R}\!\left(A\right)$ for

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix}.
\end{align*}

To build the row space, we transpose the matrix,

\begin{align*}
\displaystyle A^{t}=\begin{bmatrix}1&amp;2&amp;0&amp;-1\\
4&amp;8&amp;0&amp;-4\\
0&amp;-1&amp;2&amp;2\\
-1&amp;3&amp;-3&amp;4\\
0&amp;9&amp;-4&amp;8\\
7&amp;-13&amp;12&amp;-31\\
-9&amp;7&amp;-8&amp;37\end{bmatrix}
\end{align*}

Then the columns of this matrix are used in a span to build the row space,

\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\left&lt;\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix},\,\begin{bmatrix}-1\\
-4\\
2\\
4\\
8\\
-31\\
37\end{bmatrix}\}\right&gt;.
\end{align*}

First, row-reduce $A^{t}$,

\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-\frac{31}{7}\\
0&amp;\boxed{1}&amp;0&amp;\frac{12}{7}\\
0&amp;0&amp;\boxed{1}&amp;\frac{13}{7}\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

Since the pivot columns have indices $D=\{1,\,2,\,3\}$, the column space of $A^{t}$ can be spanned by just the first three columns of $A^{t}$,

\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\left&lt;\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix}\}\right&gt;.
\end{align*}
@end
@thm
@title{Row-Equivalent Matrices have equal Row Spaces}
@label{REMRS}
Suppose $A$ and $B$ are row-equivalent matrices. Then $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$.
@end
@proof
@col
Two matrices are row-equivalent if one can be obtained from another by a sequence of (possibly many) row operations. We will prove the theorem for two matrices that differ by a single row operation, and then this result can be applied repeatedly to get the full statement of the theorem. The row spaces of $A$ and $B$ are spans of the columns of their transposes. For each row operation we perform on a matrix, we can define an analogous operation on the columns. Perhaps we should call these <b>column operations</b>. Instead, we will still call them row operations, but we will apply them to the columns of the transposes.

Refer to the columns of $A^{t}$ and $B^{t}$ as $\mathbf{A}_{i}$ and $\mathbf{B}_{i}$, $1\leq i\leq m$. The row operation that switches rows will just switch columns of the transposed matrices. This will have no effect on the possible linear combinations formed by the columns.

Suppose that $B^{t}$ is formed from $A^{t}$ by multiplying column $\mathbf{A}_{t}$ by $\alpha\neq 0$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for all $i\neq t$. We need to establish that two sets are equal, $\mathcal{C}\!\left(A^{t}\right)=\mathcal{C}\!\left(B^{t}\right)$. We will take a generic element of one and show that it is contained in the other.

\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&amp;\displaystyle\beta_{2}\mathbf{B}_{2}+\beta_{3}\mathbf{B}_{3}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\left(\alpha\beta_{t}\right)\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}

says that $\mathcal{C}\!\left(B^{t}\right)\subseteq\mathcal{C}\!\left(A^{t}\right)$. Similarly,

\begin{align*}
\displaystyle\gamma_{1}\mathbf{A}_{1}+&amp;\displaystyle\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\left(\frac{\gamma_{t}}{\alpha}\alpha\right)\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\gamma_{3}\mathbf{B}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}

says that $\mathcal{C}\!\left(A^{t}\right)\subseteq\mathcal{C}\!\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\mathcal{C}\!\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the second type is performed.

Suppose now that $B^{t}$ is formed from $A^{t}$ by replacing $\mathbf{A}_{t}$ with $\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$ for some $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $s\neq t$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for $i\neq t$.

\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&amp;\displaystyle\beta_{2}\mathbf{B}_{2}+\cdots+\beta_{s}\mathbf{B}_{s}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\left(\beta_{s}+\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}

says that $\mathcal{C}\!\left(B^{t}\right)\subseteq\mathcal{C}\!\left(A^{t}\right)$. Similarly,

\begin{align*}
\displaystyle\gamma_{1}&amp;\displaystyle\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\left(-\alpha\gamma_{t}\mathbf{A}_{s}+\alpha\gamma_{t}\mathbf{A}_{s}\right)+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{A}_{s}+\cdots+\gamma_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{B}_{s}+\cdots+\gamma_{t}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}

says that $\mathcal{C}\!\left(A^{t}\right)\subseteq\mathcal{C}\!\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\mathcal{C}\!\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the third type is performed.

So the row space of a matrix is preserved by each row operation, and hence row spaces of row-equivalent matrices are equal sets.
∎
@end
@eg
<b>Row spaces of two row-equivalent matrices</b> We saw that the matrices

\begin{align*}
\displaystyle A&amp;\displaystyle=\begin{bmatrix}2&amp;-1&amp;3&amp;4\\
5&amp;2&amp;-2&amp;3\\
1&amp;1&amp;0&amp;6\end{bmatrix}&amp;\displaystyle B&amp;\displaystyle=\begin{bmatrix}1&amp;1&amp;0&amp;6\\
3&amp;0&amp;-2&amp;-9\\
2&amp;-1&amp;3&amp;4\end{bmatrix}
\end{align*}

are row-equivalent by demonstrating a sequence of two row operations that converted $A$ into $B$.
Hence by the above theorem

\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\left&lt;\{\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}5\\
2\\
-2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix},\,\begin{bmatrix}3\\
0\\
-2\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix}\}\right&gt;=\mathcal{R}\!\left(B\right)
\end{align*}
@end
@thm
@title{Basis for the Row Space}
@label{BRS}
Suppose that $A$ is a matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Let $S$ be the set of nonzero columns of $B^{t}$. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$\mathcal{R}\!\left(A\right)=\left&lt;S\right&gt;$.
</li>
<li class="ltx_item">
$S$ is a linearly independent set.
</li>

</ol>
@end
@proof
@col
From Theorem  @ref{REMRS}. we know that $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$. If $B$ has any zero rows, these are columns of $B^{t}$ that are the zero vector. We can safely toss out the zero vector in the span construction, since it can be recreated from the nonzero vectors by a linear combination where all the scalars are zero. So $\mathcal{R}\!\left(A\right)=\left&lt;S\right&gt;$.

Suppose $B$ has $r$ nonzero rows and let $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ denote the indices of the pivot columns of $B$. Denote the $r$ column vectors of $B^{t}$, the vectors in $S$, as $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{r}$. To show that $S$ is linearly independent, start with a relation of linear dependence

\begin{align*}
\displaystyle \alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}=\mathbf{0}
\end{align*}

Now consider this vector equality in location $d_{i}$. Since $B$ is in reduced row-echelon form, the entries of column $d_{i}$ of $B$ are all zero, except for a leading 1 in row $i$. Thus, in $B^{t}$, row $d_{i}$ is all zeros, excepting a 1 in column $i$. So, for $1\leq i\leq r$,

\begin{align*}
\displaystyle 0&amp;\displaystyle=\left[\mathbf{0}\right]_{d_{i}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{B}_{1}\right]_{d_{i}}+\left[\alpha_{2}\mathbf{B}_{2}\right]_{d_{i}}+\left[\alpha_{3}\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\left[\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&amp;\displaystyle=\alpha_{1}\left[\mathbf{B}_{1}\right]_{d_{i}}+\alpha_{2}\left[\mathbf{B}_{2}\right]_{d_{i}}+\alpha_{3}\left[\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\alpha_{r}\left[\mathbf{B}_{r}\right]_{d_{i}} \\
&amp;\displaystyle=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+\alpha_{i}(1)+\cdots+\alpha_{r}(0) \\
&amp;\displaystyle=\alpha_{i}
\end{align*}

So we conclude that $\alpha_{i}=0$ for all $1\leq i\leq r$, establishing the linear independence of $S$.
∎
@end
@eg
<b>Improving a span</b>Suppose in the course of analyzing a matrix (its column space, its null space, its …) we encounter the following set of vectors, described by a span

\begin{align*}
\displaystyle X=\left&lt;\{\begin{bmatrix}1\\
2\\
1\\
6\\
6\end{bmatrix},\,\begin{bmatrix}3\\
-1\\
2\\
-1\\
6\end{bmatrix},\,\begin{bmatrix}1\\
-1\\
0\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}-3\\
2\\
-3\\
6\\
-10\end{bmatrix}\}\right&gt;
\end{align*}

Let $A$ be the matrix whose rows are the vectors in $X$, so by design $X=\mathcal{R}\!\left(A\right)$,

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;1&amp;6&amp;6\\
3&amp;-1&amp;2&amp;-1&amp;6\\
1&amp;-1&amp;0&amp;-1&amp;-2\\
-3&amp;2&amp;-3&amp;6&amp;-10\end{bmatrix}
\end{align*}

Row-reduce $A$ to form a row-equivalent matrix in reduced row-echelon form,

\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;2&amp;-1\\
0&amp;\boxed{1}&amp;0&amp;3&amp;1\\
0&amp;0&amp;\boxed{1}&amp;-2&amp;5\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

Then the above theorem says we can grab the nonzero columns of $B^{t}$ and write

\begin{align*}
\displaystyle X=\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)=\left&lt;\{\begin{bmatrix}1\\
0\\
0\\
2\\
-1\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
3\\
1\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
-2\\
5\end{bmatrix}\}\right&gt;
\end{align*}

These three vectors provide a much-improved description of $X$. There are fewer vectors, and the pattern of zeros and ones in the first three entries makes it easier to determine membership in $X$.
@end
@thm
@title{Column Space, Row Space, Transpose}
@label{CSRST}
Suppose $A$ is a matrix. Then $\mathcal{C}\!\left(A\right)=\mathcal{R}\!\left(A^{t}\right)$.
@end
@proof
@col
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\mathcal{C}\!\left(\left(A^{t}\right)^{t}\right)=\mathcal{R}\!\left(A^{t}\right)
\end{align*}

∎
@end
@eg
<b>Column space from row operations</b> Find the column space of $A$ in example 6. <b>Method 1</b>

\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

Let

\begin{align*}
\displaystyle T=\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\}=\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\}.
\end{align*}

Then $T$ is linear independent and $\mathcal{C}\!\left(A\right)=\left&lt;T\right&gt;$.

<b>Method 2</b>
The transpose of $A$ is

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;2&amp;0&amp;-1\\
4&amp;8&amp;0&amp;-4\\
0&amp;-1&amp;2&amp;2\\
-1&amp;3&amp;-3&amp;4\\
0&amp;9&amp;-4&amp;8\\
7&amp;-13&amp;12&amp;-31\\
-9&amp;7&amp;-8&amp;37\end{bmatrix}.
\end{align*}

Row-reduced this becomes,

\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-\frac{31}{7}\\
0&amp;\boxed{1}&amp;0&amp;\frac{12}{7}\\
0&amp;0&amp;\boxed{1}&amp;\frac{13}{7}\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

Now, using Theorem  @ref{CSRST} and Theorem  @ref{BRS},

\begin{align*}
\displaystyle \mathcal{C}\!\left(I\right)=\mathcal{R}\!\left(I^{t}\right)=\left&lt;\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\}\right&gt;.
\end{align*}

This is a very nice description of the column space. Fewer vectors than the 7 involved in the definition, and the pattern of the zeros and ones in the first 3 slots can be used to advantage. For example,
let’s check if

\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}.
\end{align*}

is in $\mathcal{C}\!\left(A\right)$ or not.
If it is, then

\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=x\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+y\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+z\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}=\begin{bmatrix}x\\
y\\
z\\
-\frac{31}{7}x+\frac{12}{7}y+\frac{13}{7}z\end{bmatrix}.
\end{align*}

From the first three coordinate $x=3,y=9,z=1$. Let’s check the last coordinate:

\begin{align*}
\displaystyle -\frac{31}{7}\times 3+\frac{12}{7}\times 9+\frac{13}{7}\times 1=4.
\end{align*}

So

\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=3\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+9\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+1\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}
\end{align*}

and hence $\mathbf{b}\in\mathcal{C}\!\left(A\right)$.

<b>Important Remark</b> Both methods describe algorithms to find bases
(i.e., linear independent set the generate the column space)
for the column space.
Here are the differences.

<ol class="ltx_enumerate">
<li class="ltx_item">
In method 1, we find a subset of columns that find a basis.
However in method 2, the basis is not a subset of columns.
</li>
<li class="ltx_item">
Given a vector $\mathbf{b}\in\mathcal{C}\!\left(A\right)$, it is easier to express it as a linear combination of the basis given by method 2.
</li>

</ol>
@end

@chapter{Basis}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section B (print version p233-238), Section D (print version p245-253)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section B p.88-92 C10, C11, C12, M20 Section D p.92-96 C21, C23, C30, C31, C35, C36, C37, M20, M21.

@section{Basis}

@defn
Let $V$ be a vector space. Then a subset $S$ of $V$ is said to be a <b>basis</b> for $V$ if

<ol class="ltx_enumerate">
<li class="ltx_item">
$S$ is linearly independent.
</li>
<li class="ltx_item">
$\left&lt;S\right&gt;=V$, i.e. $S$ spans $V$.
</li>

</ol>
@end

<b>Remark</b>: Most of the time $V$ is a subspace of ${\mathbb{R}}^{m}$. Occasionally $V$ is assumed to be a subspace of $M_{mn}$ or $P_{n}$. It does not hurt to assume $V$ is a subspace of ${\mathbb{R}}^{m}$.

@eg
Let $V={\mathbb{R}}^{m}$, then $B=\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\}$ is a basis for $V$.
(recall all the entries of $\mathbf{e}_{i}$ is zero, except the $i$-th entry being $1$).
It is called the <b>standard basis</b>: Obviously $B$ is linearly independent. Also, for any $\mathbf{v}\in V$, $\mathbf{v}=[\mathbf{v}]_{1}\mathbf{e}_{1}+\cdots+[\mathbf{v}]_{m}\mathbf{e}_{m}\in\left&lt;B\right&gt;$. So $\left&lt;B\right&gt;=V$.
@end
@eg
<b>Math major only</b> $V=M_{22}$. Let

\begin{align*}
\displaystyle B_{11}=\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix},B_{12}=\begin{bmatrix}0&amp;1\\
0&amp;0\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle B_{21}=\begin{bmatrix}0&amp;0\\
1&amp;0\end{bmatrix},B_{22}=\begin{bmatrix}0&amp;0\\
0&amp;1\end{bmatrix},
\end{align*}

Then $B=\{B_{11},B_{12},B_{21},B_{22}\}$ is a basis for $V$. Check: Obviously $B$ is linearly independent (exercise). Also for any $A\in V$,

\begin{align*}
\displaystyle A=\begin{bmatrix}a&amp;b\\
c&amp;d\end{bmatrix}=aB_{11}+bB_{12}+cB_{21}+dB_{22}.
\end{align*}

So $\left&lt;B\right&gt;=M_{22}$.
@end
@eg
<b>Math major only</b> Let $V=M_{mn}$.
For $1\leq i\leq m$, $1\leq j\leq n$,
let $B_{ij}$ be the $m\times n$ matrix with $(i,j)$-th entry equal to $1$ and all other entries equal to $0$. Then $\{B_{ij}|1\leq i\leq m$, $1\leq j\leq n\}$ is a basis for $V$. (exercise).
@end
@eg
<b>Math major only</b> Let $V=P_{n}$. Then $1,x,x^{2},\ldots,x^{n}$ is a basis. It is easy to show that $S=\{1,x,x^{2},\ldots,x^{n}\}$ is linearly independent. Also any polynomial

\begin{align*}
\displaystyle f(x)=a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}
\end{align*}

is a linear combinations of $S$.
@end
@eg
A vector space can have different bases. Example for $V={\mathbb{R}}^{2}$,
$S=\{\mathbf{e}_{1},\mathbf{e}_{2}\}$ is a basis and
$S^{\prime}=\{\begin{bmatrix}1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\end{bmatrix}\}$ is also a basis.
@end

@section{Bases for spans of column vectors}

Let $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\}$ be a subset of ${\mathbb{R}}^{m}$
Recall from lecture 14, that there are several methods to find a subset $T\subseteq S$.
Such that (i) $T$ is linearly independent (ii) $\left&lt;T\right&gt;=\left&lt;S\right&gt;$. So $T$ is a basis for $\left&lt;S\right&gt;$. <b>Method 1</b> Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]\xrightarrow{\text{RREF}}B$.
Suppose $D=\{d_{1},\ldots,d_{r}\}$ be the indexes of the pivot columns of $B$.
Let $T=\{\mathbf{v}_{d_{1}},\ldots,\mathbf{v}_{d_{r}}\}$. Then $T$ is a basis for $\left&lt;S\right&gt;=\mathcal{C}\!\left(A\right)$

<b>Method 2</b> Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]$. Suppose $A^{t}\xrightarrow{\text{RREF}}B$.
Let $T$ be the nonzero columns of $B^{t}$. Then $T$ is a basis for $\left&lt;S\right&gt;=\mathcal{C}\!\left(A\right)$

This is an example from Lecture 14.

@eg
<b>Column space from row operations</b> Let

\begin{align*}
\displaystyle S=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\mathbf{v}_{6}=\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\mathbf{v}_{7}=\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\}.
\end{align*}

Find a basis for $\left&lt;S\right&gt;$.

\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{7}]=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix}.
\end{align*}

<b>Method 1</b>

\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

Let

\begin{align*}
\displaystyle T=\{\mathbf{v}_{1},\mathbf{v}_{3},\mathbf{v}_{4}\}=\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\}.
\end{align*}

Then $T$ is a basis for $\left&lt;S\right&gt;=\mathcal{C}\!\left(A\right)$.

<b>Method 2</b>
The transpose of $A$ is

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;2&amp;0&amp;-1\\
4&amp;8&amp;0&amp;-4\\
0&amp;-1&amp;2&amp;2\\
-1&amp;3&amp;-3&amp;4\\
0&amp;9&amp;-4&amp;8\\
7&amp;-13&amp;12&amp;-31\\
-9&amp;7&amp;-8&amp;37\end{bmatrix}.
\end{align*}

Row-reduced this becomes,

\begin{align*}
\displaystyle D=\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-\frac{31}{7}\\
0&amp;\boxed{1}&amp;0&amp;\frac{12}{7}\\
0&amp;0&amp;\boxed{1}&amp;\frac{13}{7}\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}

Then we can take

\begin{align*}
\displaystyle T=\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\}.
\end{align*}

$T$ is a basis for $\mathcal{C}\!\left(A\right)=\left&lt;S\right&gt;$.
@end

We have the following theorem

@thm
@label{basisexists}
Let $S$ be a finite subset of ${\mathbb{R}}^{m}$. Then basis for $\left&lt;S\right&gt;$ exists.

In fact, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left&lt;S\right&gt;$ (see Lecture 10 Theorem 2 or Lecture 14 Theorem 3).
@end

@section{Bases and nonsingular matrices}

@thm

Suppose that $A$ is a square matrix of size $m$.
Then the columns of $A$ is a basis for ${\mathbb{R}}^{m}$ if and only if $A$ is nonsingular.
@end

@proof
@col
If columns of $A$ is linearly independent, $A\mathbf{x}=\mathbf{0}$
has only trivial solution. Hence by Lecture 5 Theorem 12, $A$ is nonsingular. Next suppose $A$ is nonsingular. Let $\mathbf{b}\in{\mathbb{R}}^{m}$.
Then by Lecture 5 Theorem 12, $A\mathbf{x}=\mathbf{b}$ has solution for any $\mathbf{b}\in{\mathbb{R}}^{m}$.
Hence $\mathbf{b}\in\mathcal{C}\!\left(A\right)$. So $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{m}$.
∎
@end

@eg
$S^{\prime}=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\end{bmatrix}\}$ is a basis for ${\mathbb{R}}^{2}$. Let

\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}]=\begin{bmatrix}1&amp;1\\
0&amp;1\end{bmatrix}
\end{align*}

is nonsingular. Then $S^{\prime}$ is a basis for ${\mathbb{R}}^{2}$.
@end

@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}.
\end{align*}

Because $A$ is nonsingular, so the columns of $A$ is a basis for ${\mathbb{R}}^{3}$.
@end

@section{Dimension}

@defn
@title{Dimension}
Let $V$ be a vector space.
Suppose $\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\}$ is a basis for $V$. Then $t$ is called the <b>dimension</b>
of $V$ and is denoted by $t=\dim V$ and $V$ is called a finite dimensional vector space.
@end

<b>Warning</b>: We need to show that the dimension is well-defined, i.e.,
suppose both $\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\}$ and $\{\mathbf{u}_{1},\ldots,\mathbf{u}_{s}\}$ are bases for $V$,
then $s=t$.

@thm
@label{SSLD}
Suppose that $S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{t}\}$ is a finite set of vectors which spans the vector space $V$. Then any set of $t+1$ or more vectors from $V$ is linearly dependent.
@end
@proof
@col
We want to prove that any set of $t+1$ or more vectors from $V$ is linearly dependent. So we will begin with a totally arbitrary set of vectors from $V$, $R=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{m}\}$, where $m&gt;t$. We will now construct a nontrivial relation of linear dependence on $R$.

Each vector $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{m}$ can be written as a linear combination of the vectors $\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{t}$ since $S$ is a spanning set of $V$. This means there exist scalars $a_{ij}$, $1\leq i\leq t$, $1\leq j\leq m$, so that

\begin{align*}
\displaystyle\mathbf{u}_{1}&amp;\displaystyle=a_{11}\mathbf{v}_{1}+a_{21}\mathbf{v}_{2}+a_{31}\mathbf{v}_{3}+\cdots+a_{t1}\mathbf{v}_{t} \\
\displaystyle\mathbf{u}_{2}&amp;\displaystyle=a_{12}\mathbf{v}_{1}+a_{22}\mathbf{v}_{2}+a_{32}\mathbf{v}_{3}+\cdots+a_{t2}\mathbf{v}_{t} \\
\displaystyle\mathbf{u}_{3}&amp;\displaystyle=a_{13}\mathbf{v}_{1}+a_{23}\mathbf{v}_{2}+a_{33}\mathbf{v}_{3}+\cdots+a_{t3}\mathbf{v}_{t} \\
&amp;\displaystyle\quad\quad\vdots \\
\displaystyle\mathbf{u}_{m}&amp;\displaystyle=a_{1m}\mathbf{v}_{1}+a_{2m}\mathbf{v}_{2}+a_{3m}\mathbf{v}_{3}+\cdots+a_{tm}\mathbf{v}_{t}
\end{align*}

Now we form, unmotivated, the homogeneous system of $t$ equations in the $m$ variables, $x_{1},\,x_{2},\,x_{3},\,\ldots,\,x_{m}$, where the coefficients are the just-discovered scalars $a_{ij}$,

\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\cdots+a_{1m}x_{m}&amp;\displaystyle=0 \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\cdots+a_{2m}x_{m}&amp;\displaystyle=0 \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\cdots+a_{3m}x_{m}&amp;\displaystyle=0 \\
\displaystyle\vdots&amp; \\
\displaystyle a_{t1}x_{1}+a_{t2}x_{2}+a_{t3}x_{3}+\cdots+a_{tm}x_{m}&amp;\displaystyle=0
\end{align*}

This is a homogeneous system with more variables than equations (our hypothesis is expressed as $m&gt;t$), so there are infinitely many solutions. Choose a nontrivial solution and denote it by $x_{1}=c_{1},\,x_{2}=c_{2},\,x_{3}=c_{3},\,\ldots,\,x_{m}=c_{m}$. As a solution to the homogeneous system, we then have

\begin{align*}
\displaystyle a_{11}c_{1}+a_{12}c_{2}+a_{13}c_{3}+\cdots+a_{1m}c_{m}&amp;\displaystyle=0 \\
\displaystyle a_{21}c_{1}+a_{22}c_{2}+a_{23}c_{3}+\cdots+a_{2m}c_{m}&amp;\displaystyle=0 \\
\displaystyle a_{31}c_{1}+a_{32}c_{2}+a_{33}c_{3}+\cdots+a_{3m}c_{m}&amp;\displaystyle=0 \\
\displaystyle\vdots&amp; \\
\displaystyle a_{t1}c_{1}+a_{t2}c_{2}+a_{t3}c_{3}+\cdots+a_{tm}c_{m}&amp;\displaystyle=0
\end{align*}

As a collection of nontrivial scalars, $c_{1},\,c_{2},\,c_{3},\,\dots,\,c_{m}$ will provide the nontrivial relation of linear dependence we desire,

\begin{align*}
\displaystyle c_{1}\mathbf{u}_{1}+c_{2}\mathbf{u}_{2}+c_{3}\mathbf{u}_{3}+\cdots+c_{m}\mathbf{u}_{m} \\
\displaystyle=c_{1}\left(a_{11}\mathbf{v}_{1}+a_{21}\mathbf{v}_{2}+a_{31}\mathbf{v}_{3}+\cdots+a_{t1}\mathbf{v}_{t}\right) \\
\displaystyle\quad\quad+c_{2}\left(a_{12}\mathbf{v}_{1}+a_{22}\mathbf{v}_{2}+a_{32}\mathbf{v}_{3}+\cdots+a_{t2}\mathbf{v}_{t}\right) \\
\displaystyle\quad\quad+c_{3}\left(a_{13}\mathbf{v}_{1}+a_{23}\mathbf{v}_{2}+a_{33}\mathbf{v}_{3}+\cdots+a_{t3}\mathbf{v}_{t}\right) \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+c_{m}\left(a_{1m}\mathbf{v}_{1}+a_{2m}\mathbf{v}_{2}+a_{3m}\mathbf{v}_{3}+\cdots+a_{tm}\mathbf{v}_{t}\right) \\
\displaystyle=c_{1}a_{11}\mathbf{v}_{1}+c_{1}a_{21}\mathbf{v}_{2}+c_{1}a_{31}\mathbf{v}_{3}+\cdots+c_{1}a_{t1}\mathbf{v}_{t} \\
\displaystyle\quad\quad+c_{2}a_{12}\mathbf{v}_{1}+c_{2}a_{22}\mathbf{v}_{2}+c_{2}a_{32}\mathbf{v}_{3}+\cdots+c_{2}a_{t2}\mathbf{v}_{t} \\
\displaystyle\quad\quad+c_{3}a_{13}\mathbf{v}_{1}+c_{3}a_{23}\mathbf{v}_{2}+c_{3}a_{33}\mathbf{v}_{3}+\cdots+c_{3}a_{t3}\mathbf{v}_{t} \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+c_{m}a_{1m}\mathbf{v}_{1}+c_{m}a_{2m}\mathbf{v}_{2}+c_{m}a_{3m}\mathbf{v}_{3}+\cdots+c_{m}a_{tm}\mathbf{v}_{t} \\
\displaystyle=\left(c_{1}a_{11}+c_{2}a_{12}+c_{3}a_{13}+\cdots+c_{m}a_{1m}\right)\mathbf{v}_{1} \\
\displaystyle\quad\quad+\left(c_{1}a_{21}+c_{2}a_{22}+c_{3}a_{23}+\cdots+c_{m}a_{2m}\right)\mathbf{v}_{2} \\
\displaystyle\quad\quad+\left(c_{1}a_{31}+c_{2}a_{32}+c_{3}a_{33}+\cdots+c_{m}a_{3m}\right)\mathbf{v}_{3} \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+\left(c_{1}a_{t1}+c_{2}a_{t2}+c_{3}a_{t3}+\cdots+c_{m}a_{tm}\right)\mathbf{v}_{t} \\
\displaystyle=\left(a_{11}c_{1}+a_{12}c_{2}+a_{13}c_{3}+\cdots+a_{1m}c_{m}\right)\mathbf{v}_{1} \\
\displaystyle\quad\quad+\left(a_{21}c_{1}+a_{22}c_{2}+a_{23}c_{3}+\cdots+a_{2m}c_{m}\right)\mathbf{v}_{2} \\
\displaystyle\quad\quad+\left(a_{31}c_{1}+a_{32}c_{2}+a_{33}c_{3}+\cdots+a_{3m}c_{m}\right)\mathbf{v}_{3} \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+\left(a_{t1}c_{1}+a_{t2}c_{2}+a_{t3}c_{3}+\cdots+a_{tm}c_{m}\right)\mathbf{v}_{t} \\
\displaystyle=0\mathbf{v}_{1}+0\mathbf{v}_{2}+0\mathbf{v}_{3}+\cdots+0\mathbf{v}_{t} \\
\displaystyle=\mathbf{0}+\mathbf{0}+\mathbf{0}+\cdots+\mathbf{0} \\
\displaystyle=\mathbf{0}
\end{align*}

That does it. $R$ has been undeniably shown to be a linearly dependent set.

∎
@end
@thm

Suppose that $V$ is a vector space with a finite basis $B$ and a second basis $C$.
Then $B$ and $C$ have the same size.
@end
@proof
@col
Denote the size of $B$ by $t$. If $C$ has $\geq t+1$ vectors, then by the previous theorem, $C$ is linearly dependent. Contradict to the fact that $C$ is a basis. Denote the size of $C$ by $s$. So $s\leq t$. Because $C$ is a basis, if $t\geq s+1$, then $B$ is linearly dependent. Contradict to the fact that $B$ is a basis.
Hence $t\leq s$. So $s=t$.
∎
@end

The above theorem shows that the dimension is well-defined. No matter which basis we choose, the size is always the same.

@eg
$\dim{\mathbb{R}}^{m}=m$. See example 1.
@end
@eg
<b>Math major only</b>
$\dim M_{mn}=mn$. See example 3.
@end
@eg
<b>Math major only</b>
$\dim P_{n}=n+1$. See example 4.
@end
@eg
<b>Math major only</b>
Let $S_{2}$ be the set of $2\times 2$ symmetric matrices. For $A\in S_{2}$,

\begin{align*}
\displaystyle A=\begin{bmatrix}a&amp;b\\
b&amp;c\end{bmatrix}=a\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix}+b\begin{bmatrix}0&amp;1\\
1&amp;0\end{bmatrix}+c\begin{bmatrix}0&amp;0\\
0&amp;1\end{bmatrix}
\end{align*}

We can show that

\begin{align*}
\displaystyle T=\{\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix},\begin{bmatrix}0&amp;1\\
1&amp;0\end{bmatrix},\begin{bmatrix}0&amp;0\\
0&amp;1\end{bmatrix}\}
\end{align*}

is a basis for $S_{2}$. Hence $\dim S_{2}=3$.
@end
@eg
<b>Math major only</b>
Let $P$ be the set of all real polynomials. As $\{1,x,x^{2},x^{3},\ldots\}$ is linearly independent,
so $\dim P$ does not exists (or we can write $\dim P=\infty$).
@end
@lemma

Let $V$ be a vector space and $\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\in V$.
Suppose
$S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ is linearly independent and $\mathbf{u}\notin\left&lt;S\right&gt;$. Then
$S^{\prime}=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\}$ is linearly independent.
@end
@proof
@col
Let the relation of linear dependence of $S^{\prime}$ be

\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}+\alpha\mathbf{u}=\mathbf{0}.
\end{align*}

Suppose $\alpha\neq 0$, then

\begin{align*}
\displaystyle \mathbf{u}=-\frac{\alpha_{1}}{\alpha}\mathbf{v}_{1}-\cdots-\frac{\alpha_{k}}{\alpha}\mathbf{v}_{k}\in\left&lt;S\right&gt;.
\end{align*}

Contradiction.
So $\alpha=0$, then

\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\mathbf{0}.
\end{align*}

By the linear independence of $S$, $\alpha_{i}=0$ for all $i$. Hence the above relation of dependence of $S^{\prime}$ is trivial.
∎
@end
@thm
@label{basisexists2}
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
There exists a basis for $V$.
@end
@proof
@col
If $V$ is the zero vector space, i.e. $V=\{\mathbf{0}\}$. Then the theorem is trivial. Suppose $V$ is not the zero vector space, let $\mathbf{v}_{1}$ be a nonzero vector in $V$. If $V=\left&lt;\{\mathbf{v}_{1}\}\right&gt;$, we can take $S=\{\mathbf{v}_{1}\}$. Then obviously $\{\mathbf{v}_{1}\}$ is linearly independent and hence $S$ is a basis for $V$.
Otherwise, let $\mathbf{v}_{2}\in V$ but not in $\left&lt;\{\mathbf{v}_{1}\}\right&gt;$.
By the previous lemma, $\{\mathbf{v}_{1},\mathbf{v}_{2}\}$ is linearly independent. If $V=\left&lt;\{\mathbf{v}_{1},\mathbf{v}_{2}\}\right&gt;$, we can take $S=\{\mathbf{v}_{1},\mathbf{v}_{2}\}$.
So $S$ is a basis for $V$.
Otherwise, let $\mathbf{v}_{3}\in V$ but not in $\left&lt;\{\mathbf{v}_{1},\mathbf{v}_{2}\}\right&gt;$.
By the previous lemma, $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$ is linearly independent. Repeat the above process, inductive we can define $\mathbf{v}_{k+1}$ as following: If $V=\left&lt;\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\}\right&gt;$,
we can take $S=\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\}$.
Because $\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\}$ is linearly independent, $S$ is a basis for $V$.
Otherwise defined $\mathbf{v}_{k+1}\not\in\left&lt;\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\}\right&gt;$.
By the previous lemma, $\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k+1}\}$ is linearly independent.

If the process stops, say at step $k$, i.e., $V=\left&lt;\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\}\right&gt;$.
Then we can take $S=\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\}$.
Because $\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\}$ is linearly independent, it is a basis for $V$.
This completes the proof.
Otherwise, the process continues infinitely, in particular, we can take $k=m+1$ and
$V\neq\left&lt;\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\}\right&gt;$ and
$\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\}$ is linearly independent.

Because $\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\}$ is independent,
by theorem  @ref{SSLD} $\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\}$ is linearly dependent. Contradiction.
∎
@end
@proposition
@label{basisexistsprop}
Let $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\}\subseteq{\mathbb{R}}^{m}$. Then

\begin{align*}
\displaystyle \dim\left&lt;S\right&gt;\leq n.
\end{align*}
@end
@proof
@col
By Theorem  @ref{basisexists}, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left&lt;S\right&gt;$.

\begin{align*}
\displaystyle \dim\left&lt;S\right&gt;=\text{number of vectors in $T$}\leq\text{number of vectors in $S$}=k.
\end{align*}

∎
@end

<b>Remark</b>: both Theorem  @ref{basisexists} and Proposition  @ref{basisexistsprop} is value if we replace ${\mathbb{R}}^{m}$ by $P_{n}$, $M_{mn}$ or any finite dimensional vector space.

@section{Rank and nullity of a matrix}

@defn
@title{Nullity of a matrix}
@label{NOM}
Suppose that $A\in M_{mn}$. Then the <b>nullity</b> of $A$ is the dimension of the null space of $A$,
$n\left(A\right)=\dim({\mathcal{N}}\!\left(A\right))$.
@end
@defn
@title{Rank of a matrix}
@label{ROM}
Suppose that $A\in M_{mn}$. Then the <b>rank</b> of $A$ is the dimension of the column space of $A$,
$r\left(A\right)=\dim(\mathcal{C}\!\left(A\right))$.
@end
@eg
<b>Rank and nullity of a matrix</b>
Let us compute the rank and nullity of

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;-4&amp;-1&amp;3&amp;2&amp;1&amp;-4\\
1&amp;-2&amp;0&amp;0&amp;4&amp;0&amp;1\\
-2&amp;4&amp;1&amp;0&amp;-5&amp;-4&amp;-8\\
1&amp;-2&amp;1&amp;1&amp;6&amp;1&amp;-3\\
2&amp;-4&amp;-1&amp;1&amp;4&amp;-2&amp;-1\\
-1&amp;2&amp;3&amp;-1&amp;6&amp;3&amp;-1\end{bmatrix}
\end{align*}

To do this, we will first row-reduce the matrix since that will help us determine bases for the null space and column space.

\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;-2&amp;0&amp;0&amp;4&amp;0&amp;1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;3&amp;0&amp;-2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;-1&amp;0&amp;-3\\
0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

From this row-equivalent matrix in reduced row-echelon form we record $D=\{1,\,3,\,4,\,6\}$ and $F=\{2,\,5,\,7\}$.

By Lecture 14, Theorem 3, for each index in $D$, we can create a single basis vector.
In fact $T=\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4},\mathbf{A}_{6}\}$ is a basis for $\mathcal{C}\!\left(A\right)$.
In total the basis will have $4$ vectors, so the column space of $A$ will have dimension $4$ and we write $r\left(A\right)=4$.

Lecture 9 Section 3, for each index in $F$, we can create a single basis vector. In total the basis will have $3$ vectors, so the null space of $A$ will have dimension $3$ and we write $n\left(A\right)=3$. In fact

\begin{align*}
\displaystyle R=\{\begin{bmatrix}2\\
1\\
0\\
0\\
0\\
0\\
0\end{bmatrix},\begin{bmatrix}-4\\
0\\
-3\\
1\\
1\\
0\\
0\end{bmatrix},\begin{bmatrix}-1\\
0\\
2\\
3\\
0\\
-1\\
1\end{bmatrix}\}
\end{align*}

is a basis for ${\mathcal{N}}\!\left(A\right)$.
@end
@thm
@title{Computing rank and nullity}
Suppose $A\in M_{mn}$ and $A\xrightarrow{\text{RREF}}B$.
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).
Then $r\left(A\right)=r$ and $n\left(A\right)=n-r$.
@end
@proof
@col
Let $D=\{d_{1},\ldots,d_{r}\}$ be the indexes of the pivot columns of $B$.
By Lecture 10 Theorem 2 or Lecture 14 Theorem 3, $\{\mathbf{A}_{d_{1}},\ldots,\mathbf{A}_{d_{r}}\}$ is a basis for $\mathcal{C}\!\left(A\right)$.
So $r\left(A\right)=r$. By Lecture 9 Sect 3, each free variable corresponding to a single basis vector for the null space.
So $n\left(A\right)$ is the number of free variables $=n-r$.
∎
@end
@corollary
@title{Dimension formula}
Suppose $A\in M_{mn}$, then

\begin{align*}
\displaystyle r\left(A\right)+n\left(A\right)=n.
\end{align*}
@end
@thm

Let $A$ be a $m\times n$ matrix. Then

\begin{align*}
\displaystyle r\left(A\right)=r\left(A^{t}\right).
\end{align*}

Equivalently

\begin{align*}
\displaystyle \dim\mathcal{C}\!\left(A\right)=\dim\mathcal{R}\!\left(A\right).
\end{align*}
@end
@proof
@col
Let $A\xrightarrow{\text{RREF}}B$.
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).
Then by the above discussion $r=r\left(A\right)$. By Lecture 14 Theorem 7, the first $r$ columns of $B^{t}$ form a basis for $\mathcal{R}\!\left(A\right)$. Hence $r=r\left(A^{t}\right)$. This completes the proof.
∎
@end

Let us take a look at the rank and nullity of a square matrix.

@eg
The matrix

\begin{align*}
\displaystyle E=\begin{bmatrix}0&amp;4&amp;-1&amp;2&amp;2&amp;3&amp;1\\
2&amp;-2&amp;1&amp;-1&amp;0&amp;-4&amp;-3\\
-2&amp;-3&amp;9&amp;-3&amp;9&amp;-1&amp;9\\
-3&amp;-4&amp;9&amp;4&amp;-1&amp;6&amp;-2\\
-3&amp;-4&amp;6&amp;-2&amp;5&amp;9&amp;-4\\
9&amp;-3&amp;8&amp;-2&amp;-4&amp;2&amp;4\\
8&amp;2&amp;2&amp;9&amp;3&amp;0&amp;9\end{bmatrix}
\end{align*}

is row-equivalent to the matrix in reduced row-echelon form,

\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}\end{bmatrix}
\end{align*}

With $n=7$ columns and $r=7$ nonzero rows tells us the rank is $r\left(E\right)=7$ and the nullity is $n\left(E\right)=7-7=0$.
@end

The value of either the nullity or the rank are enough to characterize a nonsingular matrix.

@thm
@title{Rank and Nullity of a Nonsingular Matrix}
@label{RNNM}
Suppose that $A$ is a square matrix of size $n$. The following are equivalent.

<ol class="ltx_enumerate">
<li class="ltx_item">
A is nonsingular.
</li>
<li class="ltx_item">
The rank of $A$ is $n$, $r\left(A\right)=n$.
</li>
<li class="ltx_item">
The nullity of $A$ is zero, $n\left(A\right)=0$.
</li>

</ol>
@end
@proof
@col
(1 $\Rightarrow$ 2) If $A$ is nonsingular then $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$.
If $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$, then the column space has dimension $n$, so the rank of $A$ is $n$.

(2 $\Rightarrow$ 3) Suppose $r\left(A\right)=n$. Then the dimension formula gives

\begin{align*}
\displaystyle n\left(A\right)&amp;\displaystyle=n-r\left(A\right) \\
&amp;\displaystyle=n-n \\
&amp;\displaystyle=0
\end{align*}

(3 $\Rightarrow$ 1) Suppose $n\left(A\right)=0$, so a basis for the null space of $A$ is the empty set. This implies that ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$ and hence $A$ is nonsingular.
∎
@end

With a new equivalence for a nonsingular matrix, we can update our list of equivalences which now becomes a list requiring double digits to number.

@thm

Suppose that $A$ is a square matrix of size $n$. The following are equivalent.

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is nonsingular.
</li>
<li class="ltx_item">
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item">
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
</li>
<li class="ltx_item">
The linear system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>
<li class="ltx_item">
The columns of $A$ are a linearly independent set.
</li>
<li class="ltx_item">
$A$ is invertible.
</li>
<li class="ltx_item">
The column space of $A$ is ${\mathbb{R}}^{n}$, $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$.
</li>
<li class="ltx_item">
The columns of $A$ are a basis for ${\mathbb{R}}^{n}$.
</li>
<li class="ltx_item">
The rank of $A$ is $n$, $r\left(A\right)=n$.
</li>
<li class="ltx_item">
The nullity of $A$ is zero, $n\left(A\right)=0$.
</li>

</ol>
@end

@section{Linear relation of }

<b>You can skip this section. It is for math major only</b>In this section, we discuss the linear relation of $P_{n}$ or $M_{mn}$ by using the techniques used for the vector space ${\mathbb{R}}^{k}$.

Let $V=P_{n}$ and $f_{1},\ldots,f_{m},g\in P_{n}$.
Write

\begin{align*}
\displaystyle f_{i}(x)=a_{i0}+a_{i1}x+\cdots+a_{in}x^{n},
\end{align*}

\begin{align*}
\displaystyle g(x)=b_{0}+b_{1}x+\cdots+b_{n}x^{n}.
\end{align*}

By comparing coefficients,

\begin{align*}
\displaystyle g(x)=\alpha_{1}f_{1}(x)+\alpha_{2}f_{2}(x)+\cdots+\alpha_{m}f_{m}(x)
\end{align*}

if and only if

\begin{align*}
\displaystyle \alpha_{1}a_{10}+\alpha_{2}a_{20}+\cdots+\alpha_{m}a_{m0}=b_{0},
\end{align*}

\begin{align*}
\displaystyle \alpha_{1}a_{11}+\alpha_{2}a_{21}+\cdots+\alpha_{m}a_{m1}=b_{1},
\end{align*}

\begin{align*}
\displaystyle \vdots
\end{align*}

\begin{align*}
\displaystyle \alpha_{1}a_{1n}+\alpha_{2}a_{2n}+\cdots+\alpha_{m}a_{mn}=b_{n}
\end{align*}

if and only if

\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix}+\alpha_{2}\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix}+\cdots+\alpha_{m}\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*}

The above motivates us to define

\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix},\cdots,\mathbf{v}_{m}=\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix},\mathbf{u}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*}

The entries of $\mathbf{v}_{i}$ are the coefficients of $f_{i}$.
We then have the following theorem

@thm

<ol class="ltx_enumerate">
<li class="ltx_item">
$\{f_{1},\ldots,f_{m}\}$ is linearly independent if and only if $\{\mathbf{v}_{1},\ldots,\mathbf{v}_{m}\}$
is linearly independent.
</li>
<li class="ltx_item">
$g$ is a linearly combination of $f_{1},\ldots,f_{m}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{m}$
</li>

</ol>
@end

Problems regarding polynomials can therefore be transformed to problems regarding column vectors.

Similarly given $m\times n$ matrix $A_{1},\ldots,A_{k},B$. Let

\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}[A_{1}]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}[A_{2}]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix},\cdots,\mathbf{u}=\begin{bmatrix}[B]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix}.
\end{align*}

We have the following

@thm

<ol class="ltx_enumerate">
<li class="ltx_item">
$\{A_{1},\ldots,A_{k}\}$ is linearly independent if and only if $\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$
is linearly independent.
</li>
<li class="ltx_item">
$B$ is a linearly combination of $A_{1},\ldots,A_{k}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{k}$
</li>

</ol>
@end

Again, problems regarding polynomials can be transformed to problems regarding column vectors.

@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
Determine if

\begin{align*}
\displaystyle A_{1}=\begin{bmatrix}1&amp;2\\
3&amp;4\end{bmatrix},A_{2}=\begin{bmatrix}1&amp;-1\\
5&amp;6\end{bmatrix},A_{3}=\begin{bmatrix}-2&amp;0\\
-3&amp;-4\end{bmatrix}
\end{align*}

is linearly independent or not.
</li>
<li class="ltx_item">
Express

\begin{align*}
\displaystyle B=\begin{bmatrix}-3&amp;0\\
4&amp;1\end{bmatrix}
\end{align*}

as a linear combination of $A_{1},A_{2},A_{3}$.
</li>

</ol>
@end

<b>Answer.</b>

<ol class="ltx_enumerate">
<li class="ltx_item">
Let

\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
3\\
2\\
4\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
5\\
-1\\
6\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}-2\\
-3\\
0\\
-4\end{bmatrix},\mathbf{u}=\begin{bmatrix}-3\\
4\\
0\\
1\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}]=\begin{bmatrix}1&amp;1&amp;-2\\
3&amp;5&amp;-3\\
2&amp;-1&amp;0\\
4&amp;6&amp;-4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;1\\
0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*}

Obviously the columns of the RREF is linearly independent, hence $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$
is linearly independent. Therefore $\{A_{1},A_{2},A_{3}\}$ is linearly independent.
</li>
<li class="ltx_item">
Next

\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{b}]=\begin{bmatrix}1&amp;1&amp;-2&amp;-3\\
3&amp;5&amp;-3&amp;4\\
2&amp;-1&amp;0&amp;0\\
4&amp;6&amp;-5&amp;1\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;0&amp;1\\
0&amp;1&amp;0&amp;2\\
0&amp;0&amp;1&amp;3\\
0&amp;0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*}

Then $\mathbf{u}=\mathbf{v}_{1}+2\mathbf{v}_{2}+3\mathbf{v}_{3}$. Hence $B=A_{1}+2A_{2}+3A_{3}$.
</li>

</ol>

$\square$

@eg
Let $f_{1}(x)=1+x+x^{3}$, $f_{2}(x)=2+x+x^{2}$, $f_{3}(x)=4+3x+x^{2}+2x^{3}$, $f_{4}(x)=2x^{2}+x^{3}$, $f_{5}(x)=3+2x+3x^{2}+2x^{3}$.
Find a basis for $\left&lt;\{f_{1},f_{2},f_{3},f_{4},f_{5}\}\right&gt;$.
@end

<b>Answer.</b>
Let

\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\\
1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}2\\
1\\
1\\
0\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}4\\
3\\
1\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}0\\
0\\
2\\
1\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}3\\
2\\
3\\
2\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{v}_{4}|\mathbf{v}_{5}]=\begin{bmatrix}1&amp;2&amp;4&amp;0&amp;3\\
1&amp;1&amp;3&amp;0&amp;2\\
0&amp;1&amp;1&amp;2&amp;3\\
1&amp;0&amp;2&amp;1&amp;2\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;2&amp;0&amp;1\\
0&amp;1&amp;1&amp;0&amp;1\\
0&amp;0&amp;0&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*}

Therefore $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{4}\}$ is a basis for $\left&lt;\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4},\mathbf{v}_{5}\}\right&gt;$.
So $\{f_{1},f_{2},f_{4}\}$ is a basis for $\left&lt;\{f_{1},f_{2},f_{3},f_{4},f_{5}\}\right&gt;$.
$\square$

@chapter{Linear transformations and change of basis}

<b>Skip this lecture</b>

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section VR.

@section{Matrices as linear transformation}

Let $A\in M_{mn}$, $\mathbf{v}\in{\mathbb{R}}^{n}$. Then $A\mathbf{v}\in{\mathbb{R}}^{m}$. We can define define a map, denoted by $L_{A}$ (stand for left multiplying), defined as

\begin{align*}
\displaystyle L_{A}:{\mathbb{R}}^{n}\rightarrow{\mathbb{R}}^{m}.
\end{align*}

\begin{align*}
\displaystyle L_{A}(\mathbf{v})=A\mathbf{v}.
\end{align*}

@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;3\\
4&amp;5&amp;6\end{bmatrix}.
\end{align*}

Then $L_{A}$ is a map from ${\mathbb{R}}^{3}\rightarrow{\mathbb{R}}^{2}$. Define by

\begin{align*}
\displaystyle L_{A}(\begin{bmatrix}x\\
y\\
z\end{bmatrix})=\begin{bmatrix}x+2y+3z\\
4x+5y+6z\end{bmatrix}.
\end{align*}
@end

The map $L_{A}$ has the following properties:

@proposition
For $\mathbf{v},\mathbf{u}\in{\mathbb{R}}^{n}$, $\alpha\in{\mathbb{R}}^{\hbox{}}$

<ol class="ltx_enumerate">
<li class="ltx_item">
$L_{A}(\mathbf{v}+\mathbf{u})=L_{A}(\mathbf{v})+L_{A}(\mathbf{u})$
</li>
<li class="ltx_item">
$L_{A}(\alpha\mathbf{v})=\alpha L_{A}(\mathbf{v})$.
</li>

</ol>
@end
@proof
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
$L_{A}(\mathbf{v}+\mathbf{u})=A(\mathbf{v}+\mathbf{u})=A\mathbf{v}+A\mathbf{u}=L_{A}(\mathbf{v})+L_{A}(\mathbf{u})$
</li>
<li class="ltx_item">
$L_{A}(\alpha\mathbf{v})=A(\alpha\mathbf{v})=\alpha A(\mathbf{v})=\alpha L_{A}(\mathbf{v})$.
</li>

</ol>

∎
@end
@defn
Let $V,W$ be vector spaces (you can think of $V$ as subspace of ${\mathbb{R}}^{a}$ and $W$ as subspace of ${\mathbb{R}}^{b}$),
$T:V\rightarrow W$.
Then $T$ is said to be a linear transformation if for $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{u}\in W$.

<ol class="ltx_enumerate">
<li class="ltx_item">
$T(\mathbf{v}+\mathbf{u})=T(\mathbf{v})+T(\mathbf{u})$
</li>
<li class="ltx_item">
$T(\alpha\mathbf{v})=\alpha T(\mathbf{v})$.
</li>

</ol>
@end

@section{Geometric interpretation of some linear transformations}

In below we regard $\begin{bmatrix}x\\
y\end{bmatrix}$ as the point $(x,y)$ in the $xy$-plane.

@eg
<b>Rotation</b> Let

\begin{align*}
\displaystyle A=\begin{bmatrix}\cos\theta&amp;-\sin\theta\\
\sin\theta&amp;\cos\theta\end{bmatrix}.
\end{align*}

Then $L_{A}$ is $\theta$ degree anticlockwise rotation.
@end
@eg
<b>Reflection</b> Let $c$ be a real number,

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;0\\
0&amp;-1\end{bmatrix}\,\,\,(\text{resp.}A=\begin{bmatrix}-1&amp;0\\
0&amp;1\end{bmatrix})
\end{align*}

Then $L_{A}$ is a reflection through the $x$-axis (resp. $y$-axis).
@end
@eg
<b>Stretching</b> Let $c$ be real number, then

\begin{align*}
\displaystyle A=\begin{bmatrix}k&amp;0\\
0&amp;1\end{bmatrix}\,\,\,(\text{resp.}A=\begin{bmatrix}1&amp;0\\
0&amp;k\end{bmatrix})
\end{align*}

Then $L_{A}$ is a stretching of $x$-axis (resp. $y$-axis) by a factor of $k$.
@end

@section{Coordinate vector and change of basis}

Let $V$ be a vector space(you can think of it as a subspace of ${\mathbb{R}}^{n}$). Let $B=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ be a basis for $V$. Recall the following theorem

@thm

Let $\mathbf{v}\in V$ and if

\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\,\,\alpha_{i}\in{\mathbb{R}}^{\hbox{}}&amp;
\end{align*}

then $\alpha_{1},\cdots,\alpha_{k}$ are unique, i.e.

\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\beta_{1}\mathbf{v}_{1}+\cdots+\beta_{k}\mathbf{v}_{k},&amp;
\end{align*}

then

\begin{align*}
\displaystyle \alpha_{i}=\beta_{i},\,\,\,i=1,\ldots,k.
\end{align*}
@end
@proof
@col
If ( @ref{vlin2}) is true, then

\begin{align*}
\displaystyle (\alpha_{1}-\beta_{1})\mathbf{v}_{1}+\cdots+(\alpha_{k}-\beta_{k})\mathbf{v}_{k}=\mathbf{0}.
\end{align*}

Because $S$ is a basis, it is linearly independent. Therefore

\begin{align*}
\displaystyle \alpha_{i}-\beta_{i}=0.
\end{align*}

Hence

\begin{align*}
\displaystyle \alpha_{i}=\beta_{i}.
\end{align*}

∎
@end

By the theorem, using notation in ( @ref{vlin}), we can define

\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\begin{bmatrix}\alpha_{1}\\
\alpha_{2}\\
\vdots\\
\alpha_{k}\end{bmatrix}.
\end{align*}

@eg
Let $V={\mathbb{R}}^{3}$, $B=\{\mathbf{e}_{1},\mathbf{e}_{2},\mathbf{e}_{3}\}$.
Then

\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}x\\
y\\
z\end{bmatrix}=x\mathbf{e}_{1}+y\mathbf{e}_{2}+z\mathbf{e}_{3}.
\end{align*}

Hence

\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\begin{bmatrix}x\\
y\\
z\end{bmatrix}.
\end{align*}
@end
@eg
Generally if $V={\mathbb{R}}^{n}$, $B=\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}$, then

\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\mathbf{v}.
\end{align*}
@end
@eg
Let $V={\mathbb{R}}^{3}$,

\begin{align*}
\displaystyle B=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}1\\
2\\
3\end{bmatrix}\}.
\end{align*}

We can show that $B$ is linearly independent (exercise). Hence $B$ is a basis for $V$.
Let

\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}3\\
4\\
3\end{bmatrix}.
\end{align*}

Find $\rho_{B}(\mathbf{u})$. To find $\rho_{B}(\mathbf{u})$, we compute the coefficient

\begin{align*}
\displaystyle \mathbf{u}=x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3},
\end{align*}

i.e.

\begin{align*}
\displaystyle x_{1}+x_{2}+x_{3}&amp;\displaystyle=3 \\
\displaystyle x_{1}+x+2+2x_{3}&amp;\displaystyle=4 \\
\displaystyle x+2+3x_{3}&amp;\displaystyle=3
\end{align*}

Solve the system of linear equations, we have $x_{1}=2$, $x_{2}=0$, $x_{3}=1$. Hence

\begin{align*}
\displaystyle \rho_{B}(\mathbf{u})=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}=\begin{bmatrix}2\\
0\\
1\end{bmatrix}.
\end{align*}
@end
@eg
Let

\begin{align*}
\displaystyle B=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
3\\
4\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
-1\\
1\\
-1\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}-1\\
1\\
2\\
2\end{bmatrix}.\}.
\end{align*}

Let

\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}0\\
3\\
11\\
8\end{bmatrix}.
\end{align*}

<ol class="ltx_enumerate">
<li class="ltx_item">
Show that $\mathbf{u}\in\left&lt;B\right&gt;$.
</li>
<li class="ltx_item">
Hence find $\rho_{B}(\mathbf{u})$.
</li>

</ol>

Find the solution of $\mathbf{u}=x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3}$:

\begin{align*}
\displaystyle x_{1}+x_{2}-x_{3}&amp;\displaystyle=0 \\
\displaystyle 2x_{1}-x_{2}+x_{3}&amp;\displaystyle=3 \\
\displaystyle 3x_{1}+x_{2}+2x_{3}&amp;\displaystyle=11 \\
\displaystyle 4x_{1}-x_{2}+2x_{3}&amp;\displaystyle=8
\end{align*}

We find solution $x_{1}=1,x_{2}=2,x_{3}=3$. Hence

\begin{align*}
\displaystyle \rho_{B}(\mathbf{u})=\begin{bmatrix}1\\
2\\
3\end{bmatrix}.
\end{align*}
@end

@section{Change of basis}

Let $V$ be a vector space(you can think of it as a subspace of ${\mathbb{R}}^{n}$). Let $B=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ be a basis for $V$
and $B^{\prime}=\{\mathbf{v}^{\prime}_{1},\ldots,\mathbf{v}^{\prime}_{k}\}$ be another basis for $V$. For $\mathbf{v}\in V$. We want to find the relation between $\rho_{B}(\mathbf{v})$ and $\rho_{B^{\prime}}(\mathbf{v})$.

Let

\begin{align*}
\displaystyle \mathbf{v}_{i}=a_{1i}\mathbf{v}^{\prime}_{1}+a_{2i}\mathbf{v}^{\prime}_{2}+\cdots+a_{ki}\mathbf{v}^{\prime}_{k},&amp;
\end{align*}

i.e.

\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v}_{i})=\begin{bmatrix}a_{1i}\\
a_{2i}\\
\vdots\\
a_{ki}\end{bmatrix}.
\end{align*}

Define the <b>change-of-basis matrix from $S$ to $S^{\prime}$</b> as

\begin{align*}
\displaystyle C_{{B},{B^{\prime}}}=[\rho_{B^{\prime}}(\mathbf{v}_{1})|\rho_{B^{\prime}}(\mathbf{v}_{2})|\cdots|\rho_{B^{\prime}}(\mathbf{v}_{k})]=\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1k}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2k}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{k1}&amp;a_{k2}&amp;\cdots&amp;a_{kk}\end{bmatrix}.
\end{align*}

@thm

For $\mathbf{v}\in V$,

\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v})=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{v}).
\end{align*}
@end
@proof
@col
Let

\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\begin{bmatrix}\alpha_{1}\\
\alpha_{2}\\
\vdots\\
\alpha_{k}\end{bmatrix},\,\rho_{B^{\prime}}(\mathbf{v})=\begin{bmatrix}\alpha^{\prime}_{1}\\
\alpha^{\prime}_{2}\\
\vdots\\
\alpha^{\prime}_{k}\end{bmatrix},
\end{align*}

i.e.

\begin{align*}
\displaystyle\mathbf{v}&amp;\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\cdots+\alpha_{k}\mathbf{v}_{k} \\
&amp;\displaystyle=\alpha^{\prime}_{1}\mathbf{v}^{\prime}_{1}+\alpha^{\prime}_{2}\mathbf{v}^{\prime}_{2}+\cdots+\alpha^{\prime}_{k}\mathbf{v}^{\prime}_{k}.
\end{align*}

By  @ref{vv'},

\begin{align*}
\displaystyle\mathbf{v}&amp;\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\cdots+\alpha_{k}\mathbf{v}_{k} \\
&amp;\displaystyle=\alpha_{1}(a_{11}\mathbf{v}^{\prime}_{1}+a_{21}\mathbf{v}^{\prime}_{2}+\cdots+a_{k1}\mathbf{v}^{\prime}_{k}) \\
&amp;\displaystyle+\alpha_{2}(a_{12}\mathbf{v}^{\prime}_{1}+a_{22}\mathbf{v}^{\prime}_{2}+\cdots+a_{k2}\mathbf{v}^{\prime}_{k}) \\
&amp;\displaystyle+\cdots \\
&amp;\displaystyle+\alpha_{2}(a_{1k}\mathbf{v}^{\prime}_{1}+a_{2k}\mathbf{v}^{\prime}_{2}+\cdots+a_{kk}\mathbf{v}^{\prime}_{k}) \\
&amp;\displaystyle=(a_{11}\alpha_{1}+a_{12}\alpha_{2}+\cdots+a_{1k}\alpha_{k})\mathbf{v}^{\prime}_{1} \\
&amp;\displaystyle+(a_{21}\alpha_{1}+a_{22}\alpha_{2}+\cdots+a_{2k}\alpha_{k})\mathbf{v}^{\prime}_{2} \\
&amp;\displaystyle+\cdots \\
&amp;\displaystyle+(a_{k1}\alpha_{1}+a_{k2}\alpha_{2}+\cdots+a_{kk}\alpha_{k})\mathbf{v}^{\prime}_{2}
\end{align*}

Thus

\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v})=\begin{bmatrix}\alpha^{\prime}_{1}\\
\alpha^{\prime}_{2}\\
\vdots\\
\alpha^{\prime}_{k}\end{bmatrix}=\begin{bmatrix}a_{11}\alpha_{1}+a_{12}\alpha_{2}+\cdots+a_{1k}\alpha_{k}\\
a_{21}\alpha_{1}+a_{22}\alpha_{2}+\cdots+a_{2k}\alpha_{k}\\
\vdots\\
a_{k1}\alpha_{1}+a_{k2}\alpha_{2}+\cdots+a_{kk}\alpha_{k}\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle =\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1k}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2k}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{k1}&amp;a_{k2}&amp;\cdots&amp;a_{kk}\end{bmatrix}\begin{bmatrix}\alpha_{1}\\
\alpha_{2}\\
\vdots\\
\alpha_{k}\end{bmatrix}=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{v})
\end{align*}

∎
@end
@eg
In example 7,
let $V={\mathbb{R}}^{3}$,

\begin{align*}
\displaystyle B=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}1\\
2\\
3\end{bmatrix}\},
\end{align*}

\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}3\\
4\\
3\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle \rho_{B}(\mathbf{u})=\begin{bmatrix}2\\
0\\
1\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle B^{\prime}=\{\mathbf{v}^{\prime}_{1}=\begin{bmatrix}0\\
0\\
1\end{bmatrix},\mathbf{v}^{\prime}_{2}=\begin{bmatrix}1\\
0\\
1\end{bmatrix},\mathbf{v}^{\prime}_{3}=\begin{bmatrix}1\\
1\\
-1\end{bmatrix}\}.
\end{align*}

Then

\begin{align*}
\displaystyle\mathbf{v}_{1}&amp;\displaystyle=\mathbf{v}^{\prime}_{1}+\mathbf{v}^{\prime}_{3} \\
\displaystyle\mathbf{v}_{2}&amp;\displaystyle=2\mathbf{v}^{\prime}_{1}+\mathbf{v}^{\prime}_{3} \\
\displaystyle\mathbf{v}_{3}&amp;\displaystyle=6\mathbf{v}^{\prime}_{1}-\mathbf{v}^{\prime}_{2}+2\mathbf{v}^{\prime}_{3}.
\end{align*}

Then

\begin{align*}
\displaystyle C_{{B},{B^{\prime}}}=\begin{bmatrix}1&amp;2&amp;6\\
0&amp;0&amp;-1\\
1&amp;1&amp;2\end{bmatrix}
\end{align*}

So

\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{u})=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{u})=\begin{bmatrix}1&amp;2&amp;6\\
0&amp;0&amp;-1\\
1&amp;1&amp;2\end{bmatrix}\begin{bmatrix}2\\
0\\
1\end{bmatrix}=\begin{bmatrix}8\\
-1\\
4\end{bmatrix}.
\end{align*}

Verification:

\begin{align*}
\displaystyle \mathbf{u}=8\mathbf{v}^{\prime}_{1}-\mathbf{v}^{\prime}_{2}+4\mathbf{v}^{\prime}_{3},
\end{align*}

i.e.

\begin{align*}
\displaystyle \begin{bmatrix}3\\
4\\
3\end{bmatrix}=8\begin{bmatrix}0\\
0\\
1\end{bmatrix}-\begin{bmatrix}1\\
0\\
1\end{bmatrix}+4\begin{bmatrix}1\\
1\\
-1\end{bmatrix}.
\end{align*}
@end

@section{Similar matrices}

In this section, we will give the motivation of <b>similar matrices</b>.
Let $V={\mathbb{R}}^{n}$, $B=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\}$ be a basis for $V$.
Let $B^{\prime}=\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}$. Let $Q=C_{{B},{B^{\prime}}}$.
Then for any $\mathbf{v}\in V$,

\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v})=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{v})
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}=Q\rho_{B}(\mathbf{v}).
\end{align*}

Let $\mathbf{w}=L_{A}(\mathbf{v})=A\mathbf{v}$. Then

\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{w})=\rho_{B^{\prime}}(A\mathbf{v})
\end{align*}

\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{w})=A\mathbf{v}=A\rho_{B^{\prime}}(\mathbf{v})
\end{align*}

\begin{align*}
\displaystyle Q\rho_{B}(\mathbf{w})=AQ\rho_{B}(\mathbf{v}).
\end{align*}

\begin{align*}
\displaystyle \rho_{B}(\mathbf{w})=Q^{-1}AQ\rho_{B}(\mathbf{v}).
\end{align*}

So we say that two matrices $A_{1},A_{2}$ are similar if there exists an invertible matrix $Q$ such that $Q^{-1}A_{1}Q=A_{2}$.
More details will be given in lecture 19.

@chapter{Inverse}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section MISLE, Section MINM (print version p149 - p161)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection MISLE (p60-64), all. Section MINM C20, C40, M10, M11, M15, M80, T25.

@section{Solution Inverse}
@label{SI}
The inverse of a square matrix, and solutions to linear systems with square coefficient matrices, are intimately connected.

@eg
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}

We can represent this system of equations as

\begin{align*}
\displaystyle A\mathbf{x}=\mathbf{b}
\end{align*}

where

\begin{align*}
\displaystyle A=\begin{bmatrix}-7&amp;-6&amp;-12&amp;-33\\
5&amp;5&amp;7&amp;24\\
1&amp;0&amp;4&amp;5\end{bmatrix}&amp;\displaystyle\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}&amp;\displaystyle\mathbf{b}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}
\end{align*}

Now, entirely unmotivated, we define the $3\times 3$ matrix $B$,

\begin{align*}
\displaystyle \begin{bmatrix}-10&amp;-12&amp;-9\\
\frac{13}{2}&amp;8&amp;\frac{11}{2}\\
\frac{5}{2}&amp;3&amp;\frac{5}{2}\end{bmatrix}
\end{align*}

and note the remarkable fact that

\begin{align*}
\displaystyle BA=\begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;1\end{bmatrix}
\end{align*}

Now apply this computation to the problem of solving the system of equations,

\begin{align*}
\displaystyle\mathbf{x}=I_{3}\mathbf{x}=(BA)\mathbf{x}=B(A\mathbf{x})=B\mathbf{b}
\end{align*}

So we have

\begin{align*}
\displaystyle \mathbf{x}=B\mathbf{b}=\begin{bmatrix}-3\\
5\\
2\end{bmatrix}
\end{align*}

So with the help and assistance of $B$ we have been able to determine a solution to the system represented by $A\mathbf{x}=\mathbf{b}$ through judicious use of matrix multiplication. Since the coefficient matrix in this example is nonsingular, there would be a unique solution, no matter what the choice of $\mathbf{b}$. The derivation above amplifies this result, since we were forced to conclude that $\mathbf{x}=B\mathbf{b}$ and the solution could not be anything else. You should notice that this argument would hold for any particular choice of $\mathbf{b}$.
@end

The matrix $B$ of the previous example is called the inverse of $A$. When $A$ and $B$ are combined via matrix multiplication, the result is the identity matrix, which can be inserted <b>in front</b> of $\mathbf{x}$ as the first step in finding the solution. This is entirely analogous to how we might solve a single linear equation like $3x=12$.

\begin{align*}
\displaystyle x=1x=\left(\frac{1}{3}\left(3\right)\right)x=\frac{1}{3}\left(3x\right)=\frac{1}{3}\left(12\right)=4
\end{align*}

Here we have obtained a solution by employing the <b>multiplicative inverse</b> of $3$, $3^{-1}=\frac{1}{3}$. This works fine for any scalar multiple of $x$, except for zero, since zero does not have a multiplicative inverse. Consider separately the two linear equations,

\begin{align*}
\displaystyle 0x&amp;\displaystyle=12&amp;\displaystyle 0x&amp;\displaystyle=0
\end{align*}

The first has no solutions, while the second has infinitely many solutions. For matrices, it is all just a little more complicated. Some matrices have inverses, some do not. And when a matrix does have an inverse, just how would we compute it? In other words, just where did that matrix $B$ in the last example come from? Are there other matrices that might have worked just as well?

@section{Inverse of a Matrix}

@defn
Suppose $A$ and $B$ are square matrices of size $n$ such that $AB=I_{n}$ and $BA=I_{n}$. Then $A$ is <b>invertible</b> and $B$ is the <b>inverse</b> of $A$. In this situation, we write $B=A^{-1}$.
@end

Notice that if $B$ is the inverse of $A$, then we can just as easily say $A$ is the inverse of $B$, or $A$ and $B$ are inverses of each other.

Not every square matrix has an inverse.

@eg
<b>A matrix without an inverse</b>

Consider the coefficient matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;-1&amp;2\\
2&amp;1&amp;1\\
1&amp;1&amp;0\end{bmatrix}
\end{align*}

Suppose that $A$ is invertible and does have an inverse, say $B$. Choose the vector of constants

\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}1\\
3\\
2\end{bmatrix}
\end{align*}

and consider the system of equations $A\mathbf{x}=\mathbf{b}$. Just as in the previous example, this vector equation would have the unique solution $\mathbf{x}=B\mathbf{b}$.

However, the system $A\mathbf{x}=\mathbf{b}$ is inconsistent. Form the augmented matrix $\left[A|\mathbf{b}\right]$ and row-reduce to

\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;1&amp;0\\
0&amp;\boxed{1}&amp;-1&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}\end{bmatrix}
\end{align*}

which allows us to recognize the inconsistency.

So the assumption of $A$’s inverse leads to a logical inconsistency (the system cannot be both consistent and inconsistent), so our assumption is false. $A$ is not invertible.
@end

Let us look at one more matrix inverse before we embark on a more systematic study.

@eg
<b>Matrix inverse</b>

<b>1.</b>

\begin{align*}
\displaystyle A=\left[\begin{array}[]{cc}1&amp;2\\
2&amp;3\\
\end{array}\right],B=\left[\begin{array}[]{cc}-3&amp;2\\
2&amp;-1\\
\end{array}\right],
\end{align*}

Then

\begin{align*}
\displaystyle AB=BA=I_{2}.
\end{align*}

So $B$ is the inverse of $A$.

<b>2.</b>

\begin{align*}
\displaystyle A=\left[\begin{array}[]{ccc}1&amp;1&amp;1\\
1&amp;0&amp;-1\\
0&amp;1&amp;1\\
\end{array}\right],B=\left[\begin{array}[]{ccc}1&amp;0&amp;-1\\
-1&amp;1&amp;2\\
1&amp;-1&amp;-1\\
\end{array}\right]
\end{align*}

Then

\begin{align*}
\displaystyle AB=BA=I_{3}.
\end{align*}

So $B$ is the inverse of $A$.

<b>3.</b> Consider the matrices,

\begin{align*}
\displaystyle A&amp;\displaystyle=\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1\\
-2&amp;-3&amp;0&amp;-5&amp;-1\\
1&amp;1&amp;0&amp;2&amp;1\\
-2&amp;-3&amp;-1&amp;-3&amp;-2\\
-1&amp;-3&amp;-1&amp;-3&amp;1\end{bmatrix}&amp;\displaystyle B&amp;\displaystyle=\begin{bmatrix}-3&amp;3&amp;6&amp;-1&amp;-2\\
0&amp;-2&amp;-5&amp;-1&amp;1\\
1&amp;2&amp;4&amp;1&amp;-1\\
1&amp;0&amp;1&amp;1&amp;0\\
1&amp;-1&amp;-2&amp;0&amp;1\end{bmatrix}
\end{align*}

Then

\begin{align*}
\displaystyle AB&amp;\displaystyle=\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1\\
-2&amp;-3&amp;0&amp;-5&amp;-1\\
1&amp;1&amp;0&amp;2&amp;1\\
-2&amp;-3&amp;-1&amp;-3&amp;-2\\
-1&amp;-3&amp;-1&amp;-3&amp;1\end{bmatrix}\begin{bmatrix}-3&amp;3&amp;6&amp;-1&amp;-2\\
0&amp;-2&amp;-5&amp;-1&amp;1\\
1&amp;2&amp;4&amp;1&amp;-1\\
1&amp;0&amp;1&amp;1&amp;0\\
1&amp;-1&amp;-2&amp;0&amp;1\end{bmatrix}=\begin{bmatrix}1&amp;0&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0&amp;0\\
0&amp;0&amp;1&amp;0&amp;0\\
0&amp;0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;1\end{bmatrix} \\
\\
\displaystyle BA&amp;\displaystyle=\begin{bmatrix}-3&amp;3&amp;6&amp;-1&amp;-2\\
0&amp;-2&amp;-5&amp;-1&amp;1\\
1&amp;2&amp;4&amp;1&amp;-1\\
1&amp;0&amp;1&amp;1&amp;0\\
1&amp;-1&amp;-2&amp;0&amp;1\end{bmatrix}\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1\\
-2&amp;-3&amp;0&amp;-5&amp;-1\\
1&amp;1&amp;0&amp;2&amp;1\\
-2&amp;-3&amp;-1&amp;-3&amp;-2\\
-1&amp;-3&amp;-1&amp;-3&amp;1\end{bmatrix}=\begin{bmatrix}1&amp;0&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0&amp;0\\
0&amp;0&amp;1&amp;0&amp;0\\
0&amp;0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;1\end{bmatrix}
\end{align*}

so by the definition of inverse matrix, we can say that $A$ is invertible and write $B=A^{-1}$.
@end

We will now concern ourselves less with whether or not an inverse of a matrix exists, but instead with how you can find one when it does exist.
Later we will have some theorems that allow us to more quickly and easily determine just when a matrix is invertible.

@section{Computing the Inverse of a Matrix}

How would we compute an inverse? And just when is a matrix invertible, and when is it not? Writing a putative inverse with $n^{2}$ unknowns and solving the resultant $n^{2}$ equations is one approach. Applying this approach to $2\times 2$ matrices can get us somewhere, so just for fun, let us do it.

@thm
@title{Two-by-Two Matrix Inverse}
@label{TTMI}
Suppose

\begin{align*}
\displaystyle A=\begin{bmatrix}a&amp;b\\
c&amp;d\end{bmatrix}
\end{align*}

Then $A$ is invertible if and only if $ad-bc\neq 0$. When $A$ is invertible, then

\begin{align*}
\displaystyle A^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d&amp;-b\\
-c&amp;a\end{bmatrix}
\end{align*}
@end
@proof
@col
$\Leftarrow$ Assume that $ad-bc\neq 0$. We will use the definition of the inverse of a matrix to establish that $A$ has an inverse. Note that if $ad-bc\neq 0$ then the displayed formula for $A^{-1}$ is legitimate since we are not dividing by zero).
Using this proposed formula for the inverse of $A$, we compute

\begin{align*}
\displaystyle AA^{-1}&amp;\displaystyle=\begin{bmatrix}a&amp;b\\
c&amp;d\end{bmatrix}\left(\frac{1}{ad-bc}\begin{bmatrix}d&amp;-b\\
-c&amp;a\end{bmatrix}\right)=\frac{1}{ad-bc}\begin{bmatrix}ad-bc&amp;0\\
0&amp;ad-bc\end{bmatrix}=\begin{bmatrix}1&amp;0\\
0&amp;1\end{bmatrix} \\
\\
\displaystyle A^{-1}A&amp;\displaystyle=\frac{1}{ad-bc}\begin{bmatrix}d&amp;-b\\
-c&amp;a\end{bmatrix}\begin{bmatrix}a&amp;b\\
c&amp;d\end{bmatrix}=\frac{1}{ad-bc}\begin{bmatrix}ad-bc&amp;0\\
0&amp;ad-bc\end{bmatrix}=\begin{bmatrix}1&amp;0\\
0&amp;1\end{bmatrix}
\end{align*}

This is sufficient to establish that $A$ is invertible, and that the expression for $A^{-1}$ is correct.

$\Rightarrow$ Assume that $A$ is invertible, and proceed with a proof by contradiction, by assuming also that $ad-bc=0$. This translates to $ad=bc$. Let

\begin{align*}
\displaystyle B=\begin{bmatrix}e&amp;f\\
g&amp;h\end{bmatrix}
\end{align*}

be a putative inverse of $A$.

This means that

\begin{align*}
\displaystyle I_{2}=AB=\begin{bmatrix}a&amp;b\\
c&amp;d\end{bmatrix}\begin{bmatrix}e&amp;f\\
g&amp;h\end{bmatrix}=\begin{bmatrix}ae+bg&amp;af+bh\\
ce+dg&amp;cf+dh\end{bmatrix}
\end{align*}

Working on the matrices on two ends of this equation, we will multiply the top row by $c$ and the bottom row by $a$.

\begin{align*}
\displaystyle \begin{bmatrix}c&amp;0\\
0&amp;a\end{bmatrix}=\begin{bmatrix}ace+bcg&amp;acf+bch\\
ace+adg&amp;acf+adh\end{bmatrix}
\end{align*}

We are assuming that $ad=bc$, so we can replace two occurrences of $ad$ by $bc$ in the bottom row of the right matrix.

\begin{align*}
\displaystyle \begin{bmatrix}c&amp;0\\
0&amp;a\end{bmatrix}=\begin{bmatrix}ace+bcg&amp;acf+bch\\
ace+bcg&amp;acf+bch\end{bmatrix}
\end{align*}

The matrix on the right now has two rows that are identical, and therefore the same must be true of the matrix on the left. Identical rows for the matrix on the left implies that $a=0$ and $c=0$.

With this information, the product $AB$ becomes

\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0\\
0&amp;1\end{bmatrix}=I_{2}=AB=\begin{bmatrix}ae+bg&amp;af+bh\\
ce+dg&amp;cf+dh\end{bmatrix}=\begin{bmatrix}bg&amp;bh\\
dg&amp;dh\end{bmatrix}
\end{align*}

So $bg=dh=1$ and thus $b,g,d,h$ are all nonzero. But then $bh$ and $dg$ (the <b>other corners</b>) must also be nonzero, so this is (finally) a contradiction. So our assumption was false and we see that $ad-bc\neq 0$ whenever $A$ has an inverse.
∎
@end

There are several ways one could try to prove this theorem, but there is a continual temptation to divide by one of the eight entries involved ($a$ through $f$), but we can never be sure if these numbers are zero or not. This could lead to an analysis by cases, which is messy, messy, messy. Note how the above proof never divides, but always multiplies, and how zero/nonzero considerations are handled. Pay attention to the expression $ad-bc$, as we will see it again in a while.

This theorem is cute, and it is nice to have a formula for the inverse, and a condition that tells us when we can use it. However, this approach becomes impractical for larger matrices, even though it is possible to demonstrate that, in theory, there is a general formula. (Think for a minute about extending this result to just $3\times 3$ matrices. For starters, we need 18 letters!) Instead, we will work column-by-column. Let us first work an example that will motivate the main theorem and remove some of the previous mystery.

@eg
<b>Computing a matrix inverse</b>Consider

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1\\
-2&amp;-3&amp;0&amp;-5&amp;-1\\
1&amp;1&amp;0&amp;2&amp;1\\
-2&amp;-3&amp;-1&amp;-3&amp;-2\\
-1&amp;-3&amp;-1&amp;-3&amp;1\end{bmatrix}
\end{align*}

For its inverse, we desire a matrix $B$ so that $AB=I_{5}$. Emphasizing the structure of the columns and employing the definition of matrix multiplication Let $A$ be as defined in Example 3.

\begin{align*}
\displaystyle AB&amp;\displaystyle=I_{5} \\
\displaystyle A[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}|\mathbf{B}_{4}|\mathbf{B}_{5}]&amp;\displaystyle=[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}|\mathbf{e}_{4}|\mathbf{e}_{5}] \\
\displaystyle[A\mathbf{B}_{1}|A\mathbf{B}_{2}|A\mathbf{B}_{3}|A\mathbf{B}_{4}|A\mathbf{B}_{5}]&amp;\displaystyle=[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}|\mathbf{e}_{4}|\mathbf{e}_{5}]
\end{align*}

Equating the matrices column-by-column we have

\begin{align*}
\displaystyle A\mathbf{B}_{1}=\mathbf{e}_{1}&amp;\displaystyle A\mathbf{B}_{2}=\mathbf{e}_{2}&amp;\displaystyle A\mathbf{B}_{3}=\mathbf{e}_{3}&amp;\displaystyle A\mathbf{B}_{4}=\mathbf{e}_{4}&amp;\displaystyle A\mathbf{B}_{5}=\mathbf{e}_{5}.
\end{align*}

Since the matrix $B$ is what we are trying to compute, we can view each column, $\mathbf{B}_{i}$, as a column vector of unknowns. Then we have five systems of equations to solve, each with 5 equations in 5 variables. Notice that all 5 of these systems have the same coefficient matrix. We will now solve each system in turn,

\begin{align*}
\\
\displaystyle\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1&amp;1\\
-2&amp;-3&amp;0&amp;-5&amp;-1&amp;0\\
1&amp;1&amp;0&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;-1&amp;-3&amp;-2&amp;0\\
-1&amp;-3&amp;-1&amp;-3&amp;1&amp;0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;-3\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;1\end{bmatrix};\mathbf{B}_{1}=\begin{bmatrix}-3\\
0\\
1\\
1\\
1\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;0&amp;-5&amp;-1&amp;1\\
1&amp;1&amp;0&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;-1&amp;-3&amp;-2&amp;0\\
-1&amp;-3&amp;-1&amp;-3&amp;1&amp;0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;3\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;-2\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;-1\end{bmatrix};\mathbf{B}_{2}=\begin{bmatrix}3\\
-2\\
2\\
0\\
-1\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;0&amp;-5&amp;-1&amp;0\\
1&amp;1&amp;0&amp;2&amp;1&amp;1\\
-2&amp;-3&amp;-1&amp;-3&amp;-2&amp;0\\
-1&amp;-3&amp;-1&amp;-3&amp;1&amp;0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;6\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;-5\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;4\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;-2\end{bmatrix};\mathbf{B}_{3}=\begin{bmatrix}6\\
-5\\
4\\
1\\
-2\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;0&amp;-5&amp;-1&amp;0\\
1&amp;1&amp;0&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;-1&amp;-3&amp;-2&amp;1\\
-1&amp;-3&amp;-1&amp;-3&amp;1&amp;0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;-1\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;-1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;0\end{bmatrix};\mathbf{B}_{4}=\begin{bmatrix}-1\\
-1\\
1\\
1\\
0\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&amp;2&amp;1&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;0&amp;-5&amp;-1&amp;0\\
1&amp;1&amp;0&amp;2&amp;1&amp;0\\
-2&amp;-3&amp;-1&amp;-3&amp;-2&amp;0\\
-1&amp;-3&amp;-1&amp;-3&amp;1&amp;1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;-2\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;-1\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;1\end{bmatrix};\mathbf{B}_{5}=\begin{bmatrix}-2\\
1\\
-1\\
0\\
1\end{bmatrix}
\end{align*}

We can now collect our 5 solution vectors into the matrix $B$,

\begin{align*}
\displaystyle B=&amp;\displaystyle[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}|\mathbf{B}_{4}|\mathbf{B}_{5}] \\
\displaystyle=&amp;\displaystyle\left[\begin{bmatrix}-3\\
0\\
1\\
1\\
1\end{bmatrix}\left\lvert\begin{bmatrix}3\\
-2\\
2\\
0\\
-1\end{bmatrix}\right.\left\lvert\begin{bmatrix}6\\
-5\\
4\\
1\\
-2\end{bmatrix}\right.\left\lvert\begin{bmatrix}-1\\
-1\\
1\\
1\\
0\end{bmatrix}\right.\left\lvert\begin{bmatrix}-2\\
1\\
-1\\
0\\
1\end{bmatrix}\right.\right] \\
&amp;\displaystyle=\begin{bmatrix}-3&amp;3&amp;6&amp;-1&amp;-2\\
0&amp;-2&amp;-5&amp;-1&amp;1\\
1&amp;2&amp;4&amp;1&amp;-1\\
1&amp;0&amp;1&amp;1&amp;0\\
1&amp;-1&amp;-2&amp;0&amp;1\end{bmatrix}
\end{align*}

By this method, we know that $AB=I_{5}$. Check that $BA=I_{5}$, and then we will know that we have the inverse of $A$.
@end
@eg
<b>Computing a matrix inverse</b> Let

\begin{align*}
\displaystyle A=\left[\begin{array}[]{ccc}1&amp;1&amp;1\\
1&amp;0&amp;-1\\
0&amp;1&amp;1\\
\end{array}\right].
\end{align*}

Suppose $B$ is the inverse of $A$ (at this point, we don’t know inverse exists or not).

\begin{align*}
\displaystyle AB=[A\mathbf{B}_{1}|A\mathbf{B}_{2}|A\mathbf{B}_{3}]=I_{3}=[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}]
\end{align*}

So

\begin{align*}
\displaystyle A\mathbf{B}_{i}=\mathbf{e}_{i}.
\end{align*}

Hence $\mathbf{B}_{i}$ is a solution of $A\mathbf{x}=\mathbf{e}_{i}$. The solution can be obtained RREF $[A|\mathbf{e}_{i}]$. To solve for $A\mathbf{x}=\mathbf{e}_{1}$:

\begin{align*}
\displaystyle [A|\mathbf{e}_{1}]=\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;1\\
1&amp;0&amp;-1&amp;0\\
0&amp;1&amp;1&amp;0\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;1\\
0&amp;-1&amp;-2&amp;-1\\
0&amp;1&amp;1&amp;0\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;1\\
0&amp;1&amp;1&amp;0\\
0&amp;-1&amp;-2&amp;-1\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;1\\
0&amp;1&amp;1&amp;0\\
0&amp;0&amp;-1&amp;-1\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;0&amp;1\\
0&amp;1&amp;1&amp;0\\
0&amp;0&amp;-1&amp;-1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;0&amp;1\\
0&amp;1&amp;0&amp;-1\\
0&amp;0&amp;1&amp;1\\
\end{array}\right]
\end{align*}

We can take $\mathbf{B}_{1}=\begin{bmatrix}1\\
-1\\
1\end{bmatrix}$.

Next we want to find solution of $A\mathbf{x}=\mathbf{e}_{2}$ by row reducing $[A|\mathbf{e}_{2}]$. Note that we can use the <b>exact same row operations</b>.

\begin{align*}
\displaystyle [A|\mathbf{e}_{2}]=\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
1&amp;0&amp;-1&amp;1\\
0&amp;1&amp;1&amp;0\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
0&amp;-1&amp;-2&amp;1\\
0&amp;1&amp;1&amp;0\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
0&amp;1&amp;1&amp;0\\
0&amp;-1&amp;-2&amp;1\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
0&amp;1&amp;1&amp;0\\
0&amp;0&amp;-1&amp;1\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;0&amp;0\\
0&amp;1&amp;1&amp;0\\
0&amp;0&amp;-1&amp;1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;1\\
0&amp;0&amp;1&amp;-1\\
\end{array}\right]
\end{align*}

We can take $\mathbf{B}_{2}=\begin{bmatrix}0\\
1\\
-1\end{bmatrix}$.

Next we want to find solution of $A\mathbf{x}=\mathbf{e}_{3}$ by row reducing $[A|\mathbf{e}_{3}]$. Again we can use the <b>exact same row operations</b>.

\begin{align*}
\displaystyle [A|\mathbf{e}_{3}]=\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
1&amp;0&amp;-1&amp;0\\
0&amp;1&amp;1&amp;1\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
0&amp;-1&amp;-2&amp;0\\
0&amp;1&amp;1&amp;1\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
0&amp;1&amp;1&amp;1\\
0&amp;-1&amp;-2&amp;0\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}1&amp;1&amp;1&amp;0\\
0&amp;1&amp;1&amp;1\\
0&amp;0&amp;-1&amp;1\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;0&amp;-1\\
0&amp;1&amp;1&amp;1\\
0&amp;0&amp;-1&amp;1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|c}1&amp;0&amp;0&amp;-1\\
0&amp;1&amp;0&amp;2\\
0&amp;0&amp;1&amp;-1\\
\end{array}\right]
\end{align*}

We can take $\mathbf{B}_{1}=\begin{bmatrix}1\\
-1\\
1\end{bmatrix}$.

So

\begin{align*}
\displaystyle B=[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}]=\left[\begin{array}[]{ccc}1&amp;0&amp;-1\\
-1&amp;1&amp;2\\
1&amp;-1&amp;-1\\
\end{array}\right]
\end{align*}

And $AB=I_{3}$. We need to check $BA=I_{3}$ <b>but later we will show that $BA=I_{3}$ follows from $AB=I_{3}$, so we don’t have to check it</b>. We see that we follows the exact same row operations for each case. We can combine all three cases into one. <b>Better method for finding inverse</b>:

\begin{align*}
\displaystyle [A|\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}]=\left[\begin{array}[]{ccc|ccc}1&amp;1&amp;1&amp;1&amp;0&amp;0\\
1&amp;0&amp;-1&amp;0&amp;1&amp;0\\
0&amp;1&amp;1&amp;0&amp;0&amp;1\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|ccc}1&amp;1&amp;1&amp;1&amp;0&amp;0\\
0&amp;-1&amp;-2&amp;-1&amp;1&amp;0\\
0&amp;1&amp;1&amp;0&amp;0&amp;1\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|ccc}1&amp;1&amp;1&amp;1&amp;0&amp;0\\
0&amp;1&amp;1&amp;0&amp;0&amp;1\\
0&amp;-1&amp;-2&amp;-1&amp;1&amp;0\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|ccc}1&amp;1&amp;1&amp;1&amp;0&amp;0\\
0&amp;1&amp;1&amp;0&amp;0&amp;1\\
0&amp;0&amp;-1&amp;-1&amp;1&amp;1\\
\end{array}\right]
\end{align*}

\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|ccc}1&amp;0&amp;0&amp;1&amp;0&amp;-1\\
0&amp;1&amp;1&amp;0&amp;0&amp;1\\
0&amp;0&amp;-1&amp;-1&amp;1&amp;1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|ccc}1&amp;0&amp;0&amp;1&amp;0&amp;-1\\
0&amp;1&amp;0&amp;-1&amp;1&amp;2\\
0&amp;0&amp;1&amp;1&amp;-1&amp;-1\\
\end{array}\right]
\end{align*}

So

\begin{align*}
\displaystyle B=[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}]=\left[\begin{array}[]{ccc}1&amp;0&amp;-1\\
-1&amp;1&amp;2\\
1&amp;-1&amp;-1\\
\end{array}\right]
\end{align*}
@end
@thm
@title{Computing the Inverse of a Nonsingular Matrix}
@label{CINM}
Suppose $A$ is a nonsingular square matrix of size $n$. Create the $n\times 2n$ matrix $M$ by placing the $n\times n$ identity matrix $I_{n}$ to the right of the matrix $A$. Let $N$ be a matrix that is row-equivalent to $M$ and in reduced row-echelon form. Finally, let $J$ be the matrix formed from the final $n$ columns of $N$. Then $AJ=I_{n}$.
@end
@proof
@col
$A$ is nonsingular, there is a sequence of row operations that will convert $A$ into $I_{n}$. It is this same sequence of row operations that will convert $M$ into $N$, since having the identity matrix in the first $n$ columns of $N$ is sufficient to guarantee that $N$ is in reduced row-echelon form.

If we consider the systems of linear equations, $A\mathbf{x}=\mathbf{e}_{i}$, $1\leq i\leq n$, we see that the aforementioned sequence of row operations will also bring the augmented matrix of each of these systems into reduced row-echelon form. Furthermore, the unique solution to $A\mathbf{x}=\mathbf{e}_{i}$ appears in column $n+1$ of the row-reduced augmented matrix of the system and is identical to column $n+i$ of $N$. Let $\mathbf{N}_{1},\,\mathbf{N}_{2},\,\mathbf{N}_{3},\,\ldots,\,\mathbf{N}_{2n}$ denote the columns of $N$. So we find,

\begin{align*}
\displaystyle AJ=&amp;\displaystyle A[\mathbf{N}_{n+1}|\mathbf{N}_{n+2}|\mathbf{N}_{n+3}|\ldots|\mathbf{N}_{n+n}] \\
\displaystyle=&amp;\displaystyle[A\mathbf{N}_{n+1}|A\mathbf{N}_{n+2}|A\mathbf{N}_{n+3}|\ldots|A\mathbf{N}_{n+n}] \\
\displaystyle=&amp;\displaystyle[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}|\ldots|\mathbf{e}_{n}] \\
\displaystyle=&amp;\displaystyle I_{n}
\end{align*}

as desired.
∎
@end

We have to be just a bit careful here about both what this theorem says and what it does not say. If $A$ is a nonsingular matrix, then we are guaranteed a matrix $B$ such that $AB=I_{n}$, and the proof gives us a process for constructing $B$. However, the definition of the inverse of a matrix requires that $BA=I_{n}$ also. So at this juncture we must compute the matrix product in the <b>opposite</b> order before we claim $B$ as the inverse of $A$. However, we will soon see that this is always the case.

What if $A$ is singular? At this point we only know that Theorem  @ref{CINM} cannot be applied. The question of $A$’s inverse is still open.
We will solve it later.

@eg
<b>Computing a matrix inverse</b>

\begin{align*}
\displaystyle B=&amp;\displaystyle\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}. \\
\\
\displaystyle M=&amp;\displaystyle\begin{bmatrix}-7&amp;-6&amp;-12&amp;1&amp;0&amp;0\\
5&amp;5&amp;7&amp;0&amp;1&amp;0\\
1&amp;0&amp;4&amp;0&amp;0&amp;1\end{bmatrix}. \\
\\
\displaystyle N=&amp;\displaystyle\begin{bmatrix}1&amp;0&amp;0&amp;-10&amp;-12&amp;-9\\
0&amp;1&amp;0&amp;\frac{13}{2}&amp;8&amp;\frac{11}{2}\\
0&amp;0&amp;1&amp;\frac{5}{2}&amp;3&amp;\frac{5}{2}\end{bmatrix}. \\
\\
\displaystyle B^{-1}=&amp;\displaystyle\begin{bmatrix}-10&amp;-12&amp;-9\\
\frac{13}{2}&amp;8&amp;\frac{11}{2}\\
\frac{5}{2}&amp;3&amp;\frac{5}{2}\end{bmatrix}.
\end{align*}
@end

@section{Properties of Matrix Inverses}

The inverse of a matrix enjoys some nice properties. We collect a few here. First, a matrix can have but one inverse.

@thm
@title{Matrix Inverse is Unique}
@label{MIU}
Suppose the square matrix $A$ has an inverse. Then $A^{-1}$ is unique.
@end
@proof
@col
We will assume that $A$ has two inverses. The hypothesis tells there is at least one. Suppose then that $B$ and $C$ are both inverses for $A$.
Then $AB=BA=I_{n}$ and $AC=CA=I_{n}$. Then we have,

\begin{align*}
\displaystyle B&amp;\displaystyle=BI_{n} \\
&amp;\displaystyle=B(AC) \\
&amp;\displaystyle=(BA)C \\
&amp;\displaystyle=I_{n}C \\
&amp;\displaystyle=C
\end{align*}

So we conclude that $B$ and $C$ are the same, and cannot be different. So any matrix that acts like an inverse, must be the inverse.
∎
@end

When most of us dress in the morning, we put on our socks first, followed by our shoes. In the evening we must then first remove our shoes, followed by our socks. Try to connect the conclusion of the following theorem with this everyday example.

@thm
@title{Socks and Shoes}
@label{SS}
Suppose $A$ and $B$ are invertible matrices of size $n$. Then $AB$ is an invertible matrix and $(AB)^{-1}=B^{-1}A^{-1}$.
@end
@proof
@col
\begin{align*}
\displaystyle(B^{-1}A^{-1})(AB)&amp;\displaystyle=B^{-1}(A^{-1}A)B \\
&amp;\displaystyle=B^{-1}I_{n}B \\
&amp;\displaystyle=B^{-1}B \\
&amp;\displaystyle=I_{n} \\
\\
\displaystyle(AB)(B^{-1}A^{-1})&amp;\displaystyle=A(BB^{-1})A^{-1} \\
&amp;\displaystyle=AI_{n}A^{-1} \\
&amp;\displaystyle=AA^{-1} \\
&amp;\displaystyle=I_{n}
\end{align*}

So the matrix $B^{-1}A^{-1}$ has met all of the requirements to be $AB$’s inverse (date) and with the ensuing marriage proposal we can announce that $(AB)^{-1}=B^{-1}A^{-1}$.
∎
@end
@thm
@title{Matrix Inverse of a Matrix Inverse}
@label{MIMI}
Suppose $A$ is an invertible matrix. Then $A^{-1}$ is invertible and $(A^{-1})^{-1}=A$.
@end
@proof
@col
As with the proof of of the previous example, we examine if $A$ is a suitable inverse for $A^{-1}$ (by definition, the opposite is true).

\begin{align*}
\displaystyle AA^{-1}&amp;\displaystyle=I_{n} \\
\\
\displaystyle A^{-1}A&amp;\displaystyle=I_{n}
\end{align*}

The matrix $A$ has met all the requirements to be the inverse of $A^{-1}$, and so is invertible and we can write $A=(A^{-1})^{-1}$.
∎
@end
@thm
@title{Matrix Inverse of a Transpose}
@label{MIT}
Suppose $A$ is an invertible matrix. Then $A^{t}$ is invertible and $(A^{t})^{-1}=(A^{-1})^{t}$.
@end
@proof
@col
As with the proof of Theorem  @ref{SS}, we see if $(A^{-1})^{t}$ is a suitable inverse for $A^{t}$.

\begin{align*}
\displaystyle(A^{-1})^{t}A^{t}&amp;\displaystyle=(AA^{-1})^{t} \\
&amp;\displaystyle=I_{n}^{t} \\
&amp;\displaystyle=I_{n} \\
\\
\displaystyle A^{t}(A^{-1})^{t}&amp;\displaystyle=(A^{-1}A)^{t} \\
&amp;\displaystyle=I_{n}^{t} \\
&amp;\displaystyle=I_{n}
\end{align*}

The matrix $(A^{-1})^{t}$ has met all the requirements to be the inverse of $A^{t}$, and so is invertible and we can write $(A^{t})^{-1}=(A^{-1})^{t}$.
∎
@end
@thm
@title{Matrix Inverse of a Scalar Multiple}
@label{MISM}
Suppose $A$ is an invertible matrix and $\alpha$ is a nonzero scalar. Then $\left(\alpha A\right)^{-1}=\frac{1}{\alpha}A^{-1}$ and $\alpha A$ is invertible.
@end
@proof
@col
As with the proof of Theorem  @ref{SS}, we see if $\frac{1}{\alpha}A^{-1}$ is a suitable inverse for $\alpha A$.

\begin{align*}
\displaystyle\left(\frac{1}{\alpha}A^{-1}\right)\left(\alpha A\right)&amp;\displaystyle=\left(\frac{1}{\alpha}\alpha\right)\left(A^{-1}A\right) \\
&amp;\displaystyle=1I_{n} \\
&amp;\displaystyle=I_{n} \\
\\
\displaystyle\left(\alpha A\right)\left(\frac{1}{\alpha}A^{-1}\right)&amp;\displaystyle=\left(\alpha\frac{1}{\alpha}\right)\left(AA^{-1}\right) \\
&amp;\displaystyle=1I_{n} \\
&amp;\displaystyle=I_{n}
\end{align*}

The matrix $\frac{1}{\alpha}A^{-1}$ has met all the requirements to be the inverse of $\alpha A$, so we can write $\left(\alpha A\right)^{-1}=\frac{1}{\alpha}A^{-1}$.
∎
@end

Notice that there are some likely theorems that are missing here. For example, it would be tempting to think that $(A+B)^{-1}=A^{-1}+B^{-1}$, but this is false. Can you find a counterexample?

@section{Nonsingular Matrices are Invertible}
@label{NMI}
For $\alpha,\beta\in{\mathbb{R}}^{\hbox{}}$, then $\alpha\beta\neq 0$ if and only if $\alpha\neq 0$ and $\beta\neq 0$.
We have a similar result for nonsingular matrix

@thm
@title{Nonsingular Product has Nonsingular Terms}
@label{NPNT}
Suppose that $A$ and $B$ are square matrices of size $n$. The product $AB$ is nonsingular if and only if $A$ and $B$ are both nonsingular.
@end
@proof
@col
($\Rightarrow$) For this portion of the proof we will form the logically-equivalent contrapositive and prove that statement using two cases. $AB$<b> is nonsingular implies $A$ and $B$ are both nonsingular</b> becomes $A$<b> or $B$ is singular implies $AB$ is singular.</b>

Case 1. Suppose $B$ is singular. Then there is a nonzero vector $\mathbf{z}$ that is a solution to $B\mathbf{x}=\mathbf{0}$. So

\begin{align*}
\displaystyle(AB)\mathbf{z}&amp;\displaystyle=A(B\mathbf{z}) \\
&amp;\displaystyle=A\mathbf{0} \\
&amp;\displaystyle=\mathbf{0}
\end{align*}

Then $\mathbf{z}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired.

Case 2. Suppose $A$ is singular, and $B$ is not singular.
Because $A$ is singular, there is a nonzero vector $\mathbf{y}$ that is a solution to $A\mathbf{x}=\mathbf{0}$. Now consider the linear system $B\mathbf{x}=\mathbf{y}$. Since $B$ is nonsingular, the system has a unique solution, which we will denote as $\mathbf{w}$. We first claim $\mathbf{w}$ is not the zero vector either. Assuming the opposite, suppose that $\mathbf{w}=\mathbf{0}$. Then

\begin{align*}
\displaystyle\mathbf{y}&amp;\displaystyle=B\mathbf{w} \\
&amp;\displaystyle=B\mathbf{0} \\
&amp;\displaystyle=\mathbf{0} \\
\\
\displaystyle(AB)\mathbf{w}&amp;\displaystyle=A(B\mathbf{w}) \\
&amp;\displaystyle=A\mathbf{y} \\
&amp;\displaystyle=\mathbf{0}
\end{align*}

So $\mathbf{w}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired. And this conclusion holds for both cases.

($\Leftarrow$) Now assume that both $A$ and $B$ are nonsingular. Suppose that $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution to $AB\mathbf{x}=\mathbf{0}$. Then

\begin{align*}
\displaystyle\mathbf{0}&amp;\displaystyle=\left(AB\right)\mathbf{x} \\
&amp;\displaystyle=A\left(B\mathbf{x}\right)
\end{align*}

So $B\mathbf{x}$ is a solution to $A\mathbf{x}=\mathbf{0}$, and by the definition of a nonsingular matrix, we conclude that $B\mathbf{x}=\mathbf{0}$. Now, by an entirely similar argument, the nonsingularity of $B$ forces us to conclude that $\mathbf{x}=\mathbf{0}$. So the only solution to $AB\mathbf{x}=\mathbf{0}$ is the zero vector and we conclude that $AB$ is nonsingular.
∎
@end

The contrapositive of this entire result is equally interesting. It says that $A$ or $B$ (or both) is a singular matrix if and only if the product $AB$ is singular.

@thm
@title{One-Sided Inverse is Sufficient}
@label{OSIS}
Suppose $A$ and $B$ are square matrices of size $n$ such that $AB=I_{n}$. Then $BA=I_{n}$.
@end
@proof
@col
The matrix $I_{n}$ is nonsingular. So $A$ and $B$ are nonsingular by, so in particular $B$ is nonsingular. We can therefore apply Theorem  @ref{CINM} to assert the existence of a matrix $C$ so that $BC=I_{n}$.
$B$ is nonsingular, so there must be a <b>right-inverse</b> for $B$, and we are calling it $C$.

Now

\begin{align*}
\displaystyle BA&amp;\displaystyle=(BA)I_{n} \\
&amp;\displaystyle=(BA)(BC) \\
&amp;\displaystyle=B(AB)C \\
&amp;\displaystyle=BI_{n}C \\
&amp;\displaystyle=BC \\
&amp;\displaystyle=I_{n}
\end{align*}

which is the desired conclusion.

∎
@end

So above theorem tells us that if $A$ is nonsingular, then the matrix $B$ guaranteed by Theorem  @ref{CINM} will be both a <b>right-inverse</b> and a <b>left-inverse</b> for $A$, so $A$ is invertible and $A^{-1}=B$.

So if you have a nonsingular matrix, $A$, you can use the procedure described in Theorem  @ref{CINM} to find an inverse for $A$. If $A$ is singular, then the procedure in Theorem  @ref{CINM} will fail as the first $n$ columns of $M$ will not row-reduce to the identity matrix. However, we can say a bit more. When $A$ is singular, then $A$ does not have an inverse (which is very different from saying that the procedure in Theorem  @ref{CINM} fails to find an inverse).
This may feel like we are splitting hairs, but it is important that we do not make unfounded assumptions. These observations motivate the next theorem.

@thm
@title{Nonsingularity is Invertibility}
@label{NI}
Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if $A$ is invertible.
@end
@proof
@col
($\Leftarrow$) Since $A$ is invertible, we can write $I_{n}=AA^{-1}$. Notice that $I_{n}$ is nonsingular, so Theorem  @ref{NPNT} implies that $A$ (and $A^{-1}$) is nonsingular.

($\Rightarrow$) Suppose now that $A$ is nonsingular. By Theorem  @ref{CINM} we find $B$ so that $AB=I_{n}$. Then Theorem  @ref{OSIS} tells us that $BA=I_{n}$. So $B$ is $A$’s inverse, and by construction, $A$ is invertible.

∎
@end

So for a square matrix, the properties of having an inverse and of having a trivial null space are one and the same. Cannot have one without the other.

@thm
@title{Nonsingular Matrix Equivalences, Round 3}
@label{NME3}
Suppose that $A$ is a square matrix of size $n$. The following are equivalent.

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is nonsingular.
</li>
<li class="ltx_item">
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item">
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
</li>
<li class="ltx_item">
The linear system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>
<li class="ltx_item">
The columns of $A$ are a linearly independent set.
</li>
<li class="ltx_item">
$A$ is invertible.
</li>

</ol>
@end

In the case that $A$ is a nonsingular coefficient matrix of a system of equations, the inverse allows us to very quickly compute the unique solution, for any vector of constants.

@thm
@title{Solution with Nonsingular Coefficient Matrix}
Suppose that $A$ is nonsingular. Then the unique solution to $A\mathbf{x}=\mathbf{b}$ is $A^{-1}\mathbf{b}$.
@end
@proof
@col
We can show this by simply plug $A^{-1}\mathbf{b}$ in the solution.

\begin{align*}
\displaystyle A\left(A^{-1}\mathbf{b}\right)&amp;\displaystyle=\left(AA^{-1}\right)\mathbf{b} \\
&amp;\displaystyle=I_{n}\mathbf{b} \\
&amp;\displaystyle=\mathbf{b}
\end{align*}

Since $A\mathbf{x}=\mathbf{b}$ is true when we substitute $A^{-1}\mathbf{b}$ for $\mathbf{x}$, $A^{-1}\mathbf{b}$ is a (the!) solution to $A\mathbf{x}=\mathbf{b}$.
∎
@end
@eg
Using the previous theorem, solve

\begin{align*}
\displaystyle x_{1}+x_{2}-x_{3}+4x_{4}&amp;\displaystyle=1 \\
\displaystyle x_{1}-x_{2}+2x_{3}+3x_{4}&amp;\displaystyle=2 \\
\displaystyle 2x_{1}+x_{2}+x_{3}+x_{4}&amp;\displaystyle=0 \\
\displaystyle 2x_{1}+2x_{2}+2x_{3}-9x_{4}&amp;\displaystyle=-1
\end{align*}

The matrix coefficient is

\begin{align*}
\displaystyle A=\left[\begin{array}[]{cccc}1&amp;1&amp;-1&amp;4\\
1&amp;-1&amp;2&amp;3\\
2&amp;1&amp;1&amp;1\\
2&amp;2&amp;2&amp;-9\\
\end{array}\right].
\end{align*}

After some computations,

\begin{align*}
\displaystyle A^{-1}=\left[\begin{array}[]{cccc}-33&amp;-22&amp;45&amp;-17\\
35&amp;23&amp;-47&amp;18\\
25&amp;17&amp;-34&amp;13\\
6&amp;4&amp;-8&amp;3\\
\end{array}\right].
\end{align*}

Then the solution of the system of linear equations is

\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=A^{-1}\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix}=\begin{bmatrix}-60\\
63\\
46\\
11\end{bmatrix}.
\end{align*}
@end

@chapter{Determinant}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Chapter D (print version p261-282)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section DM p.98 - 101 all, Section PDM p.101-102 M30, T10, T15, T20

@section{Definition of the determinant}

The <b>determinant</b> is a function that take a square matrix as an input and
produces a scalar as an output.

Suppose $A$ is an $m\times n$ matrix. Then the submatrix $A(i|j)$ is the $(m-1)\times(n-1)$
matrix obtained from $A$ by removing row $i$ and column $j$.

@eg
Suppose

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;3&amp;4\\
5&amp;6&amp;7&amp;8\\
9&amp;10&amp;11&amp;12\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle A(2|3)=\begin{bmatrix}1&amp;2&amp;4\\
9&amp;10&amp;12\end{bmatrix}\qquad A(3|1)=\begin{bmatrix}2&amp;3&amp;4\\
6&amp;7&amp;8\end{bmatrix}
\end{align*}
@end
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;a_{14}\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;a_{24}\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;a_{34}\\
a_{41}&amp;a_{42}&amp;a_{43}&amp;a_{44}\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle A(3|2)=\begin{bmatrix}a_{11}&amp;a_{13}&amp;a_{14}\\
a_{21}&amp;a_{23}&amp;a_{24}\\
a_{41}&amp;a_{42}&amp;a_{44}\end{bmatrix}\qquad A(4|1)=\begin{bmatrix}a_{12}&amp;a_{13}&amp;a_{14}\\
a_{22}&amp;a_{23}&amp;a_{24}\\
a_{32}&amp;a_{33}&amp;a_{34}\end{bmatrix}
\end{align*}
@end
@defn
Suppose $A$ is a square matrix. Then its <b>determinant</b>, $\det\left(A\right)$ (or denoted by $|A|$), is an element of ${\mathbb{R}}^{\hbox{}}$ defined recursively by:

<ol class="ltx_enumerate">
<li class="ltx_item">
If $A$ is a $1\times 1$ matrix, then $\det\left(A\right)=\left[A\right]_{11}$.
</li>
<li class="ltx_item">
If $A$ is a matrix of size $n$ with $n\geq 2$, then

\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=\left[A\right]_{11}\det\left(A\left(1|1\right)\right)-\left[A\right]_{12}\det\left(A\left(1|2\right)\right)+\left[A\right]_{13}\det\left(A\left(1|3\right)\right)- \\
&amp;\displaystyle\quad\left[A\right]_{14}\det\left(A\left(1|4\right)\right)+\cdots+(-1)^{n+1}\left[A\right]_{1n}\det\left(A\left(1|n\right)\right) \\
&amp;\displaystyle\quad=\sum_{i=1}^{n}(-1)^{i+1}\left[A\right]_{1i}\det\left(A\left(1|i\right)\right)
\end{align*}

</li>

</ol>
@end

So to compute the determinant of a $5\times 5$ matrix we must build 5 submatrices, each of size $4$. To compute the determinants of each the $4\times 4$ matrices we need to create 4 submatrices each, these now of size $3$ and so on. To compute the determinant of a $10\times 10$ matrix would require computing the determinant of $10!=10\times 9\times 8\times 7\times 6\times 5\times 4\times 3\times 2=3,628,800$
$1\times 1$ matrices. Fortunately there are <b>better ways</b>).

Let us compute the determinant of a reasonably sized matrix by hand.

@eg
Suppose that we have the $3\times 3$ matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}3&amp;2&amp;-1\\
4&amp;1&amp;6\\
-3&amp;-1&amp;2\end{bmatrix}
\end{align*}

Then

\begin{align*}
\displaystyle\det\left(A\right)=|A|&amp;\displaystyle=\begin{vmatrix}3&amp;2&amp;-1\\
4&amp;1&amp;6\\
-3&amp;-1&amp;2\end{vmatrix} \\
&amp;\displaystyle=3\begin{vmatrix}1&amp;6\\
-1&amp;2\end{vmatrix}-2\begin{vmatrix}4&amp;6\\
-3&amp;2\end{vmatrix}+(-1)\begin{vmatrix}4&amp;1\\
-3&amp;-1\end{vmatrix} \\
&amp;\displaystyle=3\left(1\begin{vmatrix}2\\
\end{vmatrix}-6\begin{vmatrix}-1\end{vmatrix}\right)-2\left(4\begin{vmatrix}2\end{vmatrix}-6\begin{vmatrix}-3\end{vmatrix}\right)-\left(4\begin{vmatrix}-1\end{vmatrix}-1\begin{vmatrix}-3\end{vmatrix}\right) \\
&amp;\displaystyle=3\left(1(2)-6(-1)\right)-2\left(4(2)-6(-3)\right)-\left(4(-1)-1(-3)\right) \\
&amp;\displaystyle=24-52+1 \\
&amp;\displaystyle=-27
\end{align*}
@end
@thm

Suppose

\begin{align*}
\displaystyle A=\begin{bmatrix}a&amp;b\\
c&amp;d\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle \det\left(A\right)=ad-bc.
\end{align*}
@end
@proof
@col
\begin{align*}
\displaystyle \begin{vmatrix}a&amp;b\\
c&amp;d\end{vmatrix}=a\begin{vmatrix}d\end{vmatrix}-b\begin{vmatrix}c\end{vmatrix}=ad-bc
\end{align*}

∎
@end
@thm

Suppose

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}\\
a_{21}&amp;a_{22}&amp;a_{23}\\
a_{31}&amp;a_{32}&amp;a_{33}\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle \det\left(A\right)=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align*}
@end
@proof
@col
\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=a_{11}|A\left(1|1\right)|-a_{12}|A\left(1|2\right)|+a_{13}|A\left(1|3\right)| \\
&amp;\displaystyle=a_{11}\begin{vmatrix}a_{22}&amp;a_{23}\\
a_{32}&amp;a_{33}\end{vmatrix}-a_{12}\begin{vmatrix}a_{21}&amp;a_{23}\\
a_{31}&amp;a_{33}\end{vmatrix}+a_{13}\begin{vmatrix}a_{21}&amp;a_{22}\\
a_{31}&amp;a_{32}\end{vmatrix} \\
&amp;\displaystyle=a_{11}(a_{22}a_{33}-a_{23}a_{32})-a_{12}(a_{21}a_{33}-a_{23}a_{31})+a_{13}(a_{21}a_{32}-a_{22}a_{31}) \\
&amp;\displaystyle=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align*}

∎
@end

@section{Computing Determinants}

@thm
@title{Determinant Expansion about Rows}
Suppose that $A$ is a square matrix of size $n$. Then for $1\leq i\leq n$

\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=(-1)^{i+1}\left[A\right]_{i1}\det\left(A\left(i|1\right)\right)+(-1)^{i+2}\left[A\right]_{i2}\det\left(A\left(i|2\right)\right) \\
&amp;\displaystyle\quad+(-1)^{i+3}\left[A\right]_{i3}\det\left(A\left(i|3\right)\right)+\cdots+(-1)^{i+n}\left[A\right]_{in}\det\left(A\left(i|n\right)\right)
\end{align*}

which is known as <b>expansion</b> about row $i$.
@end
@proof
@col
Skip the proof. If you are interested, see Beezer, p.266.
∎
@end
@thm
@title{Determinant of the Transpose}
Suppose that $A$ is a square matrix. Then $\det\left(A^{t}\right)=\det\left(A\right)$.
@end
@proof
@col
Skip the proof.
If you are interested, see Beezer, p.267.
∎
@end
@thm
@title{Determinant Expansion about Columns}
Suppose that $A$ is a square matrix of size $n$. Then for $1\leq j\leq n$

\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=(-1)^{1+j}\left[A\right]_{1j}\det\left(A\left(1|j\right)\right)+(-1)^{2+j}\left[A\right]_{2j}\det\left(A\left(2|j\right)\right) \\
&amp;\displaystyle\quad+(-1)^{3+j}\left[A\right]_{3j}\det\left(A\left(3|j\right)\right)+\cdots+(-1)^{n+j}\left[A\right]_{nj}\det\left(A\left(n|j\right)\right)
\end{align*}

which is known as <b>expansion</b> about column $j$.
@end
@proof
@col
Skip the proof.
If you are interested, see Beezer, p.268.
∎
@end
@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}-2&amp;3&amp;0&amp;1\\
9&amp;-2&amp;0&amp;1\\
1&amp;3&amp;-2&amp;-1\\
4&amp;1&amp;2&amp;6\end{bmatrix}
\end{align*}

Then expanding about the fourth row yields,

\begin{align*}
\displaystyle|A|&amp;\displaystyle=(4)(-1)^{4+1}\begin{vmatrix}3&amp;0&amp;1\\
-2&amp;0&amp;1\\
3&amp;-2&amp;-1\end{vmatrix}+(1)(-1)^{4+2}\begin{vmatrix}-2&amp;0&amp;1\\
9&amp;0&amp;1\\
1&amp;-2&amp;-1\end{vmatrix} \\
&amp;\displaystyle\quad\quad+(2)(-1)^{4+3}\begin{vmatrix}-2&amp;3&amp;1\\
9&amp;-2&amp;1\\
1&amp;3&amp;-1\end{vmatrix}+(6)(-1)^{4+4}\begin{vmatrix}-2&amp;3&amp;0\\
9&amp;-2&amp;0\\
1&amp;3&amp;-2\end{vmatrix} \\
&amp;\displaystyle=(-4)(10)+(1)(-22)+(-2)(61)+6(46)=92
\end{align*}

Expanding about column 3 gives

\begin{align*}
\displaystyle|A|&amp;\displaystyle=(0)(-1)^{1+3}\begin{vmatrix}9&amp;-2&amp;1\\
1&amp;3&amp;-1\\
4&amp;1&amp;6\end{vmatrix}+(0)(-1)^{2+3}\begin{vmatrix}-2&amp;3&amp;1\\
1&amp;3&amp;-1\\
4&amp;1&amp;6\end{vmatrix}+ \\
&amp;\displaystyle\quad\quad(-2)(-1)^{3+3}\begin{vmatrix}-2&amp;3&amp;1\\
9&amp;-2&amp;1\\
4&amp;1&amp;6\end{vmatrix}+(2)(-1)^{4+3}\begin{vmatrix}-2&amp;3&amp;1\\
9&amp;-2&amp;1\\
1&amp;3&amp;-1\end{vmatrix} \\
&amp;\displaystyle=0+0+(-2)(-107)+(-2)(61)=92
\end{align*}

Notice how much easier the second computation was. By choosing to expand about the third column, we have two entries that are zero, so two $3\times 3$ determinants need not be computed at all!
@end

When a matrix has all zeros above (or below) the diagonal, exploiting the zeros by expanding about the proper row or column makes computing a determinant insanely easy.

@eg
Suppose that

\begin{align*}
\displaystyle T=\begin{bmatrix}2&amp;3&amp;-1&amp;3&amp;3\\
0&amp;-1&amp;5&amp;2&amp;-1\\
0&amp;0&amp;3&amp;9&amp;2\\
0&amp;0&amp;0&amp;-1&amp;3\\
0&amp;0&amp;0&amp;0&amp;5\end{bmatrix}
\end{align*}

We will compute the determinant of this $5\times 5$ matrix by consistently expanding about the first column for each submatrix that arises and does not have a zero entry multiplying it.

\begin{align*}
\displaystyle\det\left(T\right)&amp;\displaystyle=\begin{vmatrix}2&amp;3&amp;-1&amp;3&amp;3\\
0&amp;-1&amp;5&amp;2&amp;-1\\
0&amp;0&amp;3&amp;9&amp;2\\
0&amp;0&amp;0&amp;-1&amp;3\\
0&amp;0&amp;0&amp;0&amp;5\end{vmatrix} \\
&amp;\displaystyle=2(-1)^{1+1}\begin{vmatrix}-1&amp;5&amp;2&amp;-1\\
0&amp;3&amp;9&amp;2\\
0&amp;0&amp;-1&amp;3\\
0&amp;0&amp;0&amp;5\end{vmatrix} \\
&amp;\displaystyle=2(-1)(-1)^{1+1}\begin{vmatrix}3&amp;9&amp;2\\
0&amp;-1&amp;3\\
0&amp;0&amp;5\end{vmatrix} \\
&amp;\displaystyle=2(-1)(3)(-1)^{1+1}\begin{vmatrix}-1&amp;3\\
0&amp;5\end{vmatrix} \\
&amp;\displaystyle=2(-1)(3)(-1)(-1)^{1+1}\begin{vmatrix}5\end{vmatrix} \\
&amp;\displaystyle=2(-1)(3)(-1)(5)=30
\end{align*}
@end
@thm

Suppose $A$ is upper triangular matrix, i.e.

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;\cdots&amp;a_{1n}\\
0&amp;a_{22}&amp;a_{23}&amp;\cdots&amp;a_{2n}\\
0&amp;0&amp;a_{33}&amp;\cdots&amp;a_{3n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;a_{nn}\end{bmatrix}
\end{align*}

Then

\begin{align*}
\displaystyle \det\left(A\right)=a_{11}a_{22}\cdots a_{nn}.
\end{align*}
@end
@proof
@col
\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=a_{11}\det\left(\begin{bmatrix}a_{22}&amp;a_{23}&amp;\cdots&amp;a_{2n}\\
0&amp;a_{33}&amp;\cdots&amp;a_{3n}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;a_{nn}\end{bmatrix}\right)&amp;xpand along the column  \\
&amp;\displaystyle=a_{11}a_{22}\det\left(\begin{bmatrix}a_{33}&amp;\cdots&amp;a_{3n}\\
\vdots&amp;\ddots&amp;\vdots\\
0&amp;\cdots&amp;a_{nn}\end{bmatrix}\right)&amp;xpand along the column  \\
&amp;\displaystyle\cdots \\
&amp;\displaystyle=a_{11}a_{22}\cdots a_{nn}
\end{align*}

∎
@end

Similarly

@thm

Suppose $A$ is lower triangular matrix, i.e.

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;0&amp;0&amp;\cdots&amp;0\\
a_{21}&amp;a_{22}&amp;0&amp;\cdots&amp;0\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;a_{n3}&amp;\cdots&amp;a_{nn}\end{bmatrix}
\end{align*}

Then

\begin{align*}
\displaystyle \det\left(A\right)=a_{11}a_{22}\cdots a_{nn}.
\end{align*}
@end

When you consult other texts in your study of determinants, you may run into the terms <b>minor</b> and <b>cofactor</b>, especially in a discussion centered on expansion about rows and columns. We have chosen not to make these definitions formally since we have been able to get along without them. However, informally, a <b>minor</b> is a determinant of a submatrix, specifically $\det\left(A\left(i|j\right)\right)$ and is usually referenced as the minor of $\left[A\right]_{ij}$. A <b>cofactor</b> is a signed minor, specifically the cofactor of $\left[A\right]_{ij}$ is $(-1)^{i+j}\det\left(A\left(i|j\right)\right)$.

@section{Properties of Determinants of Matrces}

@thm
@title{Determinant with Zero Row or Column}
@label{DZRC}
Suppose that $A$ is a square matrix with a row where every entry is zero, or a column where every entry is zero. Then $\det\left(A\right)=0$.
@end

@proof
@col
Suppose that $A$ is a square matrix of size $n$ and row $i$ has every entry equal to zero. We compute $\det\left(A\right)$ via expansion about row $i$.

\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=\sum_{j=1}^{n}(-1)^{i+j}\left[A\right]_{ij}\det\left(A\left(i|j\right)\right) \\
&amp;\displaystyle=\sum_{j=1}^{n}(-1)^{i+j}\,0\,\det\left(A\left(i|j\right)\right)&amp;ow $i$ is zero \\
&amp;\displaystyle=\sum_{j=1}^{n}0=0
\end{align*}

The proof for the case of a zero column is entirely similar, or could be derived by the fact that
$\det\left(A\right)=\det\left(A^{t}\right)$.
∎
@end

@thm
@title{Determinant for Row or Column Swap}
@label{DRCS}
Suppose that $A$ is a square matrix. Let $B$ be the square matrix obtained from $A$ by interchanging the location of two rows, or interchanging the location of two columns. Then $\det\left(B\right)=-\det\left(A\right)$.
@end

@proof
@col
Skip the proof. If you are interested, see Beezer p.273.
∎
@end

@thm
@title{Determinant for Row or Column Multiples}
@label{DRCM}
Suppose that $A$ is a square matrix. Let $B$ be the square matrix obtained from $A$ by multiplying a single row (say, row $i$) by the scalar $\alpha$, or by multiplying a single column by the scalar $\alpha$. Then $\det\left(B\right)=\alpha\det\left(A\right)$.
@end

@proof
@col
Expand along row $i$, then

\begin{align*}
\displaystyle \det\left(B\right)=(-1)^{i+1}[B]_{i1}\det\left(B(i|1)\right)+(-1)^{i+1}[B]_{i2}\det\left(B(i|2)\right)+\cdots+(-1)^{i+n}[B]_{in}\det\left(B(i|n)\right)
\end{align*}

\begin{align*}
\displaystyle =\alpha((-1)^{i+1}[A]_{i1}\det\left(A(i|1)\right)+(-1)^{i+1}[A]_{i2}\det\left(A(i|2)\right)+\cdots+(-1)^{i+n}[A]_{in}\det\left(A(i|n)\right))=\det\left(A\right).
\end{align*}

∎
@end

@thm
@title{Determinant with Equal Rows or Columns}
@label{DERC}
Suppose that $A$ is a square matrix with two equal rows, or two equal columns. Then $\det\left(A\right)=0$.
@end

@proof
@col
Skip the proof. If you are interested, see Beezer p.274.
∎
@end

@thm
@title{Determinant for Row or Column Multiples and Addition}
@label{DRCMA}
Suppose that $A$ is a square matrix. Let $B$ be the square matrix obtained from $A$ by multiplying a row by the scalar $\alpha$ and then adding it to another row, or by multiplying a column by the scalar $\alpha$ and then adding it to another column. Then $\det\left(B\right)=\det\left(A\right)$.
@end

@proof
@col
Suppose the row operation is $\alpha R_{i}+R_{j}$, expand along row $j$. For details, see Beezer p.275.
∎
@end

@thm

\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{i-1,1}&amp;a_{i-1,2}&amp;\cdots&amp;a_{i-1,n}\\
b_{1}+c_{1}&amp;b_{2}+c_{2}&amp;\cdots&amp;b_{n}+c_{n}\\
a_{i+1,1}&amp;a_{i+1,2}&amp;\cdots&amp;a_{i+1,n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\
\end{vmatrix}=\begin{vmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{i-1,1}&amp;a_{i-1,2}&amp;\cdots&amp;a_{i-1,n}\\
b_{1}&amp;b_{2}&amp;\cdots&amp;b_{n}\\
a_{i+1,1}&amp;a_{i+1,2}&amp;\cdots&amp;a_{i+1,n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\
\end{vmatrix}+\begin{vmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{i-1,1}&amp;a_{i-1,2}&amp;\cdots&amp;a_{i-1,n}\\
c_{1}&amp;c_{2}&amp;\cdots&amp;c_{n}\\
a_{i+1,1}&amp;a_{i+1,2}&amp;\cdots&amp;a_{i+1,n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\
\end{vmatrix}
\end{align*}

Similarly

\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&amp;\cdots&amp;a_{1,i-1}&amp;b_{1}+c_{1}&amp;a_{1,i+1}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;\cdots&amp;a_{2,i-1}&amp;b_{2}+c_{2}&amp;a_{2,i+1}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;\cdots&amp;a_{n,i-1}&amp;b_{n}+c_{n}&amp;a_{n,i+1}&amp;\cdots&amp;a_{nn}\end{vmatrix}
\end{align*}

\begin{align*}
\displaystyle =\begin{vmatrix}a_{11}&amp;\cdots&amp;a_{1,i-1}&amp;b_{1}&amp;a_{1,i+1}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;\cdots&amp;a_{2,i-1}&amp;b_{2}&amp;a_{2,i+1}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;\cdots&amp;a_{n,i-1}&amp;b_{n}&amp;a_{n,i+1}&amp;\cdots&amp;a_{nn}\end{vmatrix}+\begin{vmatrix}a_{11}&amp;\cdots&amp;a_{1,i-1}&amp;c_{1}&amp;a_{1,i+1}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;\cdots&amp;a_{2,i-1}&amp;c_{2}&amp;a_{2,i+1}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;\cdots&amp;a_{n,i-1}&amp;c_{n}&amp;a_{n,i+1}&amp;\cdots&amp;a_{nn}\end{vmatrix}
\end{align*}
@end

@proof
@col
Expand along row $i$ (or column $i$).
∎
@end

@eg
Suppose we desire the determinant of the $4\times 4$ matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;0&amp;2&amp;3\\
1&amp;3&amp;-1&amp;1\\
-1&amp;1&amp;-1&amp;2\\
3&amp;5&amp;4&amp;0\end{bmatrix}
\end{align*}

We will perform a sequence of row operations on this matrix, shooting for an upper triangular matrix, whose determinant will be simply the product of its diagonal entries. For each row operation, we will track the effect on the determinant via Theorem  @ref{DRCS} Theorem  @ref{DRCM} and Theorem  @ref{DRCMA}.

\begin{align*}
\displaystyle \begin{vmatrix}2&amp;0&amp;2&amp;3\\
1&amp;3&amp;-1&amp;1\\
-1&amp;1&amp;-1&amp;2\\
3&amp;5&amp;4&amp;0\end{vmatrix}
\end{align*}

\begin{align*}
\displaystyle =-\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
2&amp;0&amp;2&amp;3\\
-1&amp;1&amp;-1&amp;2\\
3&amp;5&amp;4&amp;0\end{vmatrix}\qquad(R_{1}\leftrightarrow R_{2})
\end{align*}

\begin{align*}
\displaystyle =-\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;-6&amp;4&amp;1\\
-1&amp;1&amp;-1&amp;2\\
3&amp;5&amp;4&amp;0\end{vmatrix}\qquad(-2R_{1}+R_{2})
\end{align*}

\begin{align*}
\displaystyle =-\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;-6&amp;4&amp;1\\
0&amp;4&amp;-2&amp;3\\
3&amp;5&amp;4&amp;0\end{vmatrix}\qquad(1R_{1}+R_{3})
\end{align*}

\begin{align*}
\displaystyle =-\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;-6&amp;4&amp;1\\
0&amp;4&amp;-2&amp;3\\
0&amp;-4&amp;7&amp;-3\end{vmatrix}\qquad(-3R_{1}+R_{4})
\end{align*}

\begin{align*}
\displaystyle =-\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;-2&amp;2&amp;4\\
0&amp;4&amp;-2&amp;3\\
0&amp;-4&amp;7&amp;-3\end{vmatrix}\qquad(1R_{3}+R_{2})
\end{align*}

\begin{align*}
\displaystyle =2\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;1&amp;-1&amp;-2\\
0&amp;4&amp;-2&amp;3\\
0&amp;-4&amp;7&amp;-3\end{vmatrix}\qquad(-\frac{1}{2}R_{2})
\end{align*}

\begin{align*}
\displaystyle =2\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;1&amp;-1&amp;-2\\
0&amp;0&amp;2&amp;11\\
0&amp;-4&amp;7&amp;-3\end{vmatrix}\qquad(-4R_{2}+R_{3})
\end{align*}

\begin{align*}
\displaystyle =2\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;1&amp;-1&amp;-2\\
0&amp;0&amp;2&amp;11\\
0&amp;0&amp;3&amp;-11\end{vmatrix}\qquad(-4R_{2}+R_{4})
\end{align*}

\begin{align*}
\displaystyle =2\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;1&amp;-1&amp;-2\\
0&amp;0&amp;2&amp;11\\
0&amp;0&amp;1&amp;-22\end{vmatrix}\qquad(-1R_{3}+R_{4})
\end{align*}

\begin{align*}
\displaystyle =2\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;1&amp;-1&amp;-2\\
0&amp;0&amp;0&amp;55\\
0&amp;0&amp;1&amp;-22\end{vmatrix}\qquad(-2R_{4}+R_{3})
\end{align*}

\begin{align*}
\displaystyle =-2\begin{vmatrix}1&amp;3&amp;-1&amp;1\\
0&amp;1&amp;-1&amp;-2\\
0&amp;0&amp;1&amp;-22\\
0&amp;0&amp;0&amp;55\end{vmatrix}\qquad(R_{3}\leftrightarrow R_{4})
\end{align*}

\begin{align*}
\displaystyle =-2\times 1\times 1\times 1\times 55=-110.
\end{align*}
@end

@section{Examples}

@eg
Compute

\begin{align*}
\displaystyle \begin{vmatrix}1&amp;a_{1}&amp;a_{2}&amp;a_{3}\\
1&amp;a_{1}+b_{1}&amp;a_{2}&amp;a_{3}\\
1&amp;a_{1}&amp;a_{2}+b_{2}&amp;a_{3}\\
1&amp;a_{1}&amp;a_{2}&amp;a_{3}+b_{3}.\end{vmatrix}
\end{align*}

The above is

\begin{align*}
\displaystyle \begin{vmatrix}1&amp;a_{1}&amp;a_{2}&amp;a_{3}\\
0&amp;b_{1}&amp;0&amp;0\\
0&amp;0&amp;b_{2}&amp;0\\
0&amp;0&amp;0&amp;b_{3}\end{vmatrix}\qquad(-1R_{1}+R_{2},-1R_{1}+R_{3},-1R_{1}+R_{4})
\end{align*}

\begin{align*}
\displaystyle =b_{1}b_{2}b_{3}\qquad(\text{upper triangular matrix})
\end{align*}
@end

@eg
Compute

\begin{align*}
\displaystyle \begin{vmatrix}1&amp;1&amp;1\\
a&amp;b&amp;c\\
a^{2}&amp;b^{2}&amp;c^{2}\end{vmatrix}.
\end{align*}

\begin{align*}
\displaystyle =\begin{vmatrix}1&amp;1&amp;1\\
a&amp;b&amp;c\\
0&amp;b(b-a)&amp;c(c-a)\end{vmatrix}\qquad(-aR_{2}+R_{3})
\end{align*}

\begin{align*}
\displaystyle =\begin{vmatrix}1&amp;1&amp;1\\
0&amp;b-a&amp;c-a\\
0&amp;b(b-a)&amp;c(c-a)\end{vmatrix}\qquad(-aR_{1}+R_{2})
\end{align*}

\begin{align*}
\displaystyle =\begin{vmatrix}b-a&amp;c-a\\
b(b-a)&amp;c(c-a)\end{vmatrix}\qquad(\text{expand along the first column})
\end{align*}

\begin{align*}
\displaystyle =(b-a)\begin{vmatrix}1&amp;c-a\\
b&amp;c(c-a)\end{vmatrix}\qquad(\text{pull out $b-a$ from column 1})
\end{align*}

\begin{align*}
\displaystyle =(b-a)(c-a)\begin{vmatrix}1&amp;1\\
b&amp;c\end{vmatrix}\qquad(\text{pull out $c-a$ from column 2})
\end{align*}

\begin{align*}
\displaystyle =(b-a)(c-a)(c-b).
\end{align*}
@end

@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}0&amp;0&amp;0&amp;a_{14}\\
0&amp;0&amp;a_{23}&amp;a_{24}\\
0&amp;a_{32}&amp;a_{33}&amp;a_{34}\\
a_{41}&amp;a_{42}&amp;a_{43}&amp;a_{44}\end{bmatrix}
\end{align*}

Find $\det\left(A\right)$. $\det\left(A\right)$

\begin{align*}
\displaystyle =-\begin{vmatrix}a_{41}&amp;a_{42}&amp;a_{43}&amp;a_{44}\\
0&amp;0&amp;a_{23}&amp;a_{24}\\
0&amp;a_{32}&amp;a_{33}&amp;a_{34}\\
0&amp;0&amp;0&amp;a_{14}\end{vmatrix}\qquad(R_{1}\leftrightarrow R_{4})
\end{align*}

\begin{align*}
\displaystyle =\begin{vmatrix}a_{41}&amp;a_{42}&amp;a_{43}&amp;a_{44}\\
0&amp;a_{32}&amp;a_{33}&amp;a_{34}\\
0&amp;0&amp;a_{23}&amp;a_{24}\\
0&amp;0&amp;0&amp;a_{14}\end{vmatrix}\qquad(R_{2}\leftrightarrow R_{3})
\end{align*}

\begin{align*}
\displaystyle =a_{41}a_{32}a_{23}a_{14}.
\end{align*}
@end

@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}\\
a_{21}&amp;a_{22}&amp;a_{23}\\
a_{31}&amp;a_{32}&amp;a_{33}\end{bmatrix}
\end{align*}

Given $\det\left(A\right)=1$.
Find

\begin{align*}
\displaystyle \begin{vmatrix}2a_{11}&amp;3a_{12}&amp;4a_{13}\\
2a_{21}&amp;3a_{22}&amp;4a_{23}\\
2a_{31}&amp;3a_{32}&amp;4a_{33}\end{vmatrix}.
\end{align*}

The above is

\begin{align*}
\displaystyle =2\begin{vmatrix}a_{11}&amp;3a_{12}&amp;4a_{13}\\
a_{21}&amp;3a_{22}&amp;4a_{23}\\
a_{31}&amp;3a_{32}&amp;4a_{33}\end{vmatrix}\qquad(\frac{1}{2}C_{1})
\end{align*}

\begin{align*}
\displaystyle =2\times 3\begin{vmatrix}a_{11}&amp;a_{12}&amp;4a_{13}\\
a_{21}&amp;a_{22}&amp;4a_{23}\\
a_{31}&amp;a_{32}&amp;4a_{33}\end{vmatrix}\qquad(\frac{1}{3}C_{2})
\end{align*}

\begin{align*}
\displaystyle =2\times 3\times 4\begin{vmatrix}a_{11}&amp;a_{12}&amp;a_{13}\\
a_{21}&amp;a_{22}&amp;a_{23}\\
a_{31}&amp;a_{32}&amp;a_{33}\end{vmatrix}\qquad(\frac{1}{4}C_{3})
\end{align*}

\begin{align*}
\displaystyle =24.
\end{align*}
@end

@eg
Compute

\begin{align*}
\displaystyle \det\left(A\right)=\begin{vmatrix}1&amp;1&amp;1&amp;1&amp;1\\
2&amp;2&amp;1&amp;2&amp;2\\
1&amp;2&amp;1&amp;2&amp;3\\
1&amp;1&amp;1&amp;3&amp;2\\
1&amp;1&amp;1&amp;1&amp;4\end{vmatrix}
\end{align*}

By $-1R_{1}+R_{2}$, $-1R_{1}+R_{3}$, $-1R_{1}+R_{4}$, $-1R_{1}+R_{5}$,
the above is

\begin{align*}
\displaystyle \begin{vmatrix}1&amp;1&amp;1&amp;1&amp;1\\
1&amp;1&amp;0&amp;1&amp;1\\
0&amp;1&amp;0&amp;1&amp;2\\
0&amp;0&amp;0&amp;2&amp;1\\
0&amp;0&amp;0&amp;0&amp;3\end{vmatrix}.
\end{align*}

Expand along column 3,
the above is

\begin{align*}
\displaystyle (-1)^{1+3}\times 1\times\begin{vmatrix}1&amp;1&amp;1&amp;1\\
0&amp;1&amp;1&amp;2\\
0&amp;0&amp;2&amp;1\\
0&amp;0&amp;0&amp;3\end{vmatrix}=1\times 1\times 2\times 3=6.
\end{align*}
@end

@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}\\
a_{21}&amp;a_{22}&amp;a_{23}\\
a_{31}&amp;a_{32}&amp;a_{33}\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle B=\begin{bmatrix}b_{11}&amp;b_{12}\\
b_{21}&amp;b_{22}\end{bmatrix}.
\end{align*}

Let

\begin{align*}
\displaystyle C=\begin{bmatrix}A&amp;{\cal O}_{32}\\
{\cal O}_{23}&amp;B\end{bmatrix}=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;0&amp;0\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;0&amp;0\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;0&amp;0\\
0&amp;0&amp;0&amp;b_{11}&amp;b_{12}\\
0&amp;0&amp;0&amp;b_{21}&amp;b_{22}\end{bmatrix}
\end{align*}

Show that

\begin{align*}
\displaystyle \det\left(C\right)=\det\left(A\right)\det\left(B\right).
\end{align*}

Expand $C$ along the last row, $\det\left(C\right)=$

\begin{align*}
\displaystyle (-1)^{5+4}b_{21}\begin{vmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;0\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;0\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;0\\
0&amp;0&amp;0&amp;b_{12}\\
\end{vmatrix}+(-1)^{5+5}b_{22}\begin{vmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;0\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;0\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;0\\
0&amp;0&amp;0&amp;b_{11}\\
\end{vmatrix}.
\end{align*}

For each $4\times 4$ submatrix, expand along the last row, the above is

\begin{align*}
\displaystyle (-1)^{5+4}b_{21}(-1)^{4+4}b_{12}\begin{vmatrix}a_{11}&amp;a_{12}&amp;a_{13}\\
a_{21}&amp;a_{22}&amp;a_{23}\\
a_{31}&amp;a_{32}&amp;a_{33}\\
\end{vmatrix}+(-1)^{5+5}b_{22}(-1)^{4+4}b_{11}\begin{vmatrix}a_{11}&amp;a_{12}&amp;a_{13}\\
a_{21}&amp;a_{22}&amp;a_{23}\\
a_{31}&amp;a_{32}&amp;a_{33}\\
\end{vmatrix}
\end{align*}

\begin{align*}
\displaystyle =(b_{11}b_{22}-b_{21}b_{12})\det\left(A\right)=\det\left(A\right)\det\left(B\right).
\end{align*}

<b>Remark</b>: The result is also valid when $A$ is a square matrix of size $n$ and $B$ is a square matrix of size $m$.
@end

@eg
Compute

\begin{align*}
\displaystyle \det\left(A\right)=\begin{vmatrix}a&amp;1&amp;1&amp;1\\
1&amp;a&amp;1&amp;1\\
1&amp;1&amp;a&amp;1\\
1&amp;1&amp;1&amp;a.\end{vmatrix}
\end{align*}

By $-1R_{1}+R_{2}$, $-1R_{1}+R_{3}$, $-1R_{1}+R_{4}$, the above is

\begin{align*}
\displaystyle \begin{vmatrix}a&amp;1&amp;1&amp;1\\
1-a&amp;a-1&amp;0&amp;0\\
1-a&amp;0&amp;a-1&amp;0\\
1-a&amp;0&amp;0&amp;a-1\end{vmatrix}
\end{align*}

Take out the common factor $a-1$ of row 2, row 3 and row 4, the above is

\begin{align*}
\displaystyle (a-1)^{3}\begin{vmatrix}a&amp;1&amp;1&amp;1\\
-1&amp;1&amp;0&amp;0\\
-1&amp;0&amp;1&amp;0\\
-1&amp;0&amp;0&amp;1\end{vmatrix}
\end{align*}

\begin{align*}
\displaystyle =(a-1)^{3}\begin{vmatrix}a+3&amp;0&amp;0&amp;0\\
-1&amp;1&amp;0&amp;0\\
-1&amp;0&amp;1&amp;0\\
-1&amp;0&amp;0&amp;1\end{vmatrix}\qquad(-1R_{4}+R_{1},-1R_{3}+R_{1},-1R_{2}+R_{1})
\end{align*}

\begin{align*}
\displaystyle =(a+3)(a-1)^{3}.
\end{align*}
@end

@section{More examples}

@eg
Let $A_{n}$ be a $n\times n$ matrix

\begin{align*}
\displaystyle \left.\left[\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}x&amp;1&amp;1&amp;\cdots&amp;1\\
1&amp;x&amp;1&amp;\cdots&amp;1\\
1&amp;1&amp;x&amp;\cdots&amp;1\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;1\\
1&amp;1&amp;1&amp;\cdots&amp;x\end{array}}_{n}}\right]\right\}\,n
\end{align*}

Find $\det(A_{n})$. Add $C_{2},C_{3},\ldots,C_{n}$ to $C_{1}$.

\begin{align*}
\displaystyle \det(A_{n})=\left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}x+(n-1)&amp;1&amp;1&amp;\cdots&amp;1\\
x+(n-1)&amp;x&amp;1&amp;\cdots&amp;1\\
x+(n-1)&amp;1&amp;x&amp;\cdots&amp;1\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;1\\
x+(n-1)&amp;1&amp;1&amp;\cdots&amp;x\end{array}}_{n}}\right|\right\}\,n=(x+(n-1))\left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}1&amp;1&amp;1&amp;\cdots&amp;1\\
1&amp;x&amp;1&amp;\cdots&amp;1\\
1&amp;1&amp;x&amp;\cdots&amp;1\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;1\\
1&amp;1&amp;1&amp;\cdots&amp;x\end{array}}_{n}}\right|\right\}\,n
\end{align*}

$-C_{1}+C_{2}$, $\-C_{1}+C_{3},\ldots,-C_{1}+C_{n}$. The determinant is

\begin{align*}
\displaystyle \left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}1&amp;0&amp;0&amp;\cdots&amp;0\\
1&amp;x-1&amp;0&amp;\cdots&amp;0\\
1&amp;0&amp;x-1&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;0\\
1&amp;0&amp;0&amp;\cdots&amp;x-1\end{array}}_{n}}\right|\right\}\,n=(x+n-1)(x-1)^{n-1}.
\end{align*}

The last step follows by the fact that the matrix on the left hand side is the lower triangular matrix.
@end
@eg
Let $B_{n}$ be a $n\times n$ matrix in the form

\begin{align*}
\displaystyle \begin{bmatrix}1-a_{1}&amp;a_{2}&amp;0&amp;\cdots&amp;0&amp;0\\
-1&amp;1-a_{2}&amp;a_{3}&amp;\cdots&amp;0&amp;0\\
0&amp;-1&amp;1-a_{3}&amp;\cdots&amp;0&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;1-a_{n-1}&amp;a_{n}\\
0&amp;0&amp;0&amp;\cdots&amp;-1&amp;1-a_{n}\end{bmatrix}
\end{align*}

<ol class="ltx_enumerate">
<li class="ltx_item">
Show that $\det(B_{n})=\det(B_{n-1})+(-1)^{n}(a_{1}a_{2}\cdots a_{n})$.
</li>
<li class="ltx_item">
Hence show $\det(B_{n})=1+\sum_{i=1}^{n}(-1)^{i}(a_{1}a_{2}\cdots a_{i})$.
</li>

</ol>
@end

<b>Answer.</b>

<ol class="ltx_enumerate">
<li class="ltx_item">
$\det(B_{n})=$

\begin{align*}
\displaystyle \begin{vmatrix}1-a_{1}&amp;a_{2}&amp;0&amp;\cdots&amp;0&amp;0\\
-1&amp;1-a_{2}&amp;a_{3}&amp;\cdots&amp;0&amp;0\\
0&amp;-1&amp;1-a_{3}&amp;\cdots&amp;0&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;1-a_{n-1}&amp;a_{n}\\
-a_{1}&amp;0&amp;0&amp;\cdots&amp;0&amp;1\end{vmatrix}\,\,\,\,\text{ (add $R_{1},\ldots,R_{n-1}$ to $R_{n}$)}
\end{align*}

Expand along the last row, the above is

\begin{align*}
\displaystyle (-1)^{n+1}(-a_{1})\begin{vmatrix}a_{2}&amp;0&amp;\cdots&amp;0&amp;0\\
1-a_{2}&amp;a_{3}&amp;\cdots&amp;0&amp;0\\
-1&amp;1-a_{3}&amp;\cdots&amp;0&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\
0&amp;0&amp;\cdots&amp;1-a_{n-1}&amp;a_{n}\end{vmatrix}+
\end{align*}

\begin{align*}
\displaystyle +(-1)^{n+n}\begin{vmatrix}1-a_{1}&amp;a_{2}&amp;0&amp;\cdots&amp;0\\
-1&amp;1-a_{2}&amp;a_{3}&amp;\cdots&amp;0\\
0&amp;-1&amp;1-a_{3}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;1-a_{n-1}\\
\end{vmatrix}
\end{align*}

The first matrix is an lower triangular matrix, so the determinant is the product of the diagonal entries, the second matrix is $B_{n-1}$.

\begin{align*}
\displaystyle =(-1)^{n}(a_{1}\cdots a_{n})+\det(B_{n-1}).
\end{align*}

</li>
<li class="ltx_item">
We prove the result by mathematical induction: <b>Step 1</b>: The formula is valid for $n=1$: $\det(B_{1})=1-a_{1}$. <b>Step 2</b>: Suppose the formula is true for $n=k$, we want to show that the formula is true for $n=k+1$:

\begin{align*}
\displaystyle B_{k+1}=(-1)^{k+1}(a_{1}\cdots a_{k+1})+\det(B_{k})
\end{align*}

\begin{align*}
\displaystyle =1+\sum_{i=1}^{k}(-1)^{i}(a_{1}a_{2}\cdots a_{i})+(-1)^{k+1}(a_{1}\cdots a_{k+1})
\end{align*}

\begin{align*}
\displaystyle =1+\sum_{i=1}^{k+1}(-1)^{i}(a_{1}\cdots a_{i})
\end{align*}

\begin{align*}
\displaystyle =1+\sum_{i=1}^{n}(-1)^{i}(a_{1}\cdots a_{i}).
\end{align*}

The formulat is true for $n=k+1$. <b>Step 3</b>: By mathematical induction, the formula is valid for all positive integer. <b>Explanation</b>: the formula is true for $k=1$, then it is true for $k+1=2$, so true for $k+1=3$, etc. Hence true for all integers.
This process is called <b>mathematical induction</b>.
</li>

</ol>

$\square$

@eg
Let $C_{n}$ be a $n\times n$ matrix given by

\begin{align*}
\displaystyle C_{n}=\left.\left[\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x&amp;a&amp;a&amp;\cdots&amp;a&amp;a\\
-a&amp;x&amp;a&amp;\cdots&amp;a&amp;a\\
-a&amp;-a&amp;x&amp;\cdots&amp;a&amp;a\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a&amp;-a&amp;-a&amp;\cdots&amp;-a&amp;x\end{array}}_{n}}\right]\right\}\,n
\end{align*}

<ol class="ltx_enumerate">
<li class="ltx_item">
Show that $\det(C_{n})=a(x+a)^{n-1}+(x-a)\det(C_{n-1})$.
</li>
<li class="ltx_item">
Show that $\det(C_{n})=\frac{1}{2}((x+a)^{n}+(x-a)^{n})$.
</li>

</ol>
@end

<b>Answer.</b>

<ol class="ltx_enumerate">
<li class="ltx_item">
The last column can be written as

\begin{align*}
\displaystyle \begin{bmatrix}a\\
a\\
a\\
\vdots\\
x\end{bmatrix}=\begin{bmatrix}a\\
a\\
a\\
\vdots\\
a\end{bmatrix}+\begin{bmatrix}0\\
0\\
0\\
\vdots\\
x-a\end{bmatrix}.
\end{align*}

Then $\det(C_{n})=$

\begin{align*}
\displaystyle \left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x&amp;a&amp;a&amp;\cdots&amp;a&amp;a\\
-a&amp;x&amp;a&amp;\cdots&amp;a&amp;a\\
-a&amp;-a&amp;x&amp;\cdots&amp;a&amp;a\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a&amp;-a&amp;-a&amp;\cdots&amp;-a&amp;a\end{array}}_{n}}\right|\right\}\,n+\left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x&amp;a&amp;a&amp;\cdots&amp;a&amp;0\\
-a&amp;x&amp;a&amp;\cdots&amp;a&amp;0\\
-a&amp;-a&amp;x&amp;\cdots&amp;a&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a&amp;-a&amp;-a&amp;\cdots&amp;-a&amp;x-a\end{array}}_{n}}\right|\right\}\,n
\end{align*}

For the first determinant, pulling out $a$ from the last column, it is equal to

\begin{align*}
\displaystyle a\left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x&amp;a&amp;a&amp;\cdots&amp;a&amp;1\\
-a&amp;x&amp;a&amp;\cdots&amp;a&amp;1\\
-a&amp;-a&amp;x&amp;\cdots&amp;a&amp;1\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a&amp;-a&amp;-a&amp;\cdots&amp;-a&amp;1\end{array}}_{n}}\right|\right\}\,n
\end{align*}

\begin{align*}
\displaystyle =a\left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x+a&amp;2a&amp;2a&amp;\cdots&amp;2a&amp;0\\
0&amp;x+a&amp;2a&amp;\cdots&amp;2a&amp;0\\
0&amp;0&amp;x+a&amp;\cdots&amp;2a&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a&amp;-a&amp;-a&amp;\cdots&amp;-a&amp;1\end{array}}_{n}}\right|\right\}\,n\,\,\,(-1R_{n}+R_{1},\ldots,-1R_{n-1}+R_{n-1})
\end{align*}

\begin{align*}
\displaystyle =(-1)^{n+n}a\left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x+a&amp;2a&amp;2a&amp;\cdots&amp;2a\\
0&amp;x+a&amp;2a&amp;\cdots&amp;2a\\
0&amp;0&amp;x+a&amp;\cdots&amp;2a\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;x+a\end{array}}_{n-1}}\right|\right\}\,n-1\,\,\,\text{(expand along the last column)}
\end{align*}

\begin{align*}
\displaystyle =a(x+a)^{n-1}\,\,\,(\text{determinant of upper triangular matrix})
\end{align*}

For the second determinant,

\begin{align*}
\displaystyle \left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x&amp;a&amp;a&amp;\cdots&amp;a&amp;0\\
-a&amp;x&amp;a&amp;\cdots&amp;a&amp;0\\
-a&amp;-a&amp;x&amp;\cdots&amp;a&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a&amp;-a&amp;-a&amp;\cdots&amp;-a&amp;x-a\end{array}}_{n}}\right|\right\}\,n
\end{align*}

\begin{align*}
\displaystyle =(x-a)\left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}x&amp;a&amp;a&amp;\cdots&amp;a\\
-a&amp;x&amp;a&amp;\cdots&amp;a\\
-a&amp;-a&amp;x&amp;\cdots&amp;a\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a&amp;-a&amp;-a&amp;\cdots&amp;x-a\end{array}}_{n-1}}\right|\right\}\,n-1\,\,\,(\text{expand along the last column})
\end{align*}

\begin{align*}
\displaystyle =(x-a)\det(C_{n-1}).
\end{align*}

Adding the results

\begin{align*}
\displaystyle \det(C_{n})=a(x+a)^{n-1}+(x-a)\det(C_{n-1}).
\end{align*}

</li>
<li class="ltx_item">
We will prove the formula by induction. <b>Step 1</b>: when $n=1$, $C_{1}=[x]$, $\det(C_{1})=x=\frac{1}{2}((x+a)+(x-a))$. So the formula is valid for $n=1$.<b>Step 2</b>: Suppose the formula is valid for $n=k$, i.e.

\begin{align*}
\displaystyle \det(C_{k})=\frac{1}{2}((x+a)^{k}+(x-a)^{k}).
\end{align*}

Then for $n=k+1$,

\begin{align*}
\displaystyle \det(C_{k+1})=a(x+a)^{k}+(x-a)\det(C_{k})
\end{align*}

\begin{align*}
\displaystyle =a(x+a)^{k}+(x-a)\frac{1}{2}((x+a)^{k}+(x-a)^{k})
\end{align*}

\begin{align*}
\displaystyle =\frac{1}{2}(x+a)^{k}(2a+x-a)+\frac{1}{2}(x-a)^{k+1}
\end{align*}

\begin{align*}
\displaystyle =\frac{1}{2}((x+a)^{k+1}+(x-a)^{k+1})
\end{align*}

\begin{align*}
\displaystyle =\frac{1}{2}((x+a)^{n}+(x-a)^{n}).
\end{align*}

So the formula is valid for $n=k+1$. <b>Step 3</b> By mathematical induction, the formula is valid for all integers $n\geq 1$.
</li>

</ol>

$\square$

@eg
@title{ (}
This is the most important example of determinant. Let

\begin{align*}
\displaystyle V_{n}=\begin{bmatrix}1&amp;1&amp;1&amp;\cdots&amp;1\\
a_{1}&amp;a_{2}&amp;a_{3}&amp;\cdots&amp;a_{n}\\
a_{1}^{2}&amp;a_{2}^{2}&amp;a_{3}^{2}&amp;\cdots&amp;a_{n}^{2}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{1}^{n-2}&amp;a_{2}^{n-2}&amp;a_{3}^{n-2}&amp;\cdots&amp;a_{n}^{n-2}\\
a_{1}^{n-1}&amp;a_{2}^{n-1}&amp;a_{3}^{n-1}&amp;\cdots&amp;a_{n}^{n-1}\end{bmatrix}
\end{align*}

<ol class="ltx_enumerate">
<li class="ltx_item">
$\det(V_{n})=\det(V_{n-1})\prod_{i=1}^{n-1}(a_{n}-a_{i})$.
</li>
<li class="ltx_item">
$\det(V_{n})=\prod_{1\leq i&lt;j\leq n}(a_{j}-a_{i})$ for $n\geq 2$.
</li>

</ol>
@end

<b>Answer.</b>

<ol class="ltx_enumerate">
<li class="ltx_item">
$\det V_{n}=$(by $-a_{n}R_{n-1}+R_{n}$, $-a_{n}R_{n-2}+R_{n-1}$, …, $-a_{n}R_{1}+R_{2}$)

\begin{align*}
\displaystyle =\begin{vmatrix}1&amp;1&amp;1&amp;\cdots&amp;1\\
a_{1}-a_{n}&amp;a_{2}-a_{n}&amp;a_{3}-a_{n}&amp;\cdots&amp;0\\
a_{1}^{2}-a_{1}a_{n}&amp;a_{2}^{2}-a_{2}a_{n}&amp;a_{3}^{2}-a_{3}a_{n}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{1}^{n-2}-a_{1}^{n-3}a_{n}&amp;a_{2}^{n-2}-a_{2}^{n-3}a_{n}&amp;a_{3}^{n-2}-a_{3}^{n-3}a_{n}&amp;\cdots&amp;0\\
a_{1}^{n-1}-a_{1}^{n-2}a_{n}&amp;a_{2}^{n-1}-a_{2}^{n-2}a_{n}&amp;a_{3}^{n-1}-a_{3}^{n-2}a_{n}&amp;\cdots&amp;0\end{vmatrix}
\end{align*}

(expanding along the last column)

\begin{align*}
\displaystyle =(-1)^{1+n}\begin{vmatrix}a_{1}-a_{n}&amp;a_{2}-a_{n}&amp;a_{3}-a_{n}&amp;\cdots&amp;a_{n-1}-a_{n}\\
a_{1}(a_{1}-a_{n})&amp;a_{2}(a_{2}-a_{n})&amp;a_{3}(a_{3}-a_{n})&amp;\cdots&amp;a_{n-1}(a_{n-1}-a_{n})\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{1}^{n-3}(a_{1}-a_{n})&amp;a_{2}^{n-3}(a_{2}-a_{n})&amp;a_{3}^{n-3}(a_{3}-a_{n})&amp;\cdots&amp;a_{n-1}^{n-3}(a_{n-1}-a_{n})\\
a_{1}^{n-2}(a_{1}-a_{n})&amp;a_{2}^{n-2}(a_{2}-a_{n})&amp;a_{3}^{n-2}(a_{3}-a_{n})&amp;\cdots&amp;a_{n-1}^{n-2}(a_{n-1}-a_{n})\\
\end{vmatrix}
\end{align*}

(pull out factor $a_{1}-a_{n}$ from column 1, $a_{2}-a_{n}$ from column 2, …., $a_{n-1}-a_{n}$ from column $n-1$)

\begin{align*}
\displaystyle =(-1)^{n-1}(a_{1}-a_{n})(a_{2}-a_{n})\cdots(a_{n-1}-a_{n})\begin{vmatrix}1&amp;1&amp;1&amp;\cdots&amp;1\\
a_{1}&amp;a_{2}&amp;a_{3}&amp;\cdots&amp;a_{n-1}\\
a_{1}^{2}&amp;a_{2}^{2}&amp;a_{3}^{2}&amp;\cdots&amp;a_{n-1}^{2}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{1}^{n-2}&amp;a_{2}^{n-2}&amp;a_{3}^{n-2}&amp;\cdots&amp;a_{n-1}^{n-2}\\
\end{vmatrix}
\end{align*}

\begin{align*}
\displaystyle =(a_{n}-a_{1})\cdots(a_{n}-a_{n-1})\det(V_{n-1})=\det(V_{n-1})\prod_{i=1}^{n-1}(a_{n}-a_{i}).
\end{align*}

</li>
<li class="ltx_item">
Again by mathematical induction:<b>Step 1</b> When $n=2$,

\begin{align*}
\displaystyle \begin{vmatrix}1&amp;1\\
a_{1}&amp;a_{2}\end{vmatrix}=a_{2}-a_{1}.
\end{align*}

So the formula is valid for $n=2$. <b>Step 2</b> Suppose the statement is true for $n=k$, i.e.

\begin{align*}
\displaystyle \det(V_{k})=\prod_{1\leq i&lt;j\leq k}(a_{j}-a_{i}).
\end{align*}

Then for $n=k+1$

\begin{align*}
\displaystyle \det(V_{k+1})=\det(V_{k})\prod_{i=1}^{k}(a_{k+1}-a_{i})
\end{align*}

\begin{align*}
\displaystyle =\prod_{1\leq i&lt;j\leq k}(a_{j}-a_{i})\prod_{i=1}^{k}(a_{k+1}-a_{i})
\end{align*}

\begin{align*}
\displaystyle =\prod_{1\leq i&lt;j\leq k+1}(a_{j}-a_{i})
\end{align*}

\begin{align*}
\displaystyle =\prod_{1\leq 1&lt;j\leq n}(a_{j}-a_{i}).
\end{align*}

The formula is valid for $n=k+1$.<b>Step 3</b> By mathematical induction, the formula is valid for all $n\geq 2$. Or without mathematical induction, you can simple repeat the steps again and again until $n=2$.
</li>

</ol>

$\square$

@section{More properties of determinants}

@corollary
<ol class="ltx_enumerate">
<li class="ltx_item">
Let $I_{n}\xrightarrow{R_{i}\leftrightarrow R_{j}}J$, then $\det\left(J\right)=-1$.
</li>
<li class="ltx_item">
Let $I_{n}\xrightarrow{\alpha R_{i}}J$, then $\det\left(J\right)=\alpha$.
</li>
<li class="ltx_item">
Let $I_{n}\xrightarrow{\alpha R_{i}+R_{j}}J$, then $\det\left(J\right)=1$.
</li>

</ol>
@end
@corollary
Let $A$ be a square matrix, apply row operation on $A$ and obtain a new matrix $B$.
Let $J$ be obtained by applying the same row operation on $I_{n}$. By lecture 13, $B=JA$.
Then $\det\left(B\right)=\det\left(JA\right)=\det\left(J\right)\det\left(A\right)$.
@end

In fact we have the following stronger result:

@thm

If $A$ and $B$ are square matrices. Then

\begin{align*}
\displaystyle \det\left(AB\right)=\det\left(A\right)\det\left(B\right).
\end{align*}
@end
@thm

$A$ is nonsingular if and only if $\det\left(A\right)\neq 0$.
@end
@proof
@col
Suppose $A$ is nonsingular, then $A$ is invertible, so

\begin{align*}
\displaystyle AA^{-1}=I_{n}.
\end{align*}

\begin{align*}
\displaystyle \det\left(A\right)\det\left(A^{-1}\right)=\det\left(I_{n}\right)=1.&amp;
\end{align*}

So $\det\left(A\right)$ is nonzero. Next suppose $\det\left(A\right)\neq 0$, we want to show that $A$ is nonsingular.
If $A$ is singular, then by lecture 13, there exists an invertible matrix $J$, such that $JA=B$,
where $B$ is of RREF with rank $&lt;n$. Hence $B$ has a zero row. Therefore $\det\left(B\right)=0$.
Because $J$ is invertible, $\det\left(J\right)\neq 0$. So $\det\left(A\right)=0$. Contradiction.
∎
@end
@eg
Find $x$ such that

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;1&amp;0&amp;1\\
0&amp;1&amp;1&amp;1\\
1&amp;0&amp;0&amp;x\\
0&amp;2&amp;3&amp;1\\
\end{bmatrix}
\end{align*}

is singular.Expand along the first column

\begin{align*}
\displaystyle \det\left(A\right)=2\begin{vmatrix}1&amp;1&amp;1\\
0&amp;0&amp;x\\
2&amp;3&amp;1\end{vmatrix}+\begin{vmatrix}1&amp;0&amp;1\\
1&amp;1&amp;1\\
2&amp;3&amp;1\end{vmatrix}.
\end{align*}

Expand along the second row

\begin{align*}
\displaystyle \begin{vmatrix}1&amp;1&amp;1\\
0&amp;0&amp;x\\
2&amp;3&amp;1\end{vmatrix}=-x\begin{vmatrix}1&amp;1\\
2&amp;3\end{vmatrix}=-x.
\end{align*}

Finally

\begin{align*}
\displaystyle \begin{vmatrix}1&amp;0&amp;1\\
1&amp;1&amp;1\\
2&amp;3&amp;1\end{vmatrix}=-1.
\end{align*}

Hence

\begin{align*}
\displaystyle \det\left(A\right)=-2x-1.
\end{align*}

It is singular if and only if $\det\left(A\right)=0$ if and only if $x=-\frac{1}{2}$.
@end
@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}a&amp;b&amp;c&amp;d\\
e&amp;0&amp;0&amp;0\\
f&amp;0&amp;0&amp;0\\
g&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*}

find $\det\left(A\right)$ <b>Method 1</b>

\begin{align*}
\displaystyle a\begin{vmatrix}0&amp;0&amp;0\\
0&amp;0&amp;0\\
0&amp;0&amp;0\end{vmatrix}-b\begin{vmatrix}e&amp;0&amp;0\\
f&amp;0&amp;0\\
g&amp;0&amp;0\end{vmatrix}+c\begin{vmatrix}e&amp;0&amp;0\\
f&amp;0&amp;0\\
g&amp;0&amp;0\end{vmatrix}-d\begin{vmatrix}e&amp;0&amp;0\\
f&amp;0&amp;0\\
g&amp;0&amp;0\end{vmatrix}
\end{align*}

In each of the above matrices, there is one zero columns, so all the determinants of the $3\times 3$ submatrices must be zero.
Therefore the above is

\begin{align*}
\displaystyle a0-b0+c0-d0=0.
\end{align*}

<b>Method 2</b>
If $c=0$, then column 3 is the zero column, so $\det\left(A\right)=0$. Otherwise

\begin{align*}
\displaystyle A\begin{bmatrix}0\\
1\\
-b/c\\
0\end{bmatrix}=\mathbf{0}.
\end{align*}

So $A$ is singular and hence $\det\left(A\right)=0$.
@end
@thm

If $\det\left(A\right)\neq 0$, then $A$ is invertible and

\begin{align*}
\displaystyle \det\left(A^{-1}\right)=\frac{1}{\det\left(A\right)}.
\end{align*}
@end
@proof
@col
From the previous theorem, we know that $A$ is invertible.
The result then following from ( @ref{A-1}).
∎
@end
@thm
@title{Cramer’s rule}
Let $A$ be a invertible square matrix of size $n$. Let $\mathbf{b}\in{\mathbb{R}}^{n}$.
Let $M_{k}$ be the square matrix by replacing the $k$-th column of $A$ by $\mathbf{b}$.
If

\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}\end{bmatrix}
\end{align*}

is a solution of $A\mathbf{x}=\mathbf{b}$, then

\begin{align*}
\displaystyle x_{k}=\frac{\det\left(M_{k}\right)}{\det\left(A\right)}
\end{align*}

where $k=1,\ldots,n$.
@end
@proof
@col
Because $A$ is invertible, $A\mathbf{x}=\mathbf{b}$ has a unique solution. Let $X_{k}$ be matrix obtained from the identity matrix $I_{n}$ by replacing column $k$ with $\mathbf{x}$.
Then

\begin{align*}
\displaystyle A\mathbf{e}_{i}=\mathbf{A}_{i}\text{ if $i\neq k$}\qquad A\mathbf{x}=\mathbf{b}\text{ if $i=k$.}
\end{align*}

Hence

\begin{align*}
\displaystyle AX_{k}=M_{k}.
\end{align*}

Expanding $X_{k}$ along the row $k$, we have

\begin{align*}
\displaystyle \det\left(X_{k}\right)=x_{k}\det\left(I_{n-1}\right)=x_{k}.
\end{align*}

So

\begin{align*}
\displaystyle \det\left(M_{k}\right)=\det\left(AX_{k}\right)=\det\left(A\right)\det\left(X_{k}\right)=\det\left(A\right)x_{k}.
\end{align*}

Therefore

\begin{align*}
\displaystyle x_{k}=\frac{\det\left(M_{k}\right)}{\det\left(A\right)}.
\end{align*}

∎
@end
@eg
Using Carmaer’s rule to solve the following system of linear equation.

\begin{align*}
\displaystyle x_{1}+2x_{2}+3x_{3}=2 \\
\displaystyle x_{1}\qquad+x_{3}=3 \\
\displaystyle x_{1}+x_{2}-x_{3}=1
\end{align*}

Let

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;3\\
1&amp;0&amp;1\\
1&amp;1&amp;-1\end{bmatrix}\qquad\mathbf{b}=\begin{bmatrix}2\\
3\\
1\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle \det\left(A\right)=6.
\end{align*}

\begin{align*}
\displaystyle M_{1}=\begin{bmatrix}2&amp;2&amp;3\\
3&amp;0&amp;1\\
1&amp;1&amp;-1\end{bmatrix},\det\left(M_{1}\right)=15.
\end{align*}

\begin{align*}
\displaystyle x_{1}=\frac{\det\left(M_{1}\right)}{\det\left(A\right)}=\frac{15}{6}=\frac{5}{2}.
\end{align*}

\begin{align*}
\displaystyle M_{2}=\begin{bmatrix}1&amp;2&amp;3\\
1&amp;3&amp;1\\
1&amp;1&amp;-1\end{bmatrix},\det\left(M_{2}\right)=-6.
\end{align*}

\begin{align*}
\displaystyle x_{2}=\frac{\det\left(M_{2}\right)}{\det\left(A\right)}=\frac{-6}{6}=-1.
\end{align*}

\begin{align*}
\displaystyle M_{3}=\begin{bmatrix}1&amp;2&amp;2\\
1&amp;0&amp;3\\
1&amp;1&amp;1\end{bmatrix},\det\left(M_{3}\right)=3.
\end{align*}

\begin{align*}
\displaystyle x_{3}=\frac{\det\left(M_{3}\right)}{\det\left(A\right)}=\frac{3}{6}=\frac{1}{2}.
\end{align*}

Thus

\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}=\begin{bmatrix}\frac{5}{2}\\
-1\\
\frac{1}{2}\end{bmatrix}
\end{align*}

is a solution.
@end
@thm
@title{Formula for inverse}
Suppose $A$ is an invertible matrix. Then

\begin{align*}
\displaystyle [A^{-1}]_{ji}=\frac{(-1)^{i+j}\det\left(A(i|j)\right)}{\det\left(A\right)}.
\end{align*}

Pay attention to the order of the indexes $i$ and $j$!
@end
@proof
@col
Let $B=A^{-1}$.
Let $\mathbf{B}_{i}$ be the $i$-th column of $B$. Then

\begin{align*}
\displaystyle A\mathbf{B}_{i}=\mathbf{e}_{i}.
\end{align*}

The vector $\mathbf{B}_{i}$ is a solution of $A\mathbf{x}=\mathbf{e}_{i}$.
We can use the previous theorem to find $\mathbf{B}_{i}$.
Let $M_{j}$ be the square matrix by replacing the $j$-th column of $A$ by $\mathbf{e}_{i}$.
Expand along the $j$-th column of $M_{j}$, we have

\begin{align*}
\displaystyle \det\left(M_{j}\right)=(-1)^{i+j}\det\left(M_{j}(i|j)\right)=(-1)^{i+j}\det\left(A(i|j)\right).
\end{align*}

Then the $j$-th coordinate of $\mathbf{B}_{i}$ is given by

\begin{align*}
\displaystyle B_{ji}=[\mathbf{B}_{i}]_{j}=\frac{\det\left(M_{j}\right)}{{\det\left(A\right)}}=\frac{(-1)^{i+j}\det\left(A(i|j)\right)}{\det\left(A\right)}.
\end{align*}

∎
@end
@eg
By the above formula, find the inverse of

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;3\\
1&amp;0&amp;1\\
1&amp;1&amp;-1\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle \det\left(A\right)=6.
\end{align*}

\begin{align*}
\displaystyle A(1|1)=\begin{bmatrix}0&amp;1\\
1&amp;-1\end{bmatrix},\qquad\det\left(A(1|1)\right)=-1,
\end{align*}

\begin{align*}
\displaystyle A(1|2)=\begin{bmatrix}1&amp;1\\
1&amp;-1\end{bmatrix},\qquad\det\left(A(1|2)\right)=-2,
\end{align*}

\begin{align*}
\displaystyle A(1|3)=\begin{bmatrix}1&amp;0\\
1&amp;1\end{bmatrix},\qquad\det\left(A(1|3)\right)=1,
\end{align*}

\begin{align*}
\displaystyle A(2|1)=\begin{bmatrix}2&amp;3\\
1&amp;-1\end{bmatrix},\qquad\det\left(A(2|1)\right)=-5,
\end{align*}

\begin{align*}
\displaystyle A(2|2)=\begin{bmatrix}1&amp;3\\
1&amp;-1\end{bmatrix},\qquad\det\left(A(2|2)\right)=-4,
\end{align*}

\begin{align*}
\displaystyle A(2|3)=\begin{bmatrix}1&amp;2\\
1&amp;1\end{bmatrix},\qquad\det\left(A(2|3)\right)=-1,
\end{align*}

\begin{align*}
\displaystyle A(3|1)=\begin{bmatrix}2&amp;3\\
0&amp;1\end{bmatrix},\qquad\det\left(A(3|1)\right)=2,
\end{align*}

\begin{align*}
\displaystyle A(3|2)=\begin{bmatrix}1&amp;3\\
1&amp;1\end{bmatrix},\qquad\det\left(A(3|2)\right)=-2,
\end{align*}

\begin{align*}
\displaystyle A(3|3)=\begin{bmatrix}1&amp;2\\
1&amp;0\end{bmatrix},\qquad\det\left(A(3|3)\right)=-2.
\end{align*}
@end

\begin{align*}
\displaystyle A^{-1}=\frac{1}{\det\left(A\right)}\begin{bmatrix}\det\left(A(1|1)\right)&amp;-\det\left(A(2|1)\right)&amp;\det\left(A(3|1)\right)\\
-\det\left((A(1|2)\right)&amp;\det\left(A(2|2)\right)&amp;-\det\left(A(3|2)\right)\\
\det\left(A(1|3)\right)&amp;-\det\left(A(2|3)\right)&amp;\det\left(A(3|3)\right)\end{bmatrix}=\begin{bmatrix}-\frac{1}{6}&amp;\frac{5}{6}&amp;\frac{1}{3}\\
\frac{1}{3}&amp;-\frac{2}{3}&amp;\frac{1}{3}\\
\frac{1}{6}&amp;\frac{1}{6}&amp;-\frac{1}{3}\end{bmatrix}
\end{align*}

@section{Properties of Determinant (summary)}

Let $A$ be a square matrix with size $n$.

<ol class="ltx_enumerate">
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{vmatrix}a&amp;b\\
c&amp;d\end{vmatrix}=ad-bc.
\end{align*}

</li>
<li class="ltx_item">
\begin{align*}
\displaystyle A=\begin{vmatrix}a_{11}&amp;a_{12}&amp;a_{13}\\
a_{21}&amp;a_{22}&amp;a_{23}\\
a_{31}&amp;a_{32}&amp;a_{33}\end{vmatrix}=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align*}

</li>
<li class="ltx_item">
Expand along row $i$

\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=(-1)^{i+1}\left[A\right]_{i1}\det\left(A\left(i|1\right)\right)+(-1)^{i+2}\left[A\right]_{i2}\det\left(A\left(i|2\right)\right) \\
&amp;\displaystyle\quad+(-1)^{i+3}\left[A\right]_{i3}\det\left(A\left(i|3\right)\right)+\cdots+(-1)^{i+n}\left[A\right]_{in}\det\left(A\left(i|n\right)\right)
\end{align*}

</li>
<li class="ltx_item">
Expand along column $j$

\begin{align*}
\displaystyle\det\left(A\right)&amp;\displaystyle=(-1)^{1+j}\left[A\right]_{1j}\det\left(A\left(1|j\right)\right)+(-1)^{2+j}\left[A\right]_{2j}\det\left(A\left(2|j\right)\right) \\
&amp;\displaystyle\quad+(-1)^{3+j}\left[A\right]_{3j}\det\left(A\left(3|j\right)\right)+\cdots+(-1)^{n+j}\left[A\right]_{nj}\det\left(A\left(n|j\right)\right)
\end{align*}

</li>
<li class="ltx_item">
$\det\left(A^{t}\right)=\det\left(A\right)$
</li>
<li class="ltx_item">
Determinant of upper/lower triangular matrix.

\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;\cdots&amp;a_{1n}\\
0&amp;a_{22}&amp;a_{23}&amp;\cdots&amp;a_{2n}\\
0&amp;0&amp;a_{33}&amp;\cdots&amp;a_{3n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;a_{nn}\end{vmatrix}=a_{11}a_{22}\cdots a_{nn}.
\end{align*}

\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&amp;0&amp;0&amp;\cdots&amp;0\\
a_{21}&amp;a_{22}&amp;0&amp;\cdots&amp;0\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;a_{n3}&amp;\cdots&amp;a_{nn}\end{vmatrix}=a_{11}a_{22}\cdots a_{nn}.
\end{align*}

</li>
<li class="ltx_item">
Suppose that $A$ is a square matrix with a row where every entry is zero, or a column where every entry is zero. Then $\det\left(A\right)=0$.
</li>
<li class="ltx_item">
Suppose that $A$ is a square matrix with two equal rows, or two equal columns,
i.e., $R_{i}=R_{j}$ or $C_{i}=C_{j}$ for $i\neq j$. Then $\det\left(A\right)=0$.
</li>
<li class="ltx_item">
Let $B$ be the square matrix obtained from $A$ by interchanging the location of two rows, or interchanging the location of two columns, i.e., $R_{i}\leftrightarrow R_{j}$ or $C_{i}\leftrightarrow C_{j}$, $i\neq j$. Then $\det\left(B\right)=-\det\left(A\right)$.
</li>
<li class="ltx_item">
Let $B$ be the square matrix obtained from $A$ by multiplying a single row (say, row $i$) by the scalar $\alpha$, or by multiplying a single column by the scalar $\alpha$,
i.e., $\alpha R_{i}$ or $\alpha C_{i}$. Then $\det\left(B\right)=\alpha\det\left(A\right)$.
</li>
<li class="ltx_item">
Let $B$ be the square matrix obtained from $A$ by multiplying a row by the scalar $\alpha$ and then adding it to another row, or by multiplying a column by the scalar $\alpha$ and then adding it to another column, i.e., $\alpha R_{i}+R_{j}$ or $\alpha C_{i}+C_{j}$ for $i\neq j$. Then $\det\left(B\right)=\det\left(A\right)$.
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{i-1,1}&amp;a_{i-1,2}&amp;\cdots&amp;a_{i-1,n}\\
b_{1}+c_{1}&amp;b_{2}+c_{2}&amp;\cdots&amp;b_{n}+c_{n}\\
a_{i+1,1}&amp;a_{i+1,2}&amp;\cdots&amp;a_{i+1,n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\
\end{vmatrix}=\begin{vmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{i-1,1}&amp;a_{i-1,2}&amp;\cdots&amp;a_{i-1,n}\\
b_{1}&amp;b_{2}&amp;\cdots&amp;b_{n}\\
a_{i+1,1}&amp;a_{i+1,2}&amp;\cdots&amp;a_{i+1,n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\
\end{vmatrix}+\begin{vmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{i-1,1}&amp;a_{i-1,2}&amp;\cdots&amp;a_{i-1,n}\\
c_{1}&amp;c_{2}&amp;\cdots&amp;c_{n}\\
a_{i+1,1}&amp;a_{i+1,2}&amp;\cdots&amp;a_{i+1,n}\\
\vdots&amp;\vdots&amp;\cdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\
\end{vmatrix}
\end{align*}

Similarly

\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&amp;\cdots&amp;a_{1,i-1}&amp;b_{1}+c_{1}&amp;a_{1,i+1}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;\cdots&amp;a_{2,i-1}&amp;b_{2}+c_{2}&amp;a_{2,i+1}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;\cdots&amp;a_{n,i-1}&amp;b_{n}+c_{n}&amp;a_{n,i+1}&amp;\cdots&amp;a_{nn}\end{vmatrix}
\end{align*}

\begin{align*}
\displaystyle =\begin{vmatrix}a_{11}&amp;\cdots&amp;a_{1,i-1}&amp;b_{1}&amp;a_{1,i+1}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;\cdots&amp;a_{2,i-1}&amp;b_{2}&amp;a_{2,i+1}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;\cdots&amp;a_{n,i-1}&amp;b_{n}&amp;a_{n,i+1}&amp;\cdots&amp;a_{nn}\end{vmatrix}+\begin{vmatrix}a_{11}&amp;\cdots&amp;a_{1,i-1}&amp;c_{1}&amp;a_{1,i+1}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;\cdots&amp;a_{2,i-1}&amp;c_{2}&amp;a_{2,i+1}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;\cdots&amp;a_{n,i-1}&amp;c_{n}&amp;a_{n,i+1}&amp;\cdots&amp;a_{nn}\end{vmatrix}
\end{align*}

</li>
<li class="ltx_item">
If $A$ and $B$ are square matrices, then

\begin{align*}
\displaystyle \det\left(AB\right)=\det\left(A\right)\det\left(B\right).
\end{align*}

</li>

</ol>

@chapter{Eigenvalues and Eigenvectors}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Subsection EEM (print version p283-285), Subsection CEE and ECEE (print version p289-297)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section EE, p103-108, all except C22, M60. Note that for the questions regarding diagonalizability, use our method instead of the method in the solution manual.

@section{Eigenvalues and Eigenvectors of a Matrix}

@defn
@title{Eigenvalues and Eigenvectors of a Matrix}
@label{EEM}
Suppose that $A$ is a square matrix of size $n$, $\mathbf{x}$ a non-zero vector in ${\mathbb{R}}^{n}$, and $\lambda$ a scalar in ${\mathbb{R}}^{\hbox{}}$. We say $\mathbf{x}$ is an <b>eigenvector</b> of $A$ with <b>eigenvalue</b> $\lambda$ if

\begin{align*}
\displaystyle A\mathbf{x}=\lambda\mathbf{x}.
\end{align*}
@end

@eg
Let

\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;1&amp;1\\
1&amp;2&amp;1\\
1&amp;1&amp;2\end{bmatrix}
\end{align*}

and let

\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\,\,\mathbf{v}=\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\,\,\mathbf{w}=\begin{bmatrix}1\\
0\\
-1\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle A\mathbf{u}=\begin{bmatrix}4\\
4\\
4\end{bmatrix}=4\mathbf{u},\,\,A\mathbf{v}=\begin{bmatrix}1\\
-1\\
0\end{bmatrix}=1\mathbf{v},\,\,A\mathbf{w}=\begin{bmatrix}1\\
0\\
-1\end{bmatrix}=1\mathbf{w}.
\end{align*}

So $\mathbf{u}$ is an eigenvector of $A$ with eigenvalue $4$,
$\mathbf{v}$ is an eigenvector of $A$ with eigenvalue $1$, and
$\mathbf{w}$ is an eigenvector $A$ with eigenvalue $1$. Now let $\mathbf{x}=100\mathbf{u}$. Then

\begin{align*}
\displaystyle A\mathbf{x}=100A\mathbf{u}=400\mathbf{u}=4\mathbf{x}.
\end{align*}

So $\mathbf{x}$ is an eigenvector of $A$ with eigenvalue $4$.Next let $\mathbf{y}=\mathbf{v}+\mathbf{w}$, then

\begin{align*}
\displaystyle A\mathbf{y}=A\mathbf{v}+A\mathbf{w}=\mathbf{v}+\mathbf{w}=1\mathbf{y}.
\end{align*}

So $\mathbf{y}$ is an eigenvector of $A$ with eigenvalue $1$. Finally, let $\mathbf{z}=\mathbf{u}+\mathbf{v}=\begin{bmatrix}2\\
0\\
1\end{bmatrix}$. Then

\begin{align*}
\displaystyle A\mathbf{z}=4\mathbf{u}+\mathbf{v}=\begin{bmatrix}5\\
3\\
4\end{bmatrix}
\end{align*}

is not a scalar multiple of $\mathbf{z}$. So $\mathbf{z}$ is not an eigenvector.
This shows that sum of eigenvectors need not be an eigenvector.
@end

@eg
Consider the matrix

\begin{align*}
\displaystyle A=\begin{bmatrix}204&amp;98&amp;-26&amp;-10\\
-280&amp;-134&amp;36&amp;14\\
716&amp;348&amp;-90&amp;-36\\
-472&amp;-232&amp;60&amp;28\end{bmatrix}
\end{align*}

and the vectors

\begin{align*}
\displaystyle\mathbf{x}=\begin{bmatrix}1\\
-1\\
2\\
5\end{bmatrix}&amp;\displaystyle\mathbf{y}=\begin{bmatrix}-3\\
4\\
-10\\
4\end{bmatrix}&amp;\displaystyle\mathbf{z}=\begin{bmatrix}-3\\
7\\
0\\
8\end{bmatrix}&amp;\displaystyle\mathbf{w}=\begin{bmatrix}1\\
-1\\
4\\
0\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle A\mathbf{x}=\begin{bmatrix}204&amp;98&amp;-26&amp;-10\\
-280&amp;-134&amp;36&amp;14\\
716&amp;348&amp;-90&amp;-36\\
-472&amp;-232&amp;60&amp;28\end{bmatrix}\begin{bmatrix}1\\
-1\\
2\\
5\end{bmatrix}=\begin{bmatrix}4\\
-4\\
8\\
20\end{bmatrix}=4\begin{bmatrix}1\\
-1\\
2\\
5\end{bmatrix}=4\mathbf{x}
\end{align*}

so that $\mathbf{x}$ is an eigenvector of $A$ with eigenvalue $\lambda=4$.

Also,

\begin{align*}
\displaystyle A\mathbf{y}=\begin{bmatrix}204&amp;98&amp;-26&amp;-10\\
-280&amp;-134&amp;36&amp;14\\
716&amp;348&amp;-90&amp;-36\\
-472&amp;-232&amp;60&amp;28\end{bmatrix}\begin{bmatrix}-3\\
4\\
-10\\
4\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\\
0\end{bmatrix}=0\begin{bmatrix}-3\\
4\\
-10\\
4\end{bmatrix}=0\mathbf{y}
\end{align*}

so that $\mathbf{y}$ is an eigenvector of $A$ with eigenvalue $\lambda=0$.

Also,

\begin{align*}
\displaystyle A\mathbf{z}=\begin{bmatrix}204&amp;98&amp;-26&amp;-10\\
-280&amp;-134&amp;36&amp;14\\
716&amp;348&amp;-90&amp;-36\\
-472&amp;-232&amp;60&amp;28\end{bmatrix}\begin{bmatrix}-3\\
7\\
0\\
8\end{bmatrix}=\begin{bmatrix}-6\\
14\\
0\\
16\end{bmatrix}=2\begin{bmatrix}-3\\
7\\
0\\
8\end{bmatrix}=2\mathbf{z}
\end{align*}

so that $\mathbf{z}$ is an eigenvector of $A$ with eigenvalue $\lambda=2$.

Finally,

\begin{align*}
\displaystyle A\mathbf{w}=\begin{bmatrix}204&amp;98&amp;-26&amp;-10\\
-280&amp;-134&amp;36&amp;14\\
716&amp;348&amp;-90&amp;-36\\
-472&amp;-232&amp;60&amp;28\end{bmatrix}\begin{bmatrix}1\\
-1\\
4\\
0\end{bmatrix}=\begin{bmatrix}2\\
-2\\
8\\
0\end{bmatrix}=2\begin{bmatrix}1\\
-1\\
4\\
0\end{bmatrix}=2\mathbf{w}
\end{align*}

so that $\mathbf{w}$ is an eigenvector of $A$ with eigenvalue $\lambda=2$.

We have demonstrated four eigenvectors of $A$. Are there more? Yes, any nonzero scalar multiple of an eigenvector is again an eigenvector. In this example, setting $\mathbf{u}=30\mathbf{x}$, we have

\begin{align*}
\displaystyle A\mathbf{u}=A(30\mathbf{x})=30A\mathbf{x}=30(4\mathbf{x})=4(30\mathbf{x})=4\mathbf{u}
\end{align*}

so that $\mathbf{u}$ is also an eigenvector of $A$ with the same eigenvalue, $\lambda=4$.

The vectors $\mathbf{z}$ and $\mathbf{w}$ are both eigenvectors of $A$ for the same eigenvalue $\lambda=2$, yet this is not as simple as the two vectors just being scalar multiples of each other (they are not). Look what happens when we add them together, forming $\mathbf{v}=\mathbf{z}+\mathbf{w}$, which we then multiply by $A$:

\begin{align*}
\displaystyle A\mathbf{v}=A(\mathbf{z}+\mathbf{w})=A\mathbf{z}+A\mathbf{w}
\end{align*}

\begin{align*}
\displaystyle =2\mathbf{z}+2\mathbf{w}=2(\mathbf{z}+\mathbf{w})=2\mathbf{v}.
\end{align*}

Hence, $\mathbf{v}$ is also an eigenvector of $A$ with eigenvalue $\lambda=2$. It would appear that the set of eigenvectors that are associated with a fixed eigenvalue is closed under the vector space operations of ${\mathbb{R}}^{n}$.

The vector $\mathbf{y}$ is an eigenvector of $A$ for the eigenvalue $\lambda=0$,
so $A\mathbf{y}=0\mathbf{y}=\mathbf{0}$. But this also means that $\mathbf{y}\in{\mathcal{N}}\!\left(A\right)$. There would appear to be a connection here also.
@end

@section{Existence of Eigenvalues and Eigenvectors}

Observe that

\begin{align*}
\displaystyle A\mathbf{x}=\lambda\mathbf{x}\iff A\mathbf{x}-\lambda I_{n}\mathbf{x}=\mathbf{0}\iff\left(A-\lambda I_{n}\right)\mathbf{x}=\mathbf{0}.
\end{align*}

So, any eigenvector $x$ with eigenvalue $\lambda$ is a nonzero element of the null space of $A-\lambda I_{n}$. In particular, the matrix $A-\lambda I_{n}$ is necessarily singular, therefore having zero determinant. These ideas motivate the following definition and example.

@defn
@title{Characteristic Polynomial}
Suppose that $A$ is a square matrix of size $n$. Then the <b>characteristic polynomial</b> of $A$ is the polynomial $p_{A}\left(x\right)$ defined by

\begin{align*}
\displaystyle p_{A}\left(x\right)=\det\left(A-xI_{n}\right)
\end{align*}
@end
@eg
Consider

\begin{align*}
\displaystyle F=\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}.
\end{align*}

We compute

\begin{align*}
\displaystyle p_{F}\left(x\right)&amp;\displaystyle=\det\left(F-xI_{3}\right) \\
&amp;\displaystyle=\begin{vmatrix}-13-x&amp;-8&amp;-4\\
12&amp;7-x&amp;4\\
24&amp;16&amp;7-x\end{vmatrix} \\
&amp;\displaystyle=(-13-x)\begin{vmatrix}7-x&amp;4\\
16&amp;7-x\end{vmatrix}+(-8)(-1)\begin{vmatrix}12&amp;4\\
24&amp;7-x\end{vmatrix} \\
&amp;\displaystyle\quad\quad+(-4)\begin{vmatrix}12&amp;7-x\\
24&amp;16\end{vmatrix} \\
&amp;\displaystyle=(-13-x)((7-x)(7-x)-4(16)) \\
&amp;\displaystyle\quad\quad+(-8)(-1)(12(7-x)-4(24)) \\
&amp;\displaystyle\quad\quad+(-4)(12(16)-(7-x)(24)) \\
&amp;\displaystyle=3+5x+x^{2}-x^{3} \\
&amp;\displaystyle=-(x-3)(x+1)^{2}.
\end{align*}
@end

The characteristic polynomial is our main computational tool for finding eigenvalues, and will sometimes be used to aid us in determining the properties of eigenvalues.

@thm
@title{Eigenvalues of a Matrix are Roots of Characteristic Polynomials}
Suppose that $A$ is a square matrix.
Then $\lambda$ is an eigenvalue of $A$ if and only if $p_{A}\left(\lambda\right)=0$.
@end
@proof
@col
We have the following equivalences:

$\lambda$ is an eigenvalue of $A$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $A\mathbf{x}=\lambda\mathbf{x}$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $A\mathbf{x}-\lambda\mathbf{x}=\mathbf{0}$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $A\mathbf{x}-\lambda I_{n}\mathbf{x}=\mathbf{0}$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $(A-\lambda I_{n})\mathbf{x}=\mathbf{0}$ $\iff$ $A-\lambda I_{n}$ is singular $\iff$ $p_{A}\left(\lambda\right)=\det\left(A-\lambda I_{n}\right)=0$.
∎
@end
@thm
@title{Degree of the Characteristic Polynomial}
@label{DCP}
Suppose that $A$ is a square matrix of size $n$. Then the characteristic polynomial $p_{A}\left(x\right)$ has degree $n$.
@end
@proof
@col
You can skip the proof. The following briefly explains why the theorem is true. It is not a rigorous proof.We have

\begin{align*}
\displaystyle p_{A}\left(x\right)=\begin{vmatrix}a_{11}-x&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}-x&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}-x\end{vmatrix}.
\end{align*}

The determinant is a sum of products of entries of $A-xI_{n}$, and all such products have degree at most $n-1$, except the product of the diagonal entries,

\begin{align*}
\displaystyle (a_{11}-x)(a_{22}-x)\cdots(a_{nn}-x),
\end{align*}

which has degree $n$. <b>Remark</b>: We can also see that the leading coefficient is $(-1)^{n}$.
∎
@end
@eg
In Example 3, we found the characteristic polynomial of

\begin{align*}
\displaystyle F=\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}
\end{align*}

to be $p_{F}\left(x\right)=-(x-3)(x+1)^{2}$. Being written in factored form, we can simply read off its roots; they are $x=3$ and $x=-1$. By the previous theorem, $\lambda=3$ and $\lambda=-1$ are both eigenvalues of $F$. Moreover, these are the only eigenvalues of $F$.
@end
@defn
@title{Eigenspace of a Matrix}
Suppose that $A$ is a square matrix and $\lambda$ is an eigenvalue of $A$.
Then the <b>eigenspace</b> of $A$ for $\lambda$, denoted by ${\mathcal{E}}_{A}\left(\lambda\right)$, is the set of all eigenvectors of $A$ with eigenvalue $\lambda$, together with the zero vector.
@end
@thm
@title{Eigenspace of a Matrix is a Null Space}
Suppose that $A$ is a square matrix of size $n$ and $\lambda$ is an eigenvalue of $A$. Then

\begin{align*}
\displaystyle {\mathcal{E}}_{A}\left(\lambda\right)={\mathcal{N}}\!\left(A-\lambda I_{n}\right).
\end{align*}
@end
@proof
@col
First, notice that $\mathbf{0}\in{\mathcal{E}}_{A}\left(\lambda\right)$ (by definition) and $\mathbf{0}\in{\mathcal{N}}\!\left(A-\lambda I_{n}\right)$.
Now consider any nonzero vector $\mathbf{x}\in{\mathbb{R}}^{n}$,$\mathbf{x}\in{\mathcal{E}}_{A}\left(\lambda\right)\iff A\mathbf{x}=\lambda\mathbf{x}$$\iff A\mathbf{x}-\lambda\mathbf{x}=\mathbf{0}$ $\iff A\mathbf{x}-\lambda I_{n}\mathbf{x}=\mathbf{0}$$\iff\left(A-\lambda I_{n}\right)\mathbf{x}=\mathbf{0}$$\iff\mathbf{x}\in{\mathcal{N}}\!\left(A-\lambda I_{n}\right)$.

This completes the proof.
∎
@end
@thm
@title{Eigenspace for a Matrix is a Subspace}
Suppose that $A$ is a square matrix of size $n$ and $\lambda$ is an eigenvalue of $A$.
Then the eigenspace ${\mathcal{E}}_{A}\left(\lambda\right)$ is a subspace of the vector space ${\mathbb{R}}^{n}$.
@end
@proof
@col
This follows from the equality ${\mathcal{E}}_{A}\left(\lambda\right)={\mathcal{N}}\!\left(A-\lambda I_{n}\right)$ and the fact (Lecture 16 Theorem 4) that ${\mathcal{N}}\!\left(A-\lambda I_{n}\right)$ is a subspace.
∎
@end
@eg
Example 3 and Example 4 describe the characteristic polynomial and eigenvalues of the $3\times 3$ matrix

\begin{align*}
\displaystyle F=\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}.
\end{align*}

We will now take each eigenvalue in turn and compute its eigenspace. To do this, we row-reduce the matrix
$F-\lambda I_{3}$ in order to find all solutions to the homogeneous system $F-\lambda I_{3}\mathbf{x}=\mathbf{0}$. We then express the eigenspace ${\mathcal{E}}_{F}\left(\lambda\right)$ as the nullspace of $F-\lambda I_{3}$. Then we can write the nullspace as the span of a basis.

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=3&amp;\displaystyle F-3I_{3}&amp;\displaystyle=\begin{bmatrix}-16&amp;-8&amp;-4\\
12&amp;4&amp;4\\
24&amp;16&amp;4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;\frac{1}{2}\\
0&amp;\boxed{1}&amp;-\frac{1}{2}\\
0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{F}\left(3\right)&amp;\displaystyle={\mathcal{N}}\!\left(F-3I_{3}\right)=\left&lt;\{\begin{bmatrix}-\frac{1}{2}\\
\frac{1}{2}\\
1\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}-1\\
1\\
2\end{bmatrix}\}\right&gt; \\
\displaystyle\lambda&amp;\displaystyle=-1&amp;\displaystyle F+1I_{3}&amp;\displaystyle=\begin{bmatrix}-12&amp;-8&amp;-4\\
12&amp;8&amp;4\\
24&amp;16&amp;8\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;\frac{2}{3}&amp;\frac{1}{3}\\
0&amp;0&amp;0\\
0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{F}\left(-1\right)&amp;\displaystyle={\mathcal{N}}\!\left(F+1I_{3}\right)=\left&lt;\{\begin{bmatrix}-\frac{2}{3}\\
1\\
0\end{bmatrix},\,\begin{bmatrix}-\frac{1}{3}\\
0\\
1\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}-2\\
3\\
0\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
3\end{bmatrix}\}\right&gt;
\end{align*}

Eigenspaces in hand, we can easily compute eigenvectors by forming nontrivial linear combinations of the basis vectors describing each eigenspace. In particular, notice that we can pretty up our basis vectors by using scalar multiples to clear out fractions.
@end

@section{Examples of Computing Eigenvalues and Eigenvectors}

@eg
@label{B}
Consider the matrix

\begin{align*}
\displaystyle B=\begin{bmatrix}-2&amp;1&amp;-2&amp;-4\\
12&amp;1&amp;4&amp;9\\
6&amp;5&amp;-2&amp;-4\\
3&amp;-4&amp;5&amp;10\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle p_{B}\left(x\right)=8-20x+18x^{2}-7x^{3}+x^{4}=(x-1)(x-2)^{3}.
\end{align*}

So the eigenvalues are $\lambda=1,\,2$.
Computing eigenvectors, we find

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=1&amp;\displaystyle B-1I_{4}&amp;\displaystyle=\begin{bmatrix}-3&amp;1&amp;-2&amp;-4\\
12&amp;0&amp;4&amp;9\\
6&amp;5&amp;-3&amp;-4\\
3&amp;-4&amp;5&amp;9\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;\frac{1}{3}&amp;0\\
0&amp;\boxed{1}&amp;-1&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}\\
0&amp;0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{B}\left(1\right)&amp;\displaystyle={\mathcal{N}}\!\left(B-1I_{4}\right)=\left&lt;\{\begin{bmatrix}-\frac{1}{3}\\
1\\
1\\
0\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}-1\\
3\\
3\\
0\end{bmatrix}\}\right&gt;
\end{align*}

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=2&amp;\displaystyle B-2I_{4}&amp;\displaystyle=\begin{bmatrix}-4&amp;1&amp;-2&amp;-4\\
12&amp;-1&amp;4&amp;9\\
6&amp;5&amp;-4&amp;-4\\
3&amp;-4&amp;5&amp;8\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;1/2\\
0&amp;\boxed{1}&amp;0&amp;-1\\
0&amp;0&amp;\boxed{1}&amp;1/2\\
0&amp;0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{B}\left(2\right)&amp;\displaystyle={\mathcal{N}}\!\left(B-2I_{4}\right)=\left&lt;\{\begin{bmatrix}-\frac{1}{2}\\
1\\
-\frac{1}{2}\\
1\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}-1\\
2\\
-1\\
2\end{bmatrix}\}\right&gt;
\end{align*}
@end

@eg
@label{C}
Consider the matrix

\begin{align*}
\displaystyle C=\begin{bmatrix}1&amp;0&amp;1&amp;1\\
0&amp;1&amp;1&amp;1\\
1&amp;1&amp;1&amp;0\\
1&amp;1&amp;0&amp;1\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle p_{C}\left(x\right)=-3+4x+2x^{2}-4x^{3}+x^{4}=(x-3)(x-1)^{2}(x+1).
\end{align*}

So the eigenvalues are $\lambda=3,\,1,\,-1$.
Computing eigenvectors, we find

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=3&amp;\displaystyle C-3I_{4}&amp;\displaystyle=\begin{bmatrix}-2&amp;0&amp;1&amp;1\\
0&amp;-2&amp;1&amp;1\\
1&amp;1&amp;-2&amp;0\\
1&amp;1&amp;0&amp;-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-1\\
0&amp;\boxed{1}&amp;0&amp;-1\\
0&amp;0&amp;\boxed{1}&amp;-1\\
0&amp;0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{C}\left(3\right)&amp;\displaystyle={\mathcal{N}}\!\left(C-3I_{4}\right)=\left&lt;\{\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix}\}\right&gt; \\
\displaystyle\lambda&amp;\displaystyle=1&amp;\displaystyle C-1I_{4}&amp;\displaystyle=\begin{bmatrix}0&amp;0&amp;1&amp;1\\
0&amp;0&amp;1&amp;1\\
1&amp;1&amp;0&amp;0\\
1&amp;1&amp;0&amp;0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;1&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;1\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{C}\left(1\right)&amp;\displaystyle={\mathcal{N}}\!\left(C-1I_{4}\right)=\left&lt;\{\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\,\begin{bmatrix}0\\
0\\
-1\\
1\end{bmatrix}\}\right&gt; \\
\displaystyle\lambda&amp;\displaystyle=-1&amp;\displaystyle C+1I_{4}&amp;\displaystyle=\begin{bmatrix}2&amp;0&amp;1&amp;1\\
0&amp;2&amp;1&amp;1\\
1&amp;1&amp;2&amp;0\\
1&amp;1&amp;0&amp;2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;1\\
0&amp;\boxed{1}&amp;0&amp;1\\
0&amp;0&amp;\boxed{1}&amp;-1\\
0&amp;0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{C}\left(-1\right)&amp;\displaystyle={\mathcal{N}}\!\left(C+1I_{4}\right)=\left&lt;\{\begin{bmatrix}-1\\
-1\\
1\\
1\end{bmatrix}\}\right&gt;
\end{align*}
@end

@eg
@label{E}
Consider the matrix

\begin{align*}
\displaystyle E=\begin{bmatrix}29&amp;14&amp;2&amp;6&amp;-9\\
-47&amp;-22&amp;-1&amp;-11&amp;13\\
19&amp;10&amp;5&amp;4&amp;-8\\
-19&amp;-10&amp;-3&amp;-2&amp;8\\
7&amp;4&amp;3&amp;1&amp;-3\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle p_{E}\left(x\right)=-16+16x+8x^{2}-16x^{3}+7x^{4}-x^{5}=-(x-2)^{4}(x+1).
\end{align*}

So the eigenvalues are $\lambda=2,\,-1$. Computing eigenvectors, we find

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=2 \\
\displaystyle E-2I_{5}&amp;\displaystyle=\begin{bmatrix}27&amp;14&amp;2&amp;6&amp;-9\\
-47&amp;-24&amp;-1&amp;-11&amp;13\\
19&amp;10&amp;3&amp;4&amp;-8\\
-19&amp;-10&amp;-3&amp;-4&amp;8\\
7&amp;4&amp;3&amp;1&amp;-5\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;1&amp;0\\
0&amp;\boxed{1}&amp;0&amp;-\frac{3}{2}&amp;-\frac{1}{2}\\
0&amp;0&amp;\boxed{1}&amp;0&amp;-1\\
0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{E}\left(2\right)&amp;\displaystyle={\mathcal{N}}\!\left(E-2I_{5}\right)=\left&lt;\{\begin{bmatrix}-1\\
\frac{3}{2}\\
0\\
1\\
0\end{bmatrix},\,\begin{bmatrix}0\\
\frac{1}{2}\\
1\\
0\\
1\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}-2\\
3\\
0\\
2\\
0\end{bmatrix},\,\begin{bmatrix}0\\
1\\
2\\
0\\
2\end{bmatrix}\}\right&gt; \\
\displaystyle\lambda&amp;\displaystyle=-1 \\
\displaystyle E+1I_{5}&amp;\displaystyle=\begin{bmatrix}30&amp;14&amp;2&amp;6&amp;-9\\
-47&amp;-21&amp;-1&amp;-11&amp;13\\
19&amp;10&amp;6&amp;4&amp;-8\\
-19&amp;-10&amp;-3&amp;-1&amp;8\\
7&amp;4&amp;3&amp;1&amp;-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;2&amp;0\\
0&amp;\boxed{1}&amp;0&amp;-4&amp;0\\
0&amp;0&amp;\boxed{1}&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{E}\left(-1\right)&amp;\displaystyle={\mathcal{N}}\!\left(E+1I_{5}\right)=\left&lt;\{\begin{bmatrix}-2\\
4\\
-1\\
1\\
0\end{bmatrix}\}\right&gt;
\end{align*}
@end

@eg
@label{H}
Consider the matrix

\begin{align*}
\displaystyle H=\begin{bmatrix}15&amp;18&amp;-8&amp;6&amp;-5\\
5&amp;3&amp;1&amp;-1&amp;-3\\
0&amp;-4&amp;5&amp;-4&amp;-2\\
-43&amp;-46&amp;17&amp;-14&amp;15\\
26&amp;30&amp;-12&amp;8&amp;-10\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle p_{H}\left(x\right)=-6x+x^{2}+7x^{3}-x^{4}-x^{5}=x(x-2)(x-1)(x+1)(x+3).
\end{align*}

So the eigenvalues are $\lambda=2,\,1,\,0,\,-1,\,-3$.

Computing eigenvectors, we find

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=2 \\
&amp;\displaystyle H-2I_{5}=\begin{bmatrix}13&amp;18&amp;-8&amp;6&amp;-5\\
5&amp;1&amp;1&amp;-1&amp;-3\\
0&amp;-4&amp;3&amp;-4&amp;-2\\
-43&amp;-46&amp;17&amp;-16&amp;15\\
26&amp;30&amp;-12&amp;8&amp;-12\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;-1\\
0&amp;\boxed{1}&amp;0&amp;0&amp;1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;1\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix} \\
&amp;\displaystyle{\mathcal{E}}_{H}\left(2\right)={\mathcal{N}}\!\left(H-2I_{5}\right)=\left&lt;\{\begin{bmatrix}1\\
-1\\
-2\\
-1\\
1\end{bmatrix}\}\right&gt;
\end{align*}

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=1 \\
&amp;\displaystyle H-1I_{5}=\begin{bmatrix}14&amp;18&amp;-8&amp;6&amp;-5\\
5&amp;2&amp;1&amp;-1&amp;-3\\
0&amp;-4&amp;4&amp;-4&amp;-2\\
-43&amp;-46&amp;17&amp;-15&amp;15\\
26&amp;30&amp;-12&amp;8&amp;-11\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;-\frac{1}{2}\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0&amp;\frac{1}{2}\\
0&amp;0&amp;0&amp;\boxed{1}&amp;1\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix} \\
&amp;\displaystyle{\mathcal{E}}_{H}\left(1\right)={\mathcal{N}}\!\left(H-1I_{5}\right)=\left&lt;\{\begin{bmatrix}\frac{1}{2}\\
0\\
-\frac{1}{2}\\
-1\\
1\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}1\\
0\\
-1\\
-2\\
2\end{bmatrix}\}\right&gt;
\end{align*}

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=0 \\
&amp;\displaystyle H-0I_{5}=\begin{bmatrix}15&amp;18&amp;-8&amp;6&amp;-5\\
5&amp;3&amp;1&amp;-1&amp;-3\\
0&amp;-4&amp;5&amp;-4&amp;-2\\
-43&amp;-46&amp;17&amp;-14&amp;15\\
26&amp;30&amp;-12&amp;8&amp;-10\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;1\\
0&amp;\boxed{1}&amp;0&amp;0&amp;-2\\
0&amp;0&amp;\boxed{1}&amp;0&amp;-2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix} \\
&amp;\displaystyle{\mathcal{E}}_{H}\left(0\right)={\mathcal{N}}\!\left(H-0I_{5}\right)=\left&lt;\{\begin{bmatrix}-1\\
2\\
2\\
0\\
1\end{bmatrix}\}\right&gt;
\end{align*}

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=-1 \\
&amp;\displaystyle H+1I_{5}=\begin{bmatrix}16&amp;18&amp;-8&amp;6&amp;-5\\
5&amp;4&amp;1&amp;-1&amp;-3\\
0&amp;-4&amp;6&amp;-4&amp;-2\\
-43&amp;-46&amp;17&amp;-13&amp;15\\
26&amp;30&amp;-12&amp;8&amp;-9\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;-1/2\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;1/2\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix} \\
&amp;\displaystyle{\mathcal{E}}_{H}\left(-1\right)={\mathcal{N}}\!\left(H+1I_{5}\right)=\left&lt;\{\begin{bmatrix}\frac{1}{2}\\
0\\
0\\
-\frac{1}{2}\\
1\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}1\\
0\\
0\\
-1\\
2\end{bmatrix}\}\right&gt;
\end{align*}

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=-3 \\
&amp;\displaystyle H+3I_{5}=\begin{bmatrix}18&amp;18&amp;-8&amp;6&amp;-5\\
5&amp;6&amp;1&amp;-1&amp;-3\\
0&amp;-4&amp;8&amp;-4&amp;-2\\
-43&amp;-46&amp;17&amp;-11&amp;15\\
26&amp;30&amp;-12&amp;8&amp;-7\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;-1\\
0&amp;\boxed{1}&amp;0&amp;0&amp;\frac{1}{2}\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix} \\
&amp;\displaystyle{\mathcal{E}}_{H}\left(-3\right)={\mathcal{N}}\!\left(H+3I_{5}\right)=\left&lt;\{\begin{bmatrix}1\\
-\frac{1}{2}\\
-1\\
-2\\
1\end{bmatrix}\}\right&gt;=\left&lt;\{\begin{bmatrix}-2\\
1\\
2\\
4\\
-2\end{bmatrix}\}\right&gt;
\end{align*}
@end

@section{Similar Matrices}

@defn
@title{Similar Matrices}
@label{SIM}
Suppose that $A$ and $B$ are square matrices of size $n$. Then $A$ and $B$ are <b>similar</b> if there exists a nonsingular matrix of size $n$, $S$, such that $A=S^{-1}BS$.
We will also say $A$ is similar to $B$ via $S$.
Finally, we will refer to $S^{-1}BS$ as a <b>similarity transformation</b> when we want to emphasize the way that $S$ changes $B$.
@end

@eg
Define

\begin{align*}
\displaystyle B=\begin{bmatrix}-5&amp;-7\\
4&amp;6\end{bmatrix}&amp;\displaystyle S=\begin{bmatrix}1&amp;2\\
2&amp;3\end{bmatrix}.
\end{align*}

Check that $S$ is nonsingular and then compute

\begin{align*}
\displaystyle A=S^{-1}BS \\
\displaystyle=\begin{bmatrix}-3&amp;2\\
2&amp;-1\\
\end{bmatrix}\begin{bmatrix}-5&amp;-7\\
4&amp;6\end{bmatrix}\begin{bmatrix}1&amp;2\\
2&amp;3\end{bmatrix} \\
\displaystyle=\begin{bmatrix}89&amp;145\\
-54&amp;-88\\
\end{bmatrix}.
\end{align*}

It follows that $A$ and $B$ are similar.
@end

@eg
Define

\begin{align*}
\displaystyle B=\begin{bmatrix}-4&amp;1&amp;-3&amp;-2&amp;2\\
1&amp;2&amp;-1&amp;3&amp;-2\\
-4&amp;1&amp;3&amp;2&amp;2\\
-3&amp;4&amp;-2&amp;-1&amp;-3\\
3&amp;1&amp;-1&amp;1&amp;-4\end{bmatrix}&amp;\displaystyle S=\begin{bmatrix}1&amp;2&amp;-1&amp;1&amp;1\\
0&amp;1&amp;-1&amp;-2&amp;-1\\
1&amp;3&amp;-1&amp;1&amp;1\\
-2&amp;-3&amp;3&amp;1&amp;-2\\
1&amp;3&amp;-1&amp;2&amp;1\\
\end{bmatrix}.
\end{align*}

Check that $S$ is nonsingular and then compute

\begin{align*}
\displaystyle A=S^{-1}BS \\
\displaystyle=\begin{bmatrix}10&amp;1&amp;0&amp;2&amp;-5\\
-1&amp;0&amp;1&amp;0&amp;0\\
3&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;-1&amp;0&amp;1\\
-4&amp;-1&amp;1&amp;-1&amp;1\end{bmatrix}\begin{bmatrix}-4&amp;1&amp;-3&amp;-2&amp;2\\
1&amp;2&amp;-1&amp;3&amp;-2\\
-4&amp;1&amp;3&amp;2&amp;2\\
-3&amp;4&amp;-2&amp;-1&amp;-3\\
3&amp;1&amp;-1&amp;1&amp;-4\end{bmatrix}\begin{bmatrix}1&amp;2&amp;-1&amp;1&amp;1\\
0&amp;1&amp;-1&amp;-2&amp;-1\\
1&amp;3&amp;-1&amp;1&amp;1\\
-2&amp;-3&amp;3&amp;1&amp;-2\\
1&amp;3&amp;-1&amp;2&amp;1\end{bmatrix} \\
\displaystyle=\begin{bmatrix}-10&amp;-27&amp;-29&amp;-80&amp;-25\\
-2&amp;6&amp;6&amp;10&amp;-2\\
-3&amp;11&amp;-9&amp;-14&amp;-9\\
-1&amp;-13&amp;0&amp;-10&amp;-1\\
11&amp;35&amp;6&amp;49&amp;19\end{bmatrix}.
\end{align*}

This shows that $A$ and $B$ are similar.
@end

@eg
Define

\begin{align*}
\displaystyle B=\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}&amp;\displaystyle S=\begin{bmatrix}1&amp;1&amp;2\\
-2&amp;-1&amp;-3\\
1&amp;-2&amp;0\end{bmatrix}.
\end{align*}

Check that $S$ is nonsingular and then compute

\begin{align*}
\displaystyle A&amp;\displaystyle=S^{-1}BS \\
&amp;\displaystyle=\begin{bmatrix}-6&amp;-4&amp;-1\\
-3&amp;-2&amp;-1\\
5&amp;3&amp;1\end{bmatrix}\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}\begin{bmatrix}1&amp;1&amp;2\\
-2&amp;-1&amp;-3\\
1&amp;-2&amp;0\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}-1&amp;0&amp;0\\
0&amp;3&amp;0\\
0&amp;0&amp;-1\end{bmatrix}.
\end{align*}
@end

@thm
@title{Similarity is an Equivalence Relation}
@label{SER}
Suppose that $A$, $B$ and $C$ are square matrices of size $n$. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is similar to $A$. (Reflexive)
</li>
<li class="ltx_item">
If $A$ is similar to $B$, then $B$ is similar to $A$. (Symmetric)
</li>
<li class="ltx_item">
If $A$ is similar to $B$ and $B$ is similar to $C$, then $A$ is similar to $C$. (Transitive)
</li>

</ol>
@end

@proof
@col
To see that $A$ is similar to $A$, we need only demonstrate a nonsingular matrix that effects a similarity transformation of $A$ to $A$. We can take $I_{n}$, which is nonsingular and satisfies $I_{n}^{-1}AI_{n}=I_{n}AI_{n}=A$.

If we assume that $A$ is similar to $B$, then we know there exists is a nonsingular matrix $S$ so that $A=S^{-1}BS$.
But then $S^{-1}$ is invertible and therefore nonsingular. So

\begin{align*}
\displaystyle (S^{-1})^{-1}A(S^{-1})=SAS^{-1}=SS^{-1}BSS^{-1}
\end{align*}

\begin{align*}
\displaystyle =\left(SS^{-1}\right)B\left(SS^{-1}\right)=I_{n}BI_{n}=B
\end{align*}

and we see that $B$ is similar to $A$.

Assume that $A$ is similar to $B$ and that $B$ is similar to $C$. This gives us the existence of nonsingular matrices, $S$ and $R$, such that $A=S^{-1}BS$ and $B=R^{-1}CR$. Since $S$ and $R$ are invertible, so too is $RS$, which has inverse $S^{-1}R^{-1}$. Then we compute

\begin{align*}
\displaystyle (RS)^{-1}C(RS)=S^{-1}R^{-1}CRS=S^{-1}\left(R^{-1}CR\right)S
\end{align*}

\begin{align*}
\displaystyle =S^{-1}BS=A
\end{align*}

so $A$ is similar to $C$ via the nonsingular matrix $RS$.
∎
@end

@thm
@title{Similar Matrices have Equal Eigenvalues}
@label{SMEE}
Suppose that $A$ and $B$ are similar matrices. Then the characteristic polynomials of $A$ and $B$ are equal, that is, $p_{A}\left(x\right)=p_{B}\left(x\right)$.
@end

@proof
@col
Let $n$ denote the size of $A$ and $B$. Since $A$ and $B$ are similar, there exists a nonsingular matrix $S$, such that $A=S^{-1}BS$.
Then

\begin{align*}
\displaystyle p_{A}\left(x\right)=\det\left(A-xI_{n}\right)=\det\left(S^{-1}BS-xI_{n}\right)=\det\left(S^{-1}BS-xS^{-1}I_{n}S\right)
\end{align*}

\begin{align*}
\displaystyle =\det\left(S^{-1}BS-S^{-1}xI_{n}S\right)=\det\left(S^{-1}\left(B-xI_{n}\right)S\right)=\det\left(S^{-1}\right)\det\left(B-xI_{n}\right)\det\left(S\right)
\end{align*}

\begin{align*}
\displaystyle =\det\left(S^{-1}\right)\det\left(S\right)\det\left(B-xI_{n}\right)=\det\left(S^{-1}S\right)\det\left(B-xI_{n}\right)=\det\left(I_{n}\right)\det\left(B-xI_{n}\right)
\end{align*}

\begin{align*}
\displaystyle =1\det\left(B-xI_{n}\right)=p_{B}\left(x\right).
\end{align*}

∎
@end

@eg
We claim that the matrices

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2\\
3&amp;4\end{bmatrix},\,\,B=\begin{bmatrix}1&amp;2\\
0&amp;4\end{bmatrix}
\end{align*}

are not similar. To show this, we compute

\begin{align*}
\displaystyle p_{A}\left(x\right)=\begin{vmatrix}1-x&amp;2\\
3&amp;4-x\end{vmatrix}=(1-x)(4-x)-6=x^{2}-5x-2
\end{align*}

and

\begin{align*}
\displaystyle p_{B}\left(x\right)=\begin{vmatrix}1-x&amp;2\\
0&amp;4-x\end{vmatrix}=(1-x)(4-x)=x^{2}-5x+4.
\end{align*}

Because $p_{A}\left(x\right)\neq p_{B}\left(x\right)$, we conclude that $A$ and $B$ are not similar.
@end

@eg
<b>Same characteristic polynomial, but not similar</b>Define

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;1\\
0&amp;1\end{bmatrix}\qquad B=\begin{bmatrix}1&amp;0\\
0&amp;1\end{bmatrix}.
\end{align*}

We have

\begin{align*}
\displaystyle p_{A}\left(x\right)=p_{B}\left(x\right)=1-2x+x^{2}=(x-1)^{2},
\end{align*}

so that $A$ and $B$ have equal characteristic polynomials. If the converse of the above theorem were true, then $A$ and $B$ would be similar. Suppose this is the case. More precisely, suppose there exists is a nonsingular matrix $S$ so that $A=S^{-1}BS$. Then

\begin{align*}
\displaystyle A=S^{-1}BS=S^{-1}I_{2}S=S^{-1}S=I_{2}
\end{align*}

Clearly $A\neq I_{2}$. This contradiction tells us that the converse of the above theorem is false.
@end

@section{Diagonalizability}

Good things happen when a matrix is similar to a diagonal matrix. For example, the eigenvalues of the matrix are the entries on the diagonal of the diagonal matrix. It is also much simpler matter to compute high powers of the matrix. Diagonalizable matrices are also of interest in more abstract settings. Here are the relevant definitions, then our main theorem for this section.

@defn
@title{Diagonal Matrix}
@label{DIM}
Suppose that $A$ is a square matrix ofh size $n$. Then $A$ is a <b>diagonal matrix</b> if $\left[A\right]_{ij}=0$ whenever $i\neq j$, i.e.

\begin{align*}
\displaystyle A=\begin{bmatrix}\lambda_{1}&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;\lambda_{2}&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\lambda_{3}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;0\\
0&amp;0&amp;0&amp;\cdots&amp;\lambda_{n}\end{bmatrix}.
\end{align*}

We will often denote such a matrix $A$ by $\operatorname{diag}(\lambda_{1},\lambda_{2},\ldots,\lambda_{n})$.
@end
@defn
@title{Diagonalizable Matrix}
@label{DZM}
Suppose that $A$ is a square matrix. Then $A$ is <b>diagonalizable</b> if $A$ is similar to a diagonal matrix, i.e, there exists an invertible matrix
$S$ and real numbers $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ such that

\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}).
\end{align*}
@end
@eg
Let

\begin{align*}
\displaystyle B=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}.
\end{align*}

This matrix is similar to a diagonal matrix, as can be seen by the following computation with the nonsingular matrix $S$:

\begin{align*}
\displaystyle S^{-1}BS=\begin{bmatrix}-5&amp;-3&amp;-2\\
3&amp;2&amp;1\\
1&amp;1&amp;1\end{bmatrix}^{-1}\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}\begin{bmatrix}-5&amp;-3&amp;-2\\
3&amp;2&amp;1\\
1&amp;1&amp;1\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle =\begin{bmatrix}-1&amp;-1&amp;-1\\
2&amp;3&amp;1\\
-1&amp;-2&amp;1\end{bmatrix}\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}\begin{bmatrix}-5&amp;-3&amp;-2\\
3&amp;2&amp;1\\
1&amp;1&amp;1\end{bmatrix}=\begin{bmatrix}-1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;2\end{bmatrix}.
\end{align*}
@end
@thm
@title{Diagonalization Characterization}
@label{DC}
Suppose that $A$ is a square matrix of size $n$. Then $A$ is diagonalizable if and only if there exists a linearly independent set $T$ that contains $n$ eigenvectors of $A$.
@end
@proof
@col
($\Rightarrow$) Suppose that $A$ is diagonalizable. Then there exists an invertible matrix $S$ and real numbers $\lambda_{1},\ldots,\lambda_{n}$ such that

\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n}).
\end{align*}

Let $\mathbf{S}_{i}$ be column $i$ of $S$.
Let $T$ be the columns of $S$. Because $S$ is invertible (nonsingular), the columns of $S$ are linearly independent.
Also

\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n}).
\end{align*}

So

\begin{align*}
\displaystyle AS=S\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n})
\end{align*}

or equivalently

\begin{align*}
\displaystyle [A\mathbf{S}_{1}|A\mathbf{S}_{2}|\cdots|A\mathbf{S}_{n}]=[\lambda_{1}\mathbf{S}_{1}|\lambda_{2}\mathbf{S}_{2}|\cdots|\lambda_{n}\mathbf{S}_{n}].
\end{align*}

Hence, for $1\leq i\leq n$, we have

\begin{align*}
\displaystyle A\mathbf{S}_{i}=\lambda_{i}\mathbf{S}_{i}.
\end{align*}

Obviously $\mathbf{S}_{i}\neq\mathbf{0}$, because $S$ is nonsingular. So $\mathbf{S}_{i}$ is an eigenvector with eigenvalue $\lambda_{i}$.
Hence $T$ is a linearly independent set consisting of eigenvectors of $A$.

($\Leftarrow$). Suppose that $T=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\}$ is a linearly independent set consisting of eigenvectors of $A$
with eigenvalues $\lambda_{1},\ldots,\lambda_{n}$, i.e. $A\mathbf{v}_{i}=\lambda_{i}\mathbf{v}_{i}$ for $i=1,\ldots,n$.
Let $D=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n})$.
Let

\begin{align*}
\displaystyle S=[\mathbf{v}_{1}|\mathbf{v}_{2}|\cdots|\mathbf{v}_{n}].&amp;
\end{align*}

Because $T$ is linearly independent, $S$ is invertible. Similarly to the above computation, we compute

\begin{align*}
\displaystyle AS=[A\mathbf{v}_{1}|A\mathbf{v}_{2}|\cdots|A\mathbf{v}_{n}]=[\lambda_{1}\mathbf{v}_{1}|\lambda_{2}\mathbf{v}_{2}|\cdots|\lambda_{n}\mathbf{v}_{n}]=SD.
\end{align*}

So

\begin{align*}
\displaystyle S^{-1}AS=S^{-1}SD=D.
\end{align*}

Therefore $A$ is diagonalizable.
∎
@end

<b>Remark</b>: Notice that the proof is constructive. To diagonalize a matrix, we need only locate $n$ linearly independent eigenvectors. Then we can construct a nonsingular matrix $S$, using the eigenvectors as columns, with the property that $S^{-1}AS$ is a diagonal matrix ($D$). The entries on the diagonal of $D$ will be the eigenvalues of the eigenvectors used to create $S$, in the same order as the eigenvectors appear in $S$. We illustrate this by <b>diagonalizing</b> some matrices.

@eg
Consider the matrix

\begin{align*}
\displaystyle F=\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}
\end{align*}

from Example 3, 4, 5. The eigenvalues and eigenspaces of $F$’s are

\begin{align*}
\displaystyle\lambda&amp;\displaystyle=3&amp;\displaystyle{\mathcal{E}}_{F}\left(3\right)&amp;\displaystyle=\left&lt;\{\begin{bmatrix}-\frac{1}{2}\\
\frac{1}{2}\\
1\end{bmatrix}\}\right&gt; \\
\displaystyle\lambda&amp;\displaystyle=-1&amp;\displaystyle{\mathcal{E}}_{F}\left(-1\right)&amp;\displaystyle=\left&lt;\{\begin{bmatrix}-\frac{2}{3}\\
1\\
0\end{bmatrix},\,\begin{bmatrix}-\frac{1}{3}\\
0\\
1\end{bmatrix}\}\right&gt;
\end{align*}

Define the matrix $S$ to be the $3\times 3$ matrix whose columns are the three basis vectors in the eigenspaces for $F$:

\begin{align*}
\displaystyle S=\begin{bmatrix}-\frac{1}{2}&amp;-\frac{2}{3}&amp;-\frac{1}{3}\\
\frac{1}{2}&amp;1&amp;0\\
1&amp;0&amp;1\end{bmatrix}
\end{align*}

Check that $S$ is nonsingular (row-reduces to the identity matrix, or has a nonzero determinant).<b>Remark</b>: After we introduce Theorem  @ref{diagcond}, you don’t need to check that $S$ is nonsingular. See Example 21-24 below).

The three columns of $S$ are a linearly independent set. By Theorem  @ref{DC} we now know that $F$ is diagonalizable. Furthermore, the construction in the proof of Theorem  @ref{DC}
tells us that $S^{-1}FS=\operatorname{diag}(3,-1,-1)$. Let us check this directly:

\begin{align*}
\displaystyle S^{-1}FS&amp;\displaystyle=\begin{bmatrix}-\frac{1}{2}&amp;-\frac{2}{3}&amp;-\frac{1}{3}\\
\frac{1}{2}&amp;1&amp;0\\
1&amp;0&amp;1\end{bmatrix}^{-1}\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}\begin{bmatrix}-\frac{1}{2}&amp;-\frac{2}{3}&amp;-\frac{1}{3}\\
\frac{1}{2}&amp;1&amp;0\\
1&amp;0&amp;1\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}6&amp;4&amp;2\\
-3&amp;-1&amp;-1\\
-6&amp;-4&amp;-1\end{bmatrix}\begin{bmatrix}-13&amp;-8&amp;-4\\
12&amp;7&amp;4\\
24&amp;16&amp;7\end{bmatrix}\begin{bmatrix}-\frac{1}{2}&amp;-\frac{2}{3}&amp;-\frac{1}{3}\\
\frac{1}{2}&amp;1&amp;0\\
1&amp;0&amp;1\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}3&amp;0&amp;0\\
0&amp;-1&amp;0\\
0&amp;0&amp;-1\end{bmatrix}.
\end{align*}
@end
@thm
@label{diagcond}
Suppose that $A$ is a square matrix of size $n$. Suppose that $\lambda_{1},\ldots,\lambda_{k}$ are all of the distinct eigenvalues of $A$. Then $A$ is diagonalizable if and only if

\begin{align*}
\displaystyle \sum_{i=1}^{k}\dim{\mathcal{E}}_{A}\left(\lambda_{i}\right)=\dim{\mathcal{E}}_{A}\left(\lambda_{1}\right)+\cdots+\dim{\mathcal{E}}_{A}\left(\lambda_{k}\right)=n.&amp;
\end{align*}

Suppose that the above condition is satisfies by $A$ and let $T_{i}=\{\mathbf{v}_{i1},\,\mathbf{v}_{i2},\,\mathbf{v}_{i3},\,\ldots,\,\mathbf{v}_{id_{i}}\}$ be a basis for the eigenspace of $\lambda_{i}$, ${\mathcal{E}}_{A}\left(\lambda_{i}\right)$, for each $1\leq i\leq k$
and let $d_{i}=\dim{\mathcal{E}}_{A}\left(\lambda_{i}\right)$. Then

\begin{align*}
\displaystyle T=T_{1}\cup T_{2}\cup T_{3}\cup\cdots\cup T_{k}
\end{align*}

is a set of linearly independent eigenvectors for $A$ with size $n$. By Theorem  @ref{DC}, let $S$ be a square matrix whose $i$-th column is the $i$-th vector of the set $T$, i.e.

\begin{align*}
\displaystyle S=[\mathbf{v}_{11}|\cdots|\mathbf{v}_{1d_{1}}|\mathbf{v}_{21}|\cdots|\mathbf{v}_{2d_{2}}|\cdots|\mathbf{v}_{k1}|\cdots|\mathbf{v}_{kd_{k}}]
\end{align*}

Then

\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\underbrace{\lambda_{1},\ldots,\lambda_{1}}_{d_{1}},\underbrace{\lambda_{2},\ldots,\lambda_{2}}_{d_{2}},\ldots,\underbrace{\lambda_{k},\ldots,\lambda_{k}}_{d_{k}}).
\end{align*}
@end
@proof
@col
See the textbook ∎
@end

<b>Remark</b>: Equation ( @ref{diagconde}) can be rewritten as

\begin{align*}
\displaystyle \sum_{i=1}^{k}n\left(A-\lambda_{i}I_{n}\right)=n\left(A-\lambda_{1}I_{n}\right)+\cdots+n\left(A-\lambda_{k}I_{n}\right)=n
\end{align*}

or

\begin{align*}
\displaystyle \sum_{i=1}^{k}(n-r\left(A-\lambda_{i}I_{n}\right)=(n-r\left(A-\lambda_{1}I_{n}\right))+\cdots+(n-r\left(A-\lambda_{k}I_{n}\right))=n.
\end{align*}

@thm
@title{Distinct Eigenvalues implies Diagonalizable}
@label{DED}
Suppose that $A$ is a square matrix of size $n$ with $n$ distinct eigenvalues.
Then $A$ is diagonalizable.
@end
@proof
@col
You can skip the proof. See the textbook. ∎
@end
@eg
Determine if the matrix $B$ in Example 6
is diagonalizable. The characteristic polynomial is

\begin{align*}
\displaystyle p_{B}\left(x\right)=\det\left(B-xI_{4}\right)=(x-1)(x-2)^{3}.
\end{align*}

We conclude that $\lambda_{1}=1$ and $\lambda_{2}=2$ are all of the distinct eigenvalues of $B$.

In Example 6, we compute the RREF of $B-I_{4}$ and $B-2I_{4}$. By the RREFs, we have $r\left(B-I_{4}\right)=3$, $r\left(B-2I_{4}\right)=3$.
Therefore

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{B}\left(1\right)=n\left(B-I_{4}\right)=4-r\left(B-I_{4}\right)=1
\end{align*}

and

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{B}\left(2\right)=n\left(B-2I_{4}\right)=4-r\left(B-2I_{4}\right)=4-3=1.
\end{align*}

Now

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{B}\left(1\right)+\dim{\mathcal{E}}_{B}\left(2\right)=1+1=2\neq 4.
\end{align*}

By Theorem  @ref{diagcond}, $B$ is not diagonalizable.
@end
@eg
Determine if the matrix $C$
in Example 7 is diagonalizable.Because

\begin{align*}
\displaystyle p_{C}\left(x\right)=-3+4x+2x^{2}-4x^{3}+x^{4}=(x-3)(x-1)^{2}(x+1),
\end{align*}

all the distinct eigenvalues are $\lambda_{1}=3$, $\lambda_{2}=1$ and $\lambda_{3}=-1$. We have

\begin{align*}
\displaystyle \sum_{i=1}^{3}\dim{\mathcal{E}}_{C}\left(\lambda_{i}\right)=\sum_{i=1}^{3}(4-r\left(C-\lambda_{i}I_{4}\right))
\end{align*}

\begin{align*}
\displaystyle =(4-3)+(4-2)+(4-3)=4.
\end{align*}

By Theorem  @ref{diagcond}, $C$ is diagonalizable.
@end
@eg
Determine if the matrix $E$ in example 8 is diagonalizable.The characteristic polynomial is $p_{E}\left(x\right)=-(x-2)^{4}(x+1)$.
The eigenvalues are $\lambda_{1}=2$ and $\lambda_{2}=-1$ and we have

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{E}\left(\lambda_{1}\right)+\dim{\mathcal{E}}_{E}\left(\lambda_{2}\right)=(5-r\left(E-2I_{4}\right))+(5-r\left(E+I_{5}\right))
\end{align*}

\begin{align*}
\displaystyle =(5-3)+(5-4)=2+1=3\neq 5.
\end{align*}

So $E$ is not diagonalizable.
@end
@eg
Determine if the matrix $H$ in Example 9 is diagonalizable.Because $p_{H}\left(x\right)=x(x-2)(x-1)(x+1)(x+3)$, has $5$ distinct eigenvalues, Theorem  @ref{DED} implies that $H$ is diagonalizable.
@end
@eg
Diagonalize $C$ in Example 7 (see also Example 18). By the computation in Example 18, $C$ is diagonalizable. By the computation in Example 7, we have that$\{\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix}\}$ is a basis for ${\mathcal{E}}_{C}\left(3\right)$, $\{\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\,\begin{bmatrix}0\\
0\\
-1\\
1\end{bmatrix}\}$ is a basis for ${\mathcal{E}}_{C}\left(1\right)$, $\{\begin{bmatrix}-1\\
-1\\
1\\
1\end{bmatrix}\}$ is a basis for ${\mathcal{E}}_{C}\left(-1\right)$. By Theorem  @ref{diagcond},

\begin{align*}
\displaystyle S=\begin{bmatrix}1&amp;-1&amp;0&amp;-1\\
1&amp;1&amp;0&amp;-1\\
1&amp;0&amp;-1&amp;1\\
1&amp;0&amp;1&amp;1\end{bmatrix}.
\end{align*}

Then $S^{-1}CS=\operatorname{diag}(3,1,1,-1)$. <b>Remark</b>: Unlike Example 16, we don’t need to check that $S$ is invertible. This is guaranteed by Theorem  @ref{diagcond}.
@end
@eg
Diagonalize $H$ in Example 9 (see also Example 21).By the discussion of Example 21, $H$ is diagonalizable. By the computation in Example 9,
$\begin{bmatrix}1\\
-1\\
-2\\
-1\\
1\end{bmatrix}$, $\begin{bmatrix}1\\
0\\
-1\\
-2\\
2\end{bmatrix}$, $\begin{bmatrix}-1\\
2\\
2\\
0\\
1\end{bmatrix}$, $\begin{bmatrix}1\\
0\\
0\\
-1\\
2\end{bmatrix}$
and $\begin{bmatrix}-2\\
1\\
2\\
4\\
-2\end{bmatrix}$ are bases for ${\mathcal{E}}_{H}\left(2\right),{\mathcal{E}}_{H}\left(1\right),{\mathcal{E}}_{H}\left(0\right),{\mathcal{E}}_{H}\left(-1\right)$ and ${\mathcal{E}}_{H}\left(-3\right)$ respectively.
By Theorem  @ref{diagcond}, let

\begin{align*}
\displaystyle S=\begin{bmatrix}1&amp;1&amp;-1&amp;1&amp;-2\\
-1&amp;0&amp;2&amp;0&amp;1\\
-2&amp;-1&amp;2&amp;0&amp;2\\
-1&amp;-2&amp;0&amp;-1&amp;4\\
1&amp;2&amp;1&amp;2&amp;-2\\
\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle S^{-1}HS=\operatorname{diag}(2,1,0,-1,-3).
\end{align*}
@end
@eg
Determine if

\begin{align*}
\displaystyle J=\begin{bmatrix}2&amp;1&amp;1\\
1&amp;2&amp;1\\
1&amp;1&amp;2\end{bmatrix}
\end{align*}

is diagonalizable. If it is diagonalizable, find $S$ such that $S^{-1}JS$ is diagonal. <b>Step 1</b>:
$p_{J}\left(x\right)=-x^{3}+6x^{2}-9x+4=-(x-4)(x-1)^{2}$. All the distict eigenvalues of $J$ is $\lambda_{1}=4$, $\lambda_{2}=1$. <b>Step 2</b>

\begin{align*}
\displaystyle J-4I_{3}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;-1\\
0&amp;1&amp;-1\\
0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{J}\left(4\right)=3-r\left(J-4I_{3}\right)=3-2=1.
\end{align*}

\begin{align*}
\displaystyle J-I_{3}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;1&amp;1\\
0&amp;0&amp;0\\
0&amp;0&amp;0\\
\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{J}\left(1\right)=3-r\left(J-I_{3}\right)=3-1=2.
\end{align*}

Now

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{J}\left(4\right)+\dim{\mathcal{E}}_{J}\left(1\right)=1+2=3.
\end{align*}

By Theorem  @ref{diagcond}, $J$ is diagonalizable. <b>Step 3</b>:$\{\begin{bmatrix}1\\
1\\
1\end{bmatrix}\}$ is a basis for ${\mathcal{E}}_{J}\left(4\right)$. $\{\begin{bmatrix}-1\\
1\\
0\end{bmatrix},\begin{bmatrix}-1\\
0\\
1\end{bmatrix}\}$ is a basis for ${\mathcal{E}}_{J}\left(1\right)$. By Theorem  @ref{diagcond}, we can take

\begin{align*}
\displaystyle S=\begin{bmatrix}1&amp;-1&amp;-1\\
1&amp;1&amp;0\\
1&amp;0&amp;1\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle S^{-1}JS=\operatorname{diag}(4,1,1).
\end{align*}
@end
@eg
Determine if

\begin{align*}
\displaystyle K=\begin{bmatrix}-4&amp;-4&amp;0&amp;-11&amp;-2&amp;-5\\
138&amp;87&amp;-6&amp;248&amp;44&amp;122\\
-24&amp;-16&amp;2&amp;-44&amp;-8&amp;-20\\
-62&amp;-39&amp;2&amp;-110&amp;-20&amp;-54\\
-63&amp;-39&amp;3&amp;-114&amp;-19&amp;-57\\
56&amp;35&amp;-2&amp;101&amp;18&amp;51\\
\end{bmatrix}
\end{align*}

is diagonalizable and if it is diagonalizable, find $S$ such that $S^{-1}KS$ is diagonal.<b>Step 1</b>:
The characteristic polynomial is

\begin{align*}
\displaystyle p_{K}\left(x\right)=\det\left(K-xI_{6}\right)=(x+1)(x-1)^{2}(x-2)^{3}.
\end{align*}

The eigenvalues are $\lambda_{1}=-1,\lambda_{2}=1$ and $\lambda_{3}=2$. <b>Step 2</b>:

\begin{align*}
\displaystyle K+I_{4}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;0&amp;0&amp;0&amp;\frac{1}{9}\\
0&amp;1&amp;0&amp;0&amp;0&amp;-\frac{22}{9}\\
0&amp;0&amp;1&amp;0&amp;0&amp;\frac{4}{9}\\
0&amp;0&amp;0&amp;1&amp;0&amp;\frac{10}{9}\\
0&amp;0&amp;0&amp;0&amp;1&amp;\frac{10}{9}\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(-1\right)=n\left(K+I_{4}\right)=6-r\left(K+I_{4}\right)=6-5=1.
\end{align*}

\begin{align*}
\displaystyle A-I_{6}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;0&amp;0&amp;\frac{1}{18}&amp;\frac{1}{6}\\
0&amp;1&amp;0&amp;0&amp;\frac{5}{18}&amp;-\frac{13}{6}\\
0&amp;0&amp;1&amp;0&amp;\frac{2}{9}&amp;\frac{2}{3}\\
0&amp;0&amp;0&amp;1&amp;\frac{1}{18}&amp;\frac{7}{6}\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(1\right)=n\left(K-I_{6}\right)=6-r\left(K-I_{6}\right)=6-4=2.
\end{align*}

\begin{align*}
\displaystyle K-2I_{6}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;0&amp;\frac{1}{2}&amp;-1&amp;-\frac{1}{2}\\
0&amp;1&amp;0&amp;2&amp;2&amp;2\\
0&amp;0&amp;1&amp;-\frac{3}{2}&amp;-2&amp;-\frac{7}{2}\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(2\right)=n\left(K-2I_{6}\right)=6-r\left(K-2I_{6}\right)=6-3=3.
\end{align*}

Because

\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(-1\right)+\dim{\mathcal{E}}_{K}\left(1\right)+\dim{\mathcal{E}}_{K}\left(2\right)=1+2+3=6.
\end{align*}

By Theorem  @ref{diagcond}, $K$ is diagonalizable.<b>Step 3</b>: A basis for ${\mathcal{E}}_{K}\left(-1\right)={\mathcal{N}}\!\left(K+I_{6}\right)$ is

\begin{align*}
\displaystyle \{\begin{bmatrix}-1\\
22\\
-4\\
-10\\
-10\\
9\end{bmatrix}\}
\end{align*}

(we use the method in Lecture 8 Example 6 and multiply the result by $9$ to clear the denominator.) A basis for ${\mathcal{E}}_{K}\left(1\right)={\mathcal{N}}\!\left(K-I_{6}\right)$ is

\begin{align*}
\displaystyle \{\begin{bmatrix}-1\\
-5\\
-4\\
-1\\
18\\
0\end{bmatrix},\begin{bmatrix}-1\\
13\\
-4\\
-7\\
0\\
6\end{bmatrix}\}
\end{align*}

(Again, we use the method in Lecture 8 Example 6 and multiply the first vector by $18$ and the second vector by $6$ to clear the denominators.)

\begin{align*}
\displaystyle \{\begin{bmatrix}-1\\
-4\\
3\\
2\\
0\\
0\end{bmatrix},\begin{bmatrix}1\\
-2\\
2\\
0\\
1\\
0\end{bmatrix},\begin{bmatrix}1\\
-4\\
7\\
0\\
0\\
2\end{bmatrix}\}.
\end{align*}

(Again, we use the method in Lecture 8 Example 6 and multiply the first and the third vector by $2$ to clear the denominators.) So we can take

\begin{align*}
\displaystyle S=\begin{bmatrix}-1&amp;-1&amp;-1&amp;1&amp;1&amp;-1\\
22&amp;13&amp;-5&amp;-4&amp;-2&amp;-4\\
-4&amp;-4&amp;-4&amp;7&amp;2&amp;3\\
-10&amp;-7&amp;-1&amp;0&amp;0&amp;2\\
-10&amp;0&amp;18&amp;0&amp;1&amp;0\\
9&amp;6&amp;0&amp;2&amp;0&amp;0\\
\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(-1,1,1,2,2,2).
\end{align*}
@end
@eg
Determine if

\begin{align*}
\displaystyle J=\begin{bmatrix}2&amp;1&amp;1\\
1&amp;2&amp;1\\
1&amp;1&amp;2\end{bmatrix},\,\,L=\begin{bmatrix}-8&amp;6&amp;6\\
-9&amp;7&amp;6\\
-9&amp;6&amp;7\\
\end{bmatrix}
\end{align*}

are similar. If they are similar, Find $R$ such that $R^{-1}JR=L$. We know that $J$ is

The characteristic polynomials

\begin{align*}
\displaystyle p_{J}\left(x\right)=-(-4+x)(-1+x)^{2}=p_{L}\left(x\right).
\end{align*}

(If the characteristic polynomials are different, $J$ and $L$ are not similar, end of the story.) In example 23, we know that $J$ is diagonalizable. Follow similar method, we can show that $L$ is diagonalizable (fill the detail).

\begin{align*}
\displaystyle Q=\begin{bmatrix}1&amp;2&amp;2\\
1&amp;3&amp;0\\
1&amp;0&amp;3\\
\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle Q^{-1}LQ=\operatorname{diag}(4,1,1).
\end{align*}

So $J$ is similar to $\operatorname{diag}(4,1,1)$ which in turns similar to $L$. So $J$ is similar to $L$ (Theorem  @ref{SER}). In fact

\begin{align*}
\displaystyle S^{-1}JS=Q^{-1}LQ
\end{align*}

\begin{align*}
\displaystyle (SQ^{-1})^{-1}J(SQ^{-1})=L.
\end{align*}

So we can take

\begin{align*}
\displaystyle R=SQ^{-1}=\begin{bmatrix}-5&amp;3&amp;3\\
-2&amp;\frac{5}{3}&amp;\frac{4}{3}\\
-2&amp;\frac{4}{3}&amp;\frac{5}{3}\\
\end{bmatrix}.
\end{align*}
@end

@section{Powers of Matrices}

Suppose $s$ is a positive integer. Recall the notation

\begin{align*}
\displaystyle A^{s}=\underbrace{A\cdots A}_{s}
\end{align*}

Powers of a diagonal matrix are easy to compute. The case of a diagonalizable matrix is only slightly more difficult. Suppose that $A$ is similar to a diagonal matrix $D=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n})$. Let $S$ be an invertible matrix such that

\begin{align*}
\displaystyle S^{-1}AS=D.
\end{align*}

Then

\begin{align*}
\displaystyle A=SDS^{-1}.
\end{align*}

\begin{align*}
\displaystyle A^{s}=\underbrace{SDS^{-1}SDS^{-1}\cdots SDS^{-1}}_{s}=S\underbrace{D\cdots D}_{s}S^{-1}=SD^{s}S^{-1}
\end{align*}

\begin{align*}
\displaystyle =S\operatorname{diag}(\lambda_{1}^{s},\ldots,\lambda_{n}^{s})S^{-1}.
\end{align*}

@eg
Let $s$ be a positive integer and

\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;3\\
4&amp;2\end{bmatrix}.
\end{align*}

We want to find a closed formula for $A^{s}$. The characteristic polynomial of $A$ is

\begin{align*}
\displaystyle p_{A}\left(x\right)=\det\left(A-xI_{2}\right)=(1-x)(2-x)-12=x^{2}-3x-10=(x+2)(x-5).
\end{align*}

For $\lambda=-2$

\begin{align*}
\displaystyle A+2I_{2}=\begin{bmatrix}3&amp;3\\
4&amp;4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;1\\
0&amp;0\end{bmatrix}.
\end{align*}

So

\begin{align*}
\displaystyle \{\begin{bmatrix}1\\
-1\end{bmatrix}\}
\end{align*}

is a basis for ${\mathcal{E}}_{A}\left(-2\right)$. For $\lambda=5$

\begin{align*}
\displaystyle A-5I_{2}=\begin{bmatrix}-4&amp;3\\
4&amp;-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;-\frac{3}{4}\\
0&amp;0\end{bmatrix}.
\end{align*}

So

\begin{align*}
\displaystyle \{\begin{bmatrix}3\\
4\end{bmatrix}\}
\end{align*}

is a basis for ${\mathcal{E}}_{A}\left(5\right)$. Let

\begin{align*}
\displaystyle S=\begin{bmatrix}1&amp;3\\
-1&amp;4\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(-2,5)
\end{align*}

so that

\begin{align*}
\displaystyle A^{s}=S\operatorname{diag}((-2)^{s},5^{s})S^{-1}
\end{align*}

\begin{align*}
\displaystyle =\begin{bmatrix}1&amp;3\\
-1&amp;4\end{bmatrix}\begin{bmatrix}(-2)^{s}&amp;0\\
0&amp;5^{s}\end{bmatrix}\begin{bmatrix}\frac{4}{7}&amp;-\frac{3}{7}\\
\frac{1}{7}&amp;\frac{1}{7}\\
\end{bmatrix}=\begin{bmatrix}\frac{1}{7}(-1)^{s}2^{s+2}+\frac{3\times 5^{s}}{7}&amp;\frac{1}{7}(-3)(-2)^{s}+\frac{3\times 5^{s}}{7}\\
-\frac{1}{7}(-1)^{s}2^{s+2}+\frac{4\times 5^{s}}{7}&amp;\frac{3(-2)^{s}}{7}+\frac{4\times 5^{s}}{7}\\
\end{bmatrix}.
\end{align*}
@end
@eg
<b>High power of a diagonalizable matrix</b>Suppose that

\begin{align*}
\displaystyle A=\begin{bmatrix}19&amp;0&amp;6&amp;13\\
-33&amp;-1&amp;-9&amp;-21\\
21&amp;-4&amp;12&amp;21\\
-36&amp;2&amp;-14&amp;-28\end{bmatrix}.
\end{align*}

We wish to compute $A^{20}$. Normally this would require 19 matrix multiplications. But since $A$ is diagonalizable, we can simplify the computations substantially.

First, we diagonalize $A$. With

\begin{align*}
\displaystyle S=\begin{bmatrix}1&amp;-1&amp;2&amp;-1\\
-2&amp;3&amp;-3&amp;3\\
1&amp;1&amp;3&amp;3\\
-2&amp;1&amp;-4&amp;0\end{bmatrix},
\end{align*}

we find

\begin{align*}
\displaystyle D&amp;\displaystyle=S^{-1}AS \\
&amp;\displaystyle=\begin{bmatrix}-6&amp;1&amp;-3&amp;-6\\
0&amp;2&amp;-2&amp;-3\\
3&amp;0&amp;1&amp;2\\
-1&amp;-1&amp;1&amp;1\end{bmatrix}\begin{bmatrix}19&amp;0&amp;6&amp;13\\
-33&amp;-1&amp;-9&amp;-21\\
21&amp;-4&amp;12&amp;21\\
-36&amp;2&amp;-14&amp;-28\end{bmatrix}\begin{bmatrix}1&amp;-1&amp;2&amp;-1\\
-2&amp;3&amp;-3&amp;3\\
1&amp;1&amp;3&amp;3\\
-2&amp;1&amp;-4&amp;0\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}-1&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;2&amp;0\\
0&amp;0&amp;0&amp;1\end{bmatrix}.
\end{align*}

Using this, we compute

\begin{align*}
\displaystyle A^{20}=SD^{20}S^{-1}&amp;\displaystyle=S\begin{bmatrix}-1&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;2&amp;0\\
0&amp;0&amp;0&amp;1\end{bmatrix}^{20}S^{-1} \\
&amp;\displaystyle=S\begin{bmatrix}(-1)^{20}&amp;0&amp;0&amp;0\\
0&amp;(0)^{20}&amp;0&amp;0\\
0&amp;0&amp;(2)^{20}&amp;0\\
0&amp;0&amp;0&amp;(1)^{20}\end{bmatrix}S^{-1} \\
&amp;\displaystyle=\begin{bmatrix}1&amp;-1&amp;2&amp;-1\\
-2&amp;3&amp;-3&amp;3\\
1&amp;1&amp;3&amp;3\\
-2&amp;1&amp;-4&amp;0\end{bmatrix}\begin{bmatrix}1&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;1048576&amp;0\\
0&amp;0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}-6&amp;1&amp;-3&amp;-6\\
0&amp;2&amp;-2&amp;-3\\
3&amp;0&amp;1&amp;2\\
-1&amp;-1&amp;1&amp;1\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}6291451&amp;2&amp;2097148&amp;4194297\\
-9437175&amp;-5&amp;-3145719&amp;-6291441\\
9437175&amp;-2&amp;3145728&amp;6291453\\
-12582900&amp;-2&amp;-4194298&amp;-8388596\end{bmatrix}.
\end{align*}

Generally

\begin{align*}
\displaystyle A^{s}=SD^{s}S^{-1}=\begin{bmatrix}1-6(-1)^{s}+3\ 2^{s+1}&amp;1+(-1)^{s}&amp;-1-3(-1)^{s}+2^{s+1}&amp;-1-6(-1)^{s}+2^{s+2}\\
-3+12(-1)^{s}-9\ 2^{s}&amp;-3-2(-1)^{s}&amp;3+6(-1)^{s}-3\ 2^{s}&amp;3+12(-1)^{s}-3\ 2^{s+1}\\
-3-6(-1)^{s}+9\ 2^{s}&amp;-3+(-1)^{s}&amp;3-3(-1)^{s}+3\ 2^{s}&amp;3-6(-1)^{s}+3\ 2^{s+1}\\
12(-1)^{s}-3\ 2^{s+2}&amp;-2(-1)^{s}&amp;6(-1)^{s}-2^{s+2}&amp;12(-1)^{s}-2^{s+3}\\
\end{bmatrix}.
\end{align*}
@end

@section{Summary}

In below $A$ always denote a square matrix of size $n$

<ol class="ltx_enumerate">
<li class="ltx_item">
If $\mathbf{x}\neq 0$ and $A\mathbf{x}=\lambda x$, then $\mathbf{x}$ is called
an <b>eigenvector</b> of $A$ with eigenvalue $\lambda$.
</li>
<li class="ltx_item">
$p_{A}(x)=\det(A-xI_{n})$ is called the <b>characteristic function</b> of $A$.

<ol class="ltx_enumerate">
<li class="ltx_item">
It is a polynomial of degree $n$ with leading coefficient $(-1)^{n}$.
</li>
<li class="ltx_item">
$\lambda$ is an eigenvalue if and only if $p_{A}(\lambda)=0$, i.e., $\lambda$ is a root of $p_{A}(x)$.
</li>

</ol>
</li>
<li class="ltx_item">
${\mathcal{E}}_{A}\left(\lambda\right)$: <b>eigenspace</b> of $A$ for an eigenvalue $\lambda$.

<ol class="ltx_enumerate">
<li class="ltx_item">
It is the set of of all eigenvectors of $A$ for $\lambda$, together with the zero vector,
i.e.

\begin{align*}
\displaystyle {\mathcal{E}}_{\lambda}\left(A\right)=\{\mathbf{x}\in{\mathbb{R}}^{n}\,|\,A\mathbf{x}=\lambda\mathbf{x}\}.
\end{align*}

</li>
<li class="ltx_item">
${\mathcal{E}}_{\lambda}\left(A\right)={\mathcal{N}}\!\left(A-\lambda I_{n}\right)$.
</li>
<li class="ltx_item">
${\mathcal{E}}_{\lambda}\left(A\right)$ is a subspace of ${\mathbb{R}}^{n}$.
</li>

</ol>
</li>
<li class="ltx_item">
$\alpha_{A}\left(\lambda\right)$: <b>algebraic multiplicity</b>. The power of $(x-\lambda)$ in the factorization of $p_{A}(x)$.
</li>
<li class="ltx_item">
$\gamma_{A}\left(\lambda\right)$: <b>geometric multiplicity</b>. It is $\dim{\mathcal{E}}_{A}\left(\lambda\right)=n\left(A-\lambda I_{n}\right)$.
</li>
<li class="ltx_item">
Basic properties. Suppose $\lambda$ is an eigenvalue of $A$ and $\mathbf{x}$ is an eigenvector of $A$ with eigenvalue $\lambda$.

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is invertible if and only if $\lambda=0$ is an eigenvalue.
</li>
<li class="ltx_item">
For positive integer $s$, $\mathbf{x}$ is an eigenvector of $A^{s}$ with eigenvalue $\lambda^{s}$.
</li>
<li class="ltx_item">
If $\lambda\neq 0$, $\mathbf{x}$ is an eigenvector of $A^{-1}$ with eigenvalue $\lambda^{-1}$.
</li>
<li class="ltx_item">
$\lambda$ is an eigenvector of $A^{t}$ (but $\mathbf{x}$ may <b>not</b> be an eigenvector of $A$)
</li>

</ol>
</li>
<li class="ltx_item">
Computational questions

<ol class="ltx_enumerate">
<li class="ltx_item">
Find all the eigenvalues of $A$: find all the roots of $p_{A}(x)$.
</li>
<li class="ltx_item">
Find ${\mathcal{E}}_{A}\left(\lambda\right)$: find ${\mathcal{N}}\!\left(A-\lambda I_{n}\right)$, this can be done by finding the RREF of $A-\lambda I_{n}$.

</li>
<li class="ltx_item">
Find $\alpha_{A}\left(\lambda\right)$: Find the power of $x-\lambda$ in the factorization of $p_{A}(x)$.
</li>
<li class="ltx_item">
Find a basis for ${\mathcal{E}}_{A}\left(\lambda\right)$: again, this is same as finding basis of ${\mathcal{N}}\!\left(A-\lambda I_{n}\right)$. This can be done by $A-\lambda I_{n}\xrightarrow{\text{RREF}}B$ and use the standard method
in finding basis (see Lecture 8 Theorem 4, Example 6).
</li>
<li class="ltx_item">
Find $\gamma_{A}\left(\lambda\right)$: same as finding $n\left(A-\lambda I_{n}\right)=n-r\left(A-\lambda I_{n}\right)$. Suppose $A-\lambda I_{n}\xrightarrow{\text{RREF}}B$.
Then $\gamma_{A}\left(\lambda\right)=n-r\left(B\right)=n-\text{ number of pivot columns of $B$}$.
</li>

</ol>
</li>

</ol>

@chapter{Inner Product}

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk.

@section{Basic properties of inner products}

@defn
Given two vectors $\mathbf{v}$ and $\mathbf{w}$ in ${\mathbb{R}}^{m}$, we define

\begin{align*}
\displaystyle \left&lt;\mathbf{v},\mathbf{w}\right&gt;=\sum_{i=1}^{m}[\mathbf{v}]_{i}[\mathbf{w}]_{i}=[\mathbf{v}]_{1}[\mathbf{w}]_{1}+\cdots+[\mathbf{v}]_{m}[\mathbf{w}]_{m}.&amp;
\end{align*}

It is called the inner product of ${\mathbb{R}}^{m}$. The vector space ${\mathbb{R}}^{m}$ together with the operation $\langle-,-\rangle$ is called an inner product space.
If we regard $\mathbf{v}$ and $\mathbf{w}$ as $m\times 1$ matrice, then we can write

\begin{align*}
\displaystyle \left&lt;\mathbf{v},\mathbf{w}\right&gt;=\mathbf{v}^{t}\mathbf{w}.&amp;
\end{align*}
@end

@eg
We have

\begin{align*}
\displaystyle \left&lt;\begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}3\\
4\end{bmatrix}\right&gt;=1\times 3+2\times 4=11
\end{align*}

and

\begin{align*}
\displaystyle \left&lt;\begin{bmatrix}1\\
2\\
3\end{bmatrix},\begin{bmatrix}4\\
5\\
6\end{bmatrix}\right&gt;=1\times 4+2\times 5+3\times 6=32.
\end{align*}
@end

@proposition
@label{ipdef}
For any $\mathbf{v},\mathbf{w},\mathbf{u}\in{\mathbb{R}}^{m}$ and $\alpha\in{\mathbb{R}}^{\hbox{}}$. We have

<ol class="ltx_enumerate">
<li class="ltx_item">
$\left&lt;\mathbf{v}+\mathbf{w},\mathbf{u}\right&gt;=\left&lt;\mathbf{v},\mathbf{u}\right&gt;+\left&lt;\mathbf{w},\mathbf{u}\right&gt;$.
</li>
<li class="ltx_item">
$\left&lt;\alpha\mathbf{v},\mathbf{w}\right&gt;=\alpha\left&lt;\mathbf{v},\mathbf{w}\right&gt;$.
</li>
<li class="ltx_item">
$\left&lt;\mathbf{v},\mathbf{w}\right&gt;=\left&lt;\mathbf{w},\mathbf{v}\right&gt;$.
</li>
<li class="ltx_item">
$\left&lt;\mathbf{v},\mathbf{v}\right&gt;&gt;0$ for $\mathbf{v}\neq\mathbf{0}$.
</li>

</ol>
@end

@proof
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
We compute

\begin{align*}
\displaystyle\left&lt;\mathbf{v}+\mathbf{w},\mathbf{u}\right&gt;&amp;\displaystyle=[\mathbf{v}+\mathbf{w}]_{1}[\mathbf{u}]_{1}+[\mathbf{v}+\mathbf{w}]_{2}[\mathbf{u}]_{2}+\cdots+[\mathbf{v}+\mathbf{w}]_{m}[\mathbf{u}]_{m} \\
&amp;\displaystyle=([\mathbf{v}]_{1}+[\mathbf{w}]_{1})[\mathbf{u}]_{1}+([\mathbf{v}]_{2}+[\mathbf{w}]_{2})[\mathbf{u}]_{2}+\cdots+([\mathbf{v}]_{n}+[\mathbf{w}]_{m})[\mathbf{u}]_{m} \\
&amp;\displaystyle=[\mathbf{v}]_{1}[\mathbf{u}]_{1}+[\mathbf{w}]_{1}[\mathbf{u}]_{1}+[\mathbf{v}]_{2}[\mathbf{u}]_{2}+[\mathbf{w}]_{2}[\mathbf{u}]_{2}+\cdots+[\mathbf{v}]_{m}[\mathbf{u}]_{m}+[\mathbf{w}]_{m}[\mathbf{u}]_{m} \\
&amp;\displaystyle=[\mathbf{v}]_{1}[\mathbf{u}]_{1}+\cdots+[\mathbf{v}]_{m}[\mathbf{u}]_{m}+[\mathbf{w}]_{1}[\mathbf{u}]_{1}+\cdots+[\mathbf{w}]_{m}[\mathbf{u}]_{m} \\
&amp;\displaystyle=\left&lt;\mathbf{v},\mathbf{u}\right&gt;+\left&lt;\mathbf{w},\mathbf{u}\right&gt;.
\end{align*}

Or we can use ( @ref{ipdefe2}):

\begin{align*}
\displaystyle \left&lt;\mathbf{v}+\mathbf{w},\mathbf{u}\right&gt;=(\mathbf{v}+\mathbf{w})^{t}\mathbf{u}=(\mathbf{v}^{t}+\mathbf{w}^{t})\mathbf{u}
\end{align*}

\begin{align*}
\displaystyle =\mathbf{v}^{t}\mathbf{u}+\mathbf{w}^{t}\mathbf{u}=\left&lt;\mathbf{v},\mathbf{u}\right&gt;+\left&lt;\mathbf{w},\mathbf{u}\right&gt;.
\end{align*}

</li>
<li class="ltx_item">
We compute

\begin{align*}
\displaystyle\left&lt;\alpha\mathbf{v},\mathbf{w}\right&gt;&amp;\displaystyle=[\alpha\mathbf{v}]_{1}[\mathbf{w}]_{1}+[\alpha\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+[\alpha\mathbf{v}]_{m}[\mathbf{w}]_{m} \\
&amp;\displaystyle=\alpha[\mathbf{v}]_{1}[\mathbf{w}]_{1}+\alpha[\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+\alpha[\mathbf{v}]_{m}[\mathbf{w}]_{m} \\
&amp;\displaystyle=\alpha([\mathbf{v}]_{1}[\mathbf{w}]_{1}+[\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+[\mathbf{v}]_{m}[\mathbf{w}]_{m}) \\
&amp;\displaystyle=\alpha\left&lt;\mathbf{v},\mathbf{w}\right&gt;.
\end{align*}

Or we can use ( @ref{ipdefe2}):

\begin{align*}
\displaystyle \left&lt;\alpha\mathbf{v},\mathbf{w}\right&gt;=(\alpha\mathbf{v})^{t}\mathbf{w}=\alpha\mathbf{v}^{t}\mathbf{w}=\alpha\left&lt;\mathbf{v},\mathbf{w}\right&gt;.
\end{align*}

</li>
<li class="ltx_item">
We compute

\begin{align*}
\displaystyle\left&lt;\mathbf{v},\mathbf{w}\right&gt;&amp;\displaystyle=[\mathbf{v}]_{1}[\mathbf{w}]_{1}+[\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+[\mathbf{v}]_{m}[\mathbf{w}]_{m}. \\
&amp;\displaystyle=[\mathbf{w}]_{1}[\mathbf{v}]_{1}+[\mathbf{w}]_{2}[\mathbf{v}]_{2}+\cdots+[\mathbf{w}]_{m}[\mathbf{v}]_{m}. \\
&amp;\displaystyle=\left&lt;\mathbf{w},\mathbf{v}\right&gt;.
\end{align*}

</li>
<li class="ltx_item">
We compute

\begin{align*}
\displaystyle \left&lt;\mathbf{v},\mathbf{v}\right&gt;=[\mathbf{v}]_{1}^{2}+[\mathbf{v}]_{2}^{2}+\cdots+[\mathbf{v}]_{m}^{2}\geq 0.
\end{align*}

Noting that $\left&lt;\mathbf{v},\mathbf{v}\right&gt;=0$ if and only if $[\mathbf{v}]_{i}=0$ for all $1\leq i\leq n$, we see that $\mathbf{v}=\mathbf{0}$
</li>

</ol>

∎
@end

@proposition
Let $\alpha,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{v},\mathbf{w},\mathbf{u}\in{\mathbb{R}}^{m}$. We have

<ol class="ltx_enumerate">
<li class="ltx_item">
$\left&lt;\alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right&gt;=\alpha\left&lt;\mathbf{v},\mathbf{u}\right&gt;+\beta\left&lt;\mathbf{w},\mathbf{u}\right&gt;$.
</li>
<li class="ltx_item">
$\left&lt;\mathbf{u},\alpha\mathbf{v}+\beta\mathbf{w}\right&gt;=\alpha\left&lt;\mathbf{u},\mathbf{v}\right&gt;+\beta\left&lt;\mathbf{u},\mathbf{w}\right&gt;$.
</li>
<li class="ltx_item">
$\left&lt;\mathbf{0},\mathbf{v}\right&gt;=\left&lt;\mathbf{v},\mathbf{0}\right&gt;=0$.
</li>
<li class="ltx_item">
If $\left&lt;\mathbf{v},\mathbf{x}\right&gt;=0$ for all $\mathbf{x}\in{\mathbb{R}}^{m}$, then $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
If $\left&lt;\mathbf{v},\mathbf{x}\right&gt;=\left&lt;\mathbf{w},\mathbf{x}\right&gt;$ for all $x\in{\mathbb{R}}^{m}$, then $\mathbf{v}=\mathbf{w}$.
</li>

</ol>
@end

@proof
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
We compute

\begin{align*}
\displaystyle \left&lt;\alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right&gt;=\left&lt;\alpha\mathbf{v},\mathbf{u}\right&gt;+\left&lt;\beta\mathbf{w},\mathbf{u}\right&gt;\qquad(\text{Proposition \ref{ipdef} item 1})
\end{align*}

\begin{align*}
\displaystyle =\alpha\left&lt;\mathbf{v},\mathbf{u}\right&gt;+\beta\left&lt;\mathbf{w},\mathbf{u}\right&gt;\qquad(\text{Proposition \ref{ipdef} item 2}).
\end{align*}

Or we can also use ( @ref{ipdefe2}):

\begin{align*}
\displaystyle \left&lt;\alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right&gt;=(\alpha\mathbf{v}+\beta\mathbf{w})^{t}\mathbf{u}
\end{align*}

\begin{align*}
\displaystyle =\alpha(\mathbf{v})^{t}\mathbf{u}+\beta(\mathbf{w})^{t}\mathbf{u}=\alpha\left&lt;\mathbf{v},\mathbf{u}\right&gt;+\beta\left&lt;\mathbf{w},\mathbf{u}\right&gt;.
\end{align*}

</li>
<li class="ltx_item">
By the previous part and Proposition  @ref{ipdef} item 3, we have

\begin{align*}
\displaystyle \left&lt;\mathbf{u},\alpha\mathbf{v}+\beta\mathbf{w}\right&gt;=\left&lt;\alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right&gt;
\end{align*}

\begin{align*}
\displaystyle =\alpha\left&lt;\mathbf{v},\mathbf{u}\right&gt;+\beta\left&lt;\mathbf{w},\mathbf{u}\right&gt;=\alpha\left&lt;\mathbf{u},\mathbf{v}\right&gt;+\beta\left&lt;\mathbf{u},\mathbf{w}\right&gt;.
\end{align*}

Or we can also use ( @ref{ipdefe2}) (fill the detail).
</li>
<li class="ltx_item">
We compute

\begin{align*}
\displaystyle \left&lt;\mathbf{0},\mathbf{v}\right&gt;=0[\mathbf{v}]_{1}+\cdots+0[\mathbf{v}]_{m}=0
\end{align*}

and

\begin{align*}
\displaystyle \left&lt;\mathbf{v},\mathbf{0}\right&gt;=[\mathbf{v}]_{1}0+\cdots+[\mathbf{v}]_{m}0=0.
\end{align*}

</li>
<li class="ltx_item">
Suppose $\left&lt;\mathbf{v},\mathbf{x}\right&gt;=0$ for all $\mathbf{x}\in{\mathbb{R}}^{m}$. Let $\mathbf{x}=\mathbf{v}$. Then $\left&lt;\mathbf{v},\mathbf{v}\right&gt;=0$.
By Proposition  @ref{ipdef} item 4, $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
Suppose $\left&lt;\mathbf{v},\mathbf{x}\right&gt;=\left&lt;\mathbf{w},\mathbf{x}\right&gt;$, then $0=\left&lt;\mathbf{v},\mathbf{x}\right&gt;-\left&lt;\mathbf{w},\mathbf{x}\right&gt;=\left&lt;\mathbf{v}-\mathbf{w},\mathbf{x}\right&gt;$
for all $\mathbf{x}\in V$. By the previous part $\mathbf{v}-\mathbf{w}=\mathbf{0}$. So $\mathbf{v}=\mathbf{w}$.
</li>

</ol>

∎
@end

@defn
@title{Norm}
The <b>norm</b> (or <b>length</b>) of $\mathbf{v}\in{\mathbb{R}}^{n}$ is defined to be $\|\mathbf{v}\|=\sqrt{\left&lt;\mathbf{v},\mathbf{v}\right&gt;}$. Note that $\left&lt;\mathbf{v},\mathbf{v}\right&gt;\geq 0$. So the symbol $\sqrt{\left&lt;\mathbf{v},\mathbf{v}\right&gt;}$ is meaningful.
@end

@eg
Let $V={\mathbb{R}}^{3}$ with the standard inner product. Let

\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}1\\
2\\
3\end{bmatrix},\mathbf{w}=\begin{bmatrix}1\\
0\\
1\end{bmatrix}.
\end{align*}

Then

\begin{align*}
\displaystyle \|\mathbf{v}\|=\sqrt{\left&lt;\mathbf{v},\mathbf{v}\right&gt;}=\sqrt{1^{2}+2^{2}+3^{2}}=\sqrt{14}
\end{align*}

and

\begin{align*}
\displaystyle \|\mathbf{w}\|=\sqrt{\left&lt;\mathbf{w},\mathbf{w}\right&gt;}=\sqrt{1^{2}+0^{2}+1^{2}}=\sqrt{2}.
\end{align*}
@end

@proposition
Let $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{v}\in{\mathbb{R}}^{m}$.

<ol class="ltx_enumerate">
<li class="ltx_item">
$\|\mathbf{v}\|=0$ if and only if $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
$\|\alpha\mathbf{v}\|=|\alpha|\|\mathbf{v}\|$.
</li>
<li class="ltx_item">
Suppose that $\mathbf{v}\neq\mathbf{0}$ and let $\alpha=\frac{1}{\|\mathbf{v}\|}$. Then $\|\alpha\mathbf{v}\|=1$.
</li>

</ol>
@end

@proof
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
$\|\mathbf{v}\|=0\iff 0=\|\mathbf{v}\|^{2}=\left&lt;\mathbf{v},\mathbf{v}\right&gt;$.
By Proposition ( @ref{ipdef}), item 4, the above is true if and only if $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
$\|\alpha\mathbf{v}\|=\sqrt{\left&lt;\alpha\mathbf{v},\alpha\mathbf{v}\right&gt;}$
$=\sqrt{\alpha\left&lt;\mathbf{v},\alpha\mathbf{v}\right&gt;}=\sqrt{\alpha^{2}\left&lt;\mathbf{v},\mathbf{v}\right&gt;}$
$=|\alpha|\sqrt{\left&lt;\mathbf{v},\mathbf{v}\right&gt;}=|\alpha|\|\mathbf{v}\|$.
</li>
<li class="ltx_item">
By the previous part

\begin{align*}
\displaystyle \|\alpha\mathbf{v}\|=|\alpha|\|\mathbf{v}\|=\frac{1}{\|\mathbf{v}\|}\|\mathbf{v}\|=1.
\end{align*}

</li>

</ol>

∎
@end

@defn
@title{unit vector}
A vector $\mathbf{v}\in{\mathbb{R}}^{m}$ is said to be a <b>unit vector</b> if $\|\mathbf{v}\|=1$.A non-zero vector $\mathbf{v}$ can be <b>normalized</b> to a unit vector $\frac{\mathbf{v}}{\|\mathbf{v}\|}$ (see the previous proposition item 3).
@end

@eg
In Example 4, the vectors $\mathbf{v}$ and $\mathbf{w}$ can be normalized to

\begin{align*}
\displaystyle \frac{\mathbf{v}}{\|\mathbf{v}\|}=\frac{\mathbf{v}}{\sqrt{14}}=\begin{bmatrix}\frac{1}{\sqrt{14}}\\
\frac{2}{\sqrt{14}}\\
\frac{3}{\sqrt{14}}\end{bmatrix}
\end{align*}

and

\begin{align*}
\displaystyle \frac{\mathbf{w}}{\|\mathbf{w}\|}=\frac{\mathbf{w}}{\sqrt{2}}=\begin{bmatrix}\frac{1}{\sqrt{2}}\\
0\\
\frac{1}{\sqrt{2}}\end{bmatrix}
\end{align*}

respectively.
@end

@section{Orthogonal sets}

@defn
Two vectors $\mathbf{v}$ and $\mathbf{w}$ in $\mathbb{R}^{n}$ are said <b>orthogonal</b> or <b>perpendicular</b> if $\left&lt;\mathbf{v},\mathbf{w}\right&gt;=0$. In this case we write $\mathbf{v}\perp\mathbf{w}$.
@end
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
Let $V={\mathbb{R}}^{3}$. Then

\begin{align*}
\displaystyle \begin{bmatrix}1\\
2\\
3\end{bmatrix}\perp\begin{bmatrix}-1\\
-1\\
1\end{bmatrix},
\end{align*}

as

\begin{align*}
\displaystyle \left&lt;\begin{bmatrix}1\\
2\\
3\end{bmatrix},\begin{bmatrix}-1\\
-1\\
1\end{bmatrix}\right&gt;=1\times(-1)+2\times(-1)+3\times 1=0.
\end{align*}

</li>
<li class="ltx_item">
Let $V={\mathbb{R}}^{m}$. Then $\mathbf{e}_{i}\perp\mathbf{e}_{j}$ if $i\neq j$.
</li>

</ol>
@end
@defn
A subset $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ of ${\mathbb{R}}^{m}$ is said to be <b>orthogonal</b> if the following conditions hold:

<ol class="ltx_enumerate">
<li class="ltx_item">
$\mathbf{0}\notin S$, i.e. $\mathbf{v}_{i}\neq\mathbf{0}$ for $i=1,\ldots,k$.
</li>
<li class="ltx_item">
$\mathbf{v}_{i}\perp\mathbf{v}_{j}$ for $i\neq j$, i.e., $\left&lt;\mathbf{v}_{i},\mathbf{v}_{j}\right&gt;=0$ for $i\neq j$.
</li>

</ol>
@end
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
$S=\{\begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}-2\\
1\end{bmatrix}\}$ is orthogonal.
</li>
<li class="ltx_item">
$S=\{\begin{bmatrix}1\\
1\\
1\end{bmatrix},\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\}$ is orthogonal.
</li>
<li class="ltx_item">
For any $k\leq m$, the set $S=\{\mathbf{e}_{1},\mathbf{e}_{2},\ldots,\mathbf{e}_{k}\}\subset\mathbb{R}^{m}$ is orthogonal.
</li>

</ol>
@end
@proposition
Let $S=\{\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{k}\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$.
Let

\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},
\end{align*}

\begin{align*}
\displaystyle \mathbf{w}=\beta_{1}\mathbf{v}_{1}+\cdots+\beta_{k}\mathbf{v}_{k}.
\end{align*}

Then, for $\alpha_{i},\beta_{i}\in{\mathbb{R}}^{\hbox{}}$, $i=1,\ldots k$, we have

\begin{align*}
\displaystyle \left&lt;\mathbf{v},\mathbf{w}\right&gt;=\alpha_{1}\beta_{1}\|\mathbf{v}_{1}\|^{2}+\cdots+\alpha_{k}\beta_{k}\|\mathbf{v}_{k}\|^{2}.
\end{align*}
@end
@proof
@col
First for $1\leq i\leq k$, we compute

\begin{align*}
\displaystyle\left&lt;\mathbf{v},\mathbf{v}_{i}\right&gt;&amp;\displaystyle=\left&lt;\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right&gt; \\
&amp;\displaystyle=\alpha_{1}\left&lt;\mathbf{v}_{1},\mathbf{v}_{i}\right&gt;+\cdots+\alpha_{k}\left&lt;\mathbf{v}_{k},\mathbf{v}_{i}\right&gt; \\
&amp;\displaystyle=\alpha_{i}\left&lt;\mathbf{v}_{i},\mathbf{v}_{i}\right&gt;=\alpha_{i}\|\mathbf{v}_{i}\|^{2}.
\end{align*}

The last step follows from the fact that $\left&lt;\mathbf{v}_{j},\mathbf{v}_{i}\right&gt;=0$ for $j\neq i$. But then

\begin{align*}
\displaystyle\left&lt;\mathbf{v},\mathbf{w}\right&gt;&amp;\displaystyle=\left&lt;\mathbf{v},\beta_{1}\mathbf{v}_{1}+\cdots+\beta_{k}\mathbf{v}_{k}\right&gt; \\
&amp;\displaystyle=\beta_{1}\left&lt;\mathbf{v},\mathbf{v}_{1}\right&gt;+\cdots+\beta_{k}\left&lt;\mathbf{v},\mathbf{v}_{k}\right&gt; \\
&amp;\displaystyle=\alpha_{1}\beta_{1}\|\mathbf{v}_{1}\|^{2}+\cdots+\alpha_{k}\beta_{k}\|\mathbf{v}_{k}\|^{2}.
\end{align*}

∎
@end
@thm
@label{orthind}
Let $S=\{\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{k}\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$. Then $S$ is linearly independent.
@end
@proof
@col
Suppose that we have a relation of linear dependence:

\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\mathbf{0}.
\end{align*}

For $1\leq i\leq k$ we have

\begin{align*}
\displaystyle \left&lt;\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right&gt;=\left&lt;\mathbf{0},\mathbf{v}_{i}\right&gt;=0.
\end{align*}

i.e. for $1\leq i\leq k$,

\begin{align*}
\displaystyle\left&lt;\mathbf{v},\mathbf{v}_{i}\right&gt;&amp;\displaystyle=\left&lt;\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right&gt; \\
&amp;\displaystyle=\alpha_{1}\left&lt;\mathbf{v}_{1},\mathbf{v}_{i}\right&gt;+\cdots+\alpha_{k}\left&lt;\mathbf{v}_{k},\mathbf{v}_{i}\right&gt; \\
&amp;\displaystyle=\alpha_{i}\|\mathbf{v}_{i}\|^{2}=0.
\end{align*}

So for $1\leq i\leq k$ we have

\begin{align*}
\displaystyle \alpha_{i}=0.
\end{align*}

Therefore the relation of linear dependence is trivial. Hence $S$ is linearly independent.
∎
@end
@thm
@label{o}
Let $S=\{\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{k}\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$.
Suppose that $\mathbf{v}\in\left&lt;S\right&gt;$. Write

\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},
\end{align*}

for some $\alpha_{i}\in{\mathbb{R}}^{\hbox{}}$, $i=1,\ldots,k$.
Then

\begin{align*}
\displaystyle \alpha_{i}=\frac{\left&lt;\mathbf{v},\mathbf{v}_{i}\right&gt;}{\|\mathbf{v}_{i}\|^{2}},
\end{align*}

i.e.

\begin{align*}
\displaystyle \mathbf{v}=\frac{\left&lt;\mathbf{v},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}+\cdots+\frac{\left&lt;\mathbf{v},\mathbf{v}_{k}\right&gt;}{\|\mathbf{v}_{k}\|^{2}}\mathbf{v}_{k}.
\end{align*}
@end
@proof
@col
Suppose that $\mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}$. Then, for $1\leq i\leq k$, we compute

\begin{align*}
\displaystyle\left&lt;\mathbf{v},\mathbf{v}_{i}\right&gt;&amp;\displaystyle=\left&lt;\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right&gt; \\
&amp;\displaystyle=\alpha_{1}\left&lt;\mathbf{v}_{1},\mathbf{v}_{i}\right&gt;+\cdots+\alpha_{k}\left&lt;\mathbf{v}_{k},\mathbf{v}_{i}\right&gt; \\
&amp;\displaystyle=\alpha_{i}\left&lt;\mathbf{v}_{i},\mathbf{v}_{i}\right&gt;=\alpha_{i}\|\mathbf{v}_{i}\|^{2}.
\end{align*}

Hence

\begin{align*}
\displaystyle \alpha_{i}=\frac{\left&lt;\mathbf{v},\mathbf{v}_{i}\right&gt;}{\|\mathbf{v}_{i}\|^{2}}.
\end{align*}

∎
@end

<b>Remark</b>: The advantage of using the above method is that we don’t have to solve linear equations to find the linear combination. <b>Remark</b>: In order to use the theorem, we need to ensure that $\mathbf{v}\in\left&lt;S\right&gt;$.

@eg
We use Example 4, item 2. Let $S=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\}$.
Given that

\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}1\\
2\\
3\end{bmatrix}
\end{align*}

is in $\left&lt;S\right&gt;$, we find the following linear combinations:

\begin{align*}
\displaystyle \alpha_{1}=\frac{\left&lt;\mathbf{v},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}=\frac{6}{3}=2.
\end{align*}

\begin{align*}
\displaystyle \alpha_{2}=\frac{\left&lt;\mathbf{v},\mathbf{v}_{2}\right&gt;}{\|\mathbf{v}_{2}\|^{2}}=\frac{-1}{2}=-\frac{1}{2}.
\end{align*}

\begin{align*}
\displaystyle \alpha_{3}=\frac{\left&lt;\mathbf{v},\mathbf{v}_{3}\right&gt;}{\|\mathbf{v}_{3}\|^{2}}=\frac{-3}{6}=-\frac{1}{2}.
\end{align*}

Hence

\begin{align*}
\displaystyle \mathbf{v}=2\mathbf{v}_{1}-\frac{1}{2}\mathbf{v}_{2}-\frac{1}{2}\mathbf{v}_{3}.
\end{align*}
@end
@defn
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
A subset $S$ of $V$ is said to be an <b>orthogonal basis</b> for $V$ if $S$ is a basis of $V$ and $S$ is orthogonal.
@end

If $S$ is an orthogonal subset of $V$, then by Theorem  @ref{orthind}, it is automatically linearly independent.
So in order to check if $S$ is an orthogonal basis, we need only check that $\left&lt;S\right&gt;=V$. So we have the following result.

@thm
@label{checkob}
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
Suppose that $S$ is an orthogonal subset of $V$. Then $S$ is an orthogonal basis if and only if $\left&lt;S\right&gt;=V$.
@end
@corollary
Suppose that $S$ is an orthogonal subset of ${\mathbb{R}}^{m}$. Then $S$ is a basis of $\left&lt;S\right&gt;$.
@end
@corollary
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
Suppose that $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\}$ is an orthogonal basis of $V$. Then for any $\mathbf{v}\in V$, we have

\begin{align*}
\displaystyle \mathbf{v}=\frac{\left&lt;\mathbf{v},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}+\cdots+\frac{\left&lt;\mathbf{v},\mathbf{v}_{n}\right&gt;}{\|\mathbf{v}_{n}\|^{2}}\mathbf{v}_{n}
\end{align*}
@end
@proof
@col
This follows from Theorem  @ref{o}.
∎
@end
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
The set $S=\{\begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}-2\\
1\end{bmatrix}\}$
an orthogonal basis of ${\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
The set $S=\{\begin{bmatrix}1\\
1\\
1\end{bmatrix},\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\}$ is an orthogonal basis of ${\mathbb{R}}^{3}$.
Indeed, $\dim V=3$ and $S$, with 3 vectors, is linearly independent.
</li>
<li class="ltx_item">
The set $S=\{\mathbf{e}_{1},\mathbf{e}_{2},\ldots,\mathbf{e}_{m}\}$ is an orthogonal basis of ${\mathbb{R}}^{m}$.
It is called <b>the standard basis</b> for $V$.
</li>

</ol>
@end
@defn
A subset $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ of ${\mathbb{R}}^{m}$ is said to be <b>orthonormal</b>
if it is orthogonal and every vector in $S$ is a unit vector, i.e.

\begin{align*}
\displaystyle \left&lt;\mathbf{v}_{i},\mathbf{v}_{j}\right&gt;=\begin{cases}1&amp;\text{if $i=j$},\\
0&amp;\text{if $i\neq j$}.\end{cases}
\end{align*}

Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
The subset $S$ is said to be an <b>orthonormal basis</b> for $V$ if it is orthonormal and is a basis of $V$.
@end

Because an orthonormal set $S$ is orthogonal, the above theorems regarding orthogonal sets are also true for orthonormal sets.
In particular we have the following result.

@thm

Let $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ be an orthonormal subset of ${\mathbb{R}}^{m}$ and let $\mathbf{v}\in\left&lt;S\right&gt;$. Then

\begin{align*}
\displaystyle \mathbf{v}=\left&lt;\mathbf{v},\mathbf{v}_{1}\right&gt;\mathbf{v}_{1}+\cdots+\left&lt;\mathbf{v},\mathbf{v}_{k}\right&gt;\mathbf{v}_{k}.
\end{align*}
@end
@proof
@col
By Theorem  @ref{o} and $\|\mathbf{v}_{i}\|=1$ for $i=1,\ldots,k$.
∎
@end

If $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ is an orthogonal subset of ${\mathbb{R}}^{m}$, then
$\{\frac{\mathbf{v}_{1}}{\|\mathbf{v}_{1}\|},\ldots,\frac{\mathbf{v}_{k}}{\|\mathbf{v}_{k}\|}\}$
is an orthonormal subset. The process is called <b>normalization</b>.

@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
The set $S=\{\begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}-2\\
1\end{bmatrix}\}$ is
an orthogonal basis of ${\mathbb{R}}^{2}$. Normalizing it, we obtain an orthonormal basis

\begin{align*}
\displaystyle S^{\prime}=\{\frac{1}{\sqrt{5}}\begin{bmatrix}1\\
2\end{bmatrix},\frac{1}{\sqrt{5}}\begin{bmatrix}-2\\
1\end{bmatrix}\}.
\end{align*}

</li>
<li class="ltx_item">
The set $S=\{\begin{bmatrix}1\\
1\\
1\end{bmatrix},\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\}$ is an orthogonal basis of ${\mathbb{R}}^{3}$.
Normalizing it, we obtain an orthonormal basis

\begin{align*}
\displaystyle S^{\prime}=\{\frac{1}{\sqrt{3}}\begin{bmatrix}1\\
1\\
1\end{bmatrix},\frac{1}{\sqrt{2}}\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\frac{1}{\sqrt{6}}\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\}.
\end{align*}

</li>

</ol>
@end

@section{Gram-Schmidt Orthogonalization process}

Let $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$. If $\mathbf{w}\in\left&lt;S\right&gt;$, then

\begin{align*}
\displaystyle \mathbf{w}=\frac{\left&lt;\mathbf{w},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}+\cdots+\frac{\left&lt;\mathbf{w},\mathbf{v}_{k}\right&gt;}{\|\mathbf{v}_{k}\|^{2}}\mathbf{v}_{k}.
\end{align*}

But what if $\mathbf{w}$ is not in $\left&lt;S\right&gt;$? Let’s compare the difference. We have the following theorem.

@thm
@label{perp}
Let $S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$ and let $\mathbf{w}\in{\mathbb{R}}^{m}$. Then, for each $i=1,\ldots,k$, the vector

\begin{align*}
\displaystyle \mathbf{v}=\mathbf{w}-\frac{\left&lt;\mathbf{w},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\cdots-\frac{\left&lt;\mathbf{w},\mathbf{v}_{k}\right&gt;}{\|\mathbf{v}_{k}\|^{2}}\mathbf{v}_{k}
\end{align*}

is perpendicular to $\mathbf{v}_{i}$.
@end
@proof
@col
For $1\leq i\leq k$, we compute

\begin{align*}
\displaystyle \left&lt;\mathbf{v},\mathbf{v}_{i}\right&gt;=\left&lt;\mathbf{w},\mathbf{v}_{i}\right&gt;-\frac{\left&lt;\mathbf{w},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\left&lt;\mathbf{v}_{1},\mathbf{v}_{i}\right&gt;-\cdots-\frac{\left&lt;\mathbf{w},\mathbf{v}_{k}\right&gt;}{\|\mathbf{v}_{k}\|^{2}}\left&lt;\mathbf{v}_{k},\mathbf{v}_{i}\right&gt;.
\end{align*}

Because $\left&lt;\mathbf{v}_{j},\mathbf{v}_{i}\right&gt;$ is $0$ unless $j=i$, the above becomes

\begin{align*}
\displaystyle \left&lt;\mathbf{w},\mathbf{v}_{i}\right&gt;-\frac{\left&lt;\mathbf{w},\mathbf{v}_{i}\right&gt;}{\|\mathbf{v}_{i}\|^{2}}\left&lt;\mathbf{v}_{i},\mathbf{v}_{i}\right&gt;=\left&lt;\mathbf{w},\mathbf{v}_{i}\right&gt;-\left&lt;\mathbf{v},\mathbf{v}_{i}\right&gt;=0.
\end{align*}

Hence $\mathbf{v}\perp\mathbf{v}_{i}$ for $i=1,\ldots,k$.
∎
@end
@thm
@title{Gram-Schmidt Orthogonalization Process}
@label{GSO}
Let $S=\{\mathbf{w}_{1},\mathbf{w}_{2},\ldots,\mathbf{w}_{k}\}$ be a linearly independent subset of $V$.
Let $\mathbf{v}_{1}=\mathbf{w}_{1}$ and set

\begin{align*}
\displaystyle \mathbf{v}_{\ell}=\mathbf{w}_{\ell}-\frac{\left&lt;\mathbf{w}_{\ell},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\cdots-\frac{\left&lt;\mathbf{w}_{\ell},\mathbf{v}_{\ell-1}\right&gt;}{\|\mathbf{v}_{\ell-1}\|^{2}}\mathbf{v}_{\ell-1}\,\,\text{ for $2\leq\ell\leq k$}.
\end{align*}

Then $S^{\prime}=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ is an orthogonal set.Moreover, $\left&lt;\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell}\}\right&gt;=\left&lt;\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell}\}\right&gt;$ for $\ell=1,\ldots,k$.
In particular $\left&lt;S\right&gt;=\left&lt;S^{\prime}\right&gt;$. The process of obtaining $S^{\prime}$ by the above procedure is called the <b>Gram-Schmidt Orthogonalization process</b>.
@end
@proof
@col
We have $\left&lt;\{\mathbf{w}_{1}\}\right&gt;=\left&lt;\{\mathbf{v}_{1}\}\right&gt;$.
We are going to add one vector at a time. Suppose that $\left&lt;\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell-1}\}\right&gt;=\left&lt;\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell-1}\}\right&gt;$
and that the set $\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell-1}\}$ is orthogonal. Thus $\left&lt;\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell}\}\right&gt;=\left&lt;\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell-1},\mathbf{v}_{\ell}\}\right&gt;$
$=\left&lt;\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell-1},\mathbf{w}_{\ell}\}\right&gt;$. By Theorem  @ref{perp}, $\mathbf{v}_{\ell}\perp\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell-1}$. Thus $\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell}\}$ is orthogonal.
We repeat the process by increasing $\ell$ until $\ell=k$.
∎
@end
@corollary
Suppose that $V$ is a subspace of ${\mathbb{R}}^{m}$. Then there exists an orthogonal (orthonormal basis) of $V$.
@end
@proof
@col
By Lecture 18 Theorem 8, there exists a basis $S=\{\mathbf{w}_{1},\ldots,\mathbf{w}_{k}\}$ for $V$.
Applying Gram-Schmidt orthogonalization process to $S$, we obtain an orthogonal set $S^{\prime}=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$.
By Theorem  @ref{GSO}, $\left&lt;S^{\prime}\right&gt;=\left&lt;S\right&gt;=V$. By Theorem  @ref{checkob}, $S^{\prime}$ is an orthogonal basis. Normalizing $S^{\prime}$, we can also obtain an orthonormal basis.
∎
@end

The above proof actually describes a method to find orthogonal (orthonormal) basis of $V$.

@eg
Let $V={\mathbb{R}}^{4}$ with the standard inner product. Let

\begin{align*}
\displaystyle \mathbf{w}_{1}=\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix},\mathbf{w}_{2}=\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix},\mathbf{w}_{3}=\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}.
\end{align*}

Then $\{\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3}\}$ is linearly independent.
We can apply Gram-Schmidt orthogonalization process to this set of vectors. Take $\mathbf{v}_{1}=\mathbf{w}_{1}=\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix}$. Then

\begin{align*}
\displaystyle \mathbf{v}_{2}=\mathbf{w}_{2}-\frac{\left&lt;\mathbf{w}_{2},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix}-\frac{2}{2}\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix}=\begin{bmatrix}0\\
1\\
0\\
1\end{bmatrix}.
\end{align*}

Also

\begin{align*}
\displaystyle \mathbf{v}_{3}=\mathbf{w}_{3}-\frac{\left&lt;\mathbf{w}_{3},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\frac{\left&lt;\mathbf{w}_{3},\mathbf{v}_{2}\right&gt;}{\|\mathbf{v}_{2}\|^{2}}\mathbf{v}_{2}=\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}-\frac{2}{2}\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix}-\frac{2}{2}\begin{bmatrix}0\\
1\\
0\\
1\end{bmatrix}=\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix}.
\end{align*}

The set $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$ is an orthogonal basis of $\left&lt;\{\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3}\}\right&gt;$.
To obtain an orthonormal basis of $\left&lt;\{\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3}\}\right&gt;$, we can normalized the vectors

\begin{align*}
\displaystyle \mathbf{u}_{1}=\frac{\mathbf{v}_{1}}{\|\mathbf{v}_{1}\|}=\frac{1}{\sqrt{2}}\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix},\mathbf{u}_{2}=\frac{\mathbf{v}_{2}}{\|\mathbf{v}_{2}\|}=\frac{1}{\sqrt{2}}\begin{bmatrix}0\\
1\\
0\\
1\end{bmatrix},\mathbf{u}_{3}=\frac{\mathbf{v}_{3}}{\|\mathbf{v}_{3}\|}=\frac{1}{\sqrt{2}}\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix}.
\end{align*}
@end
@eg
Let $V={\mathcal{N}}\!\left(\begin{bmatrix}1&amp;1&amp;1&amp;1\end{bmatrix}\right)$. Find an orthonormal basis of $V$. The set

\begin{align*}
\displaystyle S=\{\mathbf{w}_{1}=\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\mathbf{w}_{2}=\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix},\mathbf{w}_{3}=\begin{bmatrix}-1\\
0\\
0\\
1\end{bmatrix}\}
\end{align*}

is a basis of $V$. We apply Gram-Schmidt orthogonalization process to the set $S$:

\begin{align*}
\displaystyle \mathbf{v}_{1}=\mathbf{w}_{1}=\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix}
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}_{2}=\mathbf{w}_{2}-\frac{\left&lt;\mathbf{w}_{2},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}=\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix}-\frac{1}{2}\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix}=\begin{bmatrix}-1/2\\
-1/2\\
1\\
0\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}_{3}=\mathbf{w}_{3}-\frac{\left&lt;\mathbf{w}_{3},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\frac{\left&lt;\mathbf{w}_{3},\mathbf{v}_{2}\right&gt;}{\|\mathbf{v}_{2}\|^{2}}\mathbf{v}_{2}=\begin{bmatrix}-1\\
0\\
0\\
1\end{bmatrix}-\frac{1}{2}\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix}-\frac{1}{3}\begin{bmatrix}-1/2\\
-1/2\\
1\\
0\end{bmatrix}=\begin{bmatrix}-1/3\\
-1/3\\
-1/3\\
1\end{bmatrix}.
\end{align*}

So

\begin{align*}
\displaystyle \{\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\begin{bmatrix}-1/2\\
-1/2\\
1\\
0\end{bmatrix},\begin{bmatrix}-1/3\\
-1/3\\
-1/3\\
1\end{bmatrix}\}.
\end{align*}

is an orthogonal basis of $V$. Normalizing it, we can obtain an orthonormal basis of $V$:

\begin{align*}
\displaystyle \{\frac{1}{\sqrt{2}}\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\frac{1}{\sqrt{6}}\begin{bmatrix}-1\\
-1\\
2\\
0\end{bmatrix},\frac{1}{\sqrt{12}}\begin{bmatrix}-1\\
-1\\
-1\\
3\end{bmatrix}\}.
\end{align*}

The above process will be easier if we start with another basis:

\begin{align*}
\displaystyle S=\{\mathbf{w}_{1}=\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix},\mathbf{w}_{2}=\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix},\mathbf{w}_{3}=\begin{bmatrix}0\\
1\\
-1\\
0\end{bmatrix}\}
\end{align*}

Now the first two vectors are perpendicular. Apply Gram-Schmidt orthogonalization process to it:

\begin{align*}
\displaystyle \mathbf{v}_{1}=\mathbf{w}_{1}=\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}_{2}=\mathbf{w}_{2}-\frac{\left&lt;\mathbf{w}_{2},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}=\mathbf{w}_{2}-0\mathbf{v}_{1}=\mathbf{w}_{2}=\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix}.
\end{align*}

\begin{align*}
\displaystyle \mathbf{v}_{3}=\mathbf{w}_{3}-\frac{\left&lt;\mathbf{w}_{3},\mathbf{v}_{1}\right&gt;}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\frac{\left&lt;\mathbf{w}_{3},\mathbf{v}_{2}\right&gt;}{\|\mathbf{v}_{2}\|^{2}}\mathbf{v}_{2}=\begin{bmatrix}1/2\\
1/2\\
-1/2\\
-1/2\end{bmatrix}.
\end{align*}

So

\begin{align*}
\displaystyle \{\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix},\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix},\begin{bmatrix}1/2\\
1/2\\
-1/2\\
-1/2\end{bmatrix}\}
\end{align*}

is an orthogonal basis of $V$. Normalizing it, we obtain an orthonormal basis

\begin{align*}
\displaystyle \{\frac{1}{\sqrt{2}}\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix},\frac{1}{\sqrt{2}}\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix},\begin{bmatrix}1/2\\
1/2\\
-1/2\\
-1/2\end{bmatrix}\}.
\end{align*}
@end

@section{Cacuhy-Schwarz inequality}

<b>Can be skipped, will not appear in final exam</b>

@thm
@title{Cauchy-Scharwz inequality}
For $\mathbf{v},\mathbf{w}\in{\mathbb{R}}^{m}$,

\begin{align*}
\displaystyle |\left&lt;\mathbf{v},\mathbf{w}\right&gt;|\leq\|\mathbf{v}\|\|\mathbf{w}\|.
\end{align*}
@end
@proof
@col
The statement is trivial if $\mathbf{w}=\mathbf{0}$. Suppose $\mathbf{w}\neq\mathbf{0}$. Let $t\in{\mathbb{R}}^{\hbox{}}$, then

\begin{align*}
\displaystyle 0\leq\|\mathbf{v}-t\mathbf{w}\|^{2}=\left&lt;\mathbf{v}-t\mathbf{w},\mathbf{v}-t\mathbf{w}\right&gt;=\left&lt;\mathbf{v},\mathbf{v}-t\mathbf{w}\right&gt;-t\left&lt;\mathbf{w},\mathbf{v}-t\mathbf{w}\right&gt;
\end{align*}

\begin{align*}
\displaystyle =\left&lt;\mathbf{v},\mathbf{v}\right&gt;-t\left&lt;\mathbf{v},\mathbf{w}\right&gt;-t\left&lt;\mathbf{w},\mathbf{v}\right&gt;+t^{2}\left&lt;\mathbf{w},\mathbf{w}\right&gt;=\left&lt;\mathbf{v},\mathbf{v}\right&gt;-2t\left&lt;\mathbf{v},\mathbf{w}\right&gt;+t^{2}\left&lt;\mathbf{w},\mathbf{w}\right&gt;
\end{align*}

Substituting

\begin{align*}
\displaystyle t=\frac{\left&lt;\mathbf{v},\mathbf{w}\right&gt;}{\left&lt;\mathbf{w},\mathbf{w}\right&gt;}
\end{align*}

into the above, we obtain

\begin{align*}
\displaystyle 0\leq\left&lt;\mathbf{v},\mathbf{v}\right&gt;-\frac{|\left&lt;\mathbf{v},\mathbf{w}\right&gt;|^{2}}{\left&lt;\mathbf{w},\mathbf{w}\right&gt;}=\|\mathbf{v}\|^{2}-\frac{|\left&lt;\mathbf{v},\mathbf{w}\right&gt;|^{2}}{\|\mathbf{w}\|^{2}}.
\end{align*}

Hence

\begin{align*}
\displaystyle |\left&lt;\mathbf{v},\mathbf{w}\right&gt;|\leq\sqrt{\|\mathbf{v}\|^{2}\|\mathbf{w}\|^{2}}=\|\mathbf{v}\|\|\mathbf{w}\|.
\end{align*}

∎
@end

<b>Remark 1</b>: The $t$ above is obtained by minimizing the quadratic equation
$\left&lt;\mathbf{v},\mathbf{v}\right&gt;-2t\left&lt;\mathbf{v},\mathbf{w}\right&gt;+t^{2}\left&lt;\mathbf{w},\mathbf{w}\right&gt;$. <b>Remark 2</b>: Following the proof, the equality occurs if (i) $\mathbf{v}=\mathbf{0}$ or (ii) $\mathbf{w}=\mathbf{0}$
or (iii) $\mathbf{v}-t\mathbf{w}=0$ $\Leftrightarrow$ $\mathbf{v}$ and $\mathbf{w}$ are parallel,
i.e., $\mathbf{v}=\alpha\mathbf{w}$ for some scalar $\alpha\in{\mathbb{R}}^{\hbox{}}$.

@thm
@title{Triangle inequality}
\begin{align*}
\displaystyle \|\mathbf{v}+\mathbf{w}\|\leq\|\mathbf{v}\|+\|\mathbf{w}\|.
\end{align*}
@end
@proof
@col
\begin{align*}
\displaystyle \|\mathbf{v}+\mathbf{w}\|^{2}=\left&lt;\mathbf{v},\mathbf{v}\right&gt;+\left&lt;\mathbf{v},\mathbf{w}\right&gt;+\left&lt;\mathbf{w},\mathbf{v}\right&gt;+\left&lt;\mathbf{w},\mathbf{w}\right&gt;=\|\mathbf{v}\|^{2}+2\left&lt;\mathbf{v},\mathbf{w}\right&gt;+\|\mathbf{w}\|^{2}.
\end{align*}

By the Cauchy-Schwarz inequality

\begin{align*}
\displaystyle |\left&lt;\mathbf{v},\mathbf{w}\right&gt;|\leq\|\mathbf{v}\|\|\mathbf{w}\|,
\end{align*}

thus

\begin{align*}
\displaystyle \|\mathbf{v}+\mathbf{w}\|^{2}\leq\|\mathbf{v}\|^{2}+2\|\mathbf{v}\|\|\mathbf{w}\|+\|\mathbf{w}\|^{2}=(\|\mathbf{v}\|+\|\mathbf{w}\|)^{2}.
\end{align*}

The result follows by taking square roots on both sides.
∎
@end
@eg
Let $\mathbf{v},\mathbf{w}\in{\mathbb{R}}^{m}$ with the standard inner product. Let

\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{m}\end{bmatrix}\qquad\mathbf{w}=\begin{bmatrix}w_{1}\\
\vdots\\
w_{m}\end{bmatrix}
\end{align*}

Cauchy-Schwarz inequality:

\begin{align*}
\displaystyle |v_{1}w_{1}+\cdots+v_{m}w_{m}|\leq\sqrt{v_{1}^{2}+\cdots+v_{m}^{2}}\sqrt{w_{1}^{2}+\cdots+w_{m}^{2}}.
\end{align*}

Triangle inequality:

\begin{align*}
\displaystyle \sqrt{(v_{1}+w_{1})^{2}+\cdots+(v_{m}+w_{m})^{2}}\leq\sqrt{v_{1}^{2}+\cdots+v_{m}^{2}}+\sqrt{w_{1}^{2}+\cdots+w_{m}^{2}}.
\end{align*}
@end
