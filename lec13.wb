@course{Math 1030}
@setchapter{13}
@chapter{Linear Dependence and Span}
<h5 class="notkw">Reference.</h5>
<ul>
<li>
Beezer, Ver 3.5 Section LDS (print version p105 - p113)
</li>
<li>
Strang, Sect 2.3
</li>
</ul>
<h5 class="notkw">Exercise</h5>
<ul>
<li>
Exercises with solutions can be downloaded at
@href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
Section LI (p.48-51) (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions)
C20, C40, C50, C51, C52, C55, C70, M10, T40.
</li>
<li>
Strang, Sect 2.3
</li>
</ul>
@section{Linearly Dependent Sets and Spans}
If we use a linearly dependent set to construct a span, then we can always create the same infinite set by starting with a set that is one vector smaller in size. We will illustrate this behaviour in @ref{RSC5}. However, this will not be possible if we build a span from a linearly independent set. So, in a certain sense, using a linearly independent set to formulate a span is the best possible way â€“ there are no any extra vectors being used to build up all the necessary linear combinations. OK, here is the theorem, and then the example.
@slide
@thm
@title{Dependency in Linearly Dependent Sets}
@label{DLDS}
Suppose that $S=\left\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\right\}$ is a set of vectors. Then $S$ is a linearly dependent set if and only if there is an index $t$, $1\leq t\leq n$, such that $\mathbf{u_{t}}$ is a linear combination of the vectors $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{t-1},\,\mathbf{u}_{t+1},\,\ldots,\,\mathbf{u}_{n}$.
@end
@proof

($\Rightarrow$)
@newcol
Suppose that $S$ is linearly dependent. Then there exists a nontrivial relation of linear dependence (Lecture 12 Definition 1). That is, there are scalars, $\alpha_{i}$, $1\leq i\leq n$, not all of which are zero, such that
\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{0}.
\end{align*}
Suppose that $\alpha_{t}$ is nonzero. Then,
\begin{align*}
\displaystyle\mathbf{u}_{t}&\displaystyle=\frac{-1}{\alpha_{t}}\left(-\alpha_{t}\mathbf{u}_{t}\right) \\
&\displaystyle=\frac{-1}{\alpha_{t}}\left(\alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{t-1}\mathbf{u}_{t-1}+\alpha_{t+1}\mathbf{u}_{t+1}+\cdots+\alpha_{n}\mathbf{u}_{n}\right) \\
&\displaystyle=\frac{-\alpha_{1}}{\alpha_{t}}\mathbf{u}_{1}+\cdots+\frac{-\alpha_{t-1}}{\alpha_{t}}\mathbf{u}_{t-1}+\frac{-\alpha_{t+1}}{\alpha_{t}}\mathbf{u}_{t+1}+\cdots+\frac{-\alpha_{n}}{\alpha_{t}}\mathbf{u}_{n}.
\end{align*}
Since $\frac{\alpha_{i}}{\alpha_{t}}$ is again a scalar, we have expressed $\mathbf{u}_{t}$ as a linear combination of the other elements of $S$.
@endcol

($\Leftarrow$)
@newcol
Assume that the vector $\mathbf{u}_{t}$ is a linear combination of the other vectors in $S$. Write such a linear combination as
\begin{align*}
\displaystyle\mathbf{u_{t}}&\displaystyle=\beta_{1}\mathbf{u}_{1}+\beta_{2}\mathbf{u}_{2}+\cdots+\beta_{t-1}\mathbf{u}_{t-1}+\beta_{t+1}\mathbf{u}_{t+1}+\cdots+\beta_{n}\mathbf{u}_{n}.
\end{align*}
@col
Then we have
\begin{align*}
\displaystyle\beta_{1}\mathbf{u}_{1}&\displaystyle+\cdots+\beta_{t-1}\mathbf{u}_{t-1}+(-1)\mathbf{u}_{t}+\beta_{t+1}\mathbf{u}_{t+1}+\cdots+\beta_{n}\mathbf{u}_{n} \\
&\displaystyle=\mathbf{u}_{t}+(-1)\mathbf{u}_{t} \\
&\displaystyle=\left(1+\left(-1\right)\right)\mathbf{u}_{t} \\
&\displaystyle=0\mathbf{u}_{t} \\
&\displaystyle=\mathbf{0}.
\end{align*}
@col
So the scalars $\beta_{1},\,\beta_{2},\,\beta_{3},\,\ldots,\,\beta_{t-1},\,\beta_{t}=-1,\beta_{t+1},\,\,\ldots,\,\beta_{n}$ provide a nontrivial relation of linear dependence of the vectors in $S$, thus establishing that $S$ is a linearly dependent set.
@endcol
@qed
@end
@col
This theorem can be used, sometimes repeatedly, to whittle down the size of a set of vectors used in a span construction.
In the next example we will examine some of the subtleties.
@slide
@eg
@label{RSC5}
<b>Reducing the generating set of a span in ${\mathbb{R}}^{5}$</b>

Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$,
\begin{align*}
\displaystyle R=
\left\{
\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\mathbf{v}_{4}\right\}
=\left\{\begin{bmatrix}1\\
2\\
-1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}0\\
-7\\
6\\
-11\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
1\\
2\\
1\\
6\end{bmatrix}\right\}.\\
\end{align*}
Define $V=\left<  R\right>$.



@col
We form a $5\times 4$ matrix, $D$, and row-reduce it to understand the solutions to the homogeneous system $\mathcal{LS}({D},{\mathbf{0}})$:
\begin{align*}
\displaystyle D=\begin{bmatrix}1&2&0&4\\
2&1&-7&1\\
-1&3&6&2\\
3&1&-11&1\\
2&2&-2&6\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&4\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&1\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
We can find infinitely many solutions to the system $\mathcal{LS}({D},{\mathbf{0}})$, most of which are nontrivial. Choose any nontrivial solution to build a nontrivial relation of linear dependence on $R$. Let us begin with $x_{4}=1$, to find the solution
\begin{align*}
\displaystyle \begin{bmatrix}-4\\
0\\
-1\\
1\end{bmatrix}.
\end{align*}
@col
The corresponding relation of linear dependence is
\begin{align*}
\displaystyle (-4)\mathbf{v}_{1}+0\mathbf{v}_{2}+(-1)\mathbf{v}_{3}+1\mathbf{v}_{4}=\mathbf{0}.
\end{align*}
@col
The theorem above guarantees that we can solve this relation of linear dependence for some vector in $R$, but the choice of which one is up to us. Notice however that $\mathbf{v}_{2}$ has a zero coefficient. In this case, we cannot choose to solve for $\mathbf{v}_{2}$. Maybe some other relation of linear dependence would produce a nonzero coefficient for $\mathbf{v}_{2}$ if we just had to solve for this vector. Unfortunately, this example has been engineered to always produce a zero coefficient here, as you can see from solving the homogeneous system. Every solution has $x_{2}=0$!




OK, if we are convinced that we cannot solve for $\mathbf{v}_{2}$, let us instead solve for $\mathbf{v}_{3}$:
\begin{align*}
\displaystyle \mathbf{v}_{3}=(-4)\mathbf{v}_{1}+0\mathbf{v}_{2}+1\mathbf{v}_{4}=(-4)\mathbf{v}_{1}+1\mathbf{v}_{4}
\end{align*}
@col
We claim that this particular equation will allow us to write
\begin{align*}
\displaystyle V=\left< R\right>=\left< \left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\mathbf{v}_{4}\right\}\right>=\left< \left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{4}\right\}\right>,
\end{align*}
in essence declaring $\mathbf{v}_{3}$ as surplus for the task of building $V$ as a span of $R$. This claim is an equality of two sets. Let $R^{\prime}=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{4}\right\}$ and $V^{\prime}=\left< R^{\prime}\right>$. We want to show that $V=V^{\prime}$.


First show that $V^{\prime}\subseteq V$. Since every vector of $R^{\prime}$ is in $R$, any vector we can construct in $V^{\prime}$ as a linear combination of vectors from $R^{\prime}$ can also be constructed as a vector in $V$ by the same linear combination of the same vectors in $R$. That was easy, now turn it around.

@col
Next show that $V\subseteq V^{\prime}$. Choose any $\mathbf{v}$ from $V$. So there are scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4}$ such that
\begin{align*}
\displaystyle\mathbf{v}&\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\mathbf{v}_{3}+\alpha_{4}\mathbf{v}_{4} \\
&\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\left((-4)\mathbf{v}_{1}+1\mathbf{v}_{4}\right)+\alpha_{4}\mathbf{v}_{4} \\
&\displaystyle=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\left((-4\alpha_{3})\mathbf{v}_{1}+\alpha_{3}\mathbf{v}_{4}\right)+\alpha_{4}\mathbf{v}_{4} \\
&\displaystyle=\left(\alpha_{1}-4\alpha_{3}\right)\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\left(\alpha_{3}+\alpha_{4}\right)\mathbf{v}_{4}.
\end{align*}
@col
This equation says that $\mathbf{v}$ can be written as a linear combination of the vectors in $R^{\prime}$ and hence qualifies for membership in $V^{\prime}$. So $V\subseteq V^{\prime}$ and we have established that $V=V^{\prime}$.


If $R^{\prime}$ was also linearly dependent (in fact, it is not), we could reduce the set $R^{\prime}$ even further. Notice that we could have chosen to eliminate any one of $\mathbf{v}_{1}$, $\mathbf{v}_{3}$ or $\mathbf{v}_{4}$, but somehow $\mathbf{v}_{2}$ is essential to the creation of $V$ since it cannot be replaced by any linear combination of $\mathbf{v}_{1}$, $\mathbf{v}_{3}$ or $\mathbf{v}_{4}$.
@end
@section{
Casting Out Vectors}
@label{COV}
In @ref{RSC5} we used four vectors to create a span. With a relation of linear dependence in hand, we were able to toss out one of these four vectors and create the same span from a subset of just three of the original set of four vectors. We did have to take some care as to just which vector we tossed out. In the next example, we will be more methodical about just how we choose to eliminate vectors from a linearly dependent set while preserving a span.
@slide
@label{slide:canonicalbasis}
In $\mathbb{R}^m$,
for $i = 1, 2, \ldots, m$, let:
\[
\vec{e}_1 = \colvector{1\\0\\0\\\vdots\\0},\;
\vec{e}_2 = \colvector{0\\1\\0\\\vdots\\0},
\;\cdots\;,\;
\vec{e}_m = \colvector{0\\0\\0\\\vdots\\1}.
\]
That is:
\[
\left[\vec{e}_m\right]_j = \begin{cases}
1 & \text{ if } j = m;\\
0 & \text{ otherwise.}
\end{cases}
\]
@col
Observe that every vector $\vec{v} = \colvector{v_1\\v_2\\ \vdots \\ v_m} \in \mathbb{R}^m$
lies in the span of $\{\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_m\}$, since:

@col
\[
\colvector{v_1\\v_2\\ \vdots \\ v_m} = v_1 \colvector{1\\0\\0\\\vdots\\0}
+ v_2 \colvector{0\\1\\0\\\vdots\\0} + \cdots + v_m \colvector{0\\0\\0\\\vdots\\1}.
\]
@col
Moreover, for any positive integer $r < n$,
and any vector $\vec{v} \in \mathbb{R}^m$ of the form:
\[
\vec{v} = \colvector{v_1\\v_2\\ \vdots \\ v_r \\ 0 \\0 \\ \vdots \\0},
\]

@col
we have:
\[
\vec{v} \in \left\langle \{\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_r\}\right\rangle.
\]

@col
<strong>Exercise.</strong>
Notice also that the vectors: $\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_r$ are linearly independent.
@endcol
@slide
@eg
@label{eg:COV}
<b>Casting out vectors</b>
@col
Consider now the following set $S$ of $n = 7$ vectors in ${\mathbb{R}}^{4}$:
\begin{align*}
\displaystyle S=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\,\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\,\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\,\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\,\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\,\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\right\}.
\end{align*}
@col
By @ref{8fc8ebb550bd42bcda3fb034a92efbd1},
the set $S$ is obviously linearly dependent, since we have $n = 7$ vectors in ${\mathbb{R}}^{4}$. So, we can slim down $S$ some and express the subspace
$\left\langle S \right\rangle$ as the span of a smaller set of vectors.

We would like to know:

<strong>
What's the smallest subset $S'$ of $S$ such that
$\left\langle S' \right\rangle = \left\langle S \right\rangle$?
</strong>

@col
Consider the matrix $A$ whose columns consist of the vectors in $S$:
\begin{align*}
\displaystyle A=\left[\mathbf{A}_{1}|\mathbf{A}_{2}|\mathbf{A}_{3}|\ldots|\mathbf{A}_{7}\right]=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}.
\end{align*}
The matrix $A$ is row-equivalent to the RREF matrix:
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}.
\end{align*}
In this case, the rank (i.e. the number of non-zero rows) of $B$ is $r = 3$.
The pivot columns of $B$ are $\mathbf{B}_1, \mathbf{B}_3, \mathbf{B}_4$.

@col
<ol class="ltx_enumerate">
<li class="ltx_item">
<strong>
$\displaystyle \mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$ are linearly independent.
</strong>
@col
The pivot columns of $B$ are precisely the vectors:
\begin{align*}
\mathbf{B}_1 &= \vec{e}_1, & \mathbf{B}_3 &= \vec{e}_2, & \mathbf{B}_4 &= \vec{e}_3.
\end{align*}
In particular, they are linearly independent.

@col
We claim that the corresponding columns of $A$
(namely $\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$) are also linearly independent.
The reason is as follows:

@col
The augmented matrix:
\[
B' = [\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4] =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
\]
is an RREF matrix which is row-equivalent to the augmented matrix:
\[
A' = [\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4].
\]
It now follows from @ref{60dd89e0db5ce8bae68e89cd2a2330de}
that the columns of $A'$ are linearly independent.
</li>
<li class="ltx_item">
<strong>
The span of $S$ is equal to
$\displaystyle
\left\langle\{\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4\}\right\rangle.
$
</strong>
@col
First, notice that every column of $B$ is a vector in $\mathbb{R}^4$.
Moreover, since the rank of $B$ is $r = 3$,
the $4$-th entry of each such column vector is zero.

@col
By the observations made earlier, we have:
\[
\mathbf{B}_i \in \left\langle \underbrace{\mathbf{B}_1}_{\vec{e}_1},
\underbrace{\mathbf{B}_3}_{\vec{e}_2}, \underbrace{\mathbf{B}_4}_{\vec{e}_3}
\right\rangle
\]
for $i = 1, 2, \ldots, 7$.

@col
In other words, for any $1 \leq i \leq 7$, the vector equation:
\[
[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\vec{x} = \mathbf{B}_i
\]
has a solution $\vec{x} \in \mathbb{R}^3$.
<!-- or equivalently the linear system:
\[
\mathcal{LS}(B', \mathbf{B}_i)
\]
is consistent.
-->

@col
On the other hand, the augmented matrix $[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4| \mathbf{B}_i]$ is row-equivalent to
$[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4| \mathbf{A}_i]$, which implies that any solution to 
$[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\vec{x} = \mathbf{B}_i$
is also a solution to 
$[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4]\vec{x} = \mathbf{A}_i$.

@col
For example, we have:
\[
[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\colvector{2\\1\\2}
=
2\mathbf{B}_1 + \mathbf{B}_3 + 2 \mathbf{B}_4
= \colvector{2\\1\\2\\0}
= \mathbf{B}_5
\]
which implies that:
\[
\vec{x} = \colvector{2\\1\\2}
\]
is a solution to 
$[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\vec{x} = \mathbf{B}_5$
and hence also a solution to
$[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4]\vec{x} = \mathbf{A}_5$.
Indeed:

@col
\[
[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4]\colvector{2\\1\\2} =
2\mathbf{A}_1 + \mathbf{A}_3 + 2 \mathbf{A}_4 = \colvector{0\\9\\-4\\8}
= \mathbf{A}_5
\]
In particular, $\mathbf{A}_5$ lies in the span of
$\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$.

@col
It now follows, since every $\mathbf{B}_i$ is in the span of the
$\mathbf{B}_1, \mathbf{B}_3, \mathbf{B}_4$, that
every $\mathbf{A}_i$ lies in the span of $\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$.

@col
Hence,
the span of $S$ is equal to the span of the vectors:
\[
\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4.
\]
</li>
</ol>
@end
@slide
The previous example motivates the following fundamental theorem:
@thm
@title{Basis of a Span}
@label{BS}
Suppose that $S=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\right\}$ is a set of column vectors. Define $W=\left< S\right>$ and let $A$ be the matrix whose columns are the vectors from $S$. Let $B$ be the reduced row-echelon form of $A$, with $D=\left\{{d}_{1},\,{d}_{2},\,{d}_{3},\,\ldots,\,{d}_{r}\right\}$ the set of indices for the pivot columns of $B$. Then
<ol class="ltx_enumerate">
<li class="ltx_item">
$T=\left\{\mathbf{v}_{d_{1}},\,\mathbf{v}_{d_{2}},\,\mathbf{v}_{d_{3}},\,\ldots\,\mathbf{v}_{d_{r}}\right\}$ is a linearly independent set.
</li>
<li class="ltx_item">
$W=\left< T\right>$.
</li>
</ol>
@end
@proof
<strong>Try to understand the example and skip the proof for now</strong>

@col
To prove that $T$ is linearly independent, begin with a relation of linear dependence on $T$,
\begin{align*}
\displaystyle \mathbf{0}=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\alpha_{3}\mathbf{v}_{d_{3}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}.
\end{align*}
@col
We will try to conclude that the only possibility for the scalars $\alpha_{i}$ is that they are all zero.
Denote the non-pivot columns of $B$ by $F=\left\{{f}_{1},\,{f}_{2},\,{f}_{3},\,\ldots,\,{f}_{n-r}\right\}$. Then we can preserve the equality by adding a big fat zero to the linear combination:
\begin{align*}
\displaystyle \mathbf{0}=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\alpha_{3}\mathbf{v}_{d_{3}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+0\mathbf{v}_{f_{1}}+0\mathbf{v}_{f_{2}}+0\mathbf{v}_{f_{3}}+\ldots+0\mathbf{v}_{f_{n-r}}.
\end{align*}
@col
The scalars in this linear combination (suitably reordered) are a solution to the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Notice that this is the solution obtained by setting each free variable to zero. In the case of a homogeneous system, we see that if all of the free variables are set to zero, then the resulting solution vector is trivial (all zeros). So it must be that $\alpha_{i}=0$, $1\leq i\leq r$. This implies, by the definition of linear independence, that $T$ is a linearly independent set.


@col
The second conclusion of this theorem is an equality of sets. Since $T$ is a subset of $S$, any linear combination of elements of the set $T$ can also be viewed as a linear combination of elements of the set $S$. So $\left< T\right>\subseteq\left< S\right>=W$. It remains to prove that $W=\left< S\right>\subseteq\left< T\right>$.


For each $k$, $1\leq k\leq n-r$, form a solution $\mathbf{x}$ to $\mathcal{LS}({A},{\mathbf{0}})$ by setting the free variables as follows:
\begin{align*}
\displaystyle x_{f_{1}}&\displaystyle=0&\displaystyle x_{f_{2}}&\displaystyle=0&\displaystyle x_{f_{3}}&\displaystyle=0&\displaystyle\ldots&\displaystyle x_{f_{k}}&\displaystyle=1&\displaystyle\ldots&\displaystyle x_{f_{n-r}}&\displaystyle=0.
\end{align*}
@col
The remainder of this solution vector is given by
\begin{align*}
\displaystyle x_{d_{1}}&\displaystyle=-\left[B\right]_{1,f_{k}}&\displaystyle x_{d_{2}}&\displaystyle=-\left[B\right]_{2,f_{k}}&\displaystyle x_{d_{3}}&\displaystyle=-\left[B\right]_{3,f_{k}}&\displaystyle\ldots&\displaystyle x_{d_{r}}&\displaystyle=-\left[B\right]_{r,f_{k}}.
\end{align*}
From this solution, we obtain a relation of linear dependence on the columns of $A$,
\begin{align*}
\displaystyle -\left[B\right]_{1,f_{k}}\mathbf{v}_{d_{1}}-\left[B\right]_{2,f_{k}}\mathbf{v}_{d_{2}}-\left[B\right]_{3,f_{k}}\mathbf{v}_{d_{3}}-\ldots-\left[B\right]_{r,f_{k}}\mathbf{v}_{d_{r}}+1\mathbf{v}_{f_{k}}=\mathbf{0},
\end{align*}
which can be arranged to the equality
\begin{align*}
\displaystyle \mathbf{v}_{f_{k}}=\left[B\right]_{1,f_{k}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{k}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{k}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{k}}\mathbf{v}_{d_{r}}.
\end{align*}
Now, suppose we take an arbitrary element $\mathbf{w}$ of $W=\left< S\right>$ and write it as a linear combination of the elements of $S$, but with the terms organized according to the indices in $D$ and $F$:
\begin{align*}
\displaystyle\mathbf{w}&\displaystyle=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+\beta_{1}\mathbf{v}_{f_{1}}+\beta_{2}\mathbf{v}_{f_{2}}+\ldots+\beta_{n-r}\mathbf{v}_{f_{n-r}}
\end{align*}
From the above, we can replace each $\mathbf{v}_{f_{j}}$ by a linear combination of the $\mathbf{v}_{d_{i}}$:
\begin{align*}
\displaystyle\mathbf{w}&\displaystyle=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+ \\
&\displaystyle\beta_{1}\left(\left[B\right]_{1,f_{1}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{1}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{1}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{1}}\mathbf{v}_{d_{r}}\right)+ \\
&\displaystyle\beta_{2}\left(\left[B\right]_{1,f_{2}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{2}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{2}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{2}}\mathbf{v}_{d_{r}}\right)+ \\
&\displaystyle\quad\quad\vdots \\
&\displaystyle\beta_{n-r}\left(\left[B\right]_{1,f_{n-r}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{n-r}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{n-r}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{n-r}}\mathbf{v}_{d_{r}}\right) \\
\displaystyle=&\displaystyle\ \left(\alpha_{1}+\beta_{1}\left[B\right]_{1,f_{1}}+\beta_{2}\left[B\right]_{1,f_{2}}+\beta_{3}\left[B\right]_{1,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{1,f_{n-r}}\right)\mathbf{v}_{d_{1}}+ \\
&\displaystyle\left(\alpha_{2}+\beta_{1}\left[B\right]_{2,f_{1}}+\beta_{2}\left[B\right]_{2,f_{2}}+\beta_{3}\left[B\right]_{2,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{2,f_{n-r}}\right)\mathbf{v}_{d_{2}}+ \\
&\displaystyle\quad\quad\vdots \\
&\displaystyle\left(\alpha_{r}+\beta_{1}\left[B\right]_{r,f_{1}}+\beta_{2}\left[B\right]_{r,f_{2}}+\beta_{3}\left[B\right]_{r,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{r,f_{n-r}}\right)\mathbf{v}_{d_{r}}.
\end{align*}
@col
This mess expresses the vector $\mathbf{w}$ as a linear combination of the vectors in
\begin{align*}
\displaystyle T=\left\{\mathbf{v}_{d_{1}},\,\mathbf{v}_{d_{2}},\,\mathbf{v}_{d_{3}},\,\ldots\,\mathbf{v}_{d_{r}}\right\},
\end{align*}
thus saying that $\mathbf{w}\in\left< T\right>$. Therefore, $W=\left< S\right>\subseteq\left< T\right>$.
@qed
@end

<!--
@slide
In @ref{eg:COV}, we tossed-out vectors one at a time. But in each instance, we rewrote the offending vector as a linear combination of those vectors with the column indices of the pivot columns of the reduced row-echelon form of the matrix of columns. In the proof of Theorem   @ref{BS}, we accomplish this reduction in one big step. In @ref{eg:COV} we arrived at a linearly independent set at exactly the same moment that we ran out of free variables to exploit. This was not a coincidence; it is the substance of our conclusion of linear independence in Theorem   @ref{BS}.
-->

@slide
Here is a straightforward application of Theorem @ref{BS}.
@eg
<b>Reducing the generating set of a span in ${\mathbb{R}}^{4}$</b>

@col
Begin with a set of five vectors in ${\mathbb{R}}^{4}$,
\begin{align*}
\displaystyle S=\left\{\begin{bmatrix}1\\
1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}2\\
2\\
4\\
2\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}7\\
1\\
-1\\
4\end{bmatrix},\,\begin{bmatrix}0\\
2\\
5\\
1\end{bmatrix}\right\}
\end{align*}
and let $W=\left< S\right>$.

@col
To arrive at a (smaller) linearly independent set, follow the procedure described in Theorem   @ref{BS}. Place the vectors from $S$ into a matrix as columns, and row-reduce:
\begin{align*}
\displaystyle \begin{bmatrix}1&2&2&7&0\\
1&2&0&1&2\\
2&4&-1&-1&5\\
1&2&1&4&1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&2&0&1&2\\
0&0&\boxed{1}&3&-1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
Columns 1 and 3 are the pivot columns ($D=\left\{1,\,3\right\}$). So the set
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1\\
1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix}\right\}
\end{align*}
is linearly independent and $\left< T\right>=\left< S\right>=W$. Boom!


Since the reduced row-echelon form of a matrix is unique, the procedure of Theorem   @ref{BS} leads us to a unique set $T$. However, there is a wide variety of possibilities for sets $T$ that are linearly independent and which can be employed in a span to create $W$. Without proof, we list two other possibilities:
\begin{align*}
\displaystyle T^{\prime}&\displaystyle=\left\{\begin{bmatrix}2\\
2\\
4\\
2\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix}\right\} \\
\displaystyle T^{*}&\displaystyle=\left\{\begin{bmatrix}3\\
1\\
1\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
3\\
0\end{bmatrix}\right\}.
\end{align*}
Can you prove that $T^{\prime}$ and $T^{*}$ are linearly independent sets and that $W=\left< S\right>=\left< T^{\prime}\right>=\left< T^{*}\right>$?
@end
@slide
@eg
<b>Reworking elements of a span</b>

Begin with a set of five vectors in ${\mathbb{R}}^{4}$
\begin{align*}
\displaystyle R=\left\{\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix},\,\begin{bmatrix}-8\\
-1\\
-9\\
-4\end{bmatrix},\,\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}-10\\
-1\\
-1\\
4\end{bmatrix}\right\}.
\end{align*}
@col
It is easy to create elements of $X=\left< R\right>$ â€“ we will create one at random,
\begin{align*}
\displaystyle \mathbf{y}=6\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix}+(-7)\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix}+1\begin{bmatrix}-8\\
-1\\
-9\\
-4\end{bmatrix}+6\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}+2\begin{bmatrix}-10\\
-1\\
-1\\
4\end{bmatrix}=\begin{bmatrix}9\\
2\\
1\\
-3\end{bmatrix}.
\end{align*}
@col
We know we can replace $R$ by a smaller set, that will create the same span. Here goes:
\begin{align*}
\displaystyle \begin{bmatrix}2&-1&-8&3&-10\\
1&1&-1&1&-1\\
3&0&-9&-1&-1\\
2&1&-4&-2&4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&-3&0&-1\\
0&\boxed{1}&2&0&2\\
0&0&0&\boxed{1}&-2\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
@col
So, if we collect the first, second and fourth vectors from $R$,


\begin{align*}
\displaystyle P=\left\{\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix},\,\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}\right\}
\end{align*}
@col
then $P$ is linearly independent and $\left< P\right>=\left< R\right>=X$ by Theorem   @ref{BS}. Since we built $\mathbf{y}$ as an element of $\left< R\right>$ it must also be an element of $\left< P\right>$. Can we write $\mathbf{y}$ as a linear combination of just the three vectors in $P$? The answer is, of course, yes. But let us compute an explicit linear combination just for fun. We can get such a linear combination by solving a system of equations with the column vectors of $R$ as the columns of a coefficient matrix, and $\mathbf{y}$ as the vector of constants.



@col
We employ an augmented matrix to solve this system:
\begin{align*}
\displaystyle \begin{bmatrix}2&-1&3&9\\
1&1&1&2\\
3&0&-1&1\\
2&1&-2&-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&1\\
0&\boxed{1}&0&-1\\
0&0&\boxed{1}&2\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
So we see, as expected, that
\begin{align*}
\displaystyle 1\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix}+(-1)\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix}+2\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}=\begin{bmatrix}9\\
2\\
1\\
-3\end{bmatrix}=\mathbf{y}.
\end{align*}
A key feature of this example is that the linear combination that expresses $\mathbf{y}$ as a linear combination of the vectors in $P$ is unique. This is a consequence of the linear independence of $P$. The linearly independent set $P$ is smaller than $R$, but still just (barely) big enough to create elements of the set $X=\left< R\right>$. There are many, many ways to write $\mathbf{y}$ as a linear combination of the five vectors in $R$ (the appropriate system of equations to verify this claim yields two free variables in the description of the solution set), yet there is precisely one way to write $\mathbf{y}$ as a linear combination of the three vectors in $P$.
@end
@section{
Uniqueness of RREF}
<strong>Math Major only. You can skip this section. Similar concept appears in the classworks. </strong>
@eg
<b>Entries of RREF $B$ gives relationship of columns of $A$</b>

@col
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&1&8&1&17\\
1&2&2&13&3&37\\
1&2&0&3&-2&-10\\
\end{bmatrix}.
\end{align*}
@col
Then $A$ can be row reduced to


\begin{align*}
\displaystyle B=\begin{bmatrix}1&2&0&3&0&4\\
0&0&1&5&0&6\\
0&0&0&0&1&7\end{bmatrix}.
\end{align*}
Let $\mathbf{A}_{i}$ (resp. $\mathbf{B}_{i}$) be the $i$-th column of $A$ (resp. $B$) for $i=1,\ldots,6$.
By the equivalence of system of linear equation $\mathcal{LS}({A},{\mathbf{0}})$ and $\mathcal{LS}({B},{\mathbf{0}})$, we have
\begin{align}
\displaystyle x_{1}\mathbf{A}_{1}+x_{2}\mathbf{A}_{2}+\cdots+x_{6}\mathbf{A}_{6}=\mathbf{0}&
\end{align}
if and only if
\begin{align}
\label{eq:B}
\displaystyle x_{1}\mathbf{B}_{1}+x_{2}\mathbf{B}_{2}+\cdots+x_{6}\mathbf{B}_{6}=\mathbf{0}.&
\end{align}
@col
<strong>Step 1</strong>
@newcol
First of all, if $(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},0,0,0,0,0)$ is a solution of \eqref{eq:B}, then
\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}=\mathbf{0}.
\end{align*}
@col
So $x_{1}$ is zero. This is equivalent to
\begin{align*}
\displaystyle x_{1}\mathbf{A}_{1}=\mathbf{0}.
\end{align*}
It has only the trivial solution, i.e. $\left\{\mathbf{A}_{1}\right\}$ is linearly independent. Hence $d_{1}=1$ is a pivot column.
@endcol

<strong>Step 2</strong>
@newcol
Letâ€™s move to $x_{2}$. Suppose that
$(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},x_{2},0,0,0,0)$.

@col
Then
\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}+x_{2}\mathbf{B}_{1}=\mathbf{0}
\end{align*}
has nontrivial solution. Say $(x_{1},x_{2})=(-2,1)$.

@col
These can also be seen as
\begin{align*}
\displaystyle -2\mathbf{A}_{1}+\mathbf{A}_{2}=\mathbf{0}
\end{align*}
or equivalently
\begin{align*}
\displaystyle \mathbf{A}_{2}=2\mathbf{A}_{1}.
\end{align*}
@endcol

<strong>Step 3</strong>
@newcol
Consider $x_{3}$. Let $(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},0,x_{3},0,0,0)$. Then
\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}+x_{3}\mathbf{B}_{3}=\mathbf{0}
\end{align*}
has only trivial solution. Equivalently $\left\{\mathbf{A}_{1},\mathbf{A}_{3}\right\}$ is linearly independent.Column $3$ of $B$ is a pivot column.
@endcol

<strong>Step 4</strong>
@newcol
Consder
\begin{align*}
\displaystyle \mathbf{B}_{4}=3\mathbf{B}_{1}+5\mathbf{B}_{3},
\end{align*}
or equivalently
\begin{align*}
\displaystyle \mathbf{A}_{4}=3\mathbf{A}_{1}+5\mathbf{A}_{3}.
\end{align*}
@col
The relation of columns of $A$ gives the entries of the column 4 of $B$.
@endcol

<strong>Step 5</strong>
@newcol
$\mathbf{B}_{5}$ is not in span of $\mathbf{B}_{1}$ and $\mathbf{B}_{3}$. Equivalently $\mathbf{A}_{5}$ is not in span of $\mathbf{A}_{1}$ and $\mathbf{A}_{3}$. Column $5$ of $B$ is a pivot column.
@endcol

<strong>Step 6</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{B}_{6}=4\mathbf{B}_{1}+6\mathbf{B}_{3}+7\mathbf{B}_{5}.
\end{align*}
Equivalently
\begin{align*}
\displaystyle \mathbf{A}_{6}=4\mathbf{A}_{1}+6\mathbf{A}_{3}+7\mathbf{A}_{5}.
\end{align*}
@col
The relation of columns of $A$ gives the entries of the column 6 of $B$.
@end

@eg
<b>Relationship of columns of $A$ determine entries of $B$</b>

@col
Row reduce
\begin{align*}
\displaystyle A=\begin{bmatrix}1&1&3&1&0&0&4\\
2&1&5&1&1&2&7\\
1&-1&1&2&1&-3&10\\
1&3&5&1&-1&1&1\\
\end{bmatrix}
\end{align*}
to a RREF $B$ by the above technique. Let $\mathbf{A}_{i}$ (resp. $\mathbf{B}_{i}$) be the $i$-th column of $A$ (resp. $B$) for $i=1,\ldots,7$.

@col
<strong>Step 1</strong>
@newcol
$\mathbf{A}_{1}$ is nonzero column. So the index $d_{1}=1$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{1}=\begin{bmatrix}1\\
0\\
0\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 2</strong>
@newcol
$\mathbf{A}_{2}$ is not in $\left< \left\{\mathbf{A}_{d_{1}}\right\}\right>$. So the index $d_{2}=2$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{2}=\begin{bmatrix}0\\
1\\
0\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 3</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{A}_{3}=2\mathbf{A}_{d_{1}}+\mathbf{A}_{d_{2}}.
\end{align*}
@col
So we have
\begin{align*}
\displaystyle \mathbf{B}_{3}=2\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}=\begin{bmatrix}2\\
1\\
0\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 4</strong>
@newcol
$\mathbf{A}_{4}$ is not in $\left< \left\{\mathbf{A}_{d_{1}},\mathbf{A}_{d_{2}}\right\}\right>$.

@col
So the index $d_{3}=4$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{4}=\begin{bmatrix}0\\
0\\
1\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 5</strong>
@newcol
$\mathbf{A}_{5}$ is not in $\left< \left\{\mathbf{A}_{d_{1}},\mathbf{A}_{d_{3}},\mathbf{A}_{d_{3}}\right\}\right>$.

@col
So the index $d_{4}=5$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{5}=\begin{bmatrix}0\\
0\\
0\\
1\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 6</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{A}_{6}=\mathbf{A}_{d_{1}}+\mathbf{A}_{d_{2}}-2\mathbf{A}_{d_{3}}+\mathbf{A}_{d_{4}}.
\end{align*}
@col
So, we have
\begin{align*}
\displaystyle \mathbf{B}_{6}=\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}-2\mathbf{B}_{d_{3}}+\mathbf{B}_{d_{4}}=\begin{bmatrix}1\\
1\\
-2\\
1\end{bmatrix}
\end{align*}
@endcol

<strong>Step 7</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{A}_{7}=2\mathbf{A}_{d_{1}}-\mathbf{A}_{d_{2}}+3\mathbf{A}_{d_{3}}+\mathbf{A}_{d_{4}}.
\end{align*}
@col
So, we have
\begin{align*}
\displaystyle \mathbf{B}_{7}=\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}-2\mathbf{B}_{d_{3}}+\mathbf{B}_{d_{4}}=\begin{bmatrix}2\\
-1\\
3\\
1\end{bmatrix}
\end{align*}
@col
Hence the RREF of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&0&2&0&0&1&2\\
0&1&1&0&0&1&-1\\
0&0&0&1&0&-2&3\\
0&0&0&0&1&1&1\\
\end{bmatrix}.
\end{align*}
@endcol

@col
<strong>Important remark</strong>: from the above computation, the entries of $B$ are uniquely determined by $A$.

@col
So the RREF $B$ is unique.

@end
