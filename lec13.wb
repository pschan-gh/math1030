@course{MATH 1030}
@setchapter{13}
@chapter{Basis}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section B (print version p233-238), Section D (print version p245-253)

<h5 class="notkw">Exercise.</h5><ul>
<li> Exercises with solutions can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf} (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)

Section B p.88-92 C10, C11, C12, M20 Section D
p.92-96 C21, C23, C30, C31, C35, C36, C37, M20, M21. </li></ul>

@section{Basis}
@defn
Let $V$ be a vector space. Then a subset $S$ of $V$ is said to be a @keyword{basis} for $V$ if <ol class="ltx_enumerate">
<li class="ltx_item"> $S$ is linearly independent. </li>
<li class="ltx_item"> $\mathrm{Span}\,S=V$, i.e. $S$ spans $V$. </li></ol>
@end
@remark
@newcol
Most of the time $V$ is a subspace of ${\mathbb{R}}^{m}$. Occasionally $V$ is assumed to be a subspace of $M_{mn}$ or $P_{n}$. It does not hurt to assume $V$ is a subspace of ${\mathbb{R}}^{m}$.
@endcol
@end
@slide
@eg
Let $V={\mathbb{R}}^{m}$, then $B=\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}$ is a basis for $V$.
(recall all the entries of $\mathbf{e}_{i}$ is zero, except the $i$-th entry being $1$).

@newcol
It is called the @keyword{standard basis}: Obviously $B$ is linearly independent. Also, for any $\mathbf{v}\in V$, $\mathbf{v}=[\mathbf{v}]_{1}\mathbf{e}_{1}+\cdots+[\mathbf{v}]_{m}\mathbf{e}_{m}\in\mathrm{Span}\,B$. So $\mathrm{Span}\,B=V$.
@endcol
@end
@slide
@skip
@eg
<strong>Math major only</strong>

@newcol
Consider $V=M_{22}$. Let:
\begin{align*}
\displaystyle B_{11}&=\begin{bmatrix}1&0\\
0&0\end{bmatrix},
&
B_{12}&=\begin{bmatrix}0&1\\
0&0\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle B_{21}&=\begin{bmatrix}0&0\\
1&0\end{bmatrix},
&
B_{22}&=\begin{bmatrix}0&0\\
0&1\end{bmatrix},
\end{align*}
@col
Then $B=\left\{B_{11},B_{12},B_{21},B_{22}\right\}$ is a basis for $V$.

@col
Check: Obviously $B$ is linearly independent (exercise). Also for any $A\in V$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
c&d\end{bmatrix}=aB_{11}+bB_{12}+cB_{21}+dB_{22}.
\end{align*}
@col
So $\mathrm{Span}\,B=M_{22}$.
@endcol
@end

@ex
<strong>Math major only</strong>

@newcol
Let $V=M_{mn}$.

For $1\leq i\leq m$, $1\leq j\leq n$,
let $B_{ij}$ be the $m\times n$ matrix with $(i,j)$-th entry equal to $1$ and all other entries equal to $0$.

Then $\left\{ B_{ij} | 1\leq i \leq m, 1\leq j\leq n\right\}$ is a basis for $V$.
@endcol
@end

@eg
<strong>Math major only</strong>

@newcol
Let $V=P_{n}$. Then $1,x,x^{2},\ldots,x^{n}$ is a basis. It is easy to show that $S=\left\{1,x,x^{2},\ldots,x^{n}\right\}$ is linearly independent. Also any polynomial
\begin{align*}
\displaystyle f(x)=a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}
\end{align*}
@col
is a linear combinations of $S$.

@endcol
@end
@slide
@eg
A vector space can have different bases.

Consider the vector space $V={\mathbb{R}}^{2}$.

Then,
\[
S=\left\{\mathbf{e}_{1},\mathbf{e}_{2}\right\}
\]
is a basis for $V$, and:

@newcol
\[
S^{\prime}=\left\{\begin{bmatrix}1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\end{bmatrix}\right\}
\]
is also a basis.
@endcol
@end
@section{Bases for Spans of Column vectors}
@subsection{Column Spaces and Systems of Equations}
@defn
@title{Column Space of a Matrix}
@label{CSM}
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$. Then the @keyword{column space} of $A$, written $\mathcal{C}\left(A\right)$, is the subset of ${\mathbb{R}}^{m}$ containing all linear combinations of the columns of $A$,
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\mathrm{Span}\,\{\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}\}\end{align*}
@end
@slide
@thm
@title{Column Spaces and Consistent Systems}
@label{CSCS}
Suppose $A$ is an $m\times n$ matrix and $\mathbf{b}$ is a vector of size $m$.
Then $\mathbf{b}\in\mathcal{C}\left(A\right)$ if and only if $A\mathbf{x} = \mathbf{b}$ is consistent.
@end
@proof
@newcol
($\Rightarrow$) Suppose $\mathbf{b}\in\mathcal{C}\left(A\right)$. Then we can write $\mathbf{b}$ as some linear combination of the columns of $A$. Then by @ref{RCLS} we can use the scalars from this linear combination to form a solution to $A\mathbf{x} = \mathbf{b}$, so this system is consistent.

($\Leftarrow$) If $A\mathbf{x} = \mathbf{b}$ is consistent, there is a solution that may be used with @ref{RCLS} to write $\mathbf{b}$ as a linear combination of the columns of $A$. This qualifies $\mathbf{b}$ for membership in $\mathcal{C}\left(A\right)$.
@qed
@endcol
@end
This theorem tells us that asking if the vector equation $A\mathbf{x} = \mathbf{b}$ has a solution is exactly the same question as asking if $\mathbf{b}$ is in the column space of $A$.

@newcol
Thus, an alternative (and popular) definition of the column space of an $m\times n$ matrix $A$ is
\begin{align*}
\displaystyle\mathcal{C}\left(A\right)&\displaystyle=\left\{\left.\mathbf{y}\in{\mathbb{R}}^{m}\,\right|\,\mathbf{y}=A\mathbf{x}\text{ for some }\mathbf{x}\in{\mathbb{R}}^{n}\right\}=\left\{\left.A\mathbf{x}\,\right|\,\mathbf{x}\in{\mathbb{R}}^{n}\right\}\subseteq{\mathbb{R}}^{m}
\end{align*}
@endcol
@slide
@eg
Consider the column space of the $3\times 4$ matrix $A$,
\begin{align*}
\displaystyle A=\begin{bmatrix}3&2&1&-4\\
-1&1&-2&3\\
2&-4&6&-8\end{bmatrix}
\end{align*}
@newcol
Show that $\mathbf{v}=\begin{bmatrix}18\\
-6\\
12\end{bmatrix}$ is in the column space of $A$, $\mathbf{v}\in\mathcal{C}\left(A\right)$.
The above theorem says that we need to check the consistency of $\mathcal{LS}({A},{v})$.
From the augmented matrix and row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}3&2&1&-4&18\\
-1&1&-2&3&-6\\
2&-4&6&-8&12\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&1&-2&6\\
0&\boxed{1}&-1&1&0\\
0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Since the last column is not a pivot column, so the system is consistent and hence $v\in\mathcal{C}\left(A\right)$.
In fact, we have
\begin{align*}
\displaystyle \mathbf{v}=6\mathbf{A}_{1}.
\end{align*}
@col
Next we show that $\mathbf{w}=\begin{bmatrix}2\\
1\\
-3\end{bmatrix}$ is not in the column space of $A$, $\mathbf{w}\not\in\mathcal{C}\left(A\right)$.
The above theorem says that we need to check the consistency of $\mathcal{LS}({A},{v})$.
From the augmented matrix and row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}3&2&1&-4&2\\
-1&1&-2&3&1\\
2&-4&6&-8&-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&1&-2&0\\
0&\boxed{1}&-1&1&0\\
0&0&0&0&\boxed{1}\end{bmatrix}
\end{align*}
@col
Since the final column is a pivot column, the system is inconsistent and therefore $\mathbf{w}\not\in\mathcal{C}\left(A\right)$.
@endcol
@end
<!--
The next two examples illustrate the main idea of describing $\mathcal{C}\left(A\right)$.
-->
<!--
@eg
<strong>Describe $\mathcal{C}\left(A\right)$ as a null space</strong>

Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&7&1&-1\\
1&1&3&1&0\\
3&2&5&-1&9\\
1&-1&-5&2&0\end{bmatrix}.
\end{align*}
@newcol
Find $\mathcal{C}\left(A\right)$. Let’s determine if $\mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{4}\end{bmatrix}\in\mathrm{Span}\,S$.

@col
Applying Gauss-Jordan elimination to the augmented matrix
\begin{align*}
\displaystyle \begin{bmatrix}1&2&7&1&-1&v_{1}\\
1&1&3&1&0&v_{2}\\
3&2&5&-1&9&v_{3}\\
1&-1&-5&2&0&v_{4}\end{bmatrix},
\end{align*}
@col
we obtain
\begin{align*}
\displaystyle \begin{bmatrix}1&0&-1&0&3&-3v_{1}+5v_{2}-v_{4}\\
0&1&4&0&-1&v_{1}-v_{2}\\
0&0&0&1&-2&2v_{1}-3v_{2}+v_{4}\\
0&0&0&0&0&9v_{1}-16v_{2}+v_{3}+4v_{4}\\
\end{bmatrix}
\end{align*}
@col
If $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$, the above is a RREF. The last column is not a pivot columns. So $\mathbf{v}\in\mathrm{Span}\,S$. If $9v_{1}-16v_{2}+v_{3}+4v_{4}\neq 0$, the equation corresponding to the last row is
\begin{align*}
\displaystyle 9v_{1}-16v_{2}+v_{3}+4v_{4}=0.
\end{align*}
@col
So the corresponding system of linear equations is inconsistent. So $\mathbf{v}\in\mathrm{Span}\,S$. Hence $\mathbf{v}\in\mathrm{Span}\,S$ if and only if $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$. Therefore
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)={\mathcal{N}}\!\left([9\,\,-16\,\,1\,\,4]\right).
\end{align*}
@endcol
@end
-->
@subsection{Column Space Spanned by Original Columns}
@label{CSSOC}
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix},
\end{align*}
@newcol
find $\mathcal{C}\left(A\right)$.
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}B=\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}.
\end{align*}
@col
The indexes of the pivot columns are $D=\left\{1,3,4\right\}$.
Hence $\mathcal{C}\left(A\right)=\mathrm{Span}\,A=\mathrm{Span}\,\left\{ \mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4} \right\}$.
@endcol
@end
@slide
@thm
@title{Basis of the Column Space}
@label{BCS}
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\left\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\right\}$ be the set of indices for the pivot columns of $B$.
Let $T=\left\{\mathbf{A}_{d_{1}},\,\mathbf{A}_{d_{2}},\,\mathbf{A}_{d_{3}},\,\ldots,\,\mathbf{A}_{d_{r}}\right\}$. Then:
<ol class="ltx_enumerate">
<li class="ltx_item"> $T$ is a linearly independent set. </li>
<li class="ltx_item"> $\mathcal{C}\left(A\right)=\mathrm{Span}\,T$. </li></ol>
@end
@lemma
@col
If a matrix $A$ is row-equivalent to $B$,
then any matrix formed with a subset of columns of $A$
is row-equivalent to the matrix formed with the corresponding subset of columns
(i.e. with the same indices and conforming to the same order) of $B$.
@end
@proof
@col
<strong>Exercise</strong>.
@end
@proof
@of{BCS}
@col
Let:
\[
A' = [\mathbf{A}_{d_1} | \mathbf{A}_{d_2} | \cdots | \mathbf{A}_{d_r}],
\]
\[
B' = [\mathbf{B}_{d_1} | \mathbf{B}_{d_2} | \cdots | \mathbf{B}_{d_r}].
\]
Then, $A'$ and $B'$ are row-equivalent by the previous lemma.

@enumerate
@item
@col
Since $d_1, d_2, \cdots d_r$ are the indices of the pivot columns of $B$, which is in RREF, we have:
\[
B' = \begin{bmatrix}
I_r\\
\hline
\mathcal{O}_{(m - r)\times r}
\end{bmatrix}.
\]
(If $r = m$, then $B' = I_r$.)

@col
Hence, $B'\vect{x} = \vect{0}$ if and only if $\vect{x} = \vect{0}$.

@col
Since $A'$ is row-equivalent to $B'$,
we have $A'\vect{x} = \vect{0}$ if and only if $\vect{x} = \vect{0}$,
which implies that the columns of $A'$, 
i.e. the elements of $T$, are linearly independent.
@item
@col
Given any integer $1 \leq j \leq n$ which does not lie in $D$ 
(i.e. the index of a non-pivot column of $B$), consider the linear system
$\mathcal{LS}(A', \vect{A}_j)$ (or equivalently $A'\vect{x} = \vect{A}_j$).

By the previous lemma,
the corresponding the augmented matrix $\left[A' | \vect{A}_j\right]$
is row-equivalent to $\left[B' | \vect{B}_j\right]$,
which has the form:

@col
\[
\left[
\begin{array}{c|c}
I_r & \begin{matrix}\ast\\\ast\\\vdots\\\ast\end{matrix}\\
\hline
\mathcal{O}_{(m - r) \times r} & \begin{matrix}0 \\ 0 \\ \vdots \\ 0\end{matrix}
\end{array}
\right].
\]
Hence, the linear system $\mathcal{LS}(A', \vect{A}_j)$ is consistent 
(in fact, with one unique solution), 
which implies that $\vect{A}_j$ lies in the span of
$\{\vect{A}_{d_1}, \vect{A}_{d_2}, \ldots, \vect{A}_{d_r}\}$.

@col
This holds for all $j \notin D$.
Hence, by @ref{thm:spanredundancy}, we have:
\[
\mathcal{C}(A) 
=
\mathrm{Span}\{\vect{A}_1, \vect{A}_2,\ldots, \vect{A}_n\}
=
\mathrm{Span}\{\vect{A}_{d_1}, \vect{A}_{d_2},\ldots, \vect{A}_{d_r}\}
=
\mathrm{Span}\,T.
\]
@endenumerate
@end
@slide
@eg
Consider the $5\times 7$ matrix $A$,
\begin{align*}
\displaystyle \begin{bmatrix}2&4&1&-1&1&4&4\\
1&2&1&0&2&4&7\\
0&0&1&4&1&8&7\\
1&2&-1&2&1&9&6\\
-2&-4&1&3&-1&-2&-2\end{bmatrix}
\end{align*}
@newcol
The column space of $A$ is
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\mathrm{Span}\,\left\{ \begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
2\\
0\\
2\\
-4\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix},\,\begin{bmatrix}4\\
4\\
8\\
9\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
7\\
7\\
6\\
-2\end{bmatrix} \right\}\end{align*}
@col
While this is a concise description of an infinite set, we might be able to describe the span with fewer than seven vectors. Now we row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}2&4&1&-1&1&4&4\\
1&2&1&0&2&4&7\\
0&0&1&4&1&8&7\\
1&2&-1&2&1&9&6\\
-2&-4&1&3&-1&-2&-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&2&0&0&0&3&1\\
0&0&\boxed{1}&0&0&-1&0\\
0&0&0&\boxed{1}&0&2&1\\
0&0&0&0&\boxed{1}&1&3\\
0&0&0&0&0&0&0\end{bmatrix}
\end{align*}
@col
The pivot columns are $D=\left\{1,\,3,\,4,\,5\right\}$, so we can create the set
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix}\right\}
\end{align*}
@col
and know that $\mathcal{C}\left(A\right)=\mathrm{Span}\,T$ and $T$ is a linearly independent set of columns from the set of columns of $A$.
@endcol
@end
@slide
@eg
<strong>Column space from row operations</strong>

Let
\begin{align*}
\displaystyle S=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\mathbf{v}_{6}=\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\mathbf{v}_{7}=\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\right\}.
\end{align*}
@newcol
Find a basis for $\mathrm{Span}\,S$.
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{7}]=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}.
\end{align*}

<strong>Method 1</strong>
@newcol
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Let
\begin{align*}
\displaystyle T=\left\{\mathbf{v}_{1},\mathbf{v}_{3},\mathbf{v}_{4}\right\}=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\right\}.
\end{align*}
@col
Then $T$ is a basis for $\mathrm{Span}\,S=\mathcal{C}\left(A\right)$.
@endcol

<strong>Method 2</strong>
@newcol
The transpose of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&2&0&-1\\
4&8&0&-4\\
0&-1&2&2\\
-1&3&-3&4\\
0&9&-4&8\\
7&-13&12&-31\\
-9&7&-8&37\end{bmatrix}.
\end{align*}
@col
Row-reduced this becomes,
\begin{align*}
\displaystyle D=\begin{bmatrix}\boxed{1}&0&0&-\frac{31}{7}\\
0&\boxed{1}&0&\frac{12}{7}\\
0&0&\boxed{1}&\frac{13}{7}\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
Then we can take
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\right\}.
\end{align*}

$T$ is a basis for $\mathcal{C}\left(A\right)=\mathrm{Span}\,S$.
@endcol
@endcol
@end
@slide
@thm
@label{basisexists}
Let $S$ be a finite subset of ${\mathbb{R}}^{m}$.
Then, a basis for $\mathrm{Span}\,S$ exists.

@newcol
In fact, there exists a subset $T$ of $S$ such that $T$ is a basis for $\mathrm{Span}\,S$ (see @ref{BCS}).
@endcol
@end
@slide
@thm
@title{Column Space of a Nonsingular Matrix}
Suppose $A$ is a square matrix of size $n$. Then $A$ is nonsingular if and only if $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$.
@end
@proof
@newcol
See @ref{thm:ANSRM}.
@qed
@endcol
@end
@slide
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}0&1&2&3\\
-1&1&2&1\\
0&1&0&2\\
1&1&1&4\\
\end{bmatrix}.
\end{align*}
@newcol
We can show that $A$ is nonsingular as $A\xrightarrow{\text{RREF}}I_{4}$.
So $\mathcal{C}\left(A\right)={\mathbb{R}}^{4}$.
@endcol
@end

@section{Row Space of a Matrix}
<!--@slide@defn
Let $n$ be a positive integer.
A @keyword{row vector} or size $n$ is an array of real numbers of the form:
\[
\left(v_1, v_2, \ldots, v_n\right),\quad v_i \in \mathbb{R}.
\]
In the context of this course, it is perfectly reasonable to view a row vector
as simply a $1 \times n$ matrix.

@col
In fact,
the standard laws of addition and scalar multiplication on row vectors
of size $n$ are those defined for $1 \times n$ matrices (see @ref{MEASM}).
That is:
\begin{multline*}
\left(v_1 , v_2 , \ldots , v_n\right) + \left(w_1, w_2, \ldots, w_n\right)
= \left(v_1 + w_1, v_2 + w_2, \ldots, v_n + w_n\right),\\
\quad v_i, w_i \in \mathbb{R};
\end{multline*}
\[
\alpha\left(v_1, v_2, \ldots, v_n\right)
=
\left(\alpha v_1, \alpha v_2, \ldots, \alpha v_n\right), \quad v_i, \alpha \in \mathbb{R}.
\]
@col
For any fixed positive integer $n$,
by @ref{fb1ac45c60c8c59135e7173658dededc} the set of all row vectors of size $n$
is a vector space under the laws of addition and scalar multiplication defined above.
@end@slide-->



@defn
@title{Row Space of a Matrix}
Suppose $A$ is an $m\times n$ matrix.
The @keyword{row space} of $A$,
$\mathcal{R}\!\left(A\right)$ is column space $\mathcal{C}\left(A^t\right)$ of $A^t$.
@end

Informally, the row space is the set of all linear combinations of the rows of $A$. However, we write the rows as column vectors, thus the necessity of using the transpose to make the rows into columns. Additionally, with the row space defined in terms of the column space, all of the previous results of this section can be applied to row spaces.

@newcol
Notice that if $A$ is a rectangular $m\times n$ matrix, then $\mathcal{C}\left(A\right)\subseteq{\mathbb{R}}^{m}$, while $\mathcal{R}\!\left(A\right)\subseteq{\mathbb{R}}^{n}$ and the two sets are not comparable since they do not even hold objects of the same type. However, when $A$ is square of size $n$, both $\mathcal{C}\left(A\right)$ and $\mathcal{R}\!\left(A\right)$ are subsets of ${\mathbb{R}}^{n}$, though usually the sets will not be equal.
@endcol
@slide
@eg
@label{eg:4x7matrix140-107-9}
Find $\mathcal{R}\!\left(A\right)$ for
\begin{align*}
\displaystyle A=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}.
\end{align*}
@newcol
To build the row space, we transpose the matrix,
\begin{align*}
\displaystyle A^{t}=\begin{bmatrix}1&2&0&-1\\
4&8&0&-4\\
0&-1&2&2\\
-1&3&-3&4\\
0&9&-4&8\\
7&-13&12&-31\\
-9&7&-8&37\end{bmatrix}
\end{align*}
@col
Then the columns of this matrix are used in a span to build the row space,
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\mathrm{Span}\,\left\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix},\,\begin{bmatrix}-1\\
-4\\
2\\
4\\
8\\
-31\\
37\end{bmatrix}\right\}.
\end{align*}
@col
First, row-reduce $A^{t}$,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&0&-\frac{31}{7}\\
0&\boxed{1}&0&\frac{12}{7}\\
0&0&\boxed{1}&\frac{13}{7}\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
Since the pivot columns have indices $D=\left\{1,\,2,\,3\right\}$, the column space of $A^{t}$ can be spanned by just the first three columns of $A^{t}$,
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\mathrm{Span}\,\left\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix}\right\}.
\end{align*}
@endcol
@end
@slide
@thm
@title{Row-Equivalent Matrices have Equal Row Spaces}
@label{REMRS}
Suppose $A$ and $B$ are row-equivalent matrices. Then $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$.
@end
@proof
@newcol
Observe that if $B$ is obtained from $A$ via a row operation of the type
$R_i \leftrightarrow R_j$,
then the rows of $B$ are the same as the rows of $A$,
and hence the columns of $B^t$ are still the same as the columns of $A^t$,
only with the order changed.  Hence,
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) = \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\]

@col
If $B$ is obtained from $A$ via a row operation of the type $\alpha R_i$ ($\alpha \neq 0$),
then the $i$-th column of $B^t$ is equal to $\alpha$ times the $i$-th column of $A^t$,
and the other columns remain the same as those of $A^t$ with the corresponding indices.

In paricular, the $i$-th column of $B^t$ is a linear combination of the columns of $A^t$.

@col
Hence, the columns of $B^t$ all lie in $\mathcal{C}(A^t)$, which in turn implies that:
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) \subseteq \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\]
@col
On the other hand, if $B$ is obtained from $A$ via $\alpha R_i$, then $A$ is obtained
from $B$ via $\left(\frac{1}{\alpha}\right) R_i$.  So, by the same argument as before we have:
\[
\mathcal{R}(A) = \mathcal{C}\left(A^t\right) \subseteq \mathcal{C}\left(B^t\right) = \mathcal{R}(B).
\]
Hence, $\mathcal{R}(B) = \mathcal{R}(A)$.

@col
If $B$ is obtained from $A$ via a row operation of the type $\alpha R_i + R_j$,
then:
\[
\left[B^t\right]_j = \alpha\left[A^t\right]_i + \left[A^t\right]_j,
\]
and the other columns of $B^t$ remain the same as those of $A^t$
with the corresponding indices.

In paricular, the $i$-th column of $B^t$ is a linear combination of the columns of $A^t$.

@col
Hence, the columns of $B^t$ all lie in $\mathcal{C}(A^t)$, which in turn implies that:
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) \subseteq \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\]
@col
On the other hand, if $B$ is obtained from $A$ via $\alpha R_i + R_j$, then $A$ is obtained
from $B$ via $(-\alpha) R_i + R_j$.  So, by the same argument as before we have:
\[
\mathcal{R}(A) = \mathcal{C}\left(A^t\right) \subseteq \mathcal{C}(B^t) = \mathcal{R}(B).
\]
Hence, $\mathcal{R}(B) = \mathcal{R}(A)$.

@col
We now see that the row space of a matrix remains unchanged after any application
of a row operation.

Hence, $\mathcal{R}(B) = \mathcal{R}(A)$ if $B$ is row-equivalent to $A$,
since by the definition of row-equivalence (@ref{dacc247eacab6b0b430f64f7de449ebe}) $B$ is obtained by $A$ via a series
of row operations.
@endcol
@end
<!--@proof@col
Two matrices are row-equivalent if one can be obtained from another by a sequence of (possibly many) row operations. We will prove the theorem for two matrices that differ by a single row operation, and then this result can be applied repeatedly to get the full statement of the theorem. The row spaces of $A$ and $B$ are spans of the columns of their transposes. For each row operation we perform on a matrix, we can define an analogous operation on the columns. Perhaps we should call these @keyword{column operations}. Instead, we will still call them row operations, but we will apply them to the columns of the transposes.

@col
Refer to the columns of $A^{t}$ and $B^{t}$ as $\mathbf{A}_{i}$ and $\mathbf{B}_{i}$, $1\leq i\leq m$. The row operation that switches rows will just switch columns of the transposed matrices. This will have no effect on the possible linear combinations formed by the columns.

@col
Suppose that $B^{t}$ is formed from $A^{t}$ by multiplying column $\mathbf{A}_{t}$ by $\alpha\neq 0$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for all $i\neq t$. We need to establish that two sets are equal, $\mathcal{C}\left(A^{t}\right)=\mathcal{C}\left(B^{t}\right)$. We will take a generic element of one and show that it is contained in the other.
\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&\displaystyle\beta_{2}\mathbf{B}_{2}+\beta_{3}\mathbf{B}_{3}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\left(\alpha\beta_{t}\right)\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(B^{t}\right)\subseteq\mathcal{C}\left(A^{t}\right)$. Similarly,
\begin{align*}
\displaystyle\gamma_{1}\mathbf{A}_{1}+&\displaystyle\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\left(\frac{\gamma_{t}}{\alpha}\alpha\right)\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&\displaystyle=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\gamma_{3}\mathbf{B}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(A^{t}\right)\subseteq\mathcal{C}\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\mathcal{C}\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the second type is performed.

@col
Suppose now that $B^{t}$ is formed from $A^{t}$ by replacing $\mathbf{A}_{t}$ with $\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$ for some $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $s\neq t$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for $i\neq t$.
\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&\displaystyle\beta_{2}\mathbf{B}_{2}+\cdots+\beta_{s}\mathbf{B}_{s}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\left(\beta_{s}+\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(B^{t}\right)\subseteq\mathcal{C}\left(A^{t}\right)$. Similarly,
\begin{align*}
\displaystyle\gamma_{1}&\displaystyle\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\left(-\alpha\gamma_{t}\mathbf{A}_{s}+\alpha\gamma_{t}\mathbf{A}_{s}\right)+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{A}_{s}+\cdots+\gamma_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&\displaystyle=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{B}_{s}+\cdots+\gamma_{t}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(A^{t}\right)\subseteq\mathcal{C}\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\mathcal{C}\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the third type is performed.

@col
So the row space of a matrix is preserved by each row operation, and hence row spaces of row-equivalent matrices are equal sets.
@qed@end-->



@slide
@eg
<strong>Row spaces of two row-equivalent matrices</strong>

The matrices
\begin{align*}
\displaystyle A&\displaystyle=\begin{bmatrix}2&-1&3&4\\
5&2&-2&3\\
1&1&0&6\end{bmatrix}&\displaystyle B&\displaystyle=\begin{bmatrix}1&1&0&6\\
3&0&-2&-9\\
2&-1&3&4\end{bmatrix}
\end{align*}
@newcol
are row-equivalent via a sequence of two row operations.

Hence by the above theorem
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathrm{Span}\,\left\{\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}5\\
2\\
-2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix}\right\}=\mathrm{Span}\,\left\{\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix},\,\begin{bmatrix}3\\
0\\
-2\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix}\right\}=\mathcal{R}\!\left(B\right)
\end{align*}
@endcol
@end
@slide
@thm
@title{Basis for the Row Space}
@label{BRS}
Suppose that $A$ is a matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Let $S$ be the set of nonzero columns of $B^{t}$. Then <ol class="ltx_enumerate">
<li class="ltx_item"> $\mathcal{R}\!\left(A\right)=\mathrm{Span}\,S$. </li>
<li class="ltx_item"> $S$ is a linearly independent set. </li></ol>
@end
@proof
@newcol
From Theorem    @ref{REMRS}. we know that $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$. If $B$ has any zero rows, these are columns of $B^{t}$ that are the zero vector. We can safely toss out the zero vector in the span construction, since it can be recreated from the nonzero vectors by a linear combination where all the scalars are zero. So $\mathcal{R}\!\left(A\right)=\mathrm{Span}\,S$.

@col
Suppose $B$ has $r$ nonzero rows and let $D=\left\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\right\}$ denote the indices of the pivot columns of $B$. Denote the $r$ column vectors of $B^{t}$, the vectors in $S$, as $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{r}$. To show that $S$ is linearly independent, start with a relation of linear dependence
\begin{align*}
\displaystyle \alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}=\mathbf{0}
\end{align*}
@col
Now consider this vector equality in location $d_{i}$. Since $B$ is in reduced row-echelon form, the entries of column $d_{i}$ of $B$ are all zero, except for a leading 1 in row $i$. Thus, in $B^{t}$, row $d_{i}$ is all zeros, excepting a 1 in column $i$. So, for $1\leq i\leq r$,
\begin{align*}
\displaystyle 0&\displaystyle=\left[\mathbf{0}\right]_{d_{i}} \\
&\displaystyle=\left[\alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&\displaystyle=\left[\alpha_{1}\mathbf{B}_{1}\right]_{d_{i}}+\left[\alpha_{2}\mathbf{B}_{2}\right]_{d_{i}}+\left[\alpha_{3}\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\left[\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&\displaystyle=\alpha_{1}\left[\mathbf{B}_{1}\right]_{d_{i}}+\alpha_{2}\left[\mathbf{B}_{2}\right]_{d_{i}}+\alpha_{3}\left[\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\alpha_{r}\left[\mathbf{B}_{r}\right]_{d_{i}} \\
&\displaystyle=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+\alpha_{i}(1)+\cdots+\alpha_{r}(0) \\
&\displaystyle=\alpha_{i}
\end{align*}
@col
So we conclude that $\alpha_{i}=0$ for all $1\leq i\leq r$, establishing the linear independence of $S$.
@qed
@endcol
@end
@slide
@eg
Suppose in the course of analyzing a matrix (its column space, its null space, its ...) we encounter the following set of vectors, described by a span:
\begin{align*}
\displaystyle X=\mathrm{Span}\,\left\{\begin{bmatrix}1\\
2\\
1\\
6\\
6\end{bmatrix},\,\begin{bmatrix}3\\
-1\\
2\\
-1\\
6\end{bmatrix},\,\begin{bmatrix}1\\
-1\\
0\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}-3\\
2\\
-3\\
6\\
-10\end{bmatrix}\right\}
\end{align*}
@newcol
Let $A$ be the matrix whose rows are the vectors in $X$, so by design $X=\mathcal{R}\!\left(A\right)$,
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&1&6&6\\
3&-1&2&-1&6\\
1&-1&0&-1&-2\\
-3&2&-3&6&-10\end{bmatrix}
\end{align*}
@col
Row-reduce $A$ to form a row-equivalent matrix in reduced row-echelon form,
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&0&2&-1\\
0&\boxed{1}&0&3&1\\
0&0&\boxed{1}&-2&5\\
0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Then the above theorem says we can grab the nonzero columns of $B^{t}$ and write
\begin{align*}
\displaystyle X=\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)=\mathrm{Span}\,\left\{\begin{bmatrix}1\\
0\\
0\\
2\\
-1\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
3\\
1\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
-2\\
5\end{bmatrix}\right\}
\end{align*}
@col
These three vectors provide a much-improved description of $X$. There are fewer vectors, and the pattern of zeros and ones in the first three entries makes it easier to determine membership in $X$.
@endcol
@end
@slide
@thm
@title{Column Space, Row Space, Transpose}
@label{CSRST}
Suppose $A$ is a matrix. Then $\mathcal{C}\left(A\right)=\mathcal{R}\!\left(A^{t}\right)$.
@end
@proof
@newcol
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\mathcal{C}\left(\left(A^{t}\right)^{t}\right)=\mathcal{R}\!\left(A^{t}\right)
\end{align*}
@qed
@endcol
@end
@slide
@eg
<strong>Column space from row operations</strong>

Find the column space of $A$ in @ref{eg:4x7matrix140-107-9}.

<strong>Method 1</strong>
@newcol
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Let
\begin{align*}
\displaystyle T=\left\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\right\}=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\right\}.
\end{align*}
@col
Then $T$ is linear independent and $\mathcal{C}\left(A\right)=\mathrm{Span}\,T$.
@endcol

<strong>Method 2</strong>
@newcol
The transpose of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&2&0&-1\\
4&8&0&-4\\
0&-1&2&2\\
-1&3&-3&4\\
0&9&-4&8\\
7&-13&12&-31\\
-9&7&-8&37\end{bmatrix}.
\end{align*}
@col
Row-reduced this becomes,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&0&-\frac{31}{7}\\
0&\boxed{1}&0&\frac{12}{7}\\
0&0&\boxed{1}&\frac{13}{7}\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
Now, using Theorem    @ref{CSRST} and Theorem    @ref{BRS},
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\mathcal{R}\!\left(A^{t}\right)=\mathrm{Span}\,\left\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\right\}.
\end{align*}
@col
This is a very nice description of the column space. Fewer vectors than the 7 involved in the definition, and the pattern of the zeros and ones in the first 3 slots can be used to advantage. For example,
let’s check if
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}
\end{align*}
is in $\mathcal{C}\left(A\right)$ or not.

@col
If it is, then
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=x\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+y\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+z\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}=\begin{bmatrix}x\\
y\\
z\\
-\frac{31}{7}x+\frac{12}{7}y+\frac{13}{7}z\end{bmatrix}.
\end{align*}
@col
From the first three coordinate $x=3,y=9,z=1$. Let’s check the last coordinate:
\begin{align*}
\displaystyle -\frac{31}{7}\times 3+\frac{12}{7}\times 9+\frac{13}{7}\times 1=4.
\end{align*}
@col
So
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=3\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+9\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+1\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}
\end{align*}
@col
and hence $\mathbf{b}\in\mathcal{C}\left(A\right)$.
@endcol
@end

@remark
@newcol
Both methods describe algorithms to find bases
(i.e., linear independent set the generate the column space)
for the column space.
Here are the differences. <ol class="ltx_enumerate">
<li class="ltx_item"> In method 1, we find a subset of columns that forms a basis.
However in method 2, the basis is not a subset of columns. </li>
<li class="ltx_item"> Given a vector $\mathbf{b}\in\mathcal{C}\left(A\right)$, it is easier to express it as a linear combination of the basis given by method 2. </li></ol>
@endcol
@end
<!--
@slide
Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}$ be a subset of ${\mathbb{R}}^{m}$
There are at least two methods to find a subset
$T\subseteq \{ S \}$ such that:

(i)
$T$ is linearly independent

(ii) $\mathrm{Span}\,T=\mathrm{Span}\,S$. (In other words $T$ is a basis for $\mathrm{Span}\,S$.)

<strong style="">Method 1</strong>
@newcol
Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]\xrightarrow{\text{RREF}}B$.

@col
Suppose $D=\left\{d_{1},\ldots,d_{r}\right\}$ be the indexes of the pivot columns of $B$.

@col
Let $T=\left\{\mathbf{v}_{d_{1}},\ldots,\mathbf{v}_{d_{r}}\right\}$. Then $T$ is a basis for $\mathrm{Span}\,S=\mathcal{C}\left(A\right)$
@endcol

<strong style="">Method 2</strong>
@newcol
Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]$. Suppose $A^{t}\xrightarrow{\text{RREF}}B$.

@col
Let $T$ be the nonzero columns of $B^{t}$. Then $T$ is a basis for $\mathrm{Span}\,S=\mathcal{C}\left(A\right)$

This is an example from Lecture 14.
@endcol
@endcol
-->
@section{Bases and Nonsingular Matrices}
@slide
@thm
Suppose that $A$ is a square matrix of size $m$.

Then, the columns of $A$ is a basis for ${\mathbb{R}}^{m}$ if and only if $A$ is nonsingular.
@end
@proof
@newcol
This is a direct consequence of the theorem  @ref{NME2}:

@col
If columns of $A$ form a basis,
then in particular they are linearly independent.
So, item 5 of the theorem holds.

It now follows from the theorem that item 1,
namely that $A$ is nonsingular, also holds.

@col
Conversely, suppose $A$ is nonsingular.

@col
Then, by Item 5 of @ref{NME2} the columns of $A$ are linearly independent,
and by Item 4 they span $\mathbb{R}^m$.
Hence, the columns of $A$ is a basis for $\mathbb{R}^m$.
@qed
@endcol
@end
@slide
In fact, we may further extend @ref{NME2} as follows:
@thm
@title{Nonsingular Matrix Equivalences}
@label{NME25}
@newcol
Suppose that $A$ is an $m\times m$ square matrix.
The following are equivalent: <ol class="ltx_enumerate">
<li class="ltx_item">
@col
$A$ is nonsingular. </li>
<li class="ltx_item">
@col
$A$ row-reduces to the identity matrix. </li>
<li class="ltx_item">
@col
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$. </li>
<li class="ltx_item">
@col
The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$. </li>
<li class="ltx_item">
@col
The columns of $A$ form a linearly independent set. </li>
<li class="ltx_item">
@col
The columns of $A$ form a basis for $\mathbb{R}^m$. </li></ol>
@endcol
@end
@slide
@skip
@eg
Consider $S^{\prime}=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\end{bmatrix}\right\}$.

Let
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}]=\begin{bmatrix}1&1\\
0&1\end{bmatrix}
\end{align*}
@newcol
<strong>Exercise.</strong> The matrix $A$ is nonsingular.

@col
Hence, $S^{\prime}$ is a basis for ${\mathbb{R}}^{2}$.
@endcol
@end
@slide
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.
\end{align*}
@newcol
It may be shown that $A$ is row equivalent to the $3\times 3$
identity matrix.

@col
Hence $A$ is nonsingular,
so the columns of $A$ form a basis for ${\mathbb{R}}^{3}$.
@endcol
@end
@section{Bases of Null Spaces}

In this section, we will find a linearly independent set that spans a null space.
Recall that, by @ref{SSNS},
there exists a particular set of $n-r$
vectors that could be used to span the null space of a matrix.

@eg
@label{LINSB}
<strong>Linear independence of null space basis</strong>

Suppose that we are interested in the null space of a $3\times 7$ matrix $A$ which row-reduces to
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&-2&4&0&3&9\\
0&\boxed{1}&5&6&0&7&1\\
0&0&0&0&\boxed{1}&8&-5\end{bmatrix}.
\end{align*}
@newcol
Then $F=\{3,\,4,\,6,\,7\}$ is the set of indices for our four free variables that would be used in a description of the solution set for the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Applying @ref{HMVEI}, we can begin to construct a set of four vectors whose span is the null space of $A$, a set of vectors we will refer to as $T$.
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)
=\mathrm{Span}\,T
&=\mathrm{Span}\,\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\mathbf{z}_{4}\}\\
&=\mathrm{Span}\,\left\{\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix}\right\}
\end{align*}
@col
So far, we have constructed as much of these individual vectors as we can, based just on the knowledge of the contents of the set $F$. This has allowed us to determine the entries in slots 3, 4, 6 and 7, while we have left slots 1, 2 and 5 blank. Without doing any more, let us ask if $T$ is linearly independent? Begin with a relation of linear dependence on $T$, and see what we can learn about the scalars.

@col
\begin{align*}
\displaystyle\mathbf{0}&\displaystyle=\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\alpha_{4}\mathbf{z}_{4} \\
\displaystyle\begin{bmatrix}0\\
0\\
0\\
0\\
0\\
0\\
0\end{bmatrix}&\displaystyle=\alpha_{1}\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix}+\alpha_{2}\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix}+\alpha_{3}\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix}+\alpha_{4}\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix} \\
&\displaystyle=\begin{bmatrix}\\
\\
\alpha_{1}\\
0\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
\alpha_{2}\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
\alpha_{3}\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
\alpha_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
\alpha_{1}\\
\alpha_{2}\\
\\
\alpha_{3}\\
\alpha_{4}\end{bmatrix}
\end{align*}
@col
Applying the equalities of vectors, we see that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$. So the only relation of linear dependence on the set $T$ is the trivial one. By the definition of linear independence, the set $T$ is linearly independent. The important feature of this example is how the @keyword{pattern of zeros and ones} in the four vectors led to the conclusion of linear independence.
@endcol
@end
@slide
@thm
@title{Basis for Null Spaces}
@label{BNS}
Suppose that $A$ is an $m\times n$ matrix, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ and $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$ be the sets of column indices of $B$ which are and are not, respectively, pivot columns. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$ as
\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&\text{if $i\in F$, $i=f_{j}$}\\
0&\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}
(In fact $\mathbf{z}_{j}$ corresponding to the solution $x_{f_{j}}=1$ and $x_{f_{k}}=0$ for $k\neq j$.) Define the set $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$. Then <ol class="ltx_enumerate">
<li class="ltx_item"> ${\mathcal{N}}\!\left(A\right)=\mathrm{Span}\,S$. </li>
<li class="ltx_item"> $S$ is a linearly independent set. </li></ol>
@end
@proof
@newcol
Study the above example. You can skip the proof for now. Notice first that the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$, are the same as the $n-r$ vectors defined in @ref{SSNS}. Also, the hypotheses of @ref{SSNS} are the same as the hypotheses of the theorem we are currently proving. So @ref{SSNS} tells us that ${\mathcal{N}}\!\left(A\right)=\mathrm{Span}\,S$. That was the easy half, but the second part is not much harder. What is new here is the claim that $S$ is a linearly independent set.

To prove the linear independence of a set, we need to start with a relation of linear dependence and somehow conclude that the scalars involved must all be zero, i.e., that the relation of linear dependence is trivial. So, we start with and equation of the form
\begin{align*}
\displaystyle \alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}=\mathbf{0}.
\end{align*}
For each $j$, $1\leq j\leq n-r$, consider the equality of the individual entries of the vectors on both sides of this equality in position $f_{j}$:
\begin{align*}
\displaystyle 0&\displaystyle=\left[\mathbf{0}\right]_{f_{j}} \\
&\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}\right]_{f_{j}}+\left[\alpha_{2}\mathbf{z}_{2}\right]_{f_{j}}+\left[\alpha_{3}\mathbf{z}_{3}\right]_{f_{j}}+\cdots+\left[\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&\displaystyle=\alpha_{1}\left[\mathbf{z}_{1}\right]_{f_{j}}+\alpha_{2}\left[\mathbf{z}_{2}\right]_{f_{j}}+\alpha_{3}\left[\mathbf{z}_{3}\right]_{f_{j}}+\cdots+ \\
&\displaystyle\quad\quad\alpha_{j-1}\left[\mathbf{z}_{j-1}\right]_{f_{j}}+\alpha_{j}\left[\mathbf{z}_{j}\right]_{f_{j}}+\alpha_{j+1}\left[\mathbf{z}_{j+1}\right]_{f_{j}}+\cdots+ \\
&\displaystyle\quad\quad\alpha_{n-r}\left[\mathbf{z}_{n-r}\right]_{f_{j}} \\
&\displaystyle=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+ \\
&\displaystyle\quad\quad\alpha_{j-1}(0)+\alpha_{j}(1)+\alpha_{j+1}(0)+\cdots+\alpha_{n-r}(0)& \text{ definition of } \mathbf{z}_{j} \\
&\displaystyle=\alpha_{j}
\end{align*}
So for all $j$, $1\leq j\leq n-r$, we have $\alpha_{j}=0$. Hence, the only relation of linear dependence on $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$ is the trivial one. By the definition of linear independence, the set is linearly independent, as desired.
@qed
@endcol
@end
@slide
@eg
Find the null space of the matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}-2&-1&-2&-4&4\\
-6&-5&-4&-4&6\\
10&7&7&10&-13\\
-7&-5&-6&-9&10\\
-4&-3&-4&-6&6\\
\end{bmatrix}.
\end{align*}
@end
@sol
@newcol
The RREF of $A$ is:
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&0&1&-2\\
0&\boxed{1}&0&-2&2\\
0&0&\boxed{1}&2&-1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
The free variables are $x_{4}$ and $x_{5}$.

Setting $x_{4}=1$ and $x_{5}=0$ gives:

@col
\begin{align*}
\displaystyle \mathbf{z}_{1}=\begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix}.
\end{align*}
@col
Setting instead $x_{4}=0$ and $x_{5}=1$ gives
\begin{align*}
\displaystyle \mathbf{z}_{2}=\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}.
\end{align*}
Hence
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\mathrm{Span}\,\left\{\begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix},\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}\right\}.
\end{align*}
@qed
@endcol
@end
