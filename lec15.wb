@course{Math 1030}
@setchapter{15}
@chapter{Basis}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section B (print version p233-238), Section D (print version p245-253)

<h5 class="notkw">Exercise.</h5><ul>
<li> Exercises with solutions can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)

Section B p.88-92 C10, C11, C12, M20 Section D
p.92-96 C21, C23, C30, C31, C35, C36, C37, M20, M21. </li></ul>

@section{Basis}
@defn
Let $V$ be a vector space. Then a subset $S$ of $V$ is said to be a @keyword{basis} for $V$ if <ol class="ltx_enumerate">
<li class="ltx_item"> $S$ is linearly independent. </li>
<li class="ltx_item"> $\left< S\right>=V$, i.e. $S$ spans $V$. </li></ol>
@end
@remark
@newcol
Most of the time $V$ is a subspace of ${\mathbb{R}}^{m}$. Occasionally $V$ is assumed to be a subspace of $M_{mn}$ or $P_{n}$. It does not hurt to assume $V$ is a subspace of ${\mathbb{R}}^{m}$.
@endcol
@end
@slide
@eg
Let $V={\mathbb{R}}^{m}$, then $B=\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}$ is a basis for $V$.
(recall all the entries of $\mathbf{e}_{i}$ is zero, except the $i$-th entry being $1$).

@newcol
It is called the @keyword{standard basis}: Obviously $B$ is linearly independent. Also, for any $\mathbf{v}\in V$, $\mathbf{v}=[\mathbf{v}]_{1}\mathbf{e}_{1}+\cdots+[\mathbf{v}]_{m}\mathbf{e}_{m}\in\left< B\right>$. So $\left< B\right>=V$.
@endcol
@end
@slide
@eg
<strong>Math major only</strong>

@newcol
Consider $V=M_{22}$. Let:
\begin{align*}
\displaystyle B_{11}&=\begin{bmatrix}1&0\\
0&0\end{bmatrix},
&
B_{12}&=\begin{bmatrix}0&1\\
0&0\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle B_{21}&=\begin{bmatrix}0&0\\
1&0\end{bmatrix},
&
B_{22}&=\begin{bmatrix}0&0\\
0&1\end{bmatrix},
\end{align*}
@col
Then $B=\left\{B_{11},B_{12},B_{21},B_{22}\right\}$ is a basis for $V$.

@col
Check: Obviously $B$ is linearly independent (exercise). Also for any $A\in V$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
c&d\end{bmatrix}=aB_{11}+bB_{12}+cB_{21}+dB_{22}.
\end{align*}
@col
So $\left< B\right>=M_{22}$.
@endcol
@end

@ex
<strong>Math major only</strong>

@newcol
Let $V=M_{mn}$.

For $1\leq i\leq m$, $1\leq j\leq n$,
let $B_{ij}$ be the $m\times n$ matrix with $(i,j)$-th entry equal to $1$ and all other entries equal to $0$.

Then $\left\{ B_{ij} | 1\leq i \leq m, 1\leq j\leq n\right\}$ is a basis for $V$.
@endcol
@end

@eg
<strong>Math major only</strong>

@newcol
Let $V=P_{n}$. Then $1,x,x^{2},\ldots,x^{n}$ is a basis. It is easy to show that $S=\left\{1,x,x^{2},\ldots,x^{n}\right\}$ is linearly independent. Also any polynomial
\begin{align*}
\displaystyle f(x)=a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}
\end{align*}
@col
is a linear combinations of $S$.

@endcol
@end
@slide
@eg
A vector space can have different bases.

Consider the vector space $V={\mathbb{R}}^{2}$.

Then,
\[
S=\left\{\mathbf{e}_{1},\mathbf{e}_{2}\right\}
\]
is a basis for $V$, and:

@newcol
\[
S^{\prime}=\left\{\begin{bmatrix}1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\end{bmatrix}\right\}
\]
is also a basis.
@endcol
@end
@section{Bases for spans of column vectors}

Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}$ be a subset of ${\mathbb{R}}^{m}$
Recall from lecture 14,
that there are several methods to find a subset
$T\subseteq \langle S \rangle$.

@newcol
Such that (i) $T$ is linearly independent (ii) $\left< T\right>=\left< S\right>$. (In other words $T$ is a basis for $\left< S\right>$.)

<strong>Method 1</strong>
@newcol
Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]\xrightarrow{\text{RREF}}B$.

@col
Suppose $D=\left\{d_{1},\ldots,d_{r}\right\}$ be the indexes of the pivot columns of $B$.

@col
Let $T=\left\{\mathbf{v}_{d_{1}},\ldots,\mathbf{v}_{d_{r}}\right\}$. Then $T$ is a basis for $\left< S\right>=\mathcal{C}\left(A\right)$
@endcol

<strong>Method 2</strong>
@newcol
Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]$. Suppose $A^{t}\xrightarrow{\text{RREF}}B$.

@col
Let $T$ be the nonzero columns of $B^{t}$. Then $T$ is a basis for $\left< S\right>=\mathcal{C}\left(A\right)$

This is an example from Lecture 14.
@endcol
@endcol
@slide
@eg
@keyword{Column space from row operations}

Let
\begin{align*}
\displaystyle S=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\mathbf{v}_{6}=\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\mathbf{v}_{7}=\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\right\}.
\end{align*}
@newcol
Find a basis for $\left< S\right>$.
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{7}]=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}.
\end{align*}

<strong>Method 1</strong>
@newcol
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Let
\begin{align*}
\displaystyle T=\left\{\mathbf{v}_{1},\mathbf{v}_{3},\mathbf{v}_{4}\right\}=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\right\}.
\end{align*}
@col
Then $T$ is a basis for $\left< S\right>=\mathcal{C}\left(A\right)$.
@endcol

<strong>Method 2</strong>
@newcol
The transpose of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&2&0&-1\\
4&8&0&-4\\
0&-1&2&2\\
-1&3&-3&4\\
0&9&-4&8\\
7&-13&12&-31\\
-9&7&-8&37\end{bmatrix}.
\end{align*}
@col
Row-reduced this becomes,
\begin{align*}
\displaystyle D=\begin{bmatrix}\boxed{1}&0&0&-\frac{31}{7}\\
0&\boxed{1}&0&\frac{12}{7}\\
0&0&\boxed{1}&\frac{13}{7}\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
Then we can take
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\right\}.
\end{align*}

$T$ is a basis for $\mathcal{C}\left(A\right)=\left< S\right>$.
@endcol
@endcol
@end
@slide
@thm
@label{basisexists}
Let $S$ be a finite subset of ${\mathbb{R}}^{m}$.
Then, a basis for $\left< S\right>$ exists.

@newcol
In fact, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left< S\right>$ (see @ref{BCS}).
@endcol
@end
@section{Bases and nonsingular matrices}
@slide
@thm
Suppose that $A$ is a square matrix of size $m$.

Then, the columns of $A$ is a basis for ${\mathbb{R}}^{m}$ if and only if $A$ is nonsingular.
@end
@proof
@newcol
This is a direct consequence of the theorem  @ref{NME2}:

@col
If columns of $A$ form a basis,
then in particular they are linearly independent.
So, item 5 of the theorem holds.

It now follows from the theorem that item 1,
namely that $A$ is nonsingular, also holds.

@col
Conversely, suppose $A$ is nonsingular.

@col
Then, by Item 5 of @ref{NME2} the columns of $A$ are linearly independent,
and by Item 4 they span $\mathbb{R}^m$.
Hence, the columns of $A$ is a basis for $\mathbb{R}^m$.
@qed
@endcol
@end
@slide
In fact, we may further extend @ref{NME2} as follows:
@thm
@title{Nonsingular Matrix Equivalences}
@label{NME25}
@newcol
Suppose that $A$ is an $m\times m$ square matrix.
The following are equivalent: <ol class="ltx_enumerate">
<li class="ltx_item">
@col
$A$ is nonsingular. </li>
<li class="ltx_item">
@col
$A$ row-reduces to the identity matrix. </li>
<li class="ltx_item">
@col
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$. </li>
<li class="ltx_item">
@col
The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$. </li>
<li class="ltx_item">
@col
The columns of $A$ form a linearly independent set. </li>
<li class="ltx_item">
@col
The columns of $A$ form a basis for $\mathbb{R}^m$. </li></ol>
@endcol
@end
@slide
@eg
Consider $S^{\prime}=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\end{bmatrix}\right\}$.

Let
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}]=\begin{bmatrix}1&1\\
0&1\end{bmatrix}
\end{align*}
@newcol
<strong>Exercise.</strong> The matrix $A$ is nonsingular.

@col
Hence, $S^{\prime}$ is a basis for ${\mathbb{R}}^{2}$.
@endcol
@end
@slide
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.
\end{align*}
@newcol
It may be shown that $A$ is row equivalent to the $3\times 3$
identity matrix.

@col
Hence $A$ is nonsingular,
so the columns of $A$ form a basis for ${\mathbb{R}}^{3}$.
@endcol
@end
@section{Dimension}

@defn
@title{Dimension}
Let $V$ be a vector space.

Suppose a finite set of vectors $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$ is a basis for $V$.

Then, we say that $V$ is a @keyword{finite dimensional vector space}.

The number $t$ (namely the number of vectors in the basis)
is called the @keyword{dimension} of $V$.
@end
@remark
@newcol
It is a non-trivial fact that the dimension is well-defined, i.e.,
If both $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$
and $\left\{\mathbf{u}_{1},\ldots,\mathbf{u}_{s}\right\}$
are bases for $V$,
then $s=t$.
@endcol
@end
@slide
@thm
@label{SSLD}
Suppose that $S=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{t}\right\}$ is a finite set of vectors which spans the vector space $V$. Then any set of $t+1$ or more vectors from $V$ is linearly dependent.
@end
@proof
@newcol
Let $\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_m$ be $m$ vectors in $V$,
where $m \geq t + 1$.
Let $A = [\vect{v}_1 | \vect{v}_2 | \cdots | \vect{v}_t ]$.
Since $S$ is a basis, for every $\vect{u}_i$ ($1\leq i \leq m$)
there exists
$\vect{w}_i \in \mathbb{R}^t$ such that:
\[
A\vect{w}_i = \vect{u}_i.
\]

@col
Now, consider the matrix:
\[
B = [\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m].
\]
This is a $t \times m$ matrix.
In particular, it has more columns than rows,
due to the assumption that $m > t$.

@col
Hence, the homogeneous linear system $\mathcal{LS}(B, \vect{0})$
has a non-trivial solution $\vect{x} \in \mathbb{R}^m$.  That is:
\[
B\vect{x} = \vect{0}.
\]

@col
The above equation implies that:
\[
A\left(B\vect{x}\right) = A\vect{0} = \vect{0}.
\]
@col
By the associativity of matrix multiplication, we have:
\[
A\left(B\vect{x}\right) = \left(AB\right)\vect{x}.
\]
@col
On the other hand:
@steps
\[
\begin{split}
AB &= A[\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m]
\\&
@nstep{=[A\vect{w}_1 | A\vect{w}_2 | \cdots | A\vect{w}_m]}
\\&
@nstep{= [\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]}
\end{split}
\]
@endsteps
@col
Hence,
\[
\left(AB\right)\vect{x} = \vect{0}
\]
is equivalent to:
\[
[\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]\vect{x} = \vect{0}
\]
@col
which is in turn equivalent to:
\[
x_1 \vect{u}_1 + x_2 \vect{u}_2 + \cdots x_m \vect{u}_m = \vect{0}.
\]
Since, $\vect{x}$ is not the zero vector,
not all the $x_i$'s are equal to zero.
We conclude that the vectors
$\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_m$ are linearly dependent.
@qed
@endcol
@end

@slide
@thm
Suppose that $V$ is a vector space with a finite basis $B$ and a second basis $C$.

@newcol
Then $B$ and $C$ have the same size.
@endcol
@end
@proof
@newcol
Denote the size of $B$ by $t$. If $C$ has $\geq t+1$ vectors,
then by the previous theorem, $C$ is linearly dependent,
in contradiction to the condition that $C$ is a basis.

@col
By the same reasoning, the linearly independent set $B$ must also not
have more vectors than $C$.

@col
So, $B$ and $C$ have the same number of vectors.
@qed
@endcol
@end
@remark
@newcol
The above theorem shows that the dimension is well-defined. No matter which basis we choose, the size is always the same.
@endcol
@end
@slide
@eg
It follows from @ref{d848ac74d9e041feff51c974a9969985} that:
\[
\dim{\mathbb{R}}^{m}=m.
\]
@end
@slide
@eg
<strong>Math major only</strong>

$\dim M_{mn}=mn$. See example 3.
@end

@eg
<strong>Math major only</strong>

$\dim P_{n}=n+1$. See example 4.
@end

@eg
<strong>Math major only</strong>

Let $S_{2}$ be the set of $2\times 2$ symmetric matrices. For $A\in S_{2}$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
b&c\end{bmatrix}=a\begin{bmatrix}1&0\\
0&0\end{bmatrix}+b\begin{bmatrix}0&1\\
1&0\end{bmatrix}+c\begin{bmatrix}0&0\\
0&1\end{bmatrix}
\end{align*}
@newcol
We can show that:
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1&0\\
0&0\end{bmatrix},\begin{bmatrix}0&1\\
1&0\end{bmatrix},\begin{bmatrix}0&0\\
0&1\end{bmatrix}\right\}
\end{align*}
@col
is a basis for $S_{2}$. Hence $\dim S_{2}=3$.
@endcol
@end

@eg
<strong>Math major only</strong>

Let $P$ be the set of all real polynomials. As $\left\{1,x,x^{2},x^{3},\ldots\right\}$ is linearly independent,
so $\dim P$ does not exists (or we can write $\dim P=\infty$).
@end
@slide
@lemma
Let $V$ be a vector space and $\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\in V$.

@newcol
Suppose
$S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent and $\mathbf{u}\notin\left< S\right>$. Then
$S^{\prime}=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\right\}$ is linearly independent.
@endcol
@end
@proof
@newcol
Let the relation of linear dependence of $S^{\prime}$ be
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}+\alpha\mathbf{u}=\mathbf{0}.
\end{align*}
@col
Suppose $\alpha\neq 0$, then
\begin{align*}
\displaystyle \mathbf{u}=-\frac{\alpha_{1}}{\alpha}\mathbf{v}_{1}-\cdots-\frac{\alpha_{k}}{\alpha}\mathbf{v}_{k}\in\left< S\right>.
\end{align*}
@col
Contradiction.

@col
So $\alpha=0$, then
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\mathbf{0}.
\end{align*}
@col
By the linear independence of $S$, $\alpha_{i}=0$ for all $i$. Hence the above relation of dependence of $S^{\prime}$ is trivial.
@qed
@endcol
@end
@slide
@thm
@label{basisexists2}
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.

@newcol
There exists a basis for $V$.
@endcol
@end
@proof
@newcol
If $V$ is the zero vector space, i.e. $V=\left\{\mathbf{0}\right\}$. Then the theorem is trivial. Suppose $V$ is not the zero vector space, let $\mathbf{v}_{1}$ be a nonzero vector in $V$. If $V=\left< \left\{\mathbf{v}_{1}\right\}\right>$, we can take $S=\left\{\mathbf{v}_{1}\right\}$. Then obviously $\left\{\mathbf{v}_{1}\right\}$ is linearly independent and hence $S$ is a basis for $V$.

@col
Otherwise, let $\mathbf{v}_{2}\in V$ but not in $\left< \left\{\mathbf{v}_{1}\right\}\right>$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$ is linearly independent. If $V=\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}\right>$, we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$.

@col
So $S$ is a basis for $V$.

@col
Otherwise, let $\mathbf{v}_{3}\in V$ but not in $\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}\right>$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$ is linearly independent. Repeat the above process, inductive we can define $\mathbf{v}_{k+1}$ as following: If $V=\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right>$,
we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, $S$ is a basis for $V$.

@col
Otherwise defined $\mathbf{v}_{k+1}\not\in\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right>$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k+1}\right\}$ is linearly independent.

@col
If the process stops, say at step $k$, i.e., $V=\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right>$.

@col
Then we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, it is a basis for $V$.

@col
This completes the proof.

@col
Otherwise, the process continues infinitely, in particular, we can take $k=m+1$ and
$V\neq\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}\right>$ and
$\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ is linearly independent.

@col
Since $\left\langle\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}\right\rangle = \mathbb{R}^m$,
by @ref{SSLD} the vectors $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ are linearly dependent. Contradiction.
@qed
@endcol
@end
@slide
@prop
@label{basisexistsprop}
Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}\subseteq{\mathbb{R}}^{m}$. Then
\begin{align*}
\displaystyle \dim\left< S\right>\leq n.
\end{align*}
@end
@proof
@newcol
By Theorem    @ref{basisexists}, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left< S\right>$.
\begin{align*}
\displaystyle \dim\left< S\right>
=
\text{number of vectors in $T$}
\leq
\text{number of vectors in $S$}=n.
\end{align*}
@qed
@endcol
@end
@remark
@newcol
Both @ref{basisexists} and @ref{basisexistsprop} is valid
if we replace ${\mathbb{R}}^{m}$ by $P_{n}$, $M_{mn}$
or any finite dimensional vector space.
@endcol
@end

@slide
@thm
Suppose a vector space $V$ has dimension $n$.
Then, any linearly independent set with $n$ vectors in $V$
is a basis for $V$.
@end
@thm
Suppose a vector space $V$ has dimension $n$.
Suppose $S$ is a set of $n$ vectors in $V$ which spans $V$
(That is, $\left\langle S \right\rangle = V$).

Then, $S$ is a basis for $V$.
@end

@section{Rank and nullity of a matrix}
@defn
@title{Nullity of a matrix}
@label{NOM}
Suppose that $A \in M_{mn}$. Then the @keyword{nullity} of $A$ is the dimension of the null space of $A$,
$\nullity{A}=\dim(\nsp{A})$.
@end

@defn
@title{Rank of a matrix}
@label{ROM}
Suppose that $A \in M_{mn}$. Then the @keyword{rank} of $A$ is the dimension of the column space of $A$,
$\rank{A}=\dim(\csp{A})$.
@end

@slide
@eg
@keyword{Rank and nullity of a matrix}

Let us compute the rank and nullity of
\[
A=\begin{bmatrix}
2 & -4 & -1 & 3 & 2 & 1 & -4\\
1 & -2 & 0 & 0 & 4 & 0 & 1\\
-2 & 4 & 1 & 0 & -5 & -4 & -8\\
1 & -2 & 1 & 1 & 6 & 1 & -3\\
2 & -4 & -1 & 1 & 4 & -2 & -1\\
-1 & 2 & 3 & -1 & 6 & 3 & -1
\end{bmatrix}
\]
@newcol
To do this, we will first row-reduce the matrix since that will help us determine bases for the null space and column space.
\[
\begin{bmatrix}
\leading{1} & -2 & 0 & 0 & 4 & 0 & 1\\
0 & 0 & \leading{1} & 0 & 3 & 0 & -2\\
0 & 0 & 0 & \leading{1} & -1 & 0 & -3\\
0 & 0 & 0 & 0 & 0 & \leading{1} & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
@col
From this row-equivalent matrix in reduced row-echelon form we record $D=\set{1,\,3,\,4,\,6}$ and $F=\set{2,\,5,\,7}$.

@col
By @ref{BCS}, for each index in $D$, we can create a single basis vector.
In fact $T=\{\vect{A}_1, \vect{A}_3, \vect{A}_4, \vect{A}_6\}$ is a basis for $\csp{A}$.
In total the basis will have $4$ vectors, so the column space of $A$ will have dimension $4$ and we write $\rank{A}=4$.

@col
By @ref{SSNS}, for each index in $F$, we can create a single basis vector.  In total the basis will have $3$ vectors, so the null space of $A$ will have dimension $3$ and we write $\nullity{A}=3$. In fact:

@col
\[
R = \left\{\colvector{ 2 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0},
\colvector{ -4 \\ 0 \\ -3 \\ 1 \\ 1 \\ 0 \\ 0},
\colvector{-1 \\ 0 \\ 2 \\ 3 \\ 0 \\ -1 \\ 1}
\right\}
\]
is a basis for $\nsp{A}$.
@endcol
@end

@slide
@thm
@title{Computing rank and nullity}
Suppose $A \in M_{mn}$ and $A \rref B$.
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).
Then $\rank{A}=r$ and $\nullity{A}=n-r$.
@end
@proof
@newcol
Let $D=\{d_1, \ldots, d_r\}$ be the indexes of the pivot columns of $B$.
By @ref{BCS}, $\{\vect{A}_{d_1}, \ldots, \vect{A}_{d_r}\}$ is a basis for $\csp{A}$.
So $\rank{A}=r$.

By @ref{SSNS}, each free variable corresponding to a single basis vector for the null space.
So $\nullity{A}$ is the number of free variables $=n-r$.
@endcol
@end

@cor
@title{Dimension formula}
Suppose $A\in M_{mn}$, then
\begin{align*}
\displaystyle r\left(A\right)+n\left(A\right)=n.
\end{align*}
@end
@slide
@thm
Let $A$ be a $m\times n$ matrix. Then
\begin{align*}
\displaystyle r\left(A\right)=r\left(A^{t}\right).
\end{align*}
@newcol
Equivalently
\begin{align*}
\displaystyle \dim\mathcal{C}\left(A\right)=\dim\mathcal{R}\!\left(A\right).
\end{align*}
@endcol
@end
@proof
@newcol
Let $A\xrightarrow{\text{RREF}}B$.

@col
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).

@col
Then by the above discussion $r=r\left(A\right)$. By @ref{BRS}, the first $r$ columns of $B^{t}$ form a basis for $\mathcal{R}\!\left(A\right) = \mathcal{C}(A^t)$.
Hence $r=r\left(A^{t}\right)$. This completes the proof.
@qed
@endcol
@end
Let us take a look at the rank and nullity of a square matrix.
@slide
@eg
The matrix
\begin{align*}
\displaystyle E=\begin{bmatrix}0&4&-1&2&2&3&1\\
2&-2&1&-1&0&-4&-3\\
-2&-3&9&-3&9&-1&9\\
-3&-4&9&4&-1&6&-2\\
-3&-4&6&-2&5&9&-4\\
9&-3&8&-2&-4&2&4\\
8&2&2&9&3&0&9\end{bmatrix}
\end{align*}
@newcol
is row-equivalent to the matrix in reduced row-echelon form,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&0&0&0&0&0\\
0&\boxed{1}&0&0&0&0&0\\
0&0&\boxed{1}&0&0&0&0\\
0&0&0&\boxed{1}&0&0&0\\
0&0&0&0&\boxed{1}&0&0\\
0&0&0&0&0&\boxed{1}&0\\
0&0&0&0&0&0&\boxed{1}\end{bmatrix}
\end{align*}
@col
With $n=7$ columns and $r=7$ nonzero rows tells us the rank is $r\left(E\right)=7$ and the nullity is $n\left(E\right)=7-7=0$.
@endcol
@end
The value of either the nullity or the rank are enough to characterize a nonsingular matrix.

@slide
@thm
@title{Rank and Nullity of a Nonsingular Matrix}
@label{RNNM}
Suppose that $A$ is a square matrix of size $n$. The following are equivalent. <ol class="ltx_enumerate">
<li class="ltx_item"> A is nonsingular. </li>
<li class="ltx_item"> The rank of $A$ is $n$, $r\left(A\right)=n$. </li>
<li class="ltx_item"> The nullity of $A$ is zero, $n\left(A\right)=0$. </li></ol>
@end
@proof
@newcol
(1 $\Rightarrow$ 2)
@newcol
If $A$ is nonsingular then $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$.

@col
If $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$, then the column space has dimension $n$, so the rank of $A$ is $n$.
@endcol

(2 $\Rightarrow$ 3)
@newcol
Suppose $r\left(A\right)=n$. Then the dimension formula gives
\begin{align*}
\displaystyle n\left(A\right)&\displaystyle=n-r\left(A\right) \\
&\displaystyle=n-n \\
&\displaystyle=0
\end{align*}
@endcol

(3 $\Rightarrow$ 1)
@newcol
Suppose $n\left(A\right)=0$, so a basis for the null space of $A$ is the empty set. This implies that ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$ and hence $A$ is nonsingular.
@endcol
@qed
@endcol
@end
With a new equivalence for a nonsingular matrix, we can update our list of equivalences which now becomes a list requiring double digits to number.
@slide
@thm
Suppose that $A$ is a square matrix of size $n$. The following are equivalent. <ol class="ltx_enumerate">
<li class="ltx_item"> $A$ is nonsingular. </li>
<li class="ltx_item"> $A$ row-reduces to the identity matrix. </li>
<li class="ltx_item"> The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$. </li>
<li class="ltx_item"> The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$. </li>
<li class="ltx_item"> The columns of $A$ are a linearly independent set. </li>
<li class="ltx_item"> $A$ is invertible. </li>
<li class="ltx_item"> The column space of $A$ is ${\mathbb{R}}^{n}$, $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$. </li>
<li class="ltx_item"> The columns of $A$ are a basis for ${\mathbb{R}}^{n}$. </li>
<li class="ltx_item"> The rank of $A$ is $n$, $r\left(A\right)=n$. </li>
<li class="ltx_item"> The nullity of $A$ is zero, $n\left(A\right)=0$. </li></ol>
@end
@section{Linear relation of $P_n$ and $M_{mn}$}
<strong>You can skip this section. It is for math major only</strong>

@newcol
In this section, we discuss the linear relation of $P_{n}$ or $M_{mn}$ by using the techniques used for the vector space ${\mathbb{R}}^{k}$.

@col
Let $V=P_{n}$ and $f_{1},\ldots,f_{m},g\in P_{n}$.

@col
Write

\begin{align*}
\displaystyle f_{i}(x)=a_{i0}+a_{i1}x+\cdots+a_{in}x^{n},
\end{align*}
\begin{align*}
\displaystyle g(x)=b_{0}+b_{1}x+\cdots+b_{n}x^{n}.
\end{align*}
@col
By comparing coefficients,
\begin{align*}
\displaystyle g(x)=\alpha_{1}f_{1}(x)+\alpha_{2}f_{2}(x)+\cdots+\alpha_{m}f_{m}(x)
\end{align*}
@col
if and only if
\begin{align*}
\displaystyle \alpha_{1}a_{10}+\alpha_{2}a_{20}+\cdots+\alpha_{m}a_{m0}=b_{0},
\end{align*}
\begin{align*}
\displaystyle \alpha_{1}a_{11}+\alpha_{2}a_{21}+\cdots+\alpha_{m}a_{m1}=b_{1},
\end{align*}
\begin{align*}
\displaystyle \vdots
\end{align*}
\begin{align*}
\displaystyle \alpha_{1}a_{1n}+\alpha_{2}a_{2n}+\cdots+\alpha_{m}a_{mn}=b_{n}
\end{align*}
@col
if and only if
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix}+\alpha_{2}\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix}+\cdots+\alpha_{m}\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*}
@col
The above motivates us to define
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix},\cdots,\mathbf{v}_{m}=\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix},\mathbf{u}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*}
@col
The entries of $\mathbf{v}_{i}$ are the coefficients of $f_{i}$.

@endcol
@slide
We then have the following theorem:
@thm
<ol class="ltx_enumerate">
<li class="ltx_item"> $\left\{f_{1},\ldots,f_{m}\right\}$ is linearly independent if and only if $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{m}\right\}$
is linearly independent. </li>
<li class="ltx_item"> $g$ is a linearly combination of $f_{1},\ldots,f_{m}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{m}$ </li></ol>
@end
Problems regarding polynomials can therefore be transformed to problems regarding column vectors.

@newcol
Similarly given $m\times n$ matrix $A_{1},\ldots,A_{k},B$. Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}[A_{1}]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}[A_{2}]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix},\cdots,\mathbf{u}=\begin{bmatrix}[B]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix}.
\end{align*}
We have the following:
@col
@thm
<ol class="ltx_enumerate">
<li class="ltx_item"> $\left\{A_{1},\ldots,A_{k}\right\}$ is linearly independent if and only if $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$
is linearly independent. </li>
<li class="ltx_item"> $B$ is a linearly combination of $A_{1},\ldots,A_{k}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{k}$ </li></ol>
@end
@col
Again, problems regarding polynomials can be transformed to problems regarding column vectors.
@endcol
@slide
@eg
<ol class="ltx_enumerate">
<li class="ltx_item"> Determine if
\begin{align*}
\displaystyle A_{1}=\begin{bmatrix}1&2\\
3&4\end{bmatrix},A_{2}=\begin{bmatrix}1&-1\\
5&6\end{bmatrix},A_{3}=\begin{bmatrix}-2&0\\
-3&-4\end{bmatrix}
\end{align*}
is linearly independent or not. </li>
<li class="ltx_item"> Express
\begin{align*}
\displaystyle B=\begin{bmatrix}-3&0\\
4&1\end{bmatrix}
\end{align*}
@newcol
as a linear combination of $A_{1},A_{2},A_{3}$.
@endcol</li></ol>
@end
@sol
<ol class="ltx_enumerate">
<li class="ltx_item"> Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
3\\
2\\
4\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
5\\
-1\\
6\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}-2\\
-3\\
0\\
-4\end{bmatrix},\mathbf{u}=\begin{bmatrix}-3\\
4\\
0\\
1\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}]=\begin{bmatrix}1&1&-2\\
3&5&-3\\
2&-1&0\\
4&6&-4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&0\\
0&1&0\\
0&0&1\\
0&0&0\\
\end{bmatrix}.
\end{align*}
@newcol
Obviously the columns of the RREF is linearly independent, hence $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$
is linearly independent. Therefore $\left\{A_{1},A_{2},A_{3}\right\}$ is linearly independent.
@endcol</li>
<li class="ltx_item"> Next
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{b}]=\begin{bmatrix}1&1&-2&-3\\
3&5&-3&4\\
2&-1&0&0\\
4&6&-5&1\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&0&1\\
0&1&0&2\\
0&0&1&3\\
0&0&0&0\\
\end{bmatrix}.
\end{align*}
@newcol
Then $\mathbf{u}=\mathbf{v}_{1}+2\mathbf{v}_{2}+3\mathbf{v}_{3}$. Hence $B=A_{1}+2A_{2}+3A_{3}$.
@endcol</li></ol>
@qed
@end
@slide
@eg
Let $f_{1}(x)=1+x+x^{3}$, $f_{2}(x)=2+x+x^{2}$, $f_{3}(x)=4+3x+x^{2}+2x^{3}$, $f_{4}(x)=2x^{2}+x^{3}$, $f_{5}(x)=3+2x+3x^{2}+2x^{3}$.

Find a basis for $\left< \left\{f_{1},f_{2},f_{3},f_{4},f_{5}\right\}\right>$.
@end
@sol
@newcol
Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\\
1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}2\\
1\\
1\\
0\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}4\\
3\\
1\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}0\\
0\\
2\\
1\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}3\\
2\\
3\\
2\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{v}_{4}|\mathbf{v}_{5}]=\begin{bmatrix}1&2&4&0&3\\
1&1&3&0&2\\
0&1&1&2&3\\
1&0&2&1&2\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&2&0&1\\
0&1&1&0&1\\
0&0&0&1&1\\
0&0&0&0&0\\
\end{bmatrix}.
\end{align*}
@col
Therefore $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{4}\right\}$ is a basis for $\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4},\mathbf{v}_{5}\right\}\right>$.

@col
So $\left\{f_{1},f_{2},f_{4}\right\}$ is a basis for $\left< \left\{f_{1},f_{2},f_{3},f_{4},f_{5}\right\}\right>$.
@qed
@endcol
@endcol
@end