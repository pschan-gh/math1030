@course{MATH 1030}
@chapter{Vector space and subspace}
<h5 class="notkw">Reference.</h5>
 Beezer, Ver 3.5 Section VO (print version p57 - p63)Subsection VS, EVS (print version p197-203)
Strang, Section 2.1 <p/><h5 class="notkw">Exercise.</h5> Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection VO (p.28-31) All questions.C10-C15, T05-T07, T13, T17, T18, T30-T32 Section VS (p.75-77) Replace $\mathbb{C}$ (the set of complex numbers) by $\mathbb{R}$ (the set of real numbers) M11, M12, M13, M14, M15, M20.

@section{Null Space of a Matrix}
@defn
@label{NSM}
 The @keyword{null space} of a an $m \times n$ matrix $A$, denoted by ${\mathcal{N}}\!\left(A\right)$,
is the set of vectors $\vec{x} \in \mathbb{R}^n$ such that:
\[
A\vec{x} = \vec{0}.
\] 
@newcol
 Equivalently,
it is the set of all the vectors which are solutions to the homogeneous system $\linearsystem{A}{\mathbf{0}}$.
That is, if:
\[A=\begin{bmatrix}a_{11}&amp;a_{12}&amp;a_{13}&amp;\dots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;a_{23}&amp;\dots&amp;a_{2n}\\
a_{31}&amp;a_{32}&amp;a_{33}&amp;\dots&amp;a_{3n}\\
\vdots&amp;\\
a_{m1}&amp;a_{m2}&amp;a_{m3}&amp;\dots&amp;a_{mn}\\
\end{bmatrix}\]
then ${\mathcal{N}}\!\left(A\right)$ is the solution set of
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&amp;=0\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&amp;=0\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&amp;=0\\
\vdots&amp;\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&amp;=0
\end{align*} 
@endcol
@end
@slide
@eg
 Suppose
\[A=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix}\]
Then
\[\mathbf{x}=\begin{bmatrix}3\\
0\\
-5\\
-6\\
0\\
0\\
1\end{bmatrix}\qquad\qquad\mathbf{y}=\begin{bmatrix}-4\\
1\\
-3\\
-2\\
1\\
1\\
1\end{bmatrix}\]
are in ${\mathcal{N}}\!\left(A\right)$ as $A\mathbf{x}=\mathbf{0}$, $A\mathbf{y}=\mathbf{0}$.

@newcol
 However, the vector
\[\mathbf{z}=\begin{bmatrix}1\\
0\\
0\\
0\\
0\\
0\\
2\end{bmatrix}\]
is not in ${\mathcal{N}}\!\left(A\right)$ as
\[A\mathbf{z}=\begin{bmatrix}-17\\
16\\
-16\\
73\end{bmatrix}\neq\mathbf{0}.\] 
@endcol
@end
@slide
@eg
@label{CNS1}
 Let us compute the null space of
\[A=\begin{bmatrix}2&amp;-1&amp;7&amp;-3&amp;-8\\
1&amp;0&amp;2&amp;4&amp;9\\
2&amp;2&amp;-2&amp;-1&amp;8\end{bmatrix}\]
which we write as ${\mathcal{N}}\!\left(A\right)$. Translating @ref{NSM},
we simply desire to solve the homogeneous system $\linearsystem{A}{\mathbf{0}}$. So we row-reduce the augmented matrix to obtain:

@newcol
 \[\left[\begin{array}[]{ccccc|c}\boxed{1}&amp;0&amp;2&amp;0&amp;1&amp;0\\
0&amp;\boxed{1}&amp;-3&amp;0&amp;4&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;0\end{array}\right]\]
The variables (of the homogeneous system) $x_{3}$ and $x_{5}$ are free (since columns 1, 2 and 4 are pivot columns), so we arrange the equations represented by the matrix in reduced row-echelon form to:

@col
 \begin{align*}
x_1&amp;=-2x_3-x_5\\
x_2&amp;=3x_3-4x_5\\
x_4&amp;=-2x_5
\end{align*}

@col
 So we can write the infinite solution set as sets using column vectors,
\[{\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}-2x_{3}-x_{5}\\
3x_{3}-4x_{5}\\
x_{3}\\
-2x_{5}\\
x_{5}\end{bmatrix}\,\right|\,x_{3},\,x_{5}\text{ real numbers}\right\}.\] 
@endcol
@end
@slide
@eg
@label{CNS2}
 Let us compute the null space of
\[C=\begin{bmatrix}-4&amp;6&amp;1\\
-1&amp;4&amp;1\\
5&amp;6&amp;7\\
4&amp;7&amp;1\end{bmatrix}\]
which we write as ${\mathcal{N}}\!\left(C\right)$. Translating definition   @ref{NSM}, we simply desire to solve the homogeneous system
$\linearsystem{C}{\mathbf{0}}$.
So we row-reduce the augmented matrix to obtain
\[\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0\\
0&amp;0&amp;0&amp;0\end{array}\right]\] 
@newcol
 There are no free variables in the homogeneous system represented by the row-reduced matrix, so there is only the trivial solution, the zero vector, $\mathbf{0}$. So we can write the (trivial) solution set as
\[{\mathcal{N}}\!\left(C\right)=\{\mathbf{0}\}=\left\{\begin{bmatrix}0\\
0\\
0\end{bmatrix}\right\}.\] 
@endcol
@end
@section{Null Space of a Nonsingular Matrix}
@label{NSNM}
@thm
@title{Nonsingular Matrices have Trivial Null Spaces}
@label{NMTNS}
 Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if the null space of $A$ is the set containing only the zero vector, i.e., ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$. 
@end
@proof
@newcol
 The null space of a square matrix, $A$,
is equal to the set of solutions to the homogeneous system,
$\linearsystem{A}{\mathbf{0}}$. A matrix is nonsingular if and only if the set of solutions to the homogeneous system,
$\linearsystem{A}{\mathbf{0}}$,
has only a trivial solution.
These two observations may be chained together to construct the two proofs necessary for each half of this theorem. 
@qed
@endcol
@end
@slide
@remark
 It is now easy to see that any nonsingular matrix $A$ is invertible:

@newcol
 If an $n\times n$ matrix $A$ is nonsingular, then it is row-equivalent to $I_n$,
which means there exists a sequence of elementary matrices $J_1, J_2, \ldots, J_l$
(each corresponding to a row operation), such that:
\[
J_l \cdots J_2 J_1 A = I.
\]
Let $J = J_l \cdots J_2 J_1$. Since each elementary matrix is invertible,
and the product of invertible matrices is also invertible, the matrix $J$ is invertible.

@col
 Hence,
\[
A = J^{-1}JA = J^{-1} I = J^{-1}
\]
The matrix $J^{-1}$, being the inverse of an invertiable matrix, is invertible.
We conclude that $A = J^{-1}$ is invertible.
(Observe that every nonsingular matrix is in particular a product of elementary matrices.) 
@endcol
@end
@slide
@thm
@title{Nonsingular Matrices and Unique Solutions}
@label{NMUS}
 Suppose that $A$ is a square matrix. $A$ is a nonsingular matrix if and only if the system $\linearsystem{A}{\mathbf{b}}$ has a unique solution for every choice of the constant vector $\mathbf{b}$. 
@end
@proof

($\Rightarrow$) 
@newcol
 The hypothesis for this half of the proof is that the system
$\linearsystem{A}{\mathbf{b}}$
has a unique solution for every choice of the constant vector $\mathbf{b}$.
We will make a very specific choice for $\mathbf{b}$: $\mathbf{b}=\mathbf{0}$.
Then we know that the system $\linearsystem{A}{\mathbf{0}}$ has a unique solution.
But this is precisely the definition of what it means for $A$
to be nonsingular. 
@endcol
($\Leftarrow$) 
@newcol
 We assume that $A$ is nonsingular of size $n\times n$,
so we know there is a sequence of row operations that will convert $A$
into the identity matrix $I_{n}$ (Theorem   @ref{NMRRI}).
Form the augmented matrix $A^{\prime}=\left[A|\mathbf{b}\right]$
and apply this same sequence of row operations to $A^{\prime}$.
The result will be the matrix $B^{\prime}=\left[I_{n}|\mathbf{c}\right]$,
which is in reduced row-echelon form with $r=n$.
Then the augmented matrix $B^{\prime}$ represents the (extremely simple)
system of equations $x_{i}=\left[\mathbf{c}\right]_{i}$, $1\leq i\leq n$.
The vector $\mathbf{c}$ is clearly a solution, so the system is consistent.
With a consistent system, we use Lecture 4 Theorem 4 to count free variables.
We find that there are $n-r=n-n=0$ free variables,
and so we therefore know that the solution is unique. 
@qed
@endcol
@end
 Alternatively, 
@proof
@newcol
 Suppose $A \mathbf{x} = \mathbf{b}$ has a unique solution for every vector $\mathbf{b}$. Then, in particular, the only solution to $A\mathbf{x} = \mathbf{0}$
is $\mathbf{x} = \mathbf{0}$. This implies by definition that $A$ is nonsingular.

@col
 Conversely,
if $A$ is nonsingular, then $A^{-1}$ exists.
So, $A\mathbf{x} = \mathbf{b}$ implies that the unique solution is $\mathbf{x} = A^{-1}\mathbf{b}$. 
@qed
@endcol
@end
@slide
@thm
@title{Nonsingular Matrix Equivalences}
@label{NME1}
 Suppose that $A$ is a square matrix. The following are equivalent. 
@enumerate
@item
 $A$ is nonsingular. 
@item
 $A$ row-reduces to the identity matrix. 
@item
 The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$. 
@item
 The linear system $\linearsystem{A}{\mathbf{b}}$ has a unique solution for every possible choice of $\mathbf{b}$. 
@endenumerate
@end
@proof
@newcol
 The statement that $A$ is nonsingular is equivalent to each of the subsequent statements by, in turn,
theorems   @ref{NMRRI},   @ref{NMTNS},   @ref{NMUS}.
So the statement of this theorem is just a convenient way to organize all these results. 
@qed
@endcol
@end
 In fact, we further have: @ref{NME3}. 
@section{Vectors}
<strong>Notation</strong>: ${\mathbb{R}}^{\hbox{}}$ is the set of real numbers.
If $X$ is a set, $x\in X$ means $x$ is an element of the set $X$. 
@defn
@title{Vector Space of Column Vectors}
@label{VSCV}
 The vector space ${\mathbb{R}}^{m}$ is the set of all column vectors of size $m$ with entries from the set of real numbers, ${\mathbb{R}}^{\hbox{}}$. ${\mathbb{R}}^{m}$ is also called the @keyword{Euclidean $m$-space}. 
@end

@defn
@title{Column Vector Equality}
@label{CVE}
@newcol
 Suppose that $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$. Then $\mathbf{u}$ and $\mathbf{v}$ are @keyword{equal}, written $\mathbf{u}=\mathbf{v}$ if
\begin{align*}
\displaystyle\left[\mathbf{u}\right]_{i}&amp;\displaystyle=\left[\mathbf{v}\right]_{i}&amp;\displaystyle 1\leq i\leq m
\end{align*}
That is,
\begin{align*}
\displaystyle \begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}=\begin{bmatrix}v_{1}\\
v_{2}\\
\vdots\\
v_{m}\end{bmatrix}
\end{align*}
if
\begin{align*}
\displaystyle u_{i}&amp;\displaystyle=v_{i}&amp;\displaystyle 1\leq i\leq m.
\end{align*} 
@endcol
@end
@slide
@eg
 The system of linear equations:
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}
can be rewritten as:

@newcol
 \begin{align*}
\displaystyle \begin{bmatrix}-7x_{1}-6x_{2}-12x_{3}\\
5x_{1}+5x_{2}+7x_{3}\\
x_{1}+4x_{3}\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*} 
@endcol
@end
@slide
@defn
@title{Column Vector Addition}
@label{CVA}
 Suppose that $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$. The @keyword{sum} of $\mathbf{u}$ and $\mathbf{v}$ is the vector $\mathbf{u}+\mathbf{v}$ defined by
\begin{align*}
\displaystyle\left[\mathbf{u}+\mathbf{v}\right]_{i}&amp;\displaystyle=\left[\mathbf{u}\right]_{i}+\left[\mathbf{v}\right]_{i}&amp;\displaystyle 1\leq i\leq m.
\end{align*}
That is
\begin{align*}
\displaystyle \begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}+\begin{bmatrix}v_{1}\\
v_{2}\\
\vdots\\
v_{m}\end{bmatrix}=\begin{bmatrix}u_{1}+v_{1}\\
u_{2}+v_{2}\\
\vdots\\
u_{m}+v_{m}\end{bmatrix}.
\end{align*} 
@end
@eg
<strong>Addition of two vectors in ${\mathbb{R}}^{4}$</strong>

@newcol
 If
\begin{align*}
\displaystyle\mathbf{u}=\begin{bmatrix}2\\
-3\\
4\\
2\end{bmatrix}&amp;
\quad&amp;
\displaystyle\mathbf{v}=\begin{bmatrix}-1\\
5\\
2\\
-7\end{bmatrix}
\end{align*}
then
\begin{align*}
\displaystyle \mathbf{u}+\mathbf{v}=\begin{bmatrix}2\\
-3\\
4\\
2\end{bmatrix}+\begin{bmatrix}-1\\
5\\
2\\
-7\end{bmatrix}=\begin{bmatrix}2+(-1)\\
-3+5\\
4+2\\
2+(-7)\end{bmatrix}=\begin{bmatrix}1\\
2\\
6\\
-5\end{bmatrix}
\end{align*} 
@endcol
@end
@slide
@defn
@title{Column Vector Scalar Multiplication}
@label{CVSM}
 Suppose $\mathbf{u}\in{\mathbb{R}}^{m}$ and $\alpha\in{\mathbb{R}}^{\hbox{}}$, then the @keyword{scalar multiple} of $\mathbf{u}$ by $\alpha$ is the vector $\alpha\mathbf{u}$ defined by
\begin{align*}
\displaystyle\left[\alpha\mathbf{u}\right]_{i}&amp;\displaystyle=\alpha\left[\mathbf{u}\right]_{i}&amp;\displaystyle 1\leq i\leq m.
\end{align*}
That is
\begin{align*}
\displaystyle \alpha\begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}=\begin{bmatrix}\alpha u_{1}\\
\alpha u_{2}\\
\vdots\\
\alpha u_{m}\end{bmatrix}.
\end{align*} 
@end
@eg
@newcol
 If
\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}3\\
1\\
-2\\
4\\
-1\end{bmatrix}
\end{align*}
and $\alpha=6$, then
\begin{align*}
\displaystyle \alpha\mathbf{u}=6\begin{bmatrix}3\\
1\\
-2\\
4\\
-1\end{bmatrix}=\begin{bmatrix}6(3)\\
6(1)\\
6(-2)\\
6(4)\\
6(-1)\end{bmatrix}=\begin{bmatrix}18\\
6\\
-12\\
24\\
-6\end{bmatrix}.
\end{align*} 
@endcol
@end
@slide
@eg
 The system of linear equations
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&amp;\displaystyle=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&amp;\displaystyle=24 \\
\displaystyle x_{1}+4x_{3}&amp;\displaystyle=5
\end{align*}
can be written as:

@newcol
 \begin{align*}
\displaystyle x_{1}\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+x_{2}\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*} 
@endcol
@end
@section{Vector Space Properties}
@label{VSP}
<strong>Warning</strong>: Read the statements of Theorem @ref{VSPCV} and skip the rest of this section <strong>unless you are/going to be</strong> a math major. The material skipped will not appear in the tests and the final exam.
With definitions of vector addition and scalar multiplication we can state, and prove, several properties of each operation, and some properties that involve their interplay. We now collect ten of them here for later reference. 
@thm
@title{Vector Space Properties of Column Vectors}
@label{VSPCV}
@label{ACC}
@label{SCC}
@label{CC}
@label{AAC}
@label{ZC}
@label{AIC}
@label{SMAC}
@label{DVAC}
@label{DSAC}
@label{OC}
 Suppose that ${\mathbb{R}}^{m}$ is the set of column vectors of size $m$ with addition and scalar multiplication as defined in Definition @ref{CVA} and Definition @ref{CVSM} .
Then: <ol class="ltx_enumerate">
<li class="ltx_item">
@keyword{ACC} <em>Additive Closure, Column Vectors</em>
If $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\mathbf{v}\in{\mathbb{R}}^{m}$. </li>
<li class="ltx_item">
@keyword{SCC} <em>Scalar Closure, Column Vectors</em>
If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then $\alpha\mathbf{u}\in{\mathbb{R}}^{m}$. </li>
<li class="ltx_item">
@keyword{CC} <em>Commutativity, Column Vectors</em>
If $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$. </li>
<li class="ltx_item">
@keyword{AAC} <em>Additive Associativity, Column Vectors</em>
If $\mathbf{u},\,\mathbf{v},\,\mathbf{w}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\left(\mathbf{v}+\mathbf{w}\right)=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}$. </li>
<li class="ltx_item">
@keyword{ZC} <em>Zero Vector, Column Vectors</em>
There is a vector, $\mathbf{0}$, called the @keyword{zero vector}, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}\in{\mathbb{R}}^{m}$. </li>
<li class="ltx_item">
@keyword{AIC} <em>Additive Inverses, Column Vectors</em>
If $\mathbf{u}\in{\mathbb{R}}^{m}$, then there exists a vector $\mathbf{-u}\in{\mathbb{R}}^{m}$ so that $\mathbf{u}+(\mathbf{-u})=\mathbf{0}$. </li>
<li class="ltx_item">
@keyword{SMAC} <em>Scalar Multiplication Associativity, Column Vectors</em>
If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then $\alpha(\beta\mathbf{u})=(\alpha\beta)\mathbf{u}$. </li>
<li class="ltx_item">
@keyword{DVAC} <em>Distributivity across Vector Addition, Column Vectors</em>
If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\alpha(\mathbf{u}+\mathbf{v})=\alpha\mathbf{u}+\alpha\mathbf{v}$. </li>
<li class="ltx_item">
@keyword{DSAC} <em>Distributivity across Scalar Addition, Column Vectors</em>
If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then
$(\alpha+\beta)\mathbf{u}=\alpha\mathbf{u}+\beta\mathbf{u}$. </li>
<li class="ltx_item">
@keyword{OC} <em>One, Column Vectors</em>
If $\mathbf{u}\in{\mathbb{R}}^{m}$, then $1\mathbf{u}=\mathbf{u}$. </li></ol>
@end
@proof
 While some of these properties seem very obvious, they all require proof. However, the proofs are not very interesting, and border on tedious.
We will prove one version of distributivity very carefully, and you can test your proof-building skills on some of the others. We need to establish an equality, so we will do so by beginning with one side of the equality, apply various definitions and theorems (listed to the right of each step) to massage the expression from the left into the expression on the right. Here we go with a proof of Property DSAC in @ref{VSPCV}.
For $1\leq i\leq m$,
\begin{align*}
\displaystyle\left[(\alpha+\beta)\mathbf{u}\right]_{i}&amp;\displaystyle=(\alpha+\beta)\left[\mathbf{u}\right]_{i}&amp;\text{definition} \\
&amp;\displaystyle=\alpha\left[\mathbf{u}\right]_{i}+\beta\left[\mathbf{u}\right]_{i} \\
&amp;\displaystyle=\left[\alpha\mathbf{u}\right]_{i}+\left[\beta\mathbf{u}\right]_{i}&amp;\text{definition} \\
&amp;\displaystyle=\left[\alpha\mathbf{u}+\beta\mathbf{u}\right]_{i}&amp;\text{definition}
\end{align*}
Since the individual components of the vectors $(\alpha+\beta)\mathbf{u}$ and $\alpha\mathbf{u}+\beta\mathbf{u}$ are equal for all $i$, $1\leq i\leq m$, @ref{CVE} tells us the vectors are equal. 
@qed
@end Many of the conclusions of our theorems can be characterized as @keyword{identities}, especially when we are establishing basic properties of operations such as those in this section. Most of the properties listed in Theorem @ref{VSPCV} are examples. So some advice about the style we use for proving identities is appropriate right now.
Be careful with the notion of the vector $\mathbf{-u}$. This is a vector that we add to $\mathbf{u}$ so that the result is the particular vector $\mathbf{0}$. This is basically a property of vector addition. It happens that we can compute $\mathbf{-u}$ using the other operation, scalar multiplication. We can prove this directly by writing that
\begin{align*}
\displaystyle \left[\mathbf{-u}\right]_{i}=-\left[\mathbf{u}\right]_{i}=(-1)\left[\mathbf{u}\right]_{i}=\left[(-1)\mathbf{u}\right]_{i}
\end{align*}
We will see later how to derive this property as a @keyword{consequence} of several of the ten properties listed in Theorem @ref{VSPCV}.
Similarly, we will often write something you would immediately recognize as @keyword{vector subtraction}. This could be placed on a firm theoretical foundation – as you can do yourself with exercise T30.
A final note. @ref{VSPCV} Property @keyword{AAC} implies that we do not have to be careful about how we <em>parenthesize</em> the addition of vectors. In other words, there is nothing to be gained by writing
$\left(\mathbf{u}+\mathbf{v}\right)+\left(\mathbf{w}+\left(\mathbf{x}+\mathbf{y}\right)\right)$
rather than
$\mathbf{u}+\mathbf{v}+\mathbf{w}+\mathbf{x}+\mathbf{y}$, since we get the same result no matter which order we choose to perform the four additions. So we will not be careful about using parentheses this way. 
@section{Vector Space}
<strong>For math major only. Non-math major can skip the rest of this section. The material will not appear in the midterms or final</strong> In this section we will give an abstract definition of vector space.
<strong>Why do we need the abstract definitions?</strong> A lot of different algebraic objects (e.g. polynomials, matrices, sequences, functions) share similar properties
with the set of column vectors. We can use
the common properties to derive similar results.
we therefore don’t need to reproof and restate the results.
<strong>One stone, kill many birds</strong>.

@defn
@label{VS}
 Suppose that $V$ is a set upon which we have defined two operations: (1) @keyword{vector addition}, which combines two elements of $V$ and is denoted by $+$, and (2) @keyword{scalar multiplication}, which combines a real number with an element of $V$ and is denoted by juxtaposition. Then $V$, along with the two operations, is a @keyword{vector space} over ${\mathbb{R}}^{\hbox{}}$ if the following ten properties hold. <ol class="ltx_enumerate">
<li class="ltx_item">
@keyword{AC}
<em>Additive Closure</em>
If $\mathbf{u},\,\mathbf{v}\in V$, then $\mathbf{u}+\mathbf{v}\in V$. </li>
<li class="ltx_item">
@keyword{SC}
<em>Scalar Closure</em>
If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then $\alpha\mathbf{u}\in V$. </li>
<li class="ltx_item">
@keyword{C}
<em>Commutativity</em>
If $\mathbf{u},\,\mathbf{v}\in V$, then $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$. </li>
<li class="ltx_item">
@keyword{AA}
<em>Additive Associativity</em>
If $\mathbf{u},\,\mathbf{v},\,\mathbf{w}\in V$, then $\mathbf{u}+\left(\mathbf{v}+\mathbf{w}\right)=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}$. </li>
<li class="ltx_item">
@keyword{Z}
<em>Zero Vector</em>
There is a vector, $\mathbf{0}$, called the @keyword{zero vector}, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}\in V$. </li>
<li class="ltx_item">
@keyword{AI}
<em>Additive Inverses</em>
If $\mathbf{u}\in V$, then there exists a vector $\mathbf{-u}\in V$ so that $\mathbf{u}+(\mathbf{-u})=\mathbf{0}$. </li>
<li class="ltx_item">
@keyword{SMA}
<em>Scalar Multiplication Associativity</em>
If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then $\alpha(\beta\mathbf{u})=(\alpha\beta)\mathbf{u}$. </li>
<li class="ltx_item">
@keyword{DVA}
<em>Distributivity across Vector Addition</em>
If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u},\,\mathbf{v}\in V$, then $\alpha(\mathbf{u}+\mathbf{v})=\alpha\mathbf{u}+\alpha\mathbf{v}$. </li>
<li class="ltx_item">
@keyword{DSA}
<em>Distributivity across Scalar Addition</em>
If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then
$(\alpha+\beta)\mathbf{u}=\alpha\mathbf{u}+\beta\mathbf{u}$. </li>
<li class="ltx_item">
@keyword{O}
<em>One</em>
If $\mathbf{u}\in V$, then $1\mathbf{u}=\mathbf{u}$. </li></ol> The objects in $V$ are called @keyword{vectors}, no matter what else they might really be, simply by virtue of being elements of a vector space. 
@end
@slide
@eg
@keyword{column vector space} The set of column vectors ${\mathbb{R}}^{n}$ is a vector space. 
@end
@eg
@keyword{Row vector space}

@newcol
 The set of row vector ($1\times n$ matrices), is a vector space with the following operations: <ul class="ltx_itemize">
<li class="ltx_item"> Vector addition: $[a_{1}\,a_{2}\,\ldots\,a_{n}]+[a_{1}\,b_{2}\,\ldots\,b_{n}]=[a_{1}+b_{1}\,a_{2}+b_{2}\,\ldots\,a_{n}+b_{n}]$ </li>
<li class="ltx_item"> Scalar multiplication $\alpha[a_{1}\,a_{2}\,\ldots,a_{n}]=[\alpha a_{1}\,\alpha a_{2},\ldots,\alpha a_{n}]$ </li></ul>
@endcol
@end
@eg
@keyword{Matrices}

@newcol
 The set of $m\times n$ matrices, denoted by $M_{mn}$, is a vector space with the following operations: <ul class="ltx_itemize">
<li class="ltx_item"> Vector addition:
\begin{align*}
\displaystyle \begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;\cdots&amp;a_{mn}\end{bmatrix}+\begin{bmatrix}b_{11}&amp;b_{12}&amp;\cdots&amp;b_{1n}\\
b_{21}&amp;b_{22}&amp;\cdots&amp;b_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
b_{m1}&amp;b_{m2}&amp;\cdots&amp;b_{mn}\end{bmatrix}=\begin{bmatrix}a_{11}+b_{11}&amp;a_{12}+b_{12}&amp;\cdots&amp;a_{1n}+b_{1n}\\
a_{21}+b_{21}&amp;a_{22}+b_{22}&amp;\cdots&amp;a_{2n}+b_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}+b_{m1}&amp;a_{m2}+b_{m2}&amp;\cdots&amp;a_{mn}+b_{mn}\end{bmatrix}
\end{align*} </li>
<li class="ltx_item"> Scalar multiplication
\begin{align*}
\displaystyle \alpha\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;\cdots&amp;a_{mn}\end{bmatrix}=\begin{bmatrix}\alpha a_{11}&amp;\alpha a_{12}&amp;\cdots&amp;\alpha a_{1n}\\
\alpha a_{21}&amp;\alpha a_{22}&amp;\cdots&amp;\alpha a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
\alpha a_{m1}&amp;\alpha a_{m2}&amp;\cdots&amp;\alpha a_{mn}\end{bmatrix}
\end{align*} </li></ul> Property Z: The zero vector is
\begin{align*}
\displaystyle \begin{bmatrix}0&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;0\end{bmatrix}
\end{align*}
Property AI:The inverse of
\begin{align*}
\displaystyle \begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
a_{m1}&amp;a_{m2}&amp;\cdots&amp;a_{mn}\end{bmatrix}
\end{align*}
is
\begin{align*}
\displaystyle \begin{bmatrix}-a_{11}&amp;-a_{12}&amp;\cdots&amp;-a_{1n}\\
-a_{21}&amp;-a_{22}&amp;\cdots&amp;-a_{2n}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
-a_{m1}&amp;-a_{m2}&amp;\cdots&amp;-a_{mn}\end{bmatrix}
\end{align*}
You can try proving all other properties. 
@endcol
@end
@eg
@keyword{The vector space of polynomials, $P_{n}$}

@newcol
 The set of all polynomials of degree $n$ or less in the variable $x$ with coefficients from ${\mathbb{R}}^{\hbox{}}$,
denoted by $P_{n}$ is a vector space. <ul class="ltx_itemize">
<li class="ltx_item"> Vector Addition:
\begin{align*}
\displaystyle (a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n})+(b_{0}+b_{1}x+b_{2}x^{2}+\cdots+b_{n}x^{n})
\end{align*}
\begin{align*}
\displaystyle =(a_{0}+b_{0})+(a_{1}+b_{1})x+(a_{2}+b_{2})x^{2}+\cdots+(a_{n}+b_{n})x^{n}
\end{align*} </li>
<li class="ltx_item"> Scalar Multiplication:
\begin{align*}
\displaystyle \alpha(a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n})=(\alpha a_{0})+(\alpha a_{1})x+(\alpha a_{2})x^{2}+\cdots+(\alpha a_{n})x^{n}
\end{align*} </li></ul> This set, with these operations, will fulfill the ten properties, though we will not work all the details here. However, we will make a few comments and prove one of the properties. First, the zero vector (property Z)
is what you might expect, and you can check that it has the required property.
\begin{align*}
\displaystyle \mathbf{0}=0+0x+0x^{2}+\cdots+0x^{n}
\end{align*}
The additive inverse (Property AI) is also no surprise, though consider how we have chosen to write it.
\begin{align*}
\displaystyle -\left(a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}\right)=(-a_{0})+(-a_{1})x+(-a_{2})x^{2}+\cdots+(-a_{n})x^{n}
\end{align*}
Now let us prove the associativity of vector addition (Property AA). This is a bit tedious, though necessary. Throughout, the plus sign ($+$) does triple-duty. You might ask yourself what each plus sign represents as you work through this proof.
\begin{align*}
\displaystyle\mathbf{u}+&amp;\displaystyle(\mathbf{v}+\mathbf{w}) \\
&amp;\displaystyle=(a_{0}+a_{1}x+\cdots+a_{n}x^{n})+\left((b_{0}+b_{1}x+\cdots+b_{n}x^{n})+(c_{0}+c_{1}x+\cdots+c_{n}x^{n})\right) \\
&amp;\displaystyle=(a_{0}+a_{1}x+\cdots+a_{n}x^{n})+((b_{0}+c_{0})+(b_{1}+c_{1})x+\cdots+(b_{n}+c_{n})x^{n}) \\
&amp;\displaystyle=(a_{0}+(b_{0}+c_{0}))+(a_{1}+(b_{1}+c_{1}))x+\cdots+(a_{n}+(b_{n}+c_{n}))x^{n} \\
&amp;\displaystyle=((a_{0}+b_{0})+c_{0})+((a_{1}+b_{1})+c_{1})x+\cdots+((a_{n}+b_{n})+c_{n})x^{n} \\
&amp;\displaystyle=((a_{0}+b_{0})+(a_{1}+b_{1})x+\cdots+(a_{n}+b_{n})x^{n})+(c_{0}+c_{1}x+\cdots+c_{n}x^{n}) \\
&amp;\displaystyle=\left((a_{0}+a_{1}x+\cdots+a_{n}x^{n})+(b_{0}+b_{1}x+\cdots+b_{n}x^{n})\right)+(c_{0}+c_{1}x+\cdots+c_{n}x^{n}) \\
&amp;\displaystyle=(\mathbf{u}+\mathbf{v})+\mathbf{w}
\end{align*}
You might try proving all the other properties. 
@endcol
@end
@eg
@keyword{The vector space of functions}

@newcol
 Let $F$ be the set of functions for ${\mathbb{R}}^{\hbox{}}$ to ${\mathbb{R}}^{\hbox{}}$
Equality: $f=g$ if and only if $f(x)=g(x)$ for all $x\in{\mathbb{R}}^{\hbox{}}$. <ul class="ltx_itemize">
<li class="ltx_item"> Vector Addition: $f+g$ is the function with outputs defined by $(f+g)(x)=f(x)+g(x)$. </li>
<li class="ltx_item"> Scalar Multiplication: $\alpha f$ is the function with outputs defined by $(\alpha f)(x)=\alpha f(x)$. </li></ul> The zero vector is a function $z$ whose definition is $z(x)=0$ for every input $x\in{\mathbb{R}}^{\hbox{}}$.
Try proving all the other properties. 
@endcol
@end
@eg
@keyword{The crazy vector space}

@newcol
 Let $C=\left\{\left.(x_{1},\,x_{2})\,\right|\,x_{1},\,x_{2}\in{\mathbb{R}}^{\hbox{}}\right\}$. <ol class="ltx_enumerate">
<li class="ltx_item"> Vector Addition: $(x_{1},\,x_{2})+(y_{1},\,y_{2})=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1)$. </li>
<li class="ltx_item"> Scalar Multiplication: $\alpha(x_{1},\,x_{2})=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)$. </li></ol> I am free to define my set and my operations any way I please. They may not look natural, or even useful, but we will now verify that they provide us with another example of a vector space. We will check all it satisfies all the definition of vector spaces. <ul class="ltx_itemize">
<li class="ltx_item">
<strong>Property AC, SC</strong>
The result of each operation is a pair of complex numbers, so these two closure properties are fulfilled. </li>
<li class="ltx_item">
<strong>Property C</strong> \begin{align*}
\displaystyle\mathbf{u}+\mathbf{v}&amp;\displaystyle=(x_{1},\,x_{2})+(y_{1},\,y_{2})=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1) \\
&amp;\displaystyle=(y_{1}+x_{1}+1,\,y_{2}+x_{2}+1)=(y_{1},\,y_{2})+(x_{1},\,x_{2}) \\
&amp;\displaystyle=\mathbf{v}+\mathbf{u}
\end{align*} </li>
<li class="ltx_item">
<strong>Property AA</strong> \begin{align*}
\displaystyle\mathbf{u}+(\mathbf{v}+\mathbf{w})&amp;\displaystyle=(x_{1},\,x_{2})+\left((y_{1},\,y_{2})+(z_{1},\,z_{2})\right) \\
&amp;\displaystyle=(x_{1},\,x_{2})+(y_{1}+z_{1}+1,\,y_{2}+z_{2}+1) \\
&amp;\displaystyle=(x_{1}+(y_{1}+z_{1}+1)+1,\,x_{2}+(y_{2}+z_{2}+1)+1) \\
&amp;\displaystyle=(x_{1}+y_{1}+z_{1}+2,\,x_{2}+y_{2}+z_{2}+2) \\
&amp;\displaystyle=((x_{1}+y_{1}+1)+z_{1}+1,\,(x_{2}+y_{2}+1)+z_{2}+1) \\
&amp;\displaystyle=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1)+(z_{1},\,z_{2}) \\
&amp;\displaystyle=\left((x_{1},\,x_{2})+(y_{1},\,y_{2})\right)+(z_{1},\,z_{2}) \\
&amp;\displaystyle=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}
\end{align*} </li>
<li class="ltx_item">
<strong>Property Z</strong>
The zero vector is $\mathbf{0}=(-1,\,-1)$ (<strong>not</strong> $(0,0)$)
\begin{align*}
\displaystyle \mathbf{u}+\mathbf{0}=(x_{1},\,x_{2})+(-1,\,-1)=(x_{1}+(-1)+1,\,x_{2}+(-1)+1)=(x_{1},\,x_{2})=\mathbf{u}
\end{align*} </li>
<li class="ltx_item">
<strong>Property AI</strong>
For each vector, $\mathbf{u}$, we must locate an additive inverse, $\mathbf{-u}$. Here it is, $-(x_{1},\,x_{2})=(-x_{1}-2,\,-x_{2}-2)$. As odd as it may look, I hope you are withholding judgment. Check:
\begin{align*}
\displaystyle\mathbf{u}+(\mathbf{-u})&amp;\displaystyle=(x_{1},\,x_{2})+(-x_{1}-2,\,-x_{2}-2) \\
&amp;\displaystyle=(x_{1}+(-x_{1}-2)+1,\,-x_{2}+(x_{2}-2)+1)=(-1,\,-1)=\mathbf{0}
\end{align*} </li>
<li class="ltx_item">
<strong>Property SMA</strong> \begin{align*}
\displaystyle\alpha(\beta\mathbf{u})&amp;\displaystyle=\alpha(\beta(x_{1},\,x_{2})) \\
&amp;\displaystyle=\alpha(\beta x_{1}+\beta-1,\,\beta x_{2}+\beta-1) \\
&amp;\displaystyle=(\alpha(\beta x_{1}+\beta-1)+\alpha-1,\,\alpha(\beta x_{2}+\beta-1)+\alpha-1) \\
&amp;\displaystyle=((\alpha\beta x_{1}+\alpha\beta-\alpha)+\alpha-1,\,(\alpha\beta x_{2}+\alpha\beta-\alpha)+\alpha-1) \\
&amp;\displaystyle=(\alpha\beta x_{1}+\alpha\beta-1,\,\alpha\beta x_{2}+\alpha\beta-1) \\
&amp;\displaystyle=(\alpha\beta)(x_{1},\,x_{2}) \\
&amp;\displaystyle=(\alpha\beta)\mathbf{u}
\end{align*} </li>
<li class="ltx_item">
<strong>Property DVA</strong>
If you have hung on so far, here is where it gets even wilder. In the next two properties we mix and mash the two operations.
\begin{align*}
\displaystyle\alpha(\mathbf{u}&amp;\displaystyle+\mathbf{v}) \\
&amp;\displaystyle=\alpha\left((x_{1},\,x_{2})+(y_{1},\,y_{2})\right) \\
&amp;\displaystyle=\alpha(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1) \\
&amp;\displaystyle=(\alpha(x_{1}+y_{1}+1)+\alpha-1,\,\alpha(x_{2}+y_{2}+1)+\alpha-1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha y_{1}+\alpha+\alpha-1,\,\alpha x_{2}+\alpha
y_{2}+\alpha+\alpha-1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1+\alpha y_{1}+\alpha-1+1,\,\alpha x_{2}+\alpha-1+\alpha y_{2}+\alpha-1+1) \\
&amp;\displaystyle=((\alpha x_{1}+\alpha-1)+(\alpha y_{1}+\alpha-1)+1,\,(\alpha x_{2}+\alpha-1)+(\alpha y_{2}+\alpha-1)+1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)+(\alpha y_{1}+\alpha-1,\,\alpha y_{2}+\alpha-1) \\
&amp;\displaystyle=\alpha(x_{1},\,x_{2})+\alpha(y_{1},\,y_{2}) \\
&amp;\displaystyle=\alpha\mathbf{u}+\alpha\mathbf{v}
\end{align*} </li>
<li class="ltx_item">
<strong>Property DSA</strong> \begin{align*}
\displaystyle(\alpha&amp;\displaystyle+\beta)\mathbf{u} \\
&amp;\displaystyle=(\alpha+\beta)(x_{1},\,x_{2}) \\
&amp;\displaystyle=((\alpha+\beta)x_{1}+(\alpha+\beta)-1,\,(\alpha+\beta)x_{2}+(\alpha+\beta)-1) \\
&amp;\displaystyle=(\alpha x_{1}+\beta x_{1}+\alpha+\beta-1,\,\alpha x_{2}+\beta x_{2}+\alpha+\beta-1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1+\beta x_{1}+\beta-1+1,\,\alpha x_{2}+\alpha-1+\beta x_{2}+\beta-1+1) \\
&amp;\displaystyle=((\alpha x_{1}+\alpha-1)+(\beta x_{1}+\beta-1)+1,\,(\alpha x_{2}+\alpha-1)+(\beta x_{2}+\beta-1)+1) \\
&amp;\displaystyle=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)+(\beta x_{1}+\beta-1,\,\beta x_{2}+\beta-1) \\
&amp;\displaystyle=\alpha(x_{1},\,x_{2})+\beta(x_{1},\,x_{2}) \\
&amp;\displaystyle=\alpha\mathbf{u}+\beta\mathbf{u}
\end{align*} </li>
<li class="ltx_item">
<strong>Property O</strong>
After all that, this one is easy, but no less pleasing.
\begin{align*}
\displaystyle 1\mathbf{u}=1(x_{1},\,x_{2})=(x_{1}+1-1,\,x_{2}+1-1)=(x_{1},\,x_{2})=\mathbf{u}
\end{align*} </li></ul> That is it, $C$ is a vector space, as crazy as that may seem.
Notice that in the case of the zero vector and additive inverses, we only had to propose possibilities and then verify that they were the correct choices. You might try to discover how you would arrive at these choices, though you should understand why the process of discovering them is not a necessary component of the proof itself. 
@endcol
@end
@section{Basic Properties of Vector Spaces}
<strong>For math major only. Non-math major can skip the rest of this section. The material will not appear in the midterms or final</strong>
@thm
@title{Cancellation Law for Vector Addition}
@label{cancell}
 if $\mathbf{v}$, $\mathbf{u}$ and $\mathbf{w}$ are vectors in a vector space $V$ such that
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{u}+\mathbf{w},
\end{align*}
then $\mathbf{v}=\mathbf{u}$ 
@end
@proof
 By Property AI, there exists a vector $-\mathbf{w}$ such that $\mathbf{w}+(-\mathbf{w})=\mathbf{0}$.
Thus,
\begin{align*}
\displaystyle (\mathbf{v}+\mathbf{w})+(-\mathbf{w})&amp;=(\mathbf{u}+\mathbf{w})+(-\mathbf{w})\\
\displaystyle \mathbf{v}+(\mathbf{w}+(-\mathbf{w}))&amp;=\mathbf{u}+(\mathbf{w}+(-\mathbf{w}))&amp;\text{Property AA}\\
\displaystyle \mathbf{v}+\mathbf{0}&amp;=\mathbf{u}+\mathbf{0}&amp;\text{Property AI}\\
\displaystyle \mathbf{v}&amp;=\mathbf{u}&amp;\text{Property Z}.
\end{align*}

@qed
@end
@thm
@title{Uniqueness of the zero vector}
 Let $V$ be a vector space.
The vector $\mathbf{0}$ described in Property Z is unique. 
@end
@proof
 Suppose both $\mathbf{0}_{1}$ and $\mathbf{0}_{2}$ satisfy the property described in Property Z.
Let $\mathbf{w}$ be an element in $V$.
\begin{align*}
\displaystyle \mathbf{0}_{1}+\mathbf{w}=\mathbf{w}=\mathbf{0}_{2}+\mathbf{w}\hskip 28.452756pt\text{Property Z}
\end{align*}
\begin{align*}
\displaystyle \mathbf{0}_{1}=\mathbf{0}_{2}\hskip 28.452756pt\text{by the previous theorem}
\end{align*}

@qed
@end
@thm
@title{Uniqueness of the additive inverse}
@label{invunique}
 Let $V$ be a vector space and $\mathbf{v},\mathbf{u},\mathbf{w}$ are vectors of $V$. If
both $\mathbf{v}$ and $\mathbf{u}$ satisfies
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{0},
\end{align*}
\begin{align*}
\displaystyle \mathbf{u}+\mathbf{w}=\mathbf{0},
\end{align*}
i.e., both $\mathbf{u}$ and $\mathbf{v}$ are additive inverse of $\mathbf{w}$ in Property AI, then
\begin{align*}
\displaystyle \mathbf{v}=\mathbf{u}.
\end{align*}
This shows that the additive inverse is unique. 
@end
@proof
 \begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{0}=\mathbf{u}+\mathbf{w}.
\end{align*}
By @ref{cancell},
\begin{align*}
\displaystyle \mathbf{v}=\mathbf{u}.
\end{align*}

@qed
@end
@thm
 Let $V$ be a vector space, $\alpha$ a real number, $\mathbf{v}$ a vector in $V$. We have the following statement. <ol class="ltx_enumerate">
<li class="ltx_item"> $0\mathbf{v}=\mathbf{0}$ . </li>
<li class="ltx_item"> $a\mathbf{0}=\mathbf{0}$. </li>
<li class="ltx_item"> $(-\alpha)\mathbf{v}=-(\alpha\mathbf{v})=\alpha(-\mathbf{v})$. </li></ol>
@end
@proof
<ol class="ltx_enumerate">
<li class="ltx_item"> \begin{align*}
\displaystyle 0\mathbf{v}+0\mathbf{v}&amp;=(0+0)\mathbf{v} &amp; \text{Property DSA}\\
\displaystyle 0\mathbf{v}+0\mathbf{v}=0\mathbf{v}&amp;=\mathbf{0}+0\mathbf{v} &amp; \text{Property Z}
\end{align*}
Hence,
\[
\displaystyle 0\mathbf{v}=\mathbf{0},
\]
by @ref{cancell}. </li>
<li class="ltx_item">
<br/> \begin{align*}
\displaystyle\alpha\mathbf{0}+a\mathbf{0}&amp;\displaystyle=\alpha(\mathbf{0}+\mathbf{0}) &amp; \text{Property DVA} \\
&amp;\displaystyle=\alpha\mathbf{0} &amp; \text{Property Z} \\
&amp;\displaystyle=\mathbf{0}+\alpha\mathbf{0} &amp; \text{Property Z}
\end{align*}
By @ref{cancell},
\begin{align*}
\displaystyle \alpha\mathbf{0}=\mathbf{0}
\end{align*} </li>
<li class="ltx_item"> \begin{align*}
\displaystyle\alpha\mathbf{v}+(-\alpha)\mathbf{v}&amp;\displaystyle=(\alpha+(-\alpha))\mathbf{v}&amp;\text{Property DSA}. \\
&amp;\displaystyle=0\mathbf{v}&amp; \\
&amp;\displaystyle=\mathbf{0}&amp;\text{item 1}
\end{align*}
By Property AI and the uniqueness of the additive inverse (@ref{invunique}),
\begin{align*}
\displaystyle (-\alpha)\mathbf{v}=-\alpha\mathbf{v}.
\end{align*}
Next
\begin{align*}
\displaystyle\alpha\mathbf{v}+\alpha(-\mathbf{v})&amp;\displaystyle=\alpha(\mathbf{v}+(-\mathbf{v})) &amp; \text{Property DVA}. \\
&amp;\displaystyle=\alpha\mathbf{0} &amp; \text{Property AI} \\
&amp;\displaystyle=\mathbf{0} &amp; \text{item 2}
\end{align*}
By Property AI and the uniqueness of the additive inverse (@ref{invunique}),
\begin{align*}
\displaystyle \alpha(-\mathbf{v})=-\alpha\mathbf{v}.
\end{align*} </li></ol>

@qed
@end
@section{Subspaces}
@defn
@label{subdef}
 Let $V$ be vector space.
A subset $W$ of $V$ is said to be a @keyword{subspace} of $V$ if <ol class="ltx_enumerate">
<li class="ltx_item"> $W$ is nonempty. </li>
<li class="ltx_item"> For $\mathbf{v},\mathbf{w}\in W$, then $\mathbf{v}+\mathbf{w}\in W$. </li>
<li class="ltx_item"> For $\alpha\in\mathbf{R}$, $\mathbf{v}\in W$, then $\alpha\mathbf{v}\in W$. </li></ol>
@end
@slide
 We will prove several theorem first before we give examples. 
@prop
@label{0}
 Let $V$ be a vector space and $W$ a subspace of $V$.
Then $\mathbf{0}$
is in $W$. 
@end
@proof
@newcol
 By @ref{subdef}, Condition 1, $W$ is nonempty. Let $\mathbf{w}\in W$.
By @ref{subdef}, Condition 3, with $\alpha=0$,
$0\mathbf{w}\in W$.
On the other hand, by @ref{1d82a47b16369aec09ebc36c7afbd598}, $0\mathbf{w} = \mathbf{0}$.
Hence, the zero vector $\mathbf{0}$ lies in $W$. 
@qed
@endcol
@end
@thm
@label{subdef2}
@newcol
 Let $V$ be a vector space
and $W$ a subset of $V$,
then $W$ is a subspace if and only if <ol class="ltx_enumerate">
<li class="ltx_item"> $W$ is nonempty. </li>
<li class="ltx_item"> For any $\alpha\in\mathbf{R}$, $\mathbf{v},\mathbf{w}\in W$, $\alpha\mathbf{v}+\mathbf{w}\in W$. </li></ol>
@endcol
@end
@proof

($\Rightarrow$) 
@newcol
 By @ref{subdef}, Condition 1, $W$ is nonempty. Next, for $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W$.
By @ref{subdef}, Condition 3, $\alpha\mathbf{v}\in W$.
By @ref{subdef}, Condition 2, $\alpha\mathbf{v}+\mathbf{w}\in W$. 
@endcol
($\Leftarrow$) 
@newcol
 By Condition 1, @ref{subdef}, Condition 1 is true.
Because $W$ is nonempty, let $\mathbf{x}\in W$. Let $\mathbf{v}=\mathbf{w}=\mathbf{x}$ and $\alpha=-1$.
Then by condition 2, $(-1)\mathbf{w}+\mathbf{w}=\mathbf{0}\in W.$

@col
 Now we want to check @ref{subdef} Condition 2, suppose $\mathbf{v},\mathbf{w}\in W$.
In condition 2, let $\alpha=1$, then $\mathbf{v}+\mathbf{w}=\alpha\mathbf{v}+\mathbf{w}\in W$.

@col
 Next we want to check @ref{subdef} Condition 3, suppose $\mathbf{v}\in W$, $\alpha\in{\mathbb{R}}^{\hbox{}}$.
Let $\mathbf{w}=\mathbf{0}$, then $\alpha\mathbf{v}=\alpha\mathbf{v}+\mathbf{w}\in W$. 
@qed
@endcol
@end
@section{Examples}
@eg
 Let $V = \mathbb{R}^3$.  Let:
\[
W = \left\{\left.\colvector{w_1\\0\\w_3} \;\right| \;w_1, w_3 \in \mathbb{R}\right\}.
\]
We now show that $W$ is a subspace of $V$:

@newcol
@ol
@li
 Clearly, $\colvector{0\\0\\0}$ lies in $W$, hence $W$ is nonempty. 
@li
 Given any $\vec{v}$ and $\vec{w}$ in $W$, by definition of $W$ we have:
\[
\vec{v} = \colvector{v_1\\0\\v_3},\quad \vec{w} = \colvector{w_1\\0\\w_3},
\quad v_1,v_2,w_1,w_3 \in \mathbb{R}.
\]
Hence,
\[
\vec{v} + \vec{w} = \colvector{v_1 + w_1\\0 + 0\\v_3 + w_3}
= \colvector{v_1 + w_1\\0\\v_3 + w_3} \in W.
\] 
@li
 Given any $\alpha \in \mathbb{R}$ and $\vec{w}$ in $W$, by definition of $W$ we have:
\[
\vec{w} = \colvector{w_1\\0\\w_3}, \quad w_1,w_3 \in \mathbb{R}.
\]
Hence,
\[
\alpha\vec{w} = \colvector{\alpha w_1\\\alpha \cdot 0\\\alpha w_3}
= \colvector{\alpha w_1\\0\\\alpha w_3} \in W.
\] 
@endol
@endcol
@end
@eg
 $V = \mathbb{R}^3$.
\[
W = \left\{\left.\colvector{x\\y\\z} \in \mathbb{R}^3 \,\right|\, x + 2y + 3z = 0
\right\}
\] 
@end

@slide
@thm
@label{thm:nullspacesubspace}
 Let $A\in M_{mn}$, then $W=\nsp{A}$ is a subspace of $\mathbb{R}^n$. 
@end

@proof
@newcol
 Because $\vect 0 \in \nsp{A}$, so $W$ is nonempty.

@col
 For $\alpha \in \mathbb{R}$, $\vect v, \vect w \in W$.
Then
\[ A \vect v = \vect 0, \qquad A \vect w = \vect 0. \] 
@col
 Then
\[ A(\alpha \vect v + \vect w) = \alpha A \vect v + A \vect w = \alpha \vect 0 + \vect 0 = \vect 0. \] 
@col
 So
\[ \alpha \vect v + \vect w \in \nsp{A}. \]
Thus by @ref{subdef2}, $W=\nsp{A}$ is a subspace. 
@qed
@endcol
@end

@slide
@eg
<strong>Skip for now, until you learn the definition of column space.</strong>
@newcol
 Let $A\in M_{mn}$, then $\mathcal{C}\!\left(A\right)$ is a subspace of ${\mathbb{R}}^{m}$. 
@endcol
@end
@proof
@newcol
 $\mathcal{C}\!\left(A\right)=\left&lt;\{\mathbf{A}_{1},\ldots,\mathbf{A}_{n}\}\right&gt;$. So by the previous theorem, $\mathcal{C}\!\left(A\right)$ is a subspace of ${\mathbb{R}}^{m}$. @keyword{Alternate proof}: Suppose For $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W=\mathcal{C}\!\left(A\right)$.
Recall
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\{A\mathbf{x}\,|\,x\in{\mathbb{R}}^{m}\}.
\end{align*}
Then there exist $\mathbf{x},\mathbf{y}$ such that $A\mathbf{x}=\mathbf{v}$, $A\mathbf{y}=\mathbf{w}$.
\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}=\alpha A\mathbf{x}+A\mathbf{y}=A(\alpha\mathbf{x}+\mathbf{y})\in\mathcal{C}\!\left(A\right).
\end{align*}
Thus by @ref{subdef2}, $W$ is a subspace. 
@qed
@endcol
@end
@eg
<strong>For math majors only</strong>

@newcol
 Let $S_{n}$ be the set of symmetric matrices of $M_{nn}$.
Then $S_{n}$ is a subspace of $M_{nn}$. Check that $W=S_{n}$ is a subspace: Because ${\cal O}\in W$, so $W$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $A,B\in W$. Then $A^{t}=A$, $B^{t}=B$.
\begin{align*}
\displaystyle (\alpha A+B)^{t}=\alpha A^{t}+B^{t}=\alpha A+B.
\end{align*}
Thus $\alpha A+B\in S_{n}$. Hence $S_{n}$ is a subspace by @ref{subdef2}. 
@endcol
@end
@eg
<strong>For math majors only</strong>

@newcol
 Let
\begin{align*}
\displaystyle F=\{f(x)\in P_{n}\,|\,f(1)=0\}.
\end{align*}
Then $F$ is a subspace of $P_{n}$ Because $0\in E$, so $E$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $f(x),g(x)\in E$. Then $f(1)=g(1)=0$.
Let $h=\alpha f+g$. Then
\begin{align*}
\displaystyle h(1)=\alpha f(1)+g(1)=\alpha 0+0=0.
\end{align*}
So $h\in F$. Hence $F$ is a subspace by @ref{subdef2}. 
@endcol
@end
@eg
<strong>For math majors only</strong>

@newcol
 Let
\begin{align*}
\displaystyle E=\{f(x)\in P_{n}\,|\,f(x)=f(-x)\}.
\end{align*}
Then $E$ is a subspace of $P_{n}$: Because $0\in E$, so $E$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $f(x),g(x)\in E$. Then $f(x)=f(-x)$, $g(x)=g(-x)$.
Let $h=\alpha f+g$. Then
\begin{align*}
\displaystyle h(-x)=\alpha f(-x)+g(-x)=\alpha f(x)+g(x)=h(x).
\end{align*}
So $h\in E$. Hence $E$ is a subspace. 
@endcol
@end
@section{Non-Examples}
 To show that $W$ is <strong>not</strong> a subspace of $V$, it suffices to show that it violates @ref{subdef} condition 1 or condition 2.
This can be done by finding counter examples to either condition. Usually before checking those conditions, we quickly check if $\mathbf{0}_{V}\in W$
(see @ref{0}). 
@eg
@newcol
 $V={\mathbb{R}}^{m}$, $W=\{\mathbf{v}\in V\,|\,[\mathbf{v}]_{1}=1\}$.
<strong>Method 1</strong> Obviously $\mathbf{0}$ is not in $W$. So by @ref{0}, $W$ is not a subspace.
<strong>Method 2</strong> For Suppose $\mathbf{v},\mathbf{w}\in W$.
Then $[\mathbf{v}+\mathbf{w}]_{1}=[\mathbf{v}]_{1}+[\mathbf{w}]_{1}=2$.
So $\mathbf{v}+\mathbf{w}\not\in W$. So $W$ violates @ref{subdef} condition 1 and hence not a subspace. 
@endcol
@end
@eg
@newcol
 $V={\mathbb{R}}^{m}$, $W=\{\mathbf{v}\in V\,|\,\sum_{i=1}^{n}[\mathbf{v}]_{i}=1\}$.
<strong>Method 1</strong> (the easiest method) Obviously $\mathbf{0}$ is not in $W$. So by @ref{0}, $W$ is not a subspace.
<strong>Method 2</strong> We will find an explicit counter example, let
\begin{align*}
\displaystyle \mathbf{v}=\mathbf{w}=\begin{bmatrix}1\\
0\\
\vdots\\
0\end{bmatrix}.
\end{align*}
Then both $\mathbf{v}$ and $\mathbf{w}$ are in $W$.
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\begin{bmatrix}2\\
0\\
\vdots\\
0\end{bmatrix}.
\end{align*}
Obvious $\mathbf{v}+\mathbf{w}\notin W$.
Therefore $W$ violates @ref{subdef} condition 1 and hence not a subspace. 
@endcol
@end
@eg
@newcol
 $V=\mathbf{R}^{n}$, $W=\{\mathbf{v}\in V\,|\,[\mathbf{v}]_{1}\geq 0\}$. Let $\alpha=-1$ and
\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}1\\
0\end{bmatrix}.
\end{align*}
Then $[\alpha\mathbf{v}]_{1}=\alpha[\mathbf{v}]_{1}=\alpha=-1&lt;0$.
So $\alpha\mathbf{v}\notin W$.
Thus $W$ violates @ref{subdef} condition 3 and hence not a subspace. 
@endcol
@end
@eg
@newcol
 $V={\mathbb{R}}^{2}$,
$W=\{\begin{bmatrix}v_{1}\\
v_{2}\end{bmatrix}\in V\,|\,v_{1}v_{2}\geq 0\}$. Let $\mathbf{v}=\begin{bmatrix}1\\
0\end{bmatrix}$ , $\mathbf{w}=\begin{bmatrix}0\\
-1\end{bmatrix}\in W$.
$\mathbf{v}+\mathbf{w}=\begin{bmatrix}1\\
-1\end{bmatrix}$. Because
$1\times(-1)=-1 &lt; 0$. So $\mathbf{v}+\mathbf{w}\notin W$.
Thus $W$ violates @ref{subdef} condition 2 and hence not a subspace. In fact, we can show that $W$ satisfies @ref{subdef} condition 3 but fails condition 2. 
@endcol
@end
@eg
<strong>For math majors only</strong>

@newcol
 Let $V=P_{n}$. Let $G$ be the set of polynomial with degree exactly equal to $n$. Let $f(x)=x^{n}$, $g(x)=-x^{n}+1$. Both $f$ and $g$ have degree exactly equal to $n$.
But
\begin{align*}
\displaystyle f(x)+g(x)=1
\end{align*}
is a polynomial with degree $0$. So $f+g$ is not in $G$. Thus $W$ violates @ref{subdef} condition 2 and hence not a subspace. 
@endcol
@end
@course{MATH 1030}
<!--DELIMITER-->
