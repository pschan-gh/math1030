@course{MATH 1030}
@setchapter{7}
@chapter{Homogeneous Systems of Equations and Nonsingular Matrices}

<h5 class="notkw">Reference.</h5>
  Beezer, Ver 3.5 Section HSE (print version p44 - p50)Section NM (print version p51 - p56)

<h5 class="notkw">Exercise.</h5> Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf  
@enumerate
@item
  Section HSE (ex p.18-23) C21-C23, C25-C27, C30-C31, M50-M52, T10-T12, T20  
@item
  Section NM (ex p.23-27) C30-C33, C50, M30, M51-M52, T10, T12, T30, T31, T90.  
@endenumerate
@section{Solutions of Homogeneous Systems}
@label{SHS}
@defn
@title{Homogeneous System}
@label{HS}
  A system of linear equations, $\mathcal{LS}(A, \mathbf{b})$ is @keyword{homogeneous} if the vector of constants is the zero vector, in other words, if $\mathbf{b}=\mathbf{0}$, i.e.
\begin{align*}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n  &=0 \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n  &=0\\
\vdots &\\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n &= 0
\end{align*}  
@end
@defn
@title{Homogeneous System corresponding to system of linear equation}
@newcol
  The @keyword{homogeneous system} corresponding to $\linearsystem{A}{\vect{b}}$:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=b_1\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=b_2\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=b_3\\
\vdots&\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=b_m
\end{align*}
is $\linearsystem{A}{\vect{0}}$:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=0\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=0\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=0\\
\vdots&\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=0
\end{align*}  
@endcol
@end
@slide
@eg
  The following is a homogeneous system of linear equations:
\begin{align*}
x_1-2x_2+3x_3 - 4x_4 &= 0 \\
x_2-x_4 &=0 \\
x_1+3x_2 - 5x_3 + 5 x_4 &= 0
\end{align*}
It is the homogeneous system of linear equations corresponding to
\begin{align*}
x_1-2x_2+3x_3 - 4x_4 &= 1 \\
x_2-x_4 &=2 \\
x_1+3x_2 - 5x_3 + 5 x_4 &= 3
\end{align*}  
@end
@slide
@thm
@title{Homogeneous Systems are Consistent}
@label{HSC}
  Suppose that a system of linear equations is homogeneous.
Then the system is consistent. In fact $\mathbf{0}$ is a solution, i.e $x_{1}=x_{2}=\cdots=x_{n}=0$ is a solution. Such solution is called a @keyword{trivial solution}.  
@end
@proof
@newcol
  Set each variable of the system to zero. The left hand side of the all equations are zero, which are equal to the right hand side.  
@qed
@endcol
@end
@slide
@eg
@enumerate
@item
<br/>
  \begin{align*}
-7x_1 -6 x_2 - 12x_3 &=0\\
5x_1  + 5x_2 + 7x_3 &=0\\
x_1 +4x_3 &=0
\end{align*}
The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&0&0\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&0\end{array}\right]\]
It has $n-r=3-3=0$ free variable. Hence it has only one solution.  
@item
@newcol
  \begin{align*}
x_1 -x_2 +2x_3 & =0\\
2x_1+ x_2 + x_3 & =0\\
x_1 + x_2 & =0
\end{align*}The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&1&0\\
0&\boxed{1}&-1&0\\
0&0&0&0\end{array}\right]\]
The system is consistent. It has $n-r=3-2=1$ free variable. The solution set is
\[S=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}\,\right|\,x_{1}=-x_{3},\,x_{2}=x_{3}\right\}=\left\{\left.%
\begin{bmatrix}-x_{3}\\
x_{3}\\
x_{3}\end{bmatrix}\,\right|\,x_{3}\text{ real number}\right\}\]
Geometrically, these are points in three dimensions that lie on a line through the origin.  
@endcol
@item
@newcol
  \begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &= 0 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &=  0 \\
x_1 +x_2 + 4x_3 - 5x_4 &=  0
\end{align*}
The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&0\\
0&0&0&0&0\end{array}\right]\]
The system is consistent. It has $n-r=4-2=2$ free variables. The solution set is
\[\displaystyle S=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{1}=-3x_{3}+2x_{4},\,x_{2}=-x_{3}+3x_{4}\right\}\]
\[\displaystyle=\left\{\left.\begin{bmatrix}-3x_{3}+2x_{4}\\
-x_{3}+3x_{4}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{3},\,x_{4}\text{ real numbers}\right\}\]  
@endcol
@endenumerate
@end
@slide
  Notice that when we do row operations on the augmented matrix of a homogeneous system of linear equations the last column of the matrix is all zeros. Any one of the three allowable row operations will convert zeros to zeros and thus, the final column of the matrix in reduced row-echelon form will also be all zeros. So in this case, we may be as likely to reference only the coefficient matrix and presume that we remember that the final column begins with zeros, and after any number of row operations is still zero.  
@slide
@thm
@label{HMVEI}
  Suppose that a homogeneous system of linear equations has $m$ equations and $n$ variables with $n>m$.
Then the system has infinitely many solutions.  
@end
@proof
@newcol
  The system is homogeneous, by theorem @ref{HSC} it is consistent.
Then the hypothesis that $n>m$, together with @ref{CMVEI}, gives infinitely many solutions.  
@qed
@endcol
@end
If $n=m$, then we can have a unique solution or infinitely many solutions (see the above examples).
@section{Null Space of a Matrix}
@defn
@label{NSM}
  The @keyword{null space} of a an $m \times n$ matrix $A$, denoted by ${\mathcal{N}}\!\left(A\right)$,
is the set of vectors $\mathbf{x} \in \mathbb{R}^n$ such that:
\[
A\mathbf{x} = \mathbf{0}.
\]  
Equivalently,
it is the set of all the vectors which are solutions to the homogeneous system $\linearsystem{A}{\mathbf{0}}$.
That is, if:
\[A=\begin{bmatrix}a_{11}&a_{12}&a_{13}&\dots&a_{1n}\\
a_{21}&a_{22}&a_{23}&\dots&a_{2n}\\
a_{31}&a_{32}&a_{33}&\dots&a_{3n}\\
\vdots&\\
a_{m1}&a_{m2}&a_{m3}&\dots&a_{mn}\\
\end{bmatrix}\]
then ${\mathcal{N}}\!\left(A\right)$ is the solution set of
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=0\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=0\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=0\\
\vdots&\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=0
\end{align*}
@end
@slide
@eg
  Suppose
\[A=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}\]
Then
\[\mathbf{x}=\begin{bmatrix}3\\
0\\
-5\\
-6\\
0\\
0\\
1\end{bmatrix}\qquad\qquad\mathbf{y}=\begin{bmatrix}-4\\
1\\
-3\\
-2\\
1\\
1\\
1\end{bmatrix}\]
are in ${\mathcal{N}}\!\left(A\right)$ as $A\mathbf{x}=\mathbf{0}$, $A\mathbf{y}=\mathbf{0}$.

@newcol
  However, the vector
\[\mathbf{z}=\begin{bmatrix}1\\
0\\
0\\
0\\
0\\
0\\
2\end{bmatrix}\]
is not in ${\mathcal{N}}\!\left(A\right)$ as
\[A\mathbf{z}=\begin{bmatrix}-17\\
16\\
-16\\
73\end{bmatrix}\neq\mathbf{0}.\]  
@endcol
@end
@slide
@eg
  Suppose
\[A=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}\]
Then
\[\mathbf{x}=\begin{bmatrix}3\\
0\\
-5\\
-6\\
0\\
0\\
1\end{bmatrix}\qquad\qquad\mathbf{y}=\begin{bmatrix}-4\\
1\\
-3\\
-2\\
1\\
1\\
1\end{bmatrix}\]
are in ${\mathcal{N}}\!\left(A\right)$ as $A\mathbf{x}=\mathbf{0}$, $A\mathbf{y}=\mathbf{0}$.

@newcol
  However, the vector
\[\mathbf{z}=\begin{bmatrix}1\\
0\\
0\\
0\\
0\\
0\\
2\end{bmatrix}\]
is not in ${\mathcal{N}}\!\left(A\right)$ as
\[A\mathbf{z}=\begin{bmatrix}-17\\
16\\
-16\\
73\end{bmatrix}\neq\mathbf{0}.\]  
@endcol
@end
@slide
@eg
@label{CNS1}
  Let us compute the null space of
\[A=\begin{bmatrix}2&-1&7&-3&-8\\
1&0&2&4&9\\
2&2&-2&-1&8\end{bmatrix}\]
which we write as ${\mathcal{N}}\!\left(A\right)$. Translating @ref{NSM},
we simply desire to solve the homogeneous system $\linearsystem{A}{\mathbf{0}}$. So we row-reduce the augmented matrix to obtain:

@newcol
  \[\left[\begin{array}[]{ccccc|c}\boxed{1}&0&2&0&1&0\\
0&\boxed{1}&-3&0&4&0\\
0&0&0&\boxed{1}&2&0\end{array}\right]\]
The variables (of the homogeneous system) $x_{3}$ and $x_{5}$ are free (since columns 1, 2 and 4 are pivot columns), so we arrange the equations represented by the matrix in reduced row-echelon form to:

@col
  \begin{align*}
x_1&=-2x_3-x_5\\
x_2&=3x_3-4x_5\\
x_4&=-2x_5
\end{align*}

@col
  So we can write the infinite solution set as sets using column vectors,
\[{\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}-2x_{3}-x_{5}\\
3x_{3}-4x_{5}\\
x_{3}\\
-2x_{5}\\
x_{5}\end{bmatrix}\,\right|\,x_{3},\,x_{5}\text{ real numbers}\right\}.\]  
@endcol
@end
@slide
@eg
@label{CNS2}
  Let us compute the null space of
\[C=\begin{bmatrix}-4&6&1\\
-1&4&1\\
5&6&7\\
4&7&1\end{bmatrix}\]
which we write as ${\mathcal{N}}\!\left(C\right)$. Translating definition   @ref{NSM}, we simply desire to solve the homogeneous system
$\linearsystem{C}{\mathbf{0}}$.
So we row-reduce the augmented matrix to obtain
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&0&0\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&0\\
0&0&0&0\end{array}\right]\]  
@newcol
  There are no free variables in the homogeneous system represented by the row-reduced matrix, so there is only the trivial solution, the zero vector, $\mathbf{0}$. So we can write the (trivial) solution set as
\[{\mathcal{N}}\!\left(C\right)=\{\mathbf{0}\}=\left\{\begin{bmatrix}0\\
0\\
0\end{bmatrix}\right\}.\]  
@endcol
@end
@section{Particular Solutions, Homogeneous Solutions}
@label{PSHS}
The next theorem tells us that in order to find all of the solutions to a linear system of equations, it is sufficient to find just one solution, and then find all of the solutions to the corresponding homogeneous system. This explains part of our interest in the null space, the set of all solutions to a homogeneous system.  
@thm
@title{Particular Solution Plus Homogeneous Solutions}
@label{PSPHS}
  Suppose that $\mathbf{w}$ is one solution to the linear system of equations
$\linearsystem{A}{\mathbf{b}}$. Then $\mathbf{y}$ is a solution to
$\linearsystem{A}{\mathbf{b}}$ if and only if $\mathbf{y}=\mathbf{w}+\mathbf{z}$ for some vector $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, i.e.  
@enumerate
@item
  If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$, then $\mathbf{y}-\mathbf{w}\in{\mathcal{N}}\!\left(A\right)$  
@item
  If $\mathbf{z} \in {\mathcal{N}}\!\left(A\right)$, then $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$  
@endenumerate In other words, there is a one-to-one correspondence between
\[\text{solution set of $A\mathbf{x}=\mathbf{b}$}\longleftrightarrow{\mathcal{N}}\!\left(A\right),\]
through
\[\mathbf{y}\rightarrow\mathbf{y}-\mathbf{w},\]
\[\mathbf{w}+\mathbf{z}\leftarrow\mathbf{z}.\]  
@end
@proof
@newcol
  Because $\mathbf{w}$ is one solution to the linear system of equations
$\linearsystem{A}{\mathbf{b}}$, $A\mathbf{w}=\mathbf{b}$.  
@enumerate
@item
  If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$,
then $A\mathbf{y}=\mathbf{b}$.
Hence $A(\mathbf{y}-\mathbf{w})=A\mathbf{y}-A\mathbf{w}=\mathbf{b}-\mathbf{b}
=\mathbf{0}$.
So $\mathbf{y}-\mathbf{w}\in{\mathcal{N}}\!\left(A\right)$.  
@item
  Suppose $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, $A\mathbf{z}=\mathbf{0}$.
So $A(\mathbf{w}+\mathbf{z})=A\mathbf{w}+A\mathbf{z}=
\mathbf{b}+\mathbf{0}=\mathbf{b}$.
Hence $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$.  
@endenumerate
@qed
@endcol
@end
@slide
@eg
  \begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &= 8 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &=  -12 \\
x_1 +x_2 + 4x_3 - 5x_4 &=  4
\end{align*}
is a consistent system of equations with a nontrivial null space.
Let $A$ denote the coefficient matrix of this system.

Consider the following three solutions to the system:

@newcol
  \begin{align*}
\vect{y}_1=\colvector{0\\1\\2\\1}&&
\vect{y}_2=\colvector{4\\0\\0\\0}&&
\vect{y}_3=\colvector{7\\8\\1\\3}
\end{align*}
Let $\mathbf{w}=\mathbf{y}_{1}$. Then,
\[
\mathbf{y}_{2}-\mathbf{w}=\begin{bmatrix}4\\
-1\\
-2\\
-1\end{bmatrix},
\quad
\mathbf{y}_{3}-\mathbf{w}=\begin{bmatrix}7\\
7\\
-1\\
2\end{bmatrix}
\]
are indeed elements in ${\mathcal{N}}\!\left(A\right)$ (check!).

@col
  To find all the solutions, we may first work out
(using Gaussian elimination on $\left[A|\mathbf{0}\right]$, for example) that:
\[{\mathcal{N}}\!\left(A\right)=\left\{\left.x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}
\]
By the theorem, the solution set to the linear system is:

@col
  \[\mathbf{w}+{\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}\]  
@endcol
@end
@section{Augmented matrix vs Coefficient Matrix}
  The augmented matrix for the homogeneous of system of linear equations
$\linearsystem{A}{\mathbf{0}}$ is $[A|\mathbf{0}]$.
Any row operators on $[A|\mathbf{0}]$ will not change the last zero columns.
If
\[A\xrightarrow{\text{row operations}}B\]
then
\[[A|\mathbf{0}]\xrightarrow{\text{same row operations}}[B|\mathbf{0}].\]  
@newcol
  Therefore, for the homogeneous system of linear equations, we can replace the augmented matrix by the coefficient matrix. Just remember there is actually a zero column as the last column. For example:

@col
  \begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &= 0 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &=  0 \\
x_1 +x_2 + 4x_3 - 5x_4 &=  0
\end{align*}
We can start with coefficient matrix:

@col
  \[A=\begin{bmatrix}2&1&6&-7\\
-3&4&-5&-6\\
1&1&4&-5\end{bmatrix}\]
The RREF is:

@col
  \[\left[\begin{array}[]{cccc}\boxed{1}&0&3&-2\\
0&\boxed{1}&1&-3\\
0&0&0&0\end{array}\right]\]
The corresponding augmented matrix is:

@col
  \[\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&0\\
0&0&0&0&0\end{array}\right]\]
The system is consistent. It has $n-r=4-2=2$ free variables.
The solution set is:

@col
  \begin{align*}
S&=\setparts{\colvector{x_1\\x_2\\x_3\\x_4}}{x_1=-3x_3+2x_4,\,x_2=-x_3+3x_4}\\
&=\setparts{\colvector{-3x_3+2x_4\\-x_3+3x_4\\x_3\\x_4}}{ x_3,\,x_4  \text{ real numbers}}
\end{align*}  
@endcol
@section{Nonsingular Matrices}
  In this section we specialize further and consider matrices with equal numbers of rows and columns,
which when considered as coefficient matrices lead to systems with equal numbers of equations and variables.  
@defn
@title{Square Matrix}
@label{SQM}
  A matrix with $m$ rows and $n$ columns is @keyword{square} if $m=n$.
In this case, we say the matrix has @keyword{size} $n$.
To emphasize the situation when a matrix is not square, we will call it @keyword{rectangular}.  
@end
@defn
@title{Nonsingular Matrix}
@label{NM}
@label{NMTNS}
Suppose $A$ is a square matrix.
Suppose further that the solution set to the homogeneous linear system of equations $\linearsystem{A}{\mathbf{0}}$ is $\{\mathbf{0}\}$, in other words, the system has only the trivial solution.
Then we say that $A$ is a @keyword{nonsingular} matrix. Otherwise we say $A$ is a @keyword{singular} matrix.

Equivalently, a matrix $A$ is nonsingular if and only if its null space
$\mathcal{N}(A)$ consists only of the zero vector $\mathbf{0}$.
@end
@slide
@eg
@enumerate
@item
@newcol
  Let
\[A=\begin{bmatrix}1&-1&2\\
2&1&1\\
1&1&0\end{bmatrix}.\]
The system of linear equations $\linearsystem{A}{\mathbf{0}}$ has nontrivial solutions.
Hence $A$ is singular.  
@endcol
@item
@newcol
  Let
\[A=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.\]
The system of linear equations $\linearsystem{A}{\mathbf{0}}$ has only trivial solutions.
So it is nonsingular.  
@endcol
@endenumerate
@end
@slide
Recall:  
@defn
@title{Identity Matrix}
@label{IM}
  The $m\times m$ @keyword{identity matrix}, $I_{m}$, is defined by
\[\displaystyle\left[I_{m}\right]_{ij}=\begin{cases}1&i=j\\
0&i\neq j\end{cases}\quad\quad 1\leq i,\,j\leq m\]
i.e.
\[I_{m}=\begin{bmatrix}1&0&0&\cdots&0\\
0&1&0&\cdots&0\\
0&0&1&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&1\end{bmatrix}.\]  
@end
@eg
@newcol
  The $4\times 4$ identity matrix is:
\[I_{4}=\begin{bmatrix}1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\end{bmatrix}.\]  
@endcol
@end
@newcol
  Notice that an identity matrix is square, and in reduced row-echelon form.
Also, every column is a pivot column,
and every possible pivot column appears once.  
@endcol
@slide
@thm
@title{Nonsingular Matrices Row Reduce to the Identity Matrix}
@label{NMRRI}
  Suppose that $A$ is a square matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Then $A$ is nonsingular if and only if $B$ is the identity matrix.  
@end
@proof

($\Leftarrow$)  
@newcol
  Suppose $B$ is the identity matrix. When the augmented matrix $\left[A|\mathbf{0}\right]$ is row-reduced, the result is $\left[B|\mathbf{0}\right]=\left[I_{n}|\mathbf{0}\right]$. The number of nonzero rows is equal to the number of variables in the linear system of equations $\linearsystem{A}{\mathbf{0}}$, so $n=r$ and has $n-r=0$ free variables.
Thus, the homogeneous system $\linearsystem{A}{\mathbf{0}}$ has just one solution, which must be the trivial solution.
This is exactly the definition of a nonsingular matrix.  
@endcol

($\Rightarrow$)  
@newcol
  If $A$ is nonsingular, then the homogeneous system
$\linearsystem{A}{\mathbf{0}}$ has a unique solution,
and has no free variables in the description of the solution set.
The homogeneous system is consistent,
by Lecture 4 Theorem 4, the homogeneous system has $n-r$ free variables.
Thus, $n-r=0$, and so $n=r$. So $B$ has $n$ pivot columns among its total of $n$ columns. This is enough to force $B$ to be the $n\times n$ identity matrix $I_{n}$ (why?).  
@qed
@endcol
@end
@slide
@eg
  \[A=\begin{bmatrix}1&-1&2\\
2&1&1\\
1&1&0\end{bmatrix}\]
is row equivalent to the reduced row echelon form
\[B=\begin{bmatrix}1&0&1\\
0&1&-1\\
0&0&0\end{bmatrix}.\]
Since $B$ is not the $3\times 3$ identity matrix, the above theorem tells us that $A$ is a singular matrix.  
@end
@eg
@newcol
  \[A=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.\]
It is row-equivalent to the reduced row echelon form
\[B = \begin{bmatrix}1&0&0\\
0&1&0\\
0&0&1\end{bmatrix}\]
Since $B$ is the $3\times 3$ identity matrix, $A$ is a nonsingular matrix by the above theorem.  
@endcol
@end
@section{Nonsingular Matrices are Invertible}
@label{NMI}
  For $\alpha,\beta\in{\mathbb{R}}^{\hbox{}}$, then $\alpha\beta\neq 0$ if and only if $\alpha\neq 0$ and $\beta\neq 0$.
We have a similar result for nonsingular matrix

@slide
@thm
@title{Nonsingular Product has Nonsingular Terms}
@label{NPNT}
  Suppose that $A$ and $B$ are square matrices of size $n$. The product $AB$ is nonsingular if and only if $A$ and $B$ are both nonsingular.  
@end
@proof
@newcol
  ($\Rightarrow$) For this portion of the proof we will form the logically-equivalent contrapositive and prove that statement using two cases.

<blockquote> $AB$ is nonsingular implies $A$ and $B$ are both nonsingular. </blockquote> becomes:

@col
<blockquote> $A$ or $B$ is singular implies $AB$ is singular.
</blockquote>
@col
  Case 1. Suppose $B$ is singular. Then there is a nonzero vector $\mathbf{z}$ that is a solution to $B\mathbf{x}=\mathbf{0}$. So
\begin{align*}
\displaystyle(AB)\mathbf{z}&\displaystyle=A(B\mathbf{z}) \\
&\displaystyle=A\mathbf{0} \\
&\displaystyle=\mathbf{0}
\end{align*}  
@col
  Then $\mathbf{z}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired.

@col
  Case 2. Suppose $A$ is singular, and $B$ is not singular.
Because $A$ is singular, there is a nonzero vector $\mathbf{y}$ that is a solution to $A\mathbf{x}=\mathbf{0}$. Now consider the linear system $B\mathbf{x}=\mathbf{y}$. Since $B$ is nonsingular, the system has a unique solution, which we will denote as $\mathbf{w}$. We first claim $\mathbf{w}$ is not the zero vector either. Assuming the opposite, suppose that $\mathbf{w}=\mathbf{0}$. Then
\begin{align*}
\displaystyle\mathbf{y}&\displaystyle=B\mathbf{w} \\
&\displaystyle=B\mathbf{0} \\
&\displaystyle=\mathbf{0} \\
\\
\displaystyle(AB)\mathbf{w}&\displaystyle=A(B\mathbf{w}) \\
&\displaystyle=A\mathbf{y} \\
&\displaystyle=\mathbf{0}
\end{align*}  
@col
  So $\mathbf{w}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired. And this conclusion holds for both cases.

($\Leftarrow$) Now assume that both $A$ and $B$ are nonsingular. Suppose that $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution to $AB\mathbf{x}=\mathbf{0}$. Then
\begin{align*}
\displaystyle\mathbf{0}&\displaystyle=\left(AB\right)\mathbf{x} \\
&\displaystyle=A\left(B\mathbf{x}\right)
\end{align*}  
@col
  So $B\mathbf{x}$ is a solution to $A\mathbf{x}=\mathbf{0}$, and by the definition of a nonsingular matrix, we conclude that $B\mathbf{x}=\mathbf{0}$. Now, by an entirely similar argument, the nonsingularity of $B$ forces us to conclude that $\mathbf{x}=\mathbf{0}$. So the only solution to $AB\mathbf{x}=\mathbf{0}$ is the zero vector and we conclude that $AB$ is nonsingular.  
@qed
@endcol
@end
@newcol
  The contrapositive of this entire result is equally interesting. It says that $A$ or $B$ (or both) is a singular matrix if and only if the product $AB$ is singular.

@endcol
@slide
@thm
@title{One-Sided Inverse is Sufficient}
@label{OSIS}
  Suppose $A$ and $B$ are square matrices of size $n$ such that $AB=I_{n}$. Then $BA=I_{n}$.  
@end

@proof
  The matrix $I_{n}$ is nonsingular. So by @ref{NPNT} $A$ is nonsingular.

@newcol
  Hence, $A$ is row equivalent to the identity matrix $I_n$,
which means there are elementary matrices $J_1, J_2, \ldots, J_k$ such that:
\[
J_k\cdots J_2 J_1 A = I_n.
\]  
@col
  Hence, $AB = I_n$ implies that:
\[
\underbrace{J_k\cdots J_2 J_1 A}_{I_n} B = \underbrace{J_k\cdots J_2 J_1 I_n}_{J_k\cdots J_2 J_1}
\]  
@col
  So, $B = J_k\cdots J_2 J_1$, and:
\[
BA = J_k\cdots J_2 J_1 A = I_n
\]  
@qed
@endcol
@end

@slide
@thm
@title{Nonsingularity is Invertibility}
@label{NI}
  Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if $A$ is invertible.  
@end
@proof
@newcol
  ($\Leftarrow$)
Suppose $A$ is invertible.
Then, $A\vect{x} = \vect{0}$ if and only if :
\[
\underbrace{A^{-1}A\vect{x}}_{\vect{x}} = A^{-1}\vect{0} = \vect{0}.
\]
This implies that the homogeneous linear system $\mathcal{LS}(A, \vect{0})$
only exactly one unique solution $\vect{x} = \vect{0}$.
Hence, $A$ is nonsingular.

@col
  ($\Rightarrow$)
Suppose $A$ is a nonsingular $n \times n$ matrix.
Then, $A$ is row-equivalent to $I_n$,
which means there are elementary matrices $J_1, J_2, \ldots, J_k$ such that:
\[
J_k\cdots J_2 J_1 A = I_n
\]
It now follows from @ref{OSIS} that $A$ is invertible, with:
\[
A^{-1} =  J_k\cdots J_2 J_1.
\]  
@qed
@endcol
@end
@remark
@newcol
  So for a square matrix, the properties of having an inverse and of having a trivial null space are one and the same. Cannot have one without the other.  
@endcol
@end
@slide
@remark
  It is now easy to see that any nonsingular matrix $A$ is invertible:

@newcol
  If an $n\times n$ matrix $A$ is nonsingular, then it is row-equivalent to $I_n$,
which means there exists a sequence of elementary matrices $J_1, J_2, \ldots, J_l$
(each corresponding to a row operation), such that:
\[
J_l \cdots J_2 J_1 A = I.
\]
Let $J = J_l \cdots J_2 J_1$. Since each elementary matrix is invertible,
and the product of invertible matrices is also invertible, the matrix $J$ is invertible.

@col
  Hence,
\[
A = J^{-1}JA = J^{-1} I = J^{-1}
\]
The matrix $J^{-1}$, being the inverse of an invertiable matrix, is invertible.
We conclude that $A = J^{-1}$ is invertible.

(Observe that every nonsingular matrix is in particular a product of elementary matrices.)  
@endcol
@end
@slide
@thm
@title{Nonsingular Matrices and Unique Solutions}
@label{NMUS}
  Suppose that $A$ is a square matrix. $A$ is a nonsingular matrix if and only if the system $\linearsystem{A}{\mathbf{b}}$ has a unique solution for every choice of the constant vector $\mathbf{b}$.  
@end
@proof

($\Rightarrow$)  
@newcol
  The hypothesis for this half of the proof is that the system
$\linearsystem{A}{\mathbf{b}}$
has a unique solution for every choice of the constant vector $\mathbf{b}$.
We will make a very specific choice for $\mathbf{b}$: $\mathbf{b}=\mathbf{0}$.
Then we know that the system $\linearsystem{A}{\mathbf{0}}$ has a unique solution.
But this is precisely the definition of what it means for $A$
to be nonsingular.  
@endcol

($\Leftarrow$)  
@newcol
  We assume that $A$ is nonsingular of size $n\times n$,
so we know there is a sequence of row operations that will convert $A$
into the identity matrix $I_{n}$ (Theorem   @ref{NMRRI}).
Form the augmented matrix $A^{\prime}=\left[A|\mathbf{b}\right]$
and apply this same sequence of row operations to $A^{\prime}$.
The result will be the matrix $B^{\prime}=\left[I_{n}|\mathbf{c}\right]$,
which is in reduced row-echelon form with $r=n$.
Then the augmented matrix $B^{\prime}$ represents the (extremely simple)
system of equations $x_{i}=\left[\mathbf{c}\right]_{i}$, $1\leq i\leq n$.
The vector $\mathbf{c}$ is clearly a solution, so the system is consistent.
With a consistent system, we use Lecture 4 Theorem 4 to count free variables.
We find that there are $n-r=n-n=0$ free variables,
and so we therefore know that the solution is unique.  
@qed
@endcol
@end
  Alternatively,  
@proof
@newcol
  Suppose $A \mathbf{x} = \mathbf{b}$ has a unique solution for every vector $\mathbf{b}$. Then, in particular, the only solution to $A\mathbf{x} = \mathbf{0}$
is $\mathbf{x} = \mathbf{0}$. This implies by definition that $A$ is nonsingular.

@col
  Conversely,
if $A$ is nonsingular, then $A^{-1}$ exists.
So, $A\mathbf{x} = \mathbf{b}$ implies that the unique solution is $\mathbf{x} = A^{-1}\mathbf{b}$.  
@qed
@endcol
@end
@slide
@thm
@title{Nonsingular Matrix Equivalences}
<!-- @label{NME3} -->
@label{NME1}
@label{NMEInverse}
Suppose that $A$ is a square matrix of size $n$. The following are equivalent.
@enumerate
@item $A$ is nonsingular.
@item $A$ row-reduces to the identity matrix.
@item The null space of $A$ contains only the zero vector,
${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
@item The linear system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every possible choice of $\mathbf{b}$.
<!--
@item The columns of $A$ are a linearly independent set.
-->
@item $A$ is invertible.
@endenumerate
@end
@proof
@newcol
The statement that $A$ is nonsingular is equivalent to each of the subsequent statements by, in turn,
theorems @ref{NMRRI}, @ref{NMTNS}, @ref{NMUS}, @ref{NI}.
@qed
@endcol
@end
