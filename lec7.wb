@course{MATH 1030}
@chapter{Homogeneous Systems of Equations and Nonsingular Matrices}

<h5 class="notkw">Reference.</h5>
 Beezer, Ver 3.5 Section HSE (print version p44 - p50)Section NM (print version p51 - p56)
<h5 class="notkw">Exercise.</h5> Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf 
@enumerate
@item
 Section HSE (ex p.18-23) C21-C23, C25-C27, C30-C31, M50-M52, T10-T12, T20 
@item
 Section NM (ex p.23-27) C30-C33, C50, M30, M51-M52, T10, T12, T30, T31, T90. 
@endenumerate
@section{Solutions of Homogeneous Systems}
@label{SHS}
@defn
@title{Homogeneous System}
@label{HS}
 A system of linear equations, $\mathcal{LS}(A, \mathbf{b})$ is @keyword{homogeneous} if the vector of constants is the zero vector, in other words, if $\mathbf{b}=\mathbf{0}$, i.e.
\begin{align*}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n  &amp;=0 \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n  &amp;=0\\
\vdots &amp;\\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n &amp;= 0
\end{align*} 
@end
@defn
@title{Homogeneous System corresponding to system of linear equation}
@newcol
 The @keyword{homogeneous system} corresponding to $\linearsystem{A}{\vect{b}}$:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&amp;=b_1\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&amp;=b_2\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&amp;=b_3\\
\vdots&amp;\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&amp;=b_m
\end{align*}
is $\linearsystem{A}{\vect{0}}$:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&amp;=0\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&amp;=0\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&amp;=0\\
\vdots&amp;\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&amp;=0
\end{align*} 
@endcol
@end
@slide
@eg
 The following is a homogeneous system of linear equations:
\begin{align*}
x_1-2x_2+3x_3 - 4x_4 &amp;= 0 \\
x_2-x_4 &amp;=0 \\
x_1+3x_2 - 5x_3 + 5 x_4 &amp;= 0
\end{align*}
It is the homogeneous system of linear equations corresponding to
\begin{align*}
x_1-2x_2+3x_3 - 4x_4 &amp;= 1 \\
x_2-x_4 &amp;=2 \\
x_1+3x_2 - 5x_3 + 5 x_4 &amp;= 3
\end{align*} 
@end
@slide
@thm
@title{Homogeneous Systems are Consistent}
@label{HSC}
 Suppose that a system of linear equations is homogeneous.
Then the system is consistent. In fact $\mathbf{0}$ is a solution, i.e $x_{1}=x_{2}=\cdots=x_{n}=0$ is a solution. Such solution is called a @keyword{trivial solution}. 
@end
@proof
@newcol
 Set each variable of the system to zero. The left hand side of the all equations are zero, which are equal to the right hand side. 
@qed
@endcol
@end
@slide
@eg
@enumerate
@item
<br/>
 \begin{align*}
-7x_1 -6 x_2 - 12x_3 &amp;=0\\
5x_1  + 5x_2 + 7x_3 &amp;=0\\
x_1 +4x_3 &amp;=0
\end{align*}
The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0\end{array}\right]\]
It has $n-r=3-3=0$ free variable. Hence it has only one solution. 
@item
@newcol
 \begin{align*}
x_1 -x_2 +2x_3 &amp; =0\\
2x_1+ x_2 + x_3 &amp; =0\\
x_1 + x_2 &amp; =0
\end{align*}The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{ccc|c}\boxed{1}&amp;0&amp;1&amp;0\\
0&amp;\boxed{1}&amp;-1&amp;0\\
0&amp;0&amp;0&amp;0\end{array}\right]\]
The system is consistent. It has $n-r=3-2=1$ free variable. The solution set is
\[S=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}\,\right|\,x_{1}=-x_{3},\,x_{2}=x_{3}\right\}=\left\{\left.%
\begin{bmatrix}-x_{3}\\
x_{3}\\
x_{3}\end{bmatrix}\,\right|\,x_{3}\text{ real number}\right\}\]
Geometrically, these are points in three dimensions that lie on a line through the origin. 
@endcol
@item
@newcol
 \begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &amp;= 0 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &amp;=  0 \\
x_1 +x_2 + 4x_3 - 5x_4 &amp;=  0
\end{align*}
The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{array}\right]\]
The system is consistent. It has $n-r=4-2=2$ free variables. The solution set is
\[\displaystyle S=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{1}=-3x_{3}+2x_{4},\,x_{2}=-x_{3}+3x_{4}\right\}\]
\[\displaystyle=\left\{\left.\begin{bmatrix}-3x_{3}+2x_{4}\\
-x_{3}+3x_{4}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{3},\,x_{4}\text{ real numbers}\right\}\] 
@endcol
@endenumerate
@end
@slide
 Notice that when we do row operations on the augmented matrix of a homogeneous system of linear equations the last column of the matrix is all zeros. Any one of the three allowable row operations will convert zeros to zeros and thus, the final column of the matrix in reduced row-echelon form will also be all zeros. So in this case, we may be as likely to reference only the coefficient matrix and presume that we remember that the final column begins with zeros, and after any number of row operations is still zero. 
@slide
@thm
@label{HMVEI}
 Suppose that a homogeneous system of linear equations has $m$ equations and $n$ variables with $n&gt;m$.
Then the system has infinitely many solutions. 
@end
@proof
@newcol
 The system is homogeneous, by theorem @ref{HSC} it is consistent.
Then the hypothesis that $n&gt;m$, together with @ref{CMVEI}, gives infinitely many solutions. 
@qed
@endcol
@end
 If $n=m$, then we can have a unique solution or infinitely many solutions (see the above examples). 
@section{Particular Solutions, Homogeneous Solutions}
@label{PSHS}
 The next theorem tells us that in order to find all of the solutions to a linear system of equations, it is sufficient to find just one solution, and then find all of the solutions to the corresponding homogeneous system. This explains part of our interest in the null space, the set of all solutions to a homogeneous system. 
@thm
@title{Particular Solution Plus Homogeneous Solutions}
@label{PSPHS}
 Suppose that $\mathbf{w}$ is one solution to the linear system of equations
$\linearsystem{A}{\mathbf{b}}$. Then $\mathbf{y}$ is a solution to
$\linearsystem{A}{\mathbf{b}}$ if and only if $\mathbf{y}=\mathbf{w}+\mathbf{z}$ for some vector $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, i.e. 
@enumerate
@item
 If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$, then $\mathbf{y}-\mathbf{w}\in{\mathcal{N}}\!\left(A\right)$ 
@item
 If $\mathbf{z} \in {\mathcal{N}}\!\left(A\right)$, then $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$ 
@endenumerate In other words, there is a one-to-one correspondence between
\[\text{solution set of $A\mathbf{x}=\mathbf{b}$}\longleftrightarrow{\mathcal{N}}\!\left(A\right),\]
through
\[\mathbf{y}\rightarrow\mathbf{y}-\mathbf{w},\]
\[\mathbf{w}+\mathbf{z}\leftarrow\mathbf{z}.\] 
@end
@proof
@newcol
 Because $\mathbf{w}$ is one solution to the linear system of equations
$\linearsystem{A}{\mathbf{b}}$, $A\mathbf{w}=\mathbf{b}$. 
@enumerate
@item
 If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$,
then $A\mathbf{y}=\mathbf{b}$.
Hence $A(\mathbf{y}-\mathbf{w})=A\mathbf{y}-A\mathbf{w}=\mathbf{b}-\mathbf{b}
=\mathbf{0}$.
So $\mathbf{y}-\mathbf{w}\in{\mathcal{N}}\!\left(A\right)$. 
@item
 Suppose $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, $A\mathbf{z}=\mathbf{0}$.
So $A(\mathbf{w}+\mathbf{z})=A\mathbf{w}+A\mathbf{z}=
\mathbf{b}+\mathbf{0}=\mathbf{b}$.
Hence $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$. 
@endenumerate
@qed
@endcol
@end
@slide
@eg
 \begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &amp;= 8 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &amp;=  -12 \\
x_1 +x_2 + 4x_3 - 5x_4 &amp;=  4
\end{align*}
is a consistent system of equations with a nontrivial null space.
Let $A$ denote the coefficient matrix of this system.
Consider the following three solutions to the system:

@newcol
 \begin{align*}
\vect{y}_1=\colvector{0\\1\\2\\1}&amp;&amp;
\vect{y}_2=\colvector{4\\0\\0\\0}&amp;&amp;
\vect{y}_3=\colvector{7\\8\\1\\3}
\end{align*}
Let $\mathbf{w}=\mathbf{y}_{1}$. Then,
\[
\mathbf{y}_{2}-\mathbf{w}=\begin{bmatrix}4\\
-1\\
-2\\
-1\end{bmatrix},
\quad
\mathbf{y}_{3}-\mathbf{w}=\begin{bmatrix}7\\
7\\
-1\\
2\end{bmatrix}
\]
are indeed elements in ${\mathcal{N}}\!\left(A\right)$ (check!).

@col
 To find all the solutions, we may first work out
(using Gaussian elimination on $\left[A|\mathbf{0}\right]$, for example) that:
\[{\mathcal{N}}\!\left(A\right)=\left\{\left.x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}
\]
By the theorem, the solution set to the linear system is:

@col
 \[\mathbf{w}+{\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}\] 
@endcol
@end
@section{Augmented matrix vs Coefficient Matrix}
 The augmented matrix for the homogeneous of system of linear equations
$\linearsystem{A}{\mathbf{0}}$ is $[A|\mathbf{0}]$.
Any row operators on $[A|\mathbf{0}]$ will not change the last zero columns.
If
\[A\xrightarrow{\text{row operations}}B\]
then
\[[A|\mathbf{0}]\xrightarrow{\text{same row operations}}[B|\mathbf{0}].\] 
@newcol
 Therefore, for the homogeneous system of linear equations, we can replace the augmented matrix by the coefficient matrix. Just remember there is actually a zero column as the last column. For example:

@col
 \begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &amp;= 0 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &amp;=  0 \\
x_1 +x_2 + 4x_3 - 5x_4 &amp;=  0
\end{align*}
We can start with coefficient matrix:

@col
 \[A=\begin{bmatrix}2&amp;1&amp;6&amp;-7\\
-3&amp;4&amp;-5&amp;-6\\
1&amp;1&amp;4&amp;-5\end{bmatrix}\]
The RREF is:

@col
 \[\left[\begin{array}[]{cccc}\boxed{1}&amp;0&amp;3&amp;-2\\
0&amp;\boxed{1}&amp;1&amp;-3\\
0&amp;0&amp;0&amp;0\end{array}\right]\]
The corresponding augmented matrix is:

@col
 \[\left[\begin{array}[]{cccc|c}\boxed{1}&amp;0&amp;3&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;1&amp;-3&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{array}\right]\]
The system is consistent. It has $n-r=4-2=2$ free variables.
The solution set is:

@col
 \begin{align*}
S&amp;=\setparts{\colvector{x_1\\x_2\\x_3\\x_4}}{x_1=-3x_3+2x_4,\,x_2=-x_3+3x_4}\\
&amp;=\setparts{\colvector{-3x_3+2x_4\\-x_3+3x_4\\x_3\\x_4}}{ x_3,\,x_4  \text{ real numbers}}
\end{align*} 
@endcol
@section{Nonsingular Matrices}
 In this section we specialize further and consider matrices with equal numbers of rows and columns,
which when considered as coefficient matrices lead to systems with equal numbers of equations and variables. 
@defn
@title{Square Matrix}
@label{SQM}
 A matrix with $m$ rows and $n$ columns is @keyword{square} if $m=n$.
In this case, we say the matrix has @keyword{size} $n$.
To emphasize the situation when a matrix is not square, we will call it @keyword{rectangular}. 
@end
@defn
@title{Nonsingular Matrix}
@label{NM}
@newcol
 Suppose $A$ is a square matrix.
Suppose further that the solution set to the homogeneous linear system of equations $\linearsystem{A}{\mathbf{0}}$ is $\{\mathbf{0}\}$, in other words, the system has only the trivial solution.
Then we say that $A$ is a @keyword{nonsingular} matrix. Otherwise we say $A$ is a @keyword{singular} matrix. 
@endcol
@end
@slide
@eg
@enumerate
@item
@newcol
 Let
\[A=\begin{bmatrix}1&amp;-1&amp;2\\
2&amp;1&amp;1\\
1&amp;1&amp;0\end{bmatrix}.\]
The system of linear equations $\linearsystem{A}{\mathbf{0}}$ has nontrivial solutions.
Hence $A$ is singular. 
@endcol
@item
@newcol
 Let
\[A=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}.\]
The system of linear equations $\linearsystem{A}{\mathbf{0}}$ has only trivial solutions.
So it is nonsingular. 
@endcol
@endenumerate
@end
@slide
 Recall: 
@defn
@title{Identity Matrix}
@label{IM}
 The $m\times m$ @keyword{identity matrix}, $I_{m}$, is defined by
\[\displaystyle\left[I_{m}\right]_{ij}=\begin{cases}1&amp;i=j\\
0&amp;i\neq j\end{cases}\quad\quad 1\leq i,\,j\leq m\]
i.e.
\[I_{m}=\begin{bmatrix}1&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;1&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;1&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;1\end{bmatrix}.\] 
@end
@eg
@newcol
 The $4\times 4$ identity matrix is:
\[I_{4}=\begin{bmatrix}1&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0\\
0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;1\end{bmatrix}.\] 
@endcol
@end
@newcol
 Notice that an identity matrix is square, and in reduced row-echelon form.
Also, every column is a pivot column,
and every possible pivot column appears once. 
@endcol
@slide
@thm
@title{Nonsingular Matrices Row Reduce to the Identity Matrix}
@label{NMRRI}
 Suppose that $A$ is a square matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Then $A$ is nonsingular if and only if $B$ is the identity matrix. 
@end
@proof

($\Leftarrow$) 
@newcol
 Suppose $B$ is the identity matrix. When the augmented matrix $\left[A|\mathbf{0}\right]$ is row-reduced, the result is $\left[B|\mathbf{0}\right]=\left[I_{n}|\mathbf{0}\right]$. The number of nonzero rows is equal to the number of variables in the linear system of equations $\linearsystem{A}{\mathbf{0}}$, so $n=r$ and has $n-r=0$ free variables.
Thus, the homogeneous system $\linearsystem{A}{\mathbf{0}}$ has just one solution, which must be the trivial solution.
This is exactly the definition of a nonsingular matrix. 
@endcol
($\Rightarrow$) 
@newcol
 If $A$ is nonsingular, then the homogeneous system
$\linearsystem{A}{\mathbf{0}}$ has a unique solution,
and has no free variables in the description of the solution set.
The homogeneous system is consistent,
by Lecture 4 Theorem 4, the homogeneous system has $n-r$ free variables.
Thus, $n-r=0$, and so $n=r$. So $B$ has $n$ pivot columns among its total of $n$ columns. This is enough to force $B$ to be the $n\times n$ identity matrix $I_{n}$ (why?). 
@qed
@endcol
@end
@slide
@eg
 \[A=\begin{bmatrix}1&amp;-1&amp;2\\
2&amp;1&amp;1\\
1&amp;1&amp;0\end{bmatrix}\]
is row equivalent to the reduced row echelon form
\[B=\begin{bmatrix}1&amp;0&amp;1\\
0&amp;1&amp;-1\\
0&amp;0&amp;0\end{bmatrix}.\]
Since $B$ is not the $3\times 3$ identity matrix, the above theorem tells us that $A$ is a singular matrix. 
@end
@eg
@newcol
 \[A=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}.\]
It is row-equivalent to the reduced row echelon form
\[B = \begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;1\end{bmatrix}\]
Since $B$ is the $3\times 3$ identity matrix, $A$ is a nonsingular matrix by the above theorem. 
@endcol
@end
@section{Nonsingular Matrices are Invertible}
@label{NMI}
 For $\alpha,\beta\in{\mathbb{R}}^{\hbox{}}$, then $\alpha\beta\neq 0$ if and only if $\alpha\neq 0$ and $\beta\neq 0$.
We have a similar result for nonsingular matrix

@slide
@thm
@title{Nonsingular Product has Nonsingular Terms}
@label{NPNT}
 Suppose that $A$ and $B$ are square matrices of size $n$. The product $AB$ is nonsingular if and only if $A$ and $B$ are both nonsingular. 
@end
@proof
@newcol
 ($\Rightarrow$) For this portion of the proof we will form the logically-equivalent contrapositive and prove that statement using two cases.
<blockquote> $AB$ is nonsingular implies $A$ and $B$ are both nonsingular. </blockquote> becomes

@col
<blockquote> $A$ or $B$ is singular implies $AB$ is singular. </blockquote>

@col
 Case 1. Suppose $B$ is singular. Then there is a nonzero vector $\mathbf{z}$ that is a solution to $B\mathbf{x}=\mathbf{0}$. So
\begin{align*}
\displaystyle(AB)\mathbf{z}&amp;\displaystyle=A(B\mathbf{z}) \\
&amp;\displaystyle=A\mathbf{0} \\
&amp;\displaystyle=\mathbf{0}
\end{align*} 
@col
 Then $\mathbf{z}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired.

@col
 Case 2. Suppose $A$ is singular, and $B$ is not singular.
Because $A$ is singular, there is a nonzero vector $\mathbf{y}$ that is a solution to $A\mathbf{x}=\mathbf{0}$. Now consider the linear system $B\mathbf{x}=\mathbf{y}$. Since $B$ is nonsingular, the system has a unique solution, which we will denote as $\mathbf{w}$. We first claim $\mathbf{w}$ is not the zero vector either. Assuming the opposite, suppose that $\mathbf{w}=\mathbf{0}$. Then
\begin{align*}
\displaystyle\mathbf{y}&amp;\displaystyle=B\mathbf{w} \\
&amp;\displaystyle=B\mathbf{0} \\
&amp;\displaystyle=\mathbf{0} \\
\\
\displaystyle(AB)\mathbf{w}&amp;\displaystyle=A(B\mathbf{w}) \\
&amp;\displaystyle=A\mathbf{y} \\
&amp;\displaystyle=\mathbf{0}
\end{align*} 
@col
 So $\mathbf{w}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired. And this conclusion holds for both cases.
($\Leftarrow$) Now assume that both $A$ and $B$ are nonsingular. Suppose that $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution to $AB\mathbf{x}=\mathbf{0}$. Then
\begin{align*}
\displaystyle\mathbf{0}&amp;\displaystyle=\left(AB\right)\mathbf{x} \\
&amp;\displaystyle=A\left(B\mathbf{x}\right)
\end{align*} 
@col
 So $B\mathbf{x}$ is a solution to $A\mathbf{x}=\mathbf{0}$, and by the definition of a nonsingular matrix, we conclude that $B\mathbf{x}=\mathbf{0}$. Now, by an entirely similar argument, the nonsingularity of $B$ forces us to conclude that $\mathbf{x}=\mathbf{0}$. So the only solution to $AB\mathbf{x}=\mathbf{0}$ is the zero vector and we conclude that $AB$ is nonsingular. 
@qed
@endcol
@end
@newcol
 The contrapositive of this entire result is equally interesting. It says that $A$ or $B$ (or both) is a singular matrix if and only if the product $AB$ is singular.

@endcol
@slide
@thm
@title{One-Sided Inverse is Sufficient}
@label{OSIS}
 Suppose $A$ and $B$ are square matrices of size $n$ such that $AB=I_{n}$. Then $BA=I_{n}$. 
@end

@proof
 The matrix $I_{n}$ is nonsingular. So by @ref{NPNT} $A$ is nonsingular.

@newcol
 Hence, $A$ is row equivalent to the identity matrix $I_n$,
which means there are elementary matrices $J_1, J_2, \ldots, J_k$ such that:
\[
J_k\cdots J_2 J_1 A = I_n.
\] 
@col
 Hence, $AB = I_n$ implies that:
\[
\underbrace{J_k\cdots J_2 J_1 A}_{I_n} B = \underbrace{J_k\cdots J_2 J_1 I_n}_{J_k\cdots J_2 J_1}
\] 
@col
 So, $B = J_k\cdots J_2 J_1$, and:
\[
BA = J_k\cdots J_2 J_1 A = I_n
\] 
@qed
@endcol
@end

@slide
@thm
@title{Nonsingularity is Invertibility}
@label{NI}
 Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if $A$ is invertible. 
@end
@proof
@newcol
 ($\Leftarrow$)
Suppose $A$ is invertible.
Then, $A\vect{x} = \vect{0}$ if and only if :
\[
\underbrace{A^{-1}A\vect{x}}_{\vect{x}} = A^{-1}\vect{0} = \vect{0}.
\]
This implies that the homogeneous linear system $\mathcal{LS}(A, \vect{0})$
only exactly one unique solution $\vect{x} = \vect{0}$.
Hence, $A$ is nonsingular.

@col
 ($\Rightarrow$)
Suppose $A$ is a nonsingular $n \times n$ matrix.
Then, $A$ is row-equivalent to $I_n$,
which means there are elementary matrices $J_1, J_2, \ldots, J_k$ such that:
\[
J_k\cdots J_2 J_1 A = I_n
\]
It now follows from @ref{OSIS} that $A$ is invertible, with:
\[
A^{-1} =  J_k\cdots J_2 J_1.
\] 
@qed
@endcol
@end
@remark
@newcol
 So for a square matrix, the properties of having an inverse and of having a trivial null space are one and the same. Cannot have one without the other. 
@endcol
@end
@course{MATH 1030}
<!--DELIMITER-->
