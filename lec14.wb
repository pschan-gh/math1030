@course{MATH 1030}
@setchapter{14}
@chapter{Dimension, Rank, Nullity}
@section{Dimension}
@defn
@title{Dimension}
Let $V$ be a vector space.

Suppose a finite set of vectors $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$ is a basis for $V$.

Then, we say that $V$ is a @keyword{finite dimensional vector space}.

The number $t$ (namely the number of vectors in the basis)
is called the @keyword{dimension} of $V$.

The dimension of the zero vector space $\{\vect{0}\}$ is defined to be $0$.
@end
@remark
@newcol
It is a non-trivial fact that the dimension is well-defined, i.e.,
If both $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$
and $\left\{\mathbf{u}_{1},\ldots,\mathbf{u}_{s}\right\}$
are bases for $V$,
then $s=t$.
@endcol
@end
@slide
@thm
@label{SSLD}
Suppose that $S=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{t}\right\}$ is a finite set of vectors which spans the vector space $V$. Then any set of $t+1$ or more vectors from $V$ is linearly dependent.
@end
@proof
@newcol
Let $\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_m$ be $m$ vectors in $V$,
where $m \geq t + 1$.
Let $A = [\vect{v}_1 | \vect{v}_2 | \cdots | \vect{v}_t ]$.
Since $S$ spans $V$, for every $\vect{u}_i$ ($1\leq i \leq m$)
there exists
$\vect{w}_i \in \mathbb{R}^t$ such that:
\[
A\vect{w}_i = \vect{u}_i.
\]

@col
Now, consider the matrix:
\[
B = [\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m].
\]
This is a $t \times m$ matrix.
In particular, it has more columns than rows,
due to the assumption that $m > t$.

@col
Hence, the homogeneous linear system $\mathcal{LS}(B, \vect{0})$
has a non-trivial solution $\vect{x} \in \mathbb{R}^m$.  That is:
\[
B\vect{x} = \vect{0}.
\]

@col
The above equation implies that:
\[
A\left(B\vect{x}\right) = A\vect{0} = \vect{0}.
\]
@col
By the associativity of matrix multiplication, we have:
\[
A\left(B\vect{x}\right) = \left(AB\right)\vect{x}.
\]
@col
On the other hand:
@steps
\[
\begin{split}
AB &= A[\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m]
\\&
\class{steps}{\cssId{step0}{=[A\vect{w}_1 | A\vect{w}_2 | \cdots | A\vect{w}_m]}}
\\&
\class{steps}{\cssId{step1}{= [\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]}}
\end{split}
\]

@endsteps
@col
Hence,
\[
\left(AB\right)\vect{x} = \vect{0}
\]
is equivalent to:
\[
[\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]\vect{x} = \vect{0}
\]
@col
which is in turn equivalent to:
\[
x_1 \vect{u}_1 + x_2 \vect{u}_2 + \cdots x_m \vect{u}_m = \vect{0}.
\]
Since, $\vect{x}$ is not the zero vector,
not all the $x_i$'s are equal to zero.
We conclude that the vectors
$\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_m$ are linearly dependent.
@qed
@endcol
@end
@slide
@thm
Suppose that $V$ is a vector space with a finite basis $B$ and a second basis $C$.

@newcol
Then $B$ and $C$ have the same size.
@endcol
@end
@proof
@newcol
Denote the size of $B$ by $t$. If $C$ has $\geq t+1$ vectors,
then by the previous theorem, $C$ is linearly dependent,
in contradiction to the condition that $C$ is a basis.

@col
By the same reasoning, the linearly independent set $B$ must also not
have more vectors than $C$.

@col
So, $B$ and $C$ have the same number of vectors.
@qed
@endcol
@end
@remark
@newcol
The above theorem shows that the dimension is well-defined. No matter which basis we choose, the size is always the same.
@endcol
@end
@slide
@prop
@label{basisexistsprop}
Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}\subseteq{\mathbb{R}}^{m}$. Then
\begin{align*}
\displaystyle \dim\mathrm{Span}\,S\leq n.
\end{align*}
@end
@proof
@newcol
By @ref{basisexists}, there exists a subset $T$ of $S$ such that $T$ is a basis for $\mathrm{Span}\,S$.
\begin{align*}
\displaystyle \dim\mathrm{Span}\,S
=
\text{number of vectors in $T$}
\leq
\text{number of vectors in $S$}=n.
\end{align*}
@qed
@endcol
@end
@remark
@newcol
@ref{basisexists} is valid
if we replace ${\mathbb{R}}^{m}$ by $P_{n}$, $M_{mn}$
or any finite dimensional vector space.
@endcol
@end
@slide
@eg
\[
\dim{\mathbb{R}}^{m}=m.
\]
@end
@corollary
@label{cor:nvecsrmlindep}
@col
Any set of $n$ vectors in $\mathbb{R}^m$ are linearly dependent if $n > m$.
@end
@proof
@col
This follows from @ref{SSLD} and the fact that $\mathbb{R}^m$ is spanned by $m$ vectors.
@end
@slide
@skip
@eg
<strong>Math major only</strong>

$\dim M_{mn}=mn$. See example 3.
@end

@eg
<strong>Math major only</strong>

$\dim P_{n}=n+1$. See example 4.
@end

@eg
<strong>Math major only</strong>

Let $S_{2}$ be the set of $2\times 2$ symmetric matrices. For $A\in S_{2}$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
b&c\end{bmatrix}=a\begin{bmatrix}1&0\\
0&0\end{bmatrix}+b\begin{bmatrix}0&1\\
1&0\end{bmatrix}+c\begin{bmatrix}0&0\\
0&1\end{bmatrix}
\end{align*}
@newcol
We can show that:
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1&0\\
0&0\end{bmatrix},\begin{bmatrix}0&1\\
1&0\end{bmatrix},\begin{bmatrix}0&0\\
0&1\end{bmatrix}\right\}
\end{align*}
@col
is a basis for $S_{2}$. Hence $\dim S_{2}=3$.
@endcol
@end

@eg
<strong>Math major only</strong>

Let $P$ be the set of all real polynomials. As $\left\{1,x,x^{2},x^{3},\ldots\right\}$ is linearly independent,
so $\dim P$ does not exists (or we can write $\dim P=\infty$).
@end
@slide
We have seen that every column space of a matrix has a basis. Does every subspace of $\mathbb{R}^m$ have a basis?
@slide
@lemma
@label{lemma:outsidespanlinindep}
Let $V$ be a vector space and $\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\in V$.

Suppose
$S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent and $\mathbf{u}\notin\mathrm{Span}\,S$. Then
$S^{\prime}=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\right\}$ is linearly independent.
@end
@proof
@newcol
Let the relation of linear dependence of $S^{\prime}$ be
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}+\alpha\mathbf{u}=\mathbf{0}.
\end{align*}
@col
Suppose $\alpha\neq 0$, then
\begin{align*}
\displaystyle \mathbf{u}=-\frac{\alpha_{1}}{\alpha}\mathbf{v}_{1}-\cdots-\frac{\alpha_{k}}{\alpha}\mathbf{v}_{k}\in\mathrm{Span}\,S.
\end{align*}
@col
Contradiction.

@col
So $\alpha=0$, then
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\mathbf{0}.
\end{align*}
@col
By the linear independence of $S$, $\alpha_{i}=0$ for all $i$. Hence the above relation of dependence of $S^{\prime}$ is trivial.
@qed
@endcol
@end
@slide
@thm
@label{basisexists2}
Let $V$ be a nonzero (i.e. contains nonzero vectors) subspace of ${\mathbb{R}}^{m}$. (That is, $V \neq \{\vect{0}\}$.)

Then, there exists a basis for $V$.
@end
@proof
Consider all nonempty linearly independent subsets $S$ of vectors in $V$.
By @ref{cor:nvecsrmlindep}, the size of any such $S$ is an integer between $1$ and $m$.

@col
Let $n$ be the largest possible size of such sets, and let:
\[
B = \{\vect{v}_1, \vect{v}_2,\ldots, \vect{v}_n\}
\]  
be a nonempty linearly independent set of $V$ with size $n$.
We claim that $\mathrm{Span}\,B = V$:

@col
If not, then there exists $\vect{u} \in V$ which does not belong to $\mathrm{Span}\,B$,
and by @ref{lemma:outsidespanlinindep} the set:
\[
B \cup \vect{u} = \{\vect{v}_1, \vect{v}_2,\ldots, \vect{v}_n, \vect{u}\}
\]
is an linearly indepedent set of size $n + 1$, which contradicts the assumpion that
$n$ is the maximum size of linearly independent subsets in $V$.

@col
Hence, the linearly independent set $B$ spans $V$, 
and it follows that $B$ is a basis of $V$.
@qed
@end
<hr/>
Alternatively,
@proof
@of{basisexists2}
@newcol
Let $V$ be a nonzero vector space.
Let $\mathbf{v}_{1}$ be a nonzero vector in $V$. If $V=\mathrm{Span}\,\left\{\mathbf{v}_{1}\right\}$, we can take $S=\left\{\mathbf{v}_{1}\right\}$. Then obviously $\left\{\mathbf{v}_{1}\right\}$ is linearly independent and hence $S$ is a basis for $V$.

@col
Otherwise, let $\mathbf{v}_{2}\in V$ but not in $\mathrm{Span}\,\left\{\mathbf{v}_{1}\right\}$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$ is linearly independent. If $V=\mathrm{Span}\,\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$, we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$.

@col
So $S$ is a basis for $V$.

@col
Otherwise, let $\mathbf{v}_{3}\in V$ but not in $\mathrm{Span}\,\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$ is linearly independent. Repeat the above process, inductive we can define $\mathbf{v}_{k+1}$ as following: If $V=\mathrm{Span}\,\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$,
we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, $S$ is a basis for $V$.

@col
Otherwise defined $\mathbf{v}_{k+1}\not\in\mathrm{Span}\,\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k+1}\right\}$ is linearly independent.

@col
If the process stops, say at step $k$, i.e., $V=\mathrm{Span}\,\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
Then we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, it is a basis for $V$.

@col
This completes the proof.

@col
Otherwise, the process continues infinitely, in particular, we can take $k=m+1$ and
$V\neq\mathrm{Span}\,\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ and
$\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ is linearly independent.

@col
Since $\left\langle\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}\right\rangle = \mathbb{R}^m$,
by @ref{SSLD} the vectors $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ are linearly dependent. Contradiction.
@qed
@endcol
@end
@slide
@thm
Suppose a vector space $V$ has dimension $n$.
Then, any linearly independent set with $n$ vectors in $V$
is a basis for $V$.
@end
@thm
Suppose a vector space $V$ has dimension $n$.
Suppose $S$ is a set of $n$ vectors in $V$ which spans $V$
(That is, $\left\langle S \right\rangle = V$).

Then, $S$ is a basis for $V$.
@end
@section{Rank and nullity of a matrix}
@defn
@title{Nullity of a matrix}
@label{NOM}
Suppose that $A \in M_{mn}$. Then the @keyword{nullity} of $A$ is the dimension of the null space of $A$,
$\nullity{A}=\dim(\nsp{A})$.
@end

@defn
@title{Rank of a matrix}
@label{ROM}
Suppose that $A \in M_{mn}$. Then the @keyword{rank} of $A$ is the dimension of the column space of $A$,
$\rank{A}=\dim(\csp{A})$.
@end

@slide
@eg
<strong>Rank and nullity of a matrix</strong>

Let us compute the rank and nullity of
\[
A=\begin{bmatrix}
2 & -4 & -1 & 3 & 2 & 1 & -4\\
1 & -2 & 0 & 0 & 4 & 0 & 1\\
-2 & 4 & 1 & 0 & -5 & -4 & -8\\
1 & -2 & 1 & 1 & 6 & 1 & -3\\
2 & -4 & -1 & 1 & 4 & -2 & -1\\
-1 & 2 & 3 & -1 & 6 & 3 & -1
\end{bmatrix}
\]
@newcol
To do this, we will first row-reduce the matrix since that will help us determine bases for the null space and column space.
\[
\begin{bmatrix}
\leading{1} & -2 & 0 & 0 & 4 & 0 & 1\\
0 & 0 & \leading{1} & 0 & 3 & 0 & -2\\
0 & 0 & 0 & \leading{1} & -1 & 0 & -3\\
0 & 0 & 0 & 0 & 0 & \leading{1} & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
@col
From this row-equivalent matrix in reduced row-echelon form we record $D=\set{1,\,3,\,4,\,6}$ and $F=\set{2,\,5,\,7}$.

@col
By @ref{BCS}, for each index in $D$, we can create a single basis vector.
In fact $T=\{\vect{A}_1, \vect{A}_3, \vect{A}_4, \vect{A}_6\}$ is a basis for $\csp{A}$.
In total the basis will have $4$ vectors, so the column space of $A$ will have dimension $4$ and we write $\rank{A}=4$.

@col
By @ref{SSNS}, for each index in $F$, we can create a single basis vector.  In total the basis will have $3$ vectors, so the null space of $A$ will have dimension $3$ and we write $\nullity{A}=3$. In fact:

@col
\[
R = \left\{\colvector{ 2 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0},
\colvector{ -4 \\ 0 \\ -3 \\ 1 \\ 1 \\ 0 \\ 0},
\colvector{-1 \\ 0 \\ 2 \\ 3 \\ 0 \\ -1 \\ 1}
\right\}
\]
is a basis for $\nsp{A}$.
@endcol
@end

@slide
@thm
@title{Computing rank and nullity}
Suppose $A \in M_{mn}$ and $A \rref B$.
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).
Then $\rank{A}=r$ and $\nullity{A}=n-r$.
@end
@proof
@newcol
Let $D=\{d_1, \ldots, d_r\}$ be the indexes of the pivot columns of $B$.
By @ref{BCS}, $\{\vect{A}_{d_1}, \ldots, \vect{A}_{d_r}\}$ is a basis for $\csp{A}$.
So $\rank{A}=r$.

By @ref{SSNS}, each free variable corresponding to a single basis vector for the null space.
So $\nullity{A}$ is the number of free variables $=n-r$.
@endcol
@end

@cor
@title{Rank-Nullity Theorem}
@title{Dimension Formula}
Suppose $A\in M_{mn}$, then
\begin{align*}
\displaystyle r\left(A\right)+n\left(A\right)=n.
\end{align*}
@end
@slide
@thm
Let $A$ be a $m\times n$ matrix. Then
\begin{align*}
\displaystyle r\left(A\right)=r\left(A^{t}\right).
\end{align*}
@newcol
Equivalently
\begin{align*}
\displaystyle \dim\mathcal{C}\left(A\right)=\dim\mathcal{R}\!\left(A\right).
\end{align*}
@endcol
@end
@proof
@newcol
Let $A\xrightarrow{\text{RREF}}B$.

@col
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).

@col
Then by the above discussion $r=r\left(A\right)$. By @ref{BRS}, the first $r$ columns of $B^{t}$ form a basis for $\mathcal{R}\!\left(A\right) = \mathcal{C}(A^t)$.
Hence $r=r\left(A^{t}\right)$. This completes the proof.
@qed
@endcol
@end
Let us take a look at the rank and nullity of a square matrix.
@slide
@eg
The matrix
\begin{align*}
\displaystyle E=\begin{bmatrix}0&4&-1&2&2&3&1\\
2&-2&1&-1&0&-4&-3\\
-2&-3&9&-3&9&-1&9\\
-3&-4&9&4&-1&6&-2\\
-3&-4&6&-2&5&9&-4\\
9&-3&8&-2&-4&2&4\\
8&2&2&9&3&0&9\end{bmatrix}
\end{align*}
@newcol
is row-equivalent to the matrix in reduced row-echelon form,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&0&0&0&0&0\\
0&\boxed{1}&0&0&0&0&0\\
0&0&\boxed{1}&0&0&0&0\\
0&0&0&\boxed{1}&0&0&0\\
0&0&0&0&\boxed{1}&0&0\\
0&0&0&0&0&\boxed{1}&0\\
0&0&0&0&0&0&\boxed{1}\end{bmatrix}
\end{align*}
@col
With $n=7$ columns and $r=7$ nonzero rows tells us the rank is $r\left(E\right)=7$ and the nullity is $n\left(E\right)=7-7=0$.
@endcol
@end
The value of either the nullity or the rank are enough to characterize a nonsingular matrix.

@slide
@thm
@title{Rank and Nullity of a Nonsingular Matrix}
@label{RNNM}
Suppose that $A$ is a square matrix of size $n$. The following are equivalent. <ol class="ltx_enumerate">
<li class="ltx_item"> A is nonsingular. </li>
<li class="ltx_item"> The rank of $A$ is $n$, $r\left(A\right)=n$. </li>
<li class="ltx_item"> The nullity of $A$ is zero, $n\left(A\right)=0$. </li></ol>
@end
@proof
@skip
@newcol
(1 $\Rightarrow$ 2)
@newcol
If $A$ is nonsingular then $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$.

@col
If $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$, then the column space has dimension $n$, so the rank of $A$ is $n$.
@endcol

(2 $\Rightarrow$ 3)
@newcol
Suppose $r\left(A\right)=n$. Then the dimension formula gives
\begin{align*}
\displaystyle n\left(A\right)&\displaystyle=n-r\left(A\right) \\
&\displaystyle=n-n \\
&\displaystyle=0
\end{align*}
@endcol

(3 $\Rightarrow$ 1)
@newcol
Suppose $n\left(A\right)=0$, so a basis for the null space of $A$ is the empty set. This implies that ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$ and hence $A$ is nonsingular.
@endcol
@qed
@endcol
@end
With a new equivalence for a nonsingular matrix, we can update our list of equivalences which now becomes a list requiring double digits to number.
@slide
@thm
Suppose that $A$ is a square matrix of size $n$. The following are equivalent. <ol class="ltx_enumerate">
<li class="ltx_item"> $A$ is nonsingular. </li>
<li class="ltx_item"> $A$ row-reduces to the identity matrix. </li>
<li class="ltx_item"> The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$. </li>
<li class="ltx_item"> The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$. </li>
<li class="ltx_item"> The columns of $A$ are a linearly independent set. </li>
<li class="ltx_item"> $A$ is invertible. </li>
<li class="ltx_item"> The column space of $A$ is ${\mathbb{R}}^{n}$, $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$. </li>
<li class="ltx_item"> The columns of $A$ are a basis for ${\mathbb{R}}^{n}$. </li>
<li class="ltx_item"> The rank of $A$ is $n$, $r\left(A\right)=n$. </li>
<li class="ltx_item"> The nullity of $A$ is zero, $n\left(A\right)=0$. </li></ol>
@end
