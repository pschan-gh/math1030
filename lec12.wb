@course{Math 1030}
@setchapter{12}
@chapter{Linear Independence}
<h5 class="notkw">Reference.</h5><ul> 	
<li> 		Beezer, Ver 3.5 Section LI (print version p95 - p104) </li>
<li> 		Strang, Section 2.3 </li></ul><h5 class="notkw">Exercise.</h5><ul> 	
<li> 		Exercises with solutions can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
Section LI (p.40-48) (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions) C20-25, C30-33, C60, M20, M21, M50, M51, T10-13, T15, T20, T50. </li>
<li> 		Strang, Section 2.3 </li></ul>
@slide
@keyword{Linear independence} is one of the most fundamental conceptual ideas in linear algebra, along with the notion of <strong>span</strong>. So this lecture, and the subsequent one, will explore this new idea.
@section{Linearly Independent Sets of Vectors}
@label{LISV}
@defn
@title{Relation of Linear Dependence}
@label{RLDCV}
Given a set of vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$, an equality of the form
\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{0}
\end{align*}
is a @keyword{relation of linear dependence} on $S$. If this equality is formed in a trivial fashion, i.e., $\alpha_{i}=0$, $1\leq i\leq n$, then we say that it is the @keyword{trivial relation of linear dependence} on $S$.
@end
@defn
@title{Linear Independence}
@label{LICV}
@newcol
The set of vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ is @keyword{linearly dependent} if there is a relation of linear dependence on $S$ that is not trivial. In the case where the only relation of linear dependence on $S$ is the trivial one, then $S$ is a @keyword{linearly independent} set of vectors.
@endcol
@end
@remark
@newcol
In short, a set of vectors $\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ is linearly independent if and only if the only solution to:
\[
x_1 \mathbf{u}_1 + x_2 \mathbf{u}_2 + \cdots + x_n \mathbf{u}_n = \mathbf{0}
\]
is:
\[
x_1 = x_2 = \cdots = x_n = 0.
\]
@endcol
@end
@slide
@thm
@title{Linearly Independent Vectors and Homogeneous Systems}
@label{LIVHS}
Suppose that $S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\}\subseteq{\mathbb{R}}^{m}$ is a set of vectors and that $A$ is the $m\times n$ matrix whose columns are the vectors in $S$. Then $S$ is a linearly independent set if and only if the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution.
@end
@proof

($\Leftarrow$)
@newcol
Suppose that $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution. Since it is a homogeneous system, this solution must be the trivial solution, $\mathbf{x}=\mathbf{0}$. This means that the only relation of linear dependence on $S$ is the trivial one. So $S$ is linearly independent.
@endcol

($\Rightarrow$)
@newcol
We will prove the contrapositive. Suppose that $\mathcal{LS}({A},{\mathbf{0}})$ does not have a unique solution. Since it is a homogeneous system, it is consistent. And so must have infinitely many solutions. One of these infinitely many solutions must be nontrivial (in fact, almost all of them are); choose one. This nontrivial solution will give a nontrivial relation of linear dependence on $S$. We therefore conclude that $S$ is a linearly dependent set.
@endcol
@qed
@end
@slide
Since the above theorem is an "if-and-only-if" statement, we can use it to determine the linear independence or dependence of any set of column vectors, just by creating a matrix and analyzing its row-reduced echelon form. Let us illustrate this with two more examples.
@slide
@eg
@label{LDS}
@keyword{Linearly dependent set in ${\mathbb{R}}^{5}$}

Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$:
\begin{align*}
\displaystyle S=
\left\{\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}
\right\}
\end{align*}
@newcol
To determine linear independence, we first form an arbitrary relation of linear dependence,
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+\alpha_{2}\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+\alpha_{3}\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+\alpha_{4}\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}=\mathbf{0}
\end{align*}
We know that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$ is a solution to this equation, but that is of no interest whatsoever. That is always the case, no matter what four vectors we might have chosen. We are curious to know if there are other, nontrivial, solutions.

In other words, are there nontrivial solutions to the homogeneous linear system
$\mathcal{LS}(A, \mathbf{0})$, where the columns of $A$ consist of the vectors in $S$.

@col
Row-reducing the matrix $A$ gives:
\begin{align*}
\displaystyle A=\begin{bmatrix}2&1&2&-6\\
-1&2&1&7\\
3&-1&-3&-1\\
1&5&6&0\\
2&2&1&1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&-2\\
0&\boxed{1}&0&4\\
0&0&\boxed{1}&-3\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
We could solve the corresponding homogeneous system completely, but for this example all we need is one nontrivial solution. Setting the lone free variable to any nonzero value, such as $x_{4}=1$, yields the nontrivial solution:
\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}2\\
-4\\
3\\
1\end{bmatrix}.
\end{align*}
@col
Hence,
\begin{align*}
\displaystyle 2\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+(-4)\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+3\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+1\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}=\mathbf{0}.
\end{align*}
This is a relation of linear dependence on $S$ that is not trivial, so we conclude that $S$ is <strong>linearly dependent</strong>.
@endcol
@end
@eg
@label{LIS}
@newcol
@keyword{Linearly independent set in ${\mathbb{R}}^{5}$}

Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$:
\begin{align*}
\displaystyle T=
\left\{\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
7\\
-1\\
1\\
1\end{bmatrix}
\right\}.
\end{align*}
To determine linear independence we first form an arbitrary relation of linear dependence,
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+\alpha_{2}\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+\alpha_{3}\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+\alpha_{4}\begin{bmatrix}-6\\
7\\
-1\\
1\\
1\end{bmatrix}=\mathbf{0}.
\end{align*}
@col
We want to know if there are solutions to the equation above besides the trivial one: $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$.

Row-reducing the associated matrix gives:
\begin{align*}
\displaystyle B=\begin{bmatrix}2&1&2&-6\\
-1&2&1&7\\
3&-1&-3&-1\\
1&5&6&1\\
2&2&1&1\end{bmatrix}&\displaystyle\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&0\\
0&0&0&\boxed{1}\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
From the form of this matrix, we see that there are no free variables. Hence the associated homogeneous linear system has only the trivial solution. So we now know that there is but one way to combine the four vectors of $T$ into a relation of linear dependence, and that this one way is the easy and obvious way. Hence, the set $T$ is <strong>linearly independent</strong>.
@endcol
@end

@subsection{More Examples}
@slide
@skip
@eg
@label{LIHS}
@keyword{Linearly independent}

@newcol
Is the set of vectors:
\begin{align*}
\displaystyle S=
\left\{\begin{bmatrix}
2\\
-1\\
3\\
4\\
2
\end{bmatrix},
\,
\begin{bmatrix}
6\\
2\\
-1\\
3\\
4
\end{bmatrix},
\,
\begin{bmatrix}
4\\
3\\
-4\\
5\\
1
\end{bmatrix}
\right\}
\end{align*}
linearly independent or linearly dependent?
@endcol
@end
@sol
@newcol
The above theorem suggests that we study the matrix $A$ whose columns are the vectors in $S$. Specifically, we are interested in the size of the solution set of the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Row-reducing $A$ gives:

@col
\begin{align*}
\displaystyle A=\begin{bmatrix}2&6&4\\
-1&2&3\\
3&-1&-4\\
4&3&5\\
2&4&1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0\\
0&\boxed{1}&0\\
0&0&\boxed{1}\\
0&0&0\\
0&0&0\end{bmatrix}.
\end{align*}
@col
We have $r=3$, so there are $n-r=3-3=0$ free variables. Hence $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution. By the above theorem, the set $S$ is linearly independent.
@endcol
@end

@slide
@skip
@eg
@label{LDHS}
@keyword{Linearly dependent}

@newcol
Is the set of vectors:
\begin{align*}
\displaystyle S=
\left\{\begin{bmatrix}2\\
-1\\
3\\
4\\
2\end{bmatrix},\,\begin{bmatrix}6\\
2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}4\\
3\\
-4\\
-1\\
2\end{bmatrix}
\right\}
\end{align*}
linearly independent or linearly dependent?
@endcol
@end
@sol

@newcol
Theorem @ref{LIVHS} suggests that we study the matrix $A$ whose columns are the vectors in $S$. Specifically, we are interested in the size of the solution set of the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Row-reducing $A$ gives
\begin{align*}
\displaystyle A=\begin{bmatrix}2&6&4\\
-1&2&3\\
3&-1&-4\\
4&3&-1\\
2&4&2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&-1\\
0&\boxed{1}&1\\
0&0&0\\
0&0&0\\
0&0&0\end{bmatrix}.
\end{align*}
We have $r=2$, so there are $n-r=3-2=1$ free variables. Hence $\mathcal{LS}({A},{\mathbf{0}})$ has infinitely many solutions. By
Theorem @ref{LIVHS}, the set $S$ is linearly dependent.
@qed
@endcol
@end

@slide
@skip
Theorem @ref{LIVHS} gives us a straightforward way to determine if a set of vectors is linearly independent or dependent.

Review the previous two examples. They are very similar, differing only in the last two slots of the third vector. This resulted in slightly different matrices when row-reduced, and different values of $r$, the number of nonzero rows. Notice, too, that we are less interested in the actual solution set, and more interested in its form or size. These observations allow us to make a slight improvement on Theorem @ref{LIVHS}.

@section{Relation between Linear Independence and the Number of Pivot Columns}
@thm
@title{Linearly Independent Vectors, $r$ and $n$}
@label{LIVRN}
Suppose that
\begin{align*}
\displaystyle S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\}\subseteq{\mathbb{R}}^{m}
\end{align*}
is a set of vectors and that $A$ is the $m\times n$ matrix whose columns are the vectors in $S$. Let $B$ be a matrix in reduced row-echelon form that is row-equivalent to $A$ and let $r$ denote the number of pivot columns in $B$. Then $S$ is linearly independent if and only if $n=r$.
@end
@proof
@newcol
Theorem @ref{LIVHS} says the linear independence of $S$ is equivalent to the homogeneous linear system $\mathcal{LS}({A},{\mathbf{0}})$ having a unique solution. Since the zero vector is a solution of $\mathcal{LS}({A},{\mathbf{0}})$,
$\mathcal{LS}({A},{\mathbf{0}})$ is consistent. We can therefore can apply @ref{CSRN} to see that the solution is unique exactly when $n=r$.
@qed
@endcol
@end
@slide
Here is an example of the most straightforward way to determine if a set of column vectors is linearly independent or linearly dependent. While this method can be quick and easy, do not forget the logical progression from the definition of linear independence through homogeneous system of equations which makes it possible.
@eg
@keyword{Linear dependence, $r$ and $n$}

@newcol
Is the set of vectors:
\begin{align*}
\displaystyle S=
\left\{\begin{bmatrix}2\\
-1\\
3\\
1\\
0\\
3\end{bmatrix},\,\begin{bmatrix}9\\
-6\\
-2\\
3\\
2\\
1\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
0\\
0\\
1\end{bmatrix},\,\begin{bmatrix}-3\\
1\\
4\\
2\\
1\\
2\end{bmatrix},\,\begin{bmatrix}6\\
-2\\
1\\
4\\
3\\
2\end{bmatrix}\right\}
\end{align*}
linearly independent or linearly dependent?
@endcol
@end
@sol
@newcol
Theorem @ref{LIVRN} suggests that we take the vectors of $S$ as the columns of a matrix and then analyze its reduced row-echelon form:
\begin{align*}
\displaystyle \begin{bmatrix}2&9&1&-3&6\\
-1&-6&1&1&-2\\
3&-2&1&4&1\\
1&3&0&2&4\\
0&2&0&1&3\\
3&1&1&2&2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&-1\\
0&\boxed{1}&0&0&1\\
0&0&\boxed{1}&0&2\\
0&0&0&\boxed{1}&1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
Now we need only compute that
$r=4<5=n$
to recognize, via Theorem @ref{LIVRN}, that $S$ is a linearly dependent set. Boom!
@qed
@endcol
@end
@slide
@eg
@keyword{Large linearly dependent set in ${\mathbb{R}}^{4}$}

@newcol
Consider the set of $n=9$ vectors from ${\mathbb{R}}^{4}$,
\begin{align*}
\displaystyle R=\left\{\begin{bmatrix}-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}7\\
1\\
-3\\
6\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}0\\
4\\
2\\
9\end{bmatrix},\,\begin{bmatrix}5\\
-2\\
4\\
3\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-6\\
4\end{bmatrix},\,\begin{bmatrix}3\\
0\\
-3\\
1\end{bmatrix},\,\begin{bmatrix}1\\
1\\
5\\
3\end{bmatrix},\,\begin{bmatrix}-6\\
-1\\
1\\
1\end{bmatrix}\right\}.
\end{align*}
@col
To employ Theorem @ref{LIVHS}, we form a $4\times 9$ matrix $C$ whose columns are the vectors in $R$:
\begin{align*}
\displaystyle C=\begin{bmatrix}-1&7&1&0&5&2&3&1&-6\\
3&1&2&4&-2&1&0&1&-1\\
1&-3&-1&2&4&-6&-3&5&1\\
2&6&-2&9&3&4&1&3&1\end{bmatrix}.
\end{align*}
@col
To determine if the homogeneous system $\mathcal{LS}({C},{\mathbf{0}})$ has a unique solution or not, we would normally row-reduce this matrix. But in this particular example, we can do better:

@col
Since the system is homogeneous with $n=9$ variables in $m=4$ equations,
and $n > m$, there are infinitely many solutions. Since there is not a unique solution, Theorem @ref{LIVHS} says the set $R$ is linearly dependent.
@endcol
@end
@slide
The following theorem generalizes the previous example.
@thm
@title{More Vectors than Size implies Linear Dependence}
@label{MVSLD}
Suppose that $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}\subseteq{\mathbb{R}}^{m}$ and $n>m$. Then $S$ is a linearly dependent set.
@end
@proof
@newcol
Form the $m\times n$ matrix $A$ whose columns are $\mathbf{u}_{i}$, $1\leq i\leq n$. Consider the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. By @ref{HMVEI} this system has infinitely many solutions. Since the system does not have a unique solution, Theorem @ref{LIVHS} says the columns of $A$ form a linearly dependent set, as desired.
@qed
@endcol
@end
@section{Linear Independence and Nonsingular Matrices}
@label{LINM}
We will now specialize to sets of $n$ vectors in ${\mathbb{R}}^{n}$.
@eg
@keyword{Linearly dependent columns} Do the columns of the matrix
\begin{align*}
\displaystyle \begin{bmatrix}1&-1&2\\
2&1&1\\
1&1&0\end{bmatrix}
\end{align*}
form a linearly independent or dependent set?
@end
@sol
@newcol
We can show that $A$ is singular.
According to the definition of nonsingular matrices, the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$ has infinitely many solutions. So, by Theorem @ref{LIVHS}, the columns of $A$ form a linearly dependent set.
@qed
@endcol
@end
@eg
@keyword{Linearly independent columns}

@newcol
Do the columns of this matrix
\begin{align*}
\displaystyle B=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}
\end{align*}
form a linearly independent or dependent set?
@endcol
@end
@sol
@newcol
We can show that $B$ is nonsingular. According to the definition of nonsingular matrices, the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution. So, by Theorem @ref{LIVHS}, the columns of $B$ form a linearly independent set.
@qed
@endcol
@end
@slide
That the previous two examples have opposite properties for the columns of their coefficient matrices is no accident. Here is the theorem, and then we will update our equivalences for nonsingular matrices.
@thm
@title{Nonsingular Matrices have Linearly Independent Columns}
@label{NMLIC}
Suppose that $A$ is a <em style="">square</em> matrix. Then $A$ is nonsingular if and only if the columns of $A$ form a linearly independent set.
@end
@proof
@newcol
This is a proof where we can chain together equivalences, rather than proving the two halves separately.
\begin{align*}
A \text{ nonsingular} &\displaystyle\iff\text{$\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution} \\
&\displaystyle\iff A\vec{x} = \vec{0} \text{ has a unique solution }     \vec{x}\\
&\displaystyle\iff\text{columns of $A$ are linearly independent}
\end{align*}
@qed
@endcol
@end
@slide
Here is the update to @ref{NME1}
@thm
@title{Nonsingular Matrix Equivalences, Round 2}
@label{NME2}
Suppose that $A$ is a square matrix. The following are equivalent. <ol class="ltx_enumerate">
<li class="ltx_item">
@newcol
$A$ is nonsingular.
@endcol</li>
<li class="ltx_item">
@newcol
$A$ row-reduces to the identity matrix.
@endcol</li>
<li class="ltx_item">
@newcol
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
@endcol</li>
<li class="ltx_item">
@newcol
The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$.
@endcol</li>
<li class="ltx_item">
@newcol
The columns of $A$ form a linearly independent set.
@endcol</li></ol>
@end
@proof
@newcol
This follows directly from @ref{NMLIC} and @ref{NME2}.
@qed
@endcol
@end
@section{Null Spaces, Spans, Linear Independence}

In this section, we will find a linearly independent set that spans a null space.
Recall that, by @ref{SSNS},
there exists a particular set of $n-r$
vectors that could be used to span the null space of a matrix.

@eg
@label{LINSB}
@keyword{Linear independence of null space basis} Suppose that we are interested in the null space of a $3\times 7$ matrix $A$ which row-reduces to
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&-2&4&0&3&9\\
0&\boxed{1}&5&6&0&7&1\\
0&0&0&0&\boxed{1}&8&-5\end{bmatrix}.
\end{align*}
@newcol
Then $F=\{3,\,4,\,6,\,7\}$ is the set of indices for our four free variables that would be used in a description of the solution set for the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Applying @ref{HMVEI}, we can begin to construct a set of four vectors whose span is the null space of $A$, a set of vectors we will refer to as $T$.
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)
=\left< T\right>
&=\left< \{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\mathbf{z}_{4}\}\right>\\
&=\left< \left\{\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix}\right\}\right>
\end{align*}
@col
So far, we have constructed as much of these individual vectors as we can, based just on the knowledge of the contents of the set $F$. This has allowed us to determine the entries in slots 3, 4, 6 and 7, while we have left slots 1, 2 and 5 blank. Without doing any more, let us ask if $T$ is linearly independent? Begin with a relation of linear dependence on $T$, and see what we can learn about the scalars.

@col
\begin{align*}
\displaystyle\mathbf{0}&\displaystyle=\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\alpha_{4}\mathbf{z}_{4} \\
\displaystyle\begin{bmatrix}0\\
0\\
0\\
0\\
0\\
0\\
0\end{bmatrix}&\displaystyle=\alpha_{1}\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix}+\alpha_{2}\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix}+\alpha_{3}\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix}+\alpha_{4}\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix} \\
&\displaystyle=\begin{bmatrix}\\
\\
\alpha_{1}\\
0\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
\alpha_{2}\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
\alpha_{3}\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
\alpha_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
\alpha_{1}\\
\alpha_{2}\\
\\
\alpha_{3}\\
\alpha_{4}\end{bmatrix}
\end{align*}
@col
Applying the equalities of vectors, we see that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$. So the only relation of linear dependence on the set $T$ is the trivial one. By the definition of linear independence, the set $T$ is linearly independent. The important feature of this example is how the @keyword{pattern of zeros and ones} in the four vectors led to the conclusion of linear independence.
@endcol
@end
@slide
@thm
@title{Basis for Null Spaces}
@label{BNS}
Suppose that $A$ is an $m\times n$ matrix, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ and $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$ be the sets of column indices of $B$ which are and are not, respectively, pivot columns. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$ as
\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&\text{if $i\in F$, $i=f_{j}$}\\
0&\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}
(In fact $\mathbf{z}_{j}$ corresponding to the solution $x_{f_{j}}=1$ and $x_{f_{k}}=0$ for $k\neq j$.) Define the set $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$. Then <ol class="ltx_enumerate">
<li class="ltx_item"> ${\mathcal{N}}\!\left(A\right)=\left< S\right>$. </li>
<li class="ltx_item"> $S$ is a linearly independent set. </li></ol>
@end
@proof
@newcol
Study the above example. You can skip the proof for now. Notice first that the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$, are the same as the $n-r$ vectors defined in @ref{SSNS}. Also, the hypotheses of @ref{SSNS} are the same as the hypotheses of the theorem we are currently proving. So @ref{SSNS} tells us that ${\mathcal{N}}\!\left(A\right)=\left< S\right>$. That was the easy half, but the second part is not much harder. What is new here is the claim that $S$ is a linearly independent set.

To prove the linear independence of a set, we need to start with a relation of linear dependence and somehow conclude that the scalars involved must all be zero, i.e., that the relation of linear dependence is trivial. So, we start with and equation of the form
\begin{align*}
\displaystyle \alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}=\mathbf{0}.
\end{align*}
For each $j$, $1\leq j\leq n-r$, consider the equality of the individual entries of the vectors on both sides of this equality in position $f_{j}$:
\begin{align*}
\displaystyle 0&\displaystyle=\left[\mathbf{0}\right]_{f_{j}} \\
&\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}\right]_{f_{j}}+\left[\alpha_{2}\mathbf{z}_{2}\right]_{f_{j}}+\left[\alpha_{3}\mathbf{z}_{3}\right]_{f_{j}}+\cdots+\left[\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&\displaystyle=\alpha_{1}\left[\mathbf{z}_{1}\right]_{f_{j}}+\alpha_{2}\left[\mathbf{z}_{2}\right]_{f_{j}}+\alpha_{3}\left[\mathbf{z}_{3}\right]_{f_{j}}+\cdots+ \\
&\displaystyle\quad\quad\alpha_{j-1}\left[\mathbf{z}_{j-1}\right]_{f_{j}}+\alpha_{j}\left[\mathbf{z}_{j}\right]_{f_{j}}+\alpha_{j+1}\left[\mathbf{z}_{j+1}\right]_{f_{j}}+\cdots+ \\
&\displaystyle\quad\quad\alpha_{n-r}\left[\mathbf{z}_{n-r}\right]_{f_{j}} \\
&\displaystyle=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+ \\
&\displaystyle\quad\quad\alpha_{j-1}(0)+\alpha_{j}(1)+\alpha_{j+1}(0)+\cdots+\alpha_{n-r}(0)& \text{ definition of } \mathbf{z}_{j} \\
&\displaystyle=\alpha_{j}
\end{align*}
So for all $j$, $1\leq j\leq n-r$, we have $\alpha_{j}=0$. Hence, the only relation of linear dependence on $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$ is the trivial one. By the definition of linear independence, the set is linearly independent, as desired.
@qed
@endcol
@end
@slide
@eg
Find the null space of the matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}-2&-1&-2&-4&4\\
-6&-5&-4&-4&6\\
10&7&7&10&-13\\
-7&-5&-6&-9&10\\
-4&-3&-4&-6&6\\
\end{bmatrix}.
\end{align*}
@end
@sol
@newcol
The RREF of $A$ is:
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&0&1&-2\\
0&\boxed{1}&0&-2&2\\
0&0&\boxed{1}&2&-1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
The free variables are $x_{4}$ and $x_{5}$.

Setting $x_{4}=1$ and $x_{5}=0$ gives:

@col
\begin{align*}
\displaystyle \mathbf{z}_{1}=\begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix}.
\end{align*}
@col
Setting instead $x_{4}=0$ and $x_{5}=1$ gives
\begin{align*}
\displaystyle \mathbf{z}_{2}=\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}.
\end{align*}
Hence
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left< \begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix},\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}\right>.
\end{align*}
@qed
@endcol
@end