@course{MATH 1030}
@chapter{Basis}
<h5 class="notkw">Reference.</h5>
 Beezer, Ver 3.5 Section B (print version p233-238), Section D (print version p245-253)
@newline<h5 class="notkw">Exercise.</h5><ul>
<li> Exercises with solutions can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf} (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)
@newlineSection B p.88-92 C10, C11, C12, M20 Section D
p.92-96 C21, C23, C30, C31, C35, C36, C37, M20, M21. </li></ul>
@newline
@section{Column Spaces and Systems of Equations}
@slide
@defn
@title{Column Space of a Matrix}
@label{CSM}
 Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$. Then the @keyword{column space} of $A$, written $\mathcal{C}\left(A\right)$, is the subset of ${\mathbb{R}}^{m}$ containing all linear combinations of the columns of $A$,
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\left&lt; \left\{\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}\right\}\right&gt;
\end{align*} 
@end
@slide
@thm
@title{Column Spaces and Consistent Systems}
@label{CSCS}
 Suppose $A$ is an $m\times n$ matrix and $\mathbf{b}$ is a vector of size $m$.
Then $\mathbf{b}\in\mathcal{C}\left(A\right)$ if and only if $\mathcal{LS}({A},{\mathbf{b}})$ is consistent. 
@end
@proof
@newcol
 ($\Rightarrow$) Suppose $\mathbf{b}\in\mathcal{C}\left(A\right)$. Then we can write $\mathbf{b}$ as some linear combination of the columns of $A$. Then by @ref{RCLS} we can use the scalars from this linear combination to form a solution to $\mathcal{LS}({A},{\mathbf{b}})$, so this system is consistent.
@newline($\Leftarrow$) If $\mathcal{LS}({A},{\mathbf{b}})$ is consistent, there is a solution that may be used with @ref{RCLS} to write $\mathbf{b}$ as a linear combination of the columns of $A$. This qualifies $\mathbf{b}$ for membership in $\mathcal{C}\left(A\right)$. 
@qed
@endcol
@end
 This theorem tells us that asking if the system $\mathcal{LS}({A},{\mathbf{b}})$ is consistent is exactly the same question as asking if $\mathbf{b}$ is in the column space of $A$. Or equivalently, it tells us that the column space of the matrix $A$ is precisely those vectors of constants, $\mathbf{b}$, that can be paired with $A$ to create a system of linear equations $\mathcal{LS}({A},{\mathbf{b}})$ that is consistent.
@newline
@newcol
 We can form the chain of equivalences
\begin{align*}
\displaystyle\mathbf{b}\in\mathcal{C}\left(A\right)\iff\mathcal{LS}({A},{\mathbf{b}})\text{ is consistent}\iff A\mathbf{x}=\mathbf{b}\text{ for some }\mathbf{x}
\end{align*} 
@col
 Thus, an alternative (and popular) definition of the column space of an $m\times n$ matrix $A$ is
\begin{align*}
\displaystyle\mathcal{C}\left(A\right)&amp;\displaystyle=\left\{\left.\mathbf{y}\in{\mathbb{R}}^{m}\,\right|\,\mathbf{y}=A\mathbf{x}\text{ for some }\mathbf{x}\in{\mathbb{R}}^{n}\right\}=\left\{\left.A\mathbf{x}\,\right|\,\mathbf{x}\in{\mathbb{R}}^{n}\right\}\subseteq{\mathbb{R}}^{m}
\end{align*} 
@endcol
@slide
@eg
 Consider the column space of the $3\times 4$ matrix $A$,
\begin{align*}
\displaystyle A=\begin{bmatrix}3&amp;2&amp;1&amp;-4\\
-1&amp;1&amp;-2&amp;3\\
2&amp;-4&amp;6&amp;-8\end{bmatrix}
\end{align*} 
@newcol
 Show that $\mathbf{v}=\begin{bmatrix}18\\
-6\\
12\end{bmatrix}$ is in the column space of $A$, $\mathbf{v}\in\mathcal{C}\left(A\right)$.
The above theorem says that we need to check the consistency of $\mathcal{LS}({A},{v})$.
From the augmented matrix and row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}3&amp;2&amp;1&amp;-4&amp;18\\
-1&amp;1&amp;-2&amp;3&amp;-6\\
2&amp;-4&amp;6&amp;-8&amp;12\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;1&amp;-2&amp;6\\
0&amp;\boxed{1}&amp;-1&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*} 
@col
 Since the last column is not a pivot column, so the system is consistent and hence $v\in\mathcal{C}\left(A\right)$.
In fact, we have
\begin{align*}
\displaystyle \mathbf{v}=6\mathbf{A}_{1}.
\end{align*} 
@col
 Next we show that $\mathbf{w}=\begin{bmatrix}2\\
1\\
-3\end{bmatrix}$ is not in the column space of $A$, $\mathbf{w}\not\in\mathcal{C}\left(A\right)$.
The above theorem says that we need to check the consistency of $\mathcal{LS}({A},{v})$.
From the augmented matrix and row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}3&amp;2&amp;1&amp;-4&amp;2\\
-1&amp;1&amp;-2&amp;3&amp;1\\
2&amp;-4&amp;6&amp;-8&amp;-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;0&amp;1&amp;-2&amp;0\\
0&amp;\boxed{1}&amp;-1&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}\end{bmatrix}
\end{align*} 
@col
 Since the final column is a pivot column, the system is inconsistent and therefore $\mathbf{w}\not\in\mathcal{C}\left(A\right)$. 
@endcol
@end
 The next two examples illustrate the main idea of describing $\mathcal{C}\left(A\right)$. 
@slide
@eg
@keyword{Describe $\mathcal{C}\left(A\right)$ as a null space}
@newlineLet
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;7&amp;1&amp;-1\\
1&amp;1&amp;3&amp;1&amp;0\\
3&amp;2&amp;5&amp;-1&amp;9\\
1&amp;-1&amp;-5&amp;2&amp;0\end{bmatrix}.
\end{align*} 
@newcol
 Find $\mathcal{C}\left(A\right)$. Let’s determine if $\mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{4}\end{bmatrix}\in\left&lt; S\right&gt;$.
@newline
@col
 Applying Gauss-Jordan elimination to the augmented matrix
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;2&amp;7&amp;1&amp;-1&amp;v_{1}\\
1&amp;1&amp;3&amp;1&amp;0&amp;v_{2}\\
3&amp;2&amp;5&amp;-1&amp;9&amp;v_{3}\\
1&amp;-1&amp;-5&amp;2&amp;0&amp;v_{4}\end{bmatrix},
\end{align*} 
@col
 we obtain
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;0&amp;-1&amp;0&amp;3&amp;-3v_{1}+5v_{2}-v_{4}\\
0&amp;1&amp;4&amp;0&amp;-1&amp;v_{1}-v_{2}\\
0&amp;0&amp;0&amp;1&amp;-2&amp;2v_{1}-3v_{2}+v_{4}\\
0&amp;0&amp;0&amp;0&amp;0&amp;9v_{1}-16v_{2}+v_{3}+4v_{4}\\
\end{bmatrix}
\end{align*} 
@col
 If $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$, the above is a RREF. The last column is not a pivot columns. So $\mathbf{v}\in\left&lt; S\right&gt;$. If $9v_{1}-16v_{2}+v_{3}+4v_{4}\neq 0$, the equation corresponding to the last row is
\begin{align*}
\displaystyle 9v_{1}-16v_{2}+v_{3}+4v_{4}=0.
\end{align*} 
@col
 So the corresponding system of linear equations is inconsistent. So $\mathbf{v}\in\left&lt; S\right&gt;$. Hence $\mathbf{v}\in\left&lt; S\right&gt;$ if and only if $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$. Therefore
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)={\mathcal{N}}\!\left([9\,\,-16\,\,1\,\,4]\right).
\end{align*} 
@endcol
@end
@slide
@eg
@keyword{Describe $\mathcal{C}\left(A\right)$ by basis}
@newlineLet
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix},
\end{align*} 
@newcol
 find $\mathcal{C}\left(A\right)$.
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}B=\begin{bmatrix}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*} 
@col
 The indexes of the pivot columns are $D=\left\{1,3,4\right\}$.
Hence $\mathcal{C}\left(A\right)=\left&lt; A\right&gt;=\left&lt; \left\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\right\}\right&gt;$. 
@endcol
@end
@section{Basis}
@defn
 Let $V$ be a vector space. Then a subset $S$ of $V$ is said to be a @keyword{basis} for $V$ if <ol class="ltx_enumerate">
<li class="ltx_item"> $S$ is linearly independent. </li>
<li class="ltx_item"> $\left&lt; S\right&gt;=V$, i.e. $S$ spans $V$. </li></ol>
@end
@remark
@newcol
 Most of the time $V$ is a subspace of ${\mathbb{R}}^{m}$. Occasionally $V$ is assumed to be a subspace of $M_{mn}$ or $P_{n}$. It does not hurt to assume $V$ is a subspace of ${\mathbb{R}}^{m}$. 
@endcol
@end
@slide
@eg
 Let $V={\mathbb{R}}^{m}$, then $B=\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}$ is a basis for $V$.
(recall all the entries of $\mathbf{e}_{i}$ is zero, except the $i$-th entry being $1$).
@newline
@newcol
 It is called the @keyword{standard basis}: Obviously $B$ is linearly independent. Also, for any $\mathbf{v}\in V$, $\mathbf{v}=[\mathbf{v}]_{1}\mathbf{e}_{1}+\cdots+[\mathbf{v}]_{m}\mathbf{e}_{m}\in\left&lt; B\right&gt;$. So $\left&lt; B\right&gt;=V$. 
@endcol
@end
@slide
@skip
@eg
<strong>Math major only</strong>
@newline
@newcol
 Consider $V=M_{22}$. Let:
\begin{align*}
\displaystyle B_{11}&amp;=\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix},
&amp;
B_{12}&amp;=\begin{bmatrix}0&amp;1\\
0&amp;0\end{bmatrix},
\end{align*}
@newline\begin{align*}
\displaystyle B_{21}&amp;=\begin{bmatrix}0&amp;0\\
1&amp;0\end{bmatrix},
&amp;
B_{22}&amp;=\begin{bmatrix}0&amp;0\\
0&amp;1\end{bmatrix},
\end{align*} 
@col
 Then $B=\left\{B_{11},B_{12},B_{21},B_{22}\right\}$ is a basis for $V$.
@newline
@col
 Check: Obviously $B$ is linearly independent (exercise). Also for any $A\in V$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&amp;b\\
c&amp;d\end{bmatrix}=aB_{11}+bB_{12}+cB_{21}+dB_{22}.
\end{align*} 
@col
 So $\left&lt; B\right&gt;=M_{22}$. 
@endcol
@end
@newline
@ex
<strong>Math major only</strong>
@newline
@newcol
 Let $V=M_{mn}$.
@newlineFor $1\leq i\leq m$, $1\leq j\leq n$,
let $B_{ij}$ be the $m\times n$ matrix with $(i,j)$-th entry equal to $1$ and all other entries equal to $0$.
@newlineThen $\left\{ B_{ij} | 1\leq i \leq m, 1\leq j\leq n\right\}$ is a basis for $V$. 
@endcol
@end
@newline
@eg
<strong>Math major only</strong>
@newline
@newcol
 Let $V=P_{n}$. Then $1,x,x^{2},\ldots,x^{n}$ is a basis. It is easy to show that $S=\left\{1,x,x^{2},\ldots,x^{n}\right\}$ is linearly independent. Also any polynomial
\begin{align*}
\displaystyle f(x)=a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}
\end{align*} 
@col
 is a linear combinations of $S$.
@newline
@endcol
@end
@slide
@eg
 A vector space can have different bases.
@newlineConsider the vector space $V={\mathbb{R}}^{2}$.
@newlineThen,
\[
S=\left\{\mathbf{e}_{1},\mathbf{e}_{2}\right\}
\]
is a basis for $V$, and:
@newline
@newcol
 \[
S^{\prime}=\left\{\begin{bmatrix}1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\end{bmatrix}\right\}
\]
is also a basis. 
@endcol
@end
@section{
@newlineColumn Space Spanned by Original Columns}
@label{CSSOC}
 So we have a foolproof, automated procedure for determining membership in $\mathcal{C}\left(A\right)$. While this works just fine a vector at a time, we would like to have a more useful description of the set $\mathcal{C}\left(A\right)$ as a whole. The next example will preview the first of two fundamental results about the column space of a matrix. 
@slide
@eg
 Consider the $5\times 7$ matrix $A$,
\begin{align*}
\displaystyle \begin{bmatrix}2&amp;4&amp;1&amp;-1&amp;1&amp;4&amp;4\\
1&amp;2&amp;1&amp;0&amp;2&amp;4&amp;7\\
0&amp;0&amp;1&amp;4&amp;1&amp;8&amp;7\\
1&amp;2&amp;-1&amp;2&amp;1&amp;9&amp;6\\
-2&amp;-4&amp;1&amp;3&amp;-1&amp;-2&amp;-2\end{bmatrix}
\end{align*} 
@newcol
 The column space of $A$ is
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\left&lt; \left\{\begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
2\\
0\\
2\\
-4\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix},\,\begin{bmatrix}4\\
4\\
8\\
9\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
7\\
7\\
6\\
-2\end{bmatrix}\right\}\right&gt;
\end{align*} 
@col
 While this is a concise description of an infinite set, we might be able to describe the span with fewer than seven vectors. Now we row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}2&amp;4&amp;1&amp;-1&amp;1&amp;4&amp;4\\
1&amp;2&amp;1&amp;0&amp;2&amp;4&amp;7\\
0&amp;0&amp;1&amp;4&amp;1&amp;8&amp;7\\
1&amp;2&amp;-1&amp;2&amp;1&amp;9&amp;6\\
-2&amp;-4&amp;1&amp;3&amp;-1&amp;-2&amp;-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;2&amp;0&amp;0&amp;0&amp;3&amp;1\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;-1&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;2&amp;1\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;1&amp;3\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*} 
@col
 The pivot columns are $D=\left\{1,\,3,\,4,\,5\right\}$, so we can create the set
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix}\right\}
\end{align*} 
@col
 and know that $\mathcal{C}\left(A\right)=\left&lt; T\right&gt;$ and $T$ is a linearly independent set of columns from the set of columns of $A$. 
@endcol
@end
@slide
 The following theorem is a direct consequence of @ref{BS}: 
@thm
@title{Basis of the Column Space}
@label{BCS}
@newcol
 Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\left\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\right\}$ be the set of indices for the pivot columns of $B$.
Let $T=\left\{\mathbf{A}_{d_{1}},\,\mathbf{A}_{d_{2}},\,\mathbf{A}_{d_{3}},\,\ldots,\,\mathbf{A}_{d_{r}}\right\}$. Then <ol class="ltx_enumerate">
<li class="ltx_item"> $T$ is a linearly independent set. </li>
<li class="ltx_item"> $\mathcal{C}\left(A\right)=\left&lt; T\right&gt;$. </li></ol>
@endcol
@end
@section{Bases for spans of column vectors}
@newline
Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}$ be a subset of ${\mathbb{R}}^{m}$
Recall from lecture 14,
that there are several methods to find a subset
$T\subseteq \{ S \}$.
@newline
@newcol
 Such that (i) $T$ is linearly independent (ii) $\left&lt; T\right&gt;=\left&lt; S\right&gt;$. (In other words $T$ is a basis for $\left&lt; S\right&gt;$.)
@newline<strong style="">Method 1</strong>
@newcol
 Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]\xrightarrow{\text{RREF}}B$.
@newline
@col
 Suppose $D=\left\{d_{1},\ldots,d_{r}\right\}$ be the indexes of the pivot columns of $B$.
@newline
@col
 Let $T=\left\{\mathbf{v}_{d_{1}},\ldots,\mathbf{v}_{d_{r}}\right\}$. Then $T$ is a basis for $\left&lt; S\right&gt;=\mathcal{C}\left(A\right)$ 
@endcol
@newline<strong style="">Method 2</strong>
@newcol
 Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]$. Suppose $A^{t}\xrightarrow{\text{RREF}}B$.
@newline
@col
 Let $T$ be the nonzero columns of $B^{t}$. Then $T$ is a basis for $\left&lt; S\right&gt;=\mathcal{C}\left(A\right)$
@newlineThis is an example from Lecture 14. 
@endcol
@endcol
@slide
@eg
@keyword{Column space from row operations}
@newlineLet
\begin{align*}
\displaystyle S=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\mathbf{v}_{6}=\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\mathbf{v}_{7}=\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\right\}.
\end{align*} 
@newcol
 Find a basis for $\left&lt; S\right&gt;$.
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{7}]=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix}.
\end{align*}
@newline<strong>Method 1</strong>
@newcol
 \begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*} 
@col
 Let
\begin{align*}
\displaystyle T=\left\{\mathbf{v}_{1},\mathbf{v}_{3},\mathbf{v}_{4}\right\}=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\right\}.
\end{align*} 
@col
 Then $T$ is a basis for $\left&lt; S\right&gt;=\mathcal{C}\left(A\right)$. 
@endcol
@newline<strong>Method 2</strong>
@newcol
 The transpose of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;2&amp;0&amp;-1\\
4&amp;8&amp;0&amp;-4\\
0&amp;-1&amp;2&amp;2\\
-1&amp;3&amp;-3&amp;4\\
0&amp;9&amp;-4&amp;8\\
7&amp;-13&amp;12&amp;-31\\
-9&amp;7&amp;-8&amp;37\end{bmatrix}.
\end{align*} 
@col
 Row-reduced this becomes,
\begin{align*}
\displaystyle D=\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-\frac{31}{7}\\
0&amp;\boxed{1}&amp;0&amp;\frac{12}{7}\\
0&amp;0&amp;\boxed{1}&amp;\frac{13}{7}\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*} 
@col
 Then we can take
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\right\}.
\end{align*}
@newline$T$ is a basis for $\mathcal{C}\left(A\right)=\left&lt; S\right&gt;$. 
@endcol
@endcol
@end
@slide
@thm
@label{basisexists}
 Let $S$ be a finite subset of ${\mathbb{R}}^{m}$.
Then, a basis for $\left&lt; S\right&gt;$ exists.
@newline
@newcol
 In fact, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left&lt; S\right&gt;$ (see @ref{BCS}). 
@endcol
@end
@section{
@newlineRow Space of a Matrix}
&lt;!--@slide@defn
Let $n$ be a positive integer.
A @keyword{row vector} or size $n$ is an array of real numbers of the form:
\[
\left(v_1, v_2, \ldots, v_n\right),\quad v_i \in \mathbb{R}.
\]
In the context of this course, it is perfectly reasonable to view a row vector
as simply a $1 \times n$ matrix.

@col
In fact,
the standard laws of addition and scalar multiplication on row vectors
of size $n$ are those defined for $1 \times n$ matrices (see @ref{MEASM}).
That is:
\begin{multline*}
\left(v_1 , v_2 , \ldots , v_n\right) + \left(w_1, w_2, \ldots, w_n\right)
= \left(v_1 + w_1, v_2 + w_2, \ldots, v_n + w_n\right),\\
 \quad v_i, w_i \in \mathbb{R};
\end{multline*}
\[
\alpha\left(v_1, v_2, \ldots, v_n\right)
=
\left(\alpha v_1, \alpha v_2, \ldots, \alpha v_n\right), \quad v_i, \alpha \in \mathbb{R}.
\]
@col
For any fixed positive integer $n$,
by @ref{fb1ac45c60c8c59135e7173658dededc} the set of all row vectors of size $n$
is a vector space under the laws of addition and scalar multiplication defined above.
@end@slide--&gt;

@defn
@title{Row Space of a Matrix}
 Suppose $A$ is an $m\times n$ matrix.
The @keyword{row space} of $A$,
$\mathcal{R}\!\left(A\right)$ is column space $\mathcal{C}\left(A^t\right)$ of $A^t$. 
@end
@newline
Informally, the row space is the set of all linear combinations of the rows of $A$. However, we write the rows as column vectors, thus the necessity of using the transpose to make the rows into columns. Additionally, with the row space defined in terms of the column space, all of the previous results of this section can be applied to row spaces.
@newline
@newcol
 Notice that if $A$ is a rectangular $m\times n$ matrix, then $\mathcal{C}\left(A\right)\subseteq{\mathbb{R}}^{m}$, while $\mathcal{R}\!\left(A\right)\subseteq{\mathbb{R}}^{n}$ and the two sets are not comparable since they do not even hold objects of the same type. However, when $A$ is square of size $n$, both $\mathcal{C}\left(A\right)$ and $\mathcal{R}\!\left(A\right)$ are subsets of ${\mathbb{R}}^{n}$, though usually the sets will not be equal. 
@endcol
@slide
@eg
 Find $\mathcal{R}\!\left(A\right)$ for
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;4&amp;0&amp;-1&amp;0&amp;7&amp;-9\\
2&amp;8&amp;-1&amp;3&amp;9&amp;-13&amp;7\\
0&amp;0&amp;2&amp;-3&amp;-4&amp;12&amp;-8\\
-1&amp;-4&amp;2&amp;4&amp;8&amp;-31&amp;37\end{bmatrix}.
\end{align*} 
@newcol
 To build the row space, we transpose the matrix,
\begin{align*}
\displaystyle A^{t}=\begin{bmatrix}1&amp;2&amp;0&amp;-1\\
4&amp;8&amp;0&amp;-4\\
0&amp;-1&amp;2&amp;2\\
-1&amp;3&amp;-3&amp;4\\
0&amp;9&amp;-4&amp;8\\
7&amp;-13&amp;12&amp;-31\\
-9&amp;7&amp;-8&amp;37\end{bmatrix}
\end{align*} 
@col
 Then the columns of this matrix are used in a span to build the row space,
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\left&lt; \left\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix},\,\begin{bmatrix}-1\\
-4\\
2\\
4\\
8\\
-31\\
37\end{bmatrix}\right\}\right&gt;.
\end{align*} 
@col
 First, row-reduce $A^{t}$,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-\frac{31}{7}\\
0&amp;\boxed{1}&amp;0&amp;\frac{12}{7}\\
0&amp;0&amp;\boxed{1}&amp;\frac{13}{7}\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*} 
@col
 Since the pivot columns have indices $D=\left\{1,\,2,\,3\right\}$, the column space of $A^{t}$ can be spanned by just the first three columns of $A^{t}$,
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\left&lt; \left\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix}\right\}\right&gt;.
\end{align*} 
@endcol
@end
@slide
@thm
@title{Row-Equivalent Matrices have Equal Row Spaces}
@label{REMRS}
 Suppose $A$ and $B$ are row-equivalent matrices. Then $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$. 
@end
@proof
@newcol
 Observe that if $B$ is obtained from $A$ via a row operation of the type
$R_i \leftrightarrow R_j$,
then the rows of $B$ are the same as the rows of $A$,
and hence the columns of $B^t$ are still the same as the columns of $A^t$,
only with the order changed.  Hence,
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) = \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\]
@newline
@col
 If $B$ is obtained from $A$ via a row operation of the type $\alpha R_i$ ($\alpha \neq 0$),
then the $i$-th column of $B^t$ is equal to $\alpha$ times the $i$-th column of $A^t$,
and the other columns remain the same as those of $A^t$ with the corresponding indices.
@newlineIn paricular, the $i$-th column of $B^t$ is a linear combination of the columns of $A^t$.
@newline
@col
 Hence, the columns of $B^t$ all lie in $\mathcal{C}(A^t)$, which in turn implies that:
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) \subseteq \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\] 
@col
 On the other hand, if $B$ is obtained from $A$ via $\alpha R_i$, then $A$ is obtained
from $B$ via $\left(\frac{1}{\alpha}\right) R_i$.  So, by the same argument as before we have:
\[
\mathcal{R}(A) = \mathcal{C}\left(A^t\right) \subseteq \mathcal{C}\left(B^t\right) = \mathcal{R}(B).
\]
Hence, $\mathcal{R}(B) = \mathcal{R}(A)$.
@newline
@col
 If $B$ is obtained from $A$ via a row operation of the type $\alpha R_i + R_j$,
then:
\[
\left[B^t\right]_j = \alpha\left[A^t\right]_i + \left[A^t\right]_j,
\]
and the other columns of $B^t$ remain the same as those of $A^t$
with the corresponding indices.
@newlineIn paricular, the $i$-th column of $B^t$ is a linear combination of the columns of $A^t$.
@newline
@col
 Hence, the columns of $B^t$ all lie in $\mathcal{C}(A^t)$, which in turn implies that:
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) \subseteq \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\] 
@col
 On the other hand, if $B$ is obtained from $A$ via $\alpha R_i + R_j$, then $A$ is obtained
from $B$ via $(-\alpha) R_i + R_j$.  So, by the same argument as before we have:
\[
\mathcal{R}(A) = \mathcal{C}\left(A^t\right) \subseteq \mathcal{C}(B^t) = \mathcal{R}(B).
\]
Hence, $\mathcal{R}(B) = \mathcal{R}(A)$.
@newline
@col
 We now see that the row space of a matrix remains unchanged after any application
of a row operation.
@newlineHence, $\mathcal{R}(B) = \mathcal{R}(A)$ if $B$ is row-equivalent to $A$,
since by the definition of row-equivalence (@ref{dacc247eacab6b0b430f64f7de449ebe}) $B$ is obtained by $A$ via a series
of row operations. 
@endcol
@end
&lt;!--@proof@col
Two matrices are row-equivalent if one can be obtained from another by a sequence of (possibly many) row operations. We will prove the theorem for two matrices that differ by a single row operation, and then this result can be applied repeatedly to get the full statement of the theorem. The row spaces of $A$ and $B$ are spans of the columns of their transposes. For each row operation we perform on a matrix, we can define an analogous operation on the columns. Perhaps we should call these @keyword{column operations}. Instead, we will still call them row operations, but we will apply them to the columns of the transposes.

@col
Refer to the columns of $A^{t}$ and $B^{t}$ as $\mathbf{A}_{i}$ and $\mathbf{B}_{i}$, $1\leq i\leq m$. The row operation that switches rows will just switch columns of the transposed matrices. This will have no effect on the possible linear combinations formed by the columns.

@col
Suppose that $B^{t}$ is formed from $A^{t}$ by multiplying column $\mathbf{A}_{t}$ by $\alpha\neq 0$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for all $i\neq t$. We need to establish that two sets are equal, $\mathcal{C}\left(A^{t}\right)=\mathcal{C}\left(B^{t}\right)$. We will take a generic element of one and show that it is contained in the other.
\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&amp;\displaystyle\beta_{2}\mathbf{B}_{2}+\beta_{3}\mathbf{B}_{3}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\left(\alpha\beta_{t}\right)\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(B^{t}\right)\subseteq\mathcal{C}\left(A^{t}\right)$. Similarly,
\begin{align*}
\displaystyle\gamma_{1}\mathbf{A}_{1}+&amp;\displaystyle\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\left(\frac{\gamma_{t}}{\alpha}\alpha\right)\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\gamma_{3}\mathbf{B}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(A^{t}\right)\subseteq\mathcal{C}\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\mathcal{C}\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the second type is performed.

@col
Suppose now that $B^{t}$ is formed from $A^{t}$ by replacing $\mathbf{A}_{t}$ with $\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$ for some $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $s\neq t$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for $i\neq t$.
\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&amp;\displaystyle\beta_{2}\mathbf{B}_{2}+\cdots+\beta_{s}\mathbf{B}_{s}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\left(\beta_{s}+\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(B^{t}\right)\subseteq\mathcal{C}\left(A^{t}\right)$. Similarly,
\begin{align*}
\displaystyle\gamma_{1}&amp;\displaystyle\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\left(-\alpha\gamma_{t}\mathbf{A}_{s}+\alpha\gamma_{t}\mathbf{A}_{s}\right)+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{A}_{s}+\cdots+\gamma_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&amp;\displaystyle=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{B}_{s}+\cdots+\gamma_{t}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}
@col
says that $\mathcal{C}\left(A^{t}\right)\subseteq\mathcal{C}\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\left(A^{t}\right)=\mathcal{C}\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the third type is performed.

@col
So the row space of a matrix is preserved by each row operation, and hence row spaces of row-equivalent matrices are equal sets.
@qed@end--&gt;

@slide
@eg
@keyword{Row spaces of two row-equivalent matrices}
@newlineThe matrices
\begin{align*}
\displaystyle A&amp;\displaystyle=\begin{bmatrix}2&amp;-1&amp;3&amp;4\\
5&amp;2&amp;-2&amp;3\\
1&amp;1&amp;0&amp;6\end{bmatrix}&amp;\displaystyle B&amp;\displaystyle=\begin{bmatrix}1&amp;1&amp;0&amp;6\\
3&amp;0&amp;-2&amp;-9\\
2&amp;-1&amp;3&amp;4\end{bmatrix}
\end{align*} 
@newcol
 are row-equivalent via a sequence of two row operations.
@newlineHence by the above theorem
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\left&lt; \left\{\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}5\\
2\\
-2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix}\right\}\right&gt;=\left&lt; \left\{\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix},\,\begin{bmatrix}3\\
0\\
-2\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix}\right\}\right&gt;=\mathcal{R}\!\left(B\right)
\end{align*} 
@endcol
@end
@slide
@thm
@title{Basis for the Row Space}
@label{BRS}
 Suppose that $A$ is a matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Let $S$ be the set of nonzero columns of $B^{t}$. Then <ol class="ltx_enumerate">
<li class="ltx_item"> $\mathcal{R}\!\left(A\right)=\left&lt; S\right&gt;$. </li>
<li class="ltx_item"> $S$ is a linearly independent set. </li></ol>
@end
@proof
@newcol
 From Theorem    @ref{REMRS}. we know that $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$. If $B$ has any zero rows, these are columns of $B^{t}$ that are the zero vector. We can safely toss out the zero vector in the span construction, since it can be recreated from the nonzero vectors by a linear combination where all the scalars are zero. So $\mathcal{R}\!\left(A\right)=\left&lt; S\right&gt;$.
@newline
@col
 Suppose $B$ has $r$ nonzero rows and let $D=\left\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\right\}$ denote the indices of the pivot columns of $B$. Denote the $r$ column vectors of $B^{t}$, the vectors in $S$, as $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{r}$. To show that $S$ is linearly independent, start with a relation of linear dependence
\begin{align*}
\displaystyle \alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}=\mathbf{0}
\end{align*} 
@col
 Now consider this vector equality in location $d_{i}$. Since $B$ is in reduced row-echelon form, the entries of column $d_{i}$ of $B$ are all zero, except for a leading 1 in row $i$. Thus, in $B^{t}$, row $d_{i}$ is all zeros, excepting a 1 in column $i$. So, for $1\leq i\leq r$,
\begin{align*}
\displaystyle 0&amp;\displaystyle=\left[\mathbf{0}\right]_{d_{i}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{B}_{1}\right]_{d_{i}}+\left[\alpha_{2}\mathbf{B}_{2}\right]_{d_{i}}+\left[\alpha_{3}\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\left[\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&amp;\displaystyle=\alpha_{1}\left[\mathbf{B}_{1}\right]_{d_{i}}+\alpha_{2}\left[\mathbf{B}_{2}\right]_{d_{i}}+\alpha_{3}\left[\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\alpha_{r}\left[\mathbf{B}_{r}\right]_{d_{i}} \\
&amp;\displaystyle=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+\alpha_{i}(1)+\cdots+\alpha_{r}(0) \\
&amp;\displaystyle=\alpha_{i}
\end{align*} 
@col
 So we conclude that $\alpha_{i}=0$ for all $1\leq i\leq r$, establishing the linear independence of $S$. 
@qed
@endcol
@end
@slide
@eg
@keyword{Improving a span}
@newlineSuppose in the course of analyzing a matrix (its column space, its null space, its ...) we encounter the following set of vectors, described by a span
\begin{align*}
\displaystyle X=\left&lt; \left\{\begin{bmatrix}1\\
2\\
1\\
6\\
6\end{bmatrix},\,\begin{bmatrix}3\\
-1\\
2\\
-1\\
6\end{bmatrix},\,\begin{bmatrix}1\\
-1\\
0\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}-3\\
2\\
-3\\
6\\
-10\end{bmatrix}\right\}\right&gt;
\end{align*} 
@newcol
 Let $A$ be the matrix whose rows are the vectors in $X$, so by design $X=\mathcal{R}\!\left(A\right)$,
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;2&amp;1&amp;6&amp;6\\
3&amp;-1&amp;2&amp;-1&amp;6\\
1&amp;-1&amp;0&amp;-1&amp;-2\\
-3&amp;2&amp;-3&amp;6&amp;-10\end{bmatrix}
\end{align*} 
@col
 Row-reduce $A$ to form a row-equivalent matrix in reduced row-echelon form,
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;2&amp;-1\\
0&amp;\boxed{1}&amp;0&amp;3&amp;1\\
0&amp;0&amp;\boxed{1}&amp;-2&amp;5\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*} 
@col
 Then the above theorem says we can grab the nonzero columns of $B^{t}$ and write
\begin{align*}
\displaystyle X=\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)=\left&lt; \left\{\begin{bmatrix}1\\
0\\
0\\
2\\
-1\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
3\\
1\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
-2\\
5\end{bmatrix}\right\}\right&gt;
\end{align*} 
@col
 These three vectors provide a much-improved description of $X$. There are fewer vectors, and the pattern of zeros and ones in the first three entries makes it easier to determine membership in $X$. 
@endcol
@end
@slide
@thm
@title{Column Space, Row Space, Transpose}
@label{CSRST}
 Suppose $A$ is a matrix. Then $\mathcal{C}\left(A\right)=\mathcal{R}\!\left(A^{t}\right)$. 
@end
@proof
@newcol
 \begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\mathcal{C}\left(\left(A^{t}\right)^{t}\right)=\mathcal{R}\!\left(A^{t}\right)
\end{align*} 
@qed
@endcol
@end
@slide
@eg
@keyword{Column space from row operations}
@newlineFind the column space of $A$ in @ref{cb807061818215f8c700c6df6edc1229}.
@newline<strong>Method 1</strong>
@newcol
 \begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&amp;4&amp;0&amp;0&amp;2&amp;1&amp;-3\\
0&amp;0&amp;\boxed{1}&amp;0&amp;1&amp;-3&amp;5\\
0&amp;0&amp;0&amp;\boxed{1}&amp;2&amp;-6&amp;6\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}
\end{align*} 
@col
 Let
\begin{align*}
\displaystyle T=\left\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\right\}=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\right\}.
\end{align*} 
@col
 Then $T$ is linear independent and $\mathcal{C}\left(A\right)=\left&lt; T\right&gt;$. 
@endcol
@newline<strong>Method 2</strong>
@newcol
 The transpose of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&amp;2&amp;0&amp;-1\\
4&amp;8&amp;0&amp;-4\\
0&amp;-1&amp;2&amp;2\\
-1&amp;3&amp;-3&amp;4\\
0&amp;9&amp;-4&amp;8\\
7&amp;-13&amp;12&amp;-31\\
-9&amp;7&amp;-8&amp;37\end{bmatrix}.
\end{align*} 
@col
 Row-reduced this becomes,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;-\frac{31}{7}\\
0&amp;\boxed{1}&amp;0&amp;\frac{12}{7}\\
0&amp;0&amp;\boxed{1}&amp;\frac{13}{7}\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*} 
@col
 Now, using Theorem    @ref{CSRST} and Theorem    @ref{BRS},
\begin{align*}
\displaystyle \mathcal{C}\left(A\right)=\mathcal{R}\!\left(A^{t}\right)=\left&lt; \left\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\right\}\right&gt;.
\end{align*} 
@col
 This is a very nice description of the column space. Fewer vectors than the 7 involved in the definition, and the pattern of the zeros and ones in the first 3 slots can be used to advantage. For example,
let’s check if
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}
\end{align*}
is in $\mathcal{C}\left(A\right)$ or not.
@newline
@col
 If it is, then
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=x\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+y\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+z\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}=\begin{bmatrix}x\\
y\\
z\\
-\frac{31}{7}x+\frac{12}{7}y+\frac{13}{7}z\end{bmatrix}.
\end{align*} 
@col
 From the first three coordinate $x=3,y=9,z=1$. Let’s check the last coordinate:
\begin{align*}
\displaystyle -\frac{31}{7}\times 3+\frac{12}{7}\times 9+\frac{13}{7}\times 1=4.
\end{align*} 
@col
 So
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=3\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+9\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+1\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}
\end{align*} 
@col
 and hence $\mathbf{b}\in\mathcal{C}\left(A\right)$. 
@endcol
@end
@newline
@remark
@newcol
 Both methods describe algorithms to find bases
(i.e., linear independent set the generate the column space)
for the column space.
Here are the differences. <ol class="ltx_enumerate">
<li class="ltx_item"> In method 1, we find a subset of columns that forms a basis.
However in method 2, the basis is not a subset of columns. </li>
<li class="ltx_item"> Given a vector $\mathbf{b}\in\mathcal{C}\left(A\right)$, it is easier to express it as a linear combination of the basis given by method 2. </li></ol>
@endcol
@end
@section{Spanning Sets of Null Spaces}
 Recall @ref{thm:nullspacesubspace}
&lt;!-- The following theorem describe how to express a null space as $\left&lt; S \right&gt;$. --&gt;

@thm
@label{SSNS}
@newcol
 Spanning
Sets for Null Spaces
Suppose that $A$ is an $m\times n$ matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Suppose that $B$ has $r$ pivot columns, with indices given by $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$, while the $n-r$ non-pivot columns have indices $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$,
\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&amp;\text{if $i\in F$, $i=f_{j}$}\\
0&amp;\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&amp;\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}
Then the null space of $A$ is given by
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left&lt;\left\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\right\}\right&gt;
\end{align*} 
@endcol
@end
@proof
@newcol
 The can be seen by moving the free variables to another side. For details. See Beezer p88. Don’t memorize this theorem. Instead, study the examples below. 
@qed
@endcol
@end
@slide
@eg
@keyword{Spanning set of a null space}
@newlineFind a set of vectors, $S$, so that the null space of the matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}1&amp;3&amp;3&amp;-1&amp;-5\\
2&amp;5&amp;7&amp;1&amp;1\\
1&amp;1&amp;5&amp;1&amp;5\\
-1&amp;-4&amp;-2&amp;0&amp;4\end{bmatrix}
\end{align*}
is the span of $S$, that is, $\left&lt; S \right&gt;={\mathcal{N}}\!\left(A\right)$. 
<hr/>
@newcol
 The null space of $A$ is the set of all solutions to the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$.
Begin by row-reducing $A$. The result is
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;6&amp;0&amp;4\\
0&amp;\boxed{1}&amp;-1&amp;0&amp;-2\\
0&amp;0&amp;0&amp;\boxed{1}&amp;3\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}
We have $D=\{1,\,2,\,4\}$ and $F=\{3,\,5\}$. Hence $x_{3}$ and $x_{5}$ are free variables and we can interpret each nonzero row as an expression for the dependent variables $x_{1}$, $x_{2}$, $x_{4}$ (respectively) in the free variables $x_{3}$ and $x_{5}$. With this we can write the vector form of a solution vector as
\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\end{bmatrix}=\begin{bmatrix}-6x_{3}-4x_{5}\\
x_{3}+2x_{5}\\
x_{3}\\
-3x_{5}\\
x_{5}\end{bmatrix}=x_{3}\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix}+x_{5}\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}.
\end{align*}
Then, in the notation of the above theorem, we have
\begin{align*}
\displaystyle\mathbf{z}_{1}&amp;\displaystyle=\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{2}&amp;\displaystyle=\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}
\end{align*}
and
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)
=\left&lt;
\left\{\mathbf{z}_{1},\,\mathbf{z}_{2}
\right\}
\right&gt;
=\left&lt;
\left
\{\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix},\,\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}
\right\}
\right&gt;.
\end{align*} 
@endcol
@end
@eg
@newcol
 Consider the matrix:
\begin{align*}
\displaystyle A=\begin{bmatrix}2&amp;1&amp;5&amp;1&amp;5&amp;1\\
1&amp;1&amp;3&amp;1&amp;6&amp;-1\\
-1&amp;1&amp;-1&amp;0&amp;4&amp;-3\\
-3&amp;2&amp;-4&amp;-4&amp;-7&amp;0\\
3&amp;-1&amp;5&amp;2&amp;2&amp;3\end{bmatrix}.
\end{align*}
Row-reducing $A$ gives the matrix
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;2&amp;0&amp;-1&amp;2\\
0&amp;\boxed{1}&amp;1&amp;0&amp;3&amp;-1\\
0&amp;0&amp;0&amp;\boxed{1}&amp;4&amp;-2\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}
First, the non-pivot columns have indices $F=\{3,\,5,\,6\}$, so we will construct the $n-r=6-3=3$ vectors with a pattern of zeros and ones dictated by the indices in $F$. This is the realization of the first two lines of the three-case definition of the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$.
\begin{align*}
\displaystyle\mathbf{z}_{1}&amp;\displaystyle=\begin{bmatrix}\\
\\
1\\
\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{2}&amp;\displaystyle=\begin{bmatrix}\\
\\
0\\
\\
1\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{3}&amp;\displaystyle=\begin{bmatrix}\\
\\
0\\
\\
0\\
1\end{bmatrix}.
\end{align*}
Each of these vectors arises due to the presence of a non-pivot column. The remaining entries of each vector are the entries of the non-pivot column, negated, and distributed into the empty slots in order (these slots have indices in the set $D$, so also refer to pivot columns). This is the realization of the third line of the three-case definition of the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$.
\begin{align*}
\displaystyle\mathbf{z}_{1}&amp;\displaystyle=\begin{bmatrix}-2\\
-1\\
1\\
0\\
0\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{2}&amp;\displaystyle=\begin{bmatrix}1\\
-3\\
0\\
-4\\
1\\
0\end{bmatrix}&amp;\displaystyle\mathbf{z}_{3}&amp;\displaystyle=\begin{bmatrix}-2\\
1\\
0\\
2\\
0\\
1\end{bmatrix}.
\end{align*}
So we have
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left&lt;
\left\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3}
\right\}
\right&gt;
=
\left\langle
\left\{\begin{bmatrix}-2\\
-1\\
1\\
0\\
0\\
0\end{bmatrix},\,\begin{bmatrix}1\\
-3\\
0\\
-4\\
1\\
0\end{bmatrix},\,\begin{bmatrix}-2\\
1\\
0\\
2\\
0\\
1\end{bmatrix}
\right\}
\right\rangle.
\end{align*} 
@endcol
@end
@section{Null Spaces, Spans, Linear Independence}
@newline
In this section, we will find a linearly independent set that spans a null space.
Recall that, by @ref{SSNS},
there exists a particular set of $n-r$
vectors that could be used to span the null space of a matrix.
@newline
@eg
@label{LINSB}
@keyword{Linear independence of null space basis} Suppose that we are interested in the null space of a $3\times 7$ matrix $A$ which row-reduces to
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;-2&amp;4&amp;0&amp;3&amp;9\\
0&amp;\boxed{1}&amp;5&amp;6&amp;0&amp;7&amp;1\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;8&amp;-5\end{bmatrix}.
\end{align*} 
@newcol
 Then $F=\{3,\,4,\,6,\,7\}$ is the set of indices for our four free variables that would be used in a description of the solution set for the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Applying @ref{HMVEI}, we can begin to construct a set of four vectors whose span is the null space of $A$, a set of vectors we will refer to as $T$.
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)
=\left&lt; T\right&gt;
&amp;=\left&lt; \{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\mathbf{z}_{4}\}\right&gt;\\
&amp;=\left&lt; \left\{\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix}\right\}\right&gt;
\end{align*} 
@col
 So far, we have constructed as much of these individual vectors as we can, based just on the knowledge of the contents of the set $F$. This has allowed us to determine the entries in slots 3, 4, 6 and 7, while we have left slots 1, 2 and 5 blank. Without doing any more, let us ask if $T$ is linearly independent? Begin with a relation of linear dependence on $T$, and see what we can learn about the scalars.
@newline
@col
 \begin{align*}
\displaystyle\mathbf{0}&amp;\displaystyle=\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\alpha_{4}\mathbf{z}_{4} \\
\displaystyle\begin{bmatrix}0\\
0\\
0\\
0\\
0\\
0\\
0\end{bmatrix}&amp;\displaystyle=\alpha_{1}\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix}+\alpha_{2}\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix}+\alpha_{3}\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix}+\alpha_{4}\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix} \\
&amp;\displaystyle=\begin{bmatrix}\\
\\
\alpha_{1}\\
0\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
\alpha_{2}\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
\alpha_{3}\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
\alpha_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
\alpha_{1}\\
\alpha_{2}\\
\\
\alpha_{3}\\
\alpha_{4}\end{bmatrix}
\end{align*} 
@col
 Applying the equalities of vectors, we see that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$. So the only relation of linear dependence on the set $T$ is the trivial one. By the definition of linear independence, the set $T$ is linearly independent. The important feature of this example is how the @keyword{pattern of zeros and ones} in the four vectors led to the conclusion of linear independence. 
@endcol
@end
@slide
@thm
@title{Basis for Null Spaces}
@label{BNS}
 Suppose that $A$ is an $m\times n$ matrix, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ and $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$ be the sets of column indices of $B$ which are and are not, respectively, pivot columns. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$ as
\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&amp;\text{if $i\in F$, $i=f_{j}$}\\
0&amp;\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&amp;\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}
(In fact $\mathbf{z}_{j}$ corresponding to the solution $x_{f_{j}}=1$ and $x_{f_{k}}=0$ for $k\neq j$.) Define the set $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$. Then <ol class="ltx_enumerate">
<li class="ltx_item"> ${\mathcal{N}}\!\left(A\right)=\left&lt; S\right&gt;$. </li>
<li class="ltx_item"> $S$ is a linearly independent set. </li></ol>
@end
@proof
@newcol
 Study the above example. You can skip the proof for now. Notice first that the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$, are the same as the $n-r$ vectors defined in @ref{SSNS}. Also, the hypotheses of @ref{SSNS} are the same as the hypotheses of the theorem we are currently proving. So @ref{SSNS} tells us that ${\mathcal{N}}\!\left(A\right)=\left&lt; S\right&gt;$. That was the easy half, but the second part is not much harder. What is new here is the claim that $S$ is a linearly independent set.
@newlineTo prove the linear independence of a set, we need to start with a relation of linear dependence and somehow conclude that the scalars involved must all be zero, i.e., that the relation of linear dependence is trivial. So, we start with and equation of the form
\begin{align*}
\displaystyle \alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}=\mathbf{0}.
\end{align*}
For each $j$, $1\leq j\leq n-r$, consider the equality of the individual entries of the vectors on both sides of this equality in position $f_{j}$:
\begin{align*}
\displaystyle 0&amp;\displaystyle=\left[\mathbf{0}\right]_{f_{j}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&amp;\displaystyle=\left[\alpha_{1}\mathbf{z}_{1}\right]_{f_{j}}+\left[\alpha_{2}\mathbf{z}_{2}\right]_{f_{j}}+\left[\alpha_{3}\mathbf{z}_{3}\right]_{f_{j}}+\cdots+\left[\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&amp;\displaystyle=\alpha_{1}\left[\mathbf{z}_{1}\right]_{f_{j}}+\alpha_{2}\left[\mathbf{z}_{2}\right]_{f_{j}}+\alpha_{3}\left[\mathbf{z}_{3}\right]_{f_{j}}+\cdots+ \\
&amp;\displaystyle\quad\quad\alpha_{j-1}\left[\mathbf{z}_{j-1}\right]_{f_{j}}+\alpha_{j}\left[\mathbf{z}_{j}\right]_{f_{j}}+\alpha_{j+1}\left[\mathbf{z}_{j+1}\right]_{f_{j}}+\cdots+ \\
&amp;\displaystyle\quad\quad\alpha_{n-r}\left[\mathbf{z}_{n-r}\right]_{f_{j}} \\
&amp;\displaystyle=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+ \\
&amp;\displaystyle\quad\quad\alpha_{j-1}(0)+\alpha_{j}(1)+\alpha_{j+1}(0)+\cdots+\alpha_{n-r}(0)&amp; \text{ definition of } \mathbf{z}_{j} \\
&amp;\displaystyle=\alpha_{j}
\end{align*}
So for all $j$, $1\leq j\leq n-r$, we have $\alpha_{j}=0$. Hence, the only relation of linear dependence on $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$ is the trivial one. By the definition of linear independence, the set is linearly independent, as desired. 
@qed
@endcol
@end
@slide
@eg
 Find the null space of the matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}-2&amp;-1&amp;-2&amp;-4&amp;4\\
-6&amp;-5&amp;-4&amp;-4&amp;6\\
10&amp;7&amp;7&amp;10&amp;-13\\
-7&amp;-5&amp;-6&amp;-9&amp;10\\
-4&amp;-3&amp;-4&amp;-6&amp;6\\
\end{bmatrix}.
\end{align*} 
@end
@sol
@newcol
 The RREF of $A$ is:
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;1&amp;-2\\
0&amp;\boxed{1}&amp;0&amp;-2&amp;2\\
0&amp;0&amp;\boxed{1}&amp;2&amp;-1\\
0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}.
\end{align*}
The free variables are $x_{4}$ and $x_{5}$.
@newlineSetting $x_{4}=1$ and $x_{5}=0$ gives:
@newline
@col
 \begin{align*}
\displaystyle \mathbf{z}_{1}=\begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix}.
\end{align*} 
@col
 Setting instead $x_{4}=0$ and $x_{5}=1$ gives
\begin{align*}
\displaystyle \mathbf{z}_{2}=\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}.
\end{align*}
Hence
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left&lt; \begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix},\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}\right&gt;.
\end{align*} 
@qed
@endcol
@end
@section{Dimension}
@newline
@defn
@title{Dimension}
 Let $V$ be a vector space.
@newlineSuppose a finite set of vectors $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$ is a basis for $V$.
@newlineThen, we say that $V$ is a @keyword{finite dimensional vector space}.
@newlineThe number $t$ (namely the number of vectors in the basis)
is called the @keyword{dimension} of $V$.
@newlineThe dimension of the zero vector space $\{\vect{0}\}$ is defined to be $0$. 
@end
@remark
@newcol
 It is a non-trivial fact that the dimension is well-defined, i.e.,
If both $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$
and $\left\{\mathbf{u}_{1},\ldots,\mathbf{u}_{s}\right\}$
are bases for $V$,
then $s=t$. 
@endcol
@end
@slide
@thm
@label{SSLD}
 Suppose that $S=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{t}\right\}$ is a finite set of vectors which spans the vector space $V$. Then any set of $t+1$ or more vectors from $V$ is linearly dependent. 
@end
@proof
@newcol
 Let $\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_m$ be $m$ vectors in $V$,
where $m \geq t + 1$.
Let $A = [\vect{v}_1 | \vect{v}_2 | \cdots | \vect{v}_t ]$.
Since $S$ spans $V$, for every $\vect{u}_i$ ($1\leq i \leq m$)
there exists
$\vect{w}_i \in \mathbb{R}^t$ such that:
\[
A\vect{w}_i = \vect{u}_i.
\]
@newline
@col
 Now, consider the matrix:
\[
B = [\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m].
\]
This is a $t \times m$ matrix.
In particular, it has more columns than rows,
due to the assumption that $m &gt; t$.
@newline
@col
 Hence, the homogeneous linear system $\mathcal{LS}(B, \vect{0})$
has a non-trivial solution $\vect{x} \in \mathbb{R}^m$.  That is:
\[
B\vect{x} = \vect{0}.
\]
@newline
@col
 The above equation implies that:
\[
A\left(B\vect{x}\right) = A\vect{0} = \vect{0}.
\] 
@col
 By the associativity of matrix multiplication, we have:
\[
A\left(B\vect{x}\right) = \left(AB\right)\vect{x}.
\] 
@col
 On the other hand: 
@steps
\[
\begin{split}
AB &amp;= A[\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m]
\\&amp;
\class{steps}{\cssId{step0}{=[A\vect{w}_1 | A\vect{w}_2 | \cdots | A\vect{w}_m]}}
\\&amp;
\class{steps}{\cssId{step1}{= [\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]}}
\end{split}
\]

@endsteps
@col
 Hence,
\[
\left(AB\right)\vect{x} = \vect{0}
\]
is equivalent to:
\[
[\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]\vect{x} = \vect{0}
\] 
@col
 which is in turn equivalent to:
\[
x_1 \vect{u}_1 + x_2 \vect{u}_2 + \cdots x_m \vect{u}_m = \vect{0}.
\]
Since, $\vect{x}$ is not the zero vector,
not all the $x_i$'s are equal to zero.
We conclude that the vectors
$\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_m$ are linearly dependent. 
@qed
@endcol
@end
@newline
@slide
@thm
 Suppose that $V$ is a vector space with a finite basis $B$ and a second basis $C$.
@newline
@newcol
 Then $B$ and $C$ have the same size. 
@endcol
@end
@proof
@newcol
 Denote the size of $B$ by $t$. If $C$ has $\geq t+1$ vectors,
then by the previous theorem, $C$ is linearly dependent,
in contradiction to the condition that $C$ is a basis.
@newline
@col
 By the same reasoning, the linearly independent set $B$ must also not
have more vectors than $C$.
@newline
@col
 So, $B$ and $C$ have the same number of vectors. 
@qed
@endcol
@end
@remark
@newcol
 The above theorem shows that the dimension is well-defined. No matter which basis we choose, the size is always the same. 
@endcol
@end
@slide
@eg
 It follows from @ref{d848ac74d9e041feff51c974a9969985} that:
\[
\dim{\mathbb{R}}^{m}=m.
\] 
@end
@slide
@skip
@eg
<strong>Math major only</strong>
@newline$\dim M_{mn}=mn$. See example 3. 
@end
@newline
@eg
<strong>Math major only</strong>
@newline$\dim P_{n}=n+1$. See example 4. 
@end
@newline
@eg
<strong>Math major only</strong>
@newlineLet $S_{2}$ be the set of $2\times 2$ symmetric matrices. For $A\in S_{2}$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&amp;b\\
b&amp;c\end{bmatrix}=a\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix}+b\begin{bmatrix}0&amp;1\\
1&amp;0\end{bmatrix}+c\begin{bmatrix}0&amp;0\\
0&amp;1\end{bmatrix}
\end{align*} 
@newcol
 We can show that:
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix},\begin{bmatrix}0&amp;1\\
1&amp;0\end{bmatrix},\begin{bmatrix}0&amp;0\\
0&amp;1\end{bmatrix}\right\}
\end{align*} 
@col
 is a basis for $S_{2}$. Hence $\dim S_{2}=3$. 
@endcol
@end
@newline
@eg
<strong>Math major only</strong>
@newlineLet $P$ be the set of all real polynomials. As $\left\{1,x,x^{2},x^{3},\ldots\right\}$ is linearly independent,
so $\dim P$ does not exists (or we can write $\dim P=\infty$). 
@end
@slide
@lemma
 Let $V$ be a vector space and $\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\in V$.
@newline
@newcol
 Suppose
$S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent and $\mathbf{u}\notin\left&lt; S\right&gt;$. Then
$S^{\prime}=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\right\}$ is linearly independent. 
@endcol
@end
@proof
@newcol
 Let the relation of linear dependence of $S^{\prime}$ be
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}+\alpha\mathbf{u}=\mathbf{0}.
\end{align*} 
@col
 Suppose $\alpha\neq 0$, then
\begin{align*}
\displaystyle \mathbf{u}=-\frac{\alpha_{1}}{\alpha}\mathbf{v}_{1}-\cdots-\frac{\alpha_{k}}{\alpha}\mathbf{v}_{k}\in\left&lt; S\right&gt;.
\end{align*} 
@col
 Contradiction.
@newline
@col
 So $\alpha=0$, then
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\mathbf{0}.
\end{align*} 
@col
 By the linear independence of $S$, $\alpha_{i}=0$ for all $i$. Hence the above relation of dependence of $S^{\prime}$ is trivial. 
@qed
@endcol
@end
@slide
@thm
@label{basisexists2}
 Let $V$ be a nonzero subspace of ${\mathbb{R}}^{m}$. (That is, $V \neq \{\vect{0}\}$.)
@newlineThen, there exists a basis for $V$. 
@end
@proof
@newcol
 Let $V$ be a nonzero vector space.
Let $\mathbf{v}_{1}$ be a nonzero vector in $V$. If $V=\left&lt; \left\{\mathbf{v}_{1}\right\}\right&gt;$, we can take $S=\left\{\mathbf{v}_{1}\right\}$. Then obviously $\left\{\mathbf{v}_{1}\right\}$ is linearly independent and hence $S$ is a basis for $V$.
@newline
@col
 Otherwise, let $\mathbf{v}_{2}\in V$ but not in $\left&lt; \left\{\mathbf{v}_{1}\right\}\right&gt;$.
@newline
@col
 By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$ is linearly independent. If $V=\left&lt; \left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}\right&gt;$, we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$.
@newline
@col
 So $S$ is a basis for $V$.
@newline
@col
 Otherwise, let $\mathbf{v}_{3}\in V$ but not in $\left&lt; \left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}\right&gt;$.
@newline
@col
 By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$ is linearly independent. Repeat the above process, inductive we can define $\mathbf{v}_{k+1}$ as following: If $V=\left&lt; \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right&gt;$,
we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.
@newline
@col
 Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, $S$ is a basis for $V$.
@newline
@col
 Otherwise defined $\mathbf{v}_{k+1}\not\in\left&lt; \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right&gt;$.
@newline
@col
 By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k+1}\right\}$ is linearly independent.
@newline
@col
 If the process stops, say at step $k$, i.e., $V=\left&lt; \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right&gt;$.
@newline
@col
 Then we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.
@newline
@col
 Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, it is a basis for $V$.
@newline
@col
 This completes the proof.
@newline
@col
 Otherwise, the process continues infinitely, in particular, we can take $k=m+1$ and
$V\neq\left&lt; \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}\right&gt;$ and
$\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ is linearly independent.
@newline
@col
 Since $\left\langle\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}\right\rangle = \mathbb{R}^m$,
by @ref{SSLD} the vectors $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ are linearly dependent. Contradiction. 
@qed
@endcol
@end
@slide
@prop
@label{basisexistsprop}
 Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}\subseteq{\mathbb{R}}^{m}$. Then
\begin{align*}
\displaystyle \dim\left&lt; S\right&gt;\leq n.
\end{align*} 
@end
@proof
@newcol
 By Theorem    @ref{basisexists}, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left&lt; S\right&gt;$.
\begin{align*}
\displaystyle \dim\left&lt; S\right&gt;
=
\text{number of vectors in $T$}
\leq
\text{number of vectors in $S$}=n.
\end{align*} 
@qed
@endcol
@end
@remark
@newcol
 Both @ref{basisexists} and @ref{basisexistsprop} is valid
if we replace ${\mathbb{R}}^{m}$ by $P_{n}$, $M_{mn}$
or any finite dimensional vector space. 
@endcol
@end
@newline
@slide
@thm
 Suppose a vector space $V$ has dimension $n$.
Then, any linearly independent set with $n$ vectors in $V$
is a basis for $V$. 
@end
@thm
 Suppose a vector space $V$ has dimension $n$.
Suppose $S$ is a set of $n$ vectors in $V$ which spans $V$
(That is, $\left\langle S \right\rangle = V$).
@newlineThen, $S$ is a basis for $V$. 
@end
@newline
@section{Rank and nullity of a matrix}
@defn
@title{Nullity of a matrix}
@label{NOM}
 Suppose that $A \in M_{mn}$. Then the @keyword{nullity} of $A$ is the dimension of the null space of $A$,
$\nullity{A}=\dim(\nsp{A})$. 
@end
@newline
@defn
@title{Rank of a matrix}
@label{ROM}
 Suppose that $A \in M_{mn}$. Then the @keyword{rank} of $A$ is the dimension of the column space of $A$,
$\rank{A}=\dim(\csp{A})$. 
@end
@newline
@slide
@eg
@keyword{Rank and nullity of a matrix}
@newlineLet us compute the rank and nullity of
\[
A=\begin{bmatrix}
2 &amp; -4 &amp; -1 &amp; 3 &amp; 2 &amp; 1 &amp; -4\\
1 &amp; -2 &amp; 0 &amp; 0 &amp; 4 &amp; 0 &amp; 1\\
-2 &amp; 4 &amp; 1 &amp; 0 &amp; -5 &amp; -4 &amp; -8\\
1 &amp; -2 &amp; 1 &amp; 1 &amp; 6 &amp; 1 &amp; -3\\
2 &amp; -4 &amp; -1 &amp; 1 &amp; 4 &amp; -2 &amp; -1\\
-1 &amp; 2 &amp; 3 &amp; -1 &amp; 6 &amp; 3 &amp; -1
\end{bmatrix}
\] 
@newcol
 To do this, we will first row-reduce the matrix since that will help us determine bases for the null space and column space.
\[
\begin{bmatrix}
\leading{1} &amp; -2 &amp; 0 &amp; 0 &amp; 4 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; \leading{1} &amp; 0 &amp; 3 &amp; 0 &amp; -2\\
0 &amp; 0 &amp; 0 &amp; \leading{1} &amp; -1 &amp; 0 &amp; -3\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \leading{1} &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}
\] 
@col
 From this row-equivalent matrix in reduced row-echelon form we record $D=\set{1,\,3,\,4,\,6}$ and $F=\set{2,\,5,\,7}$.
@newline
@col
 By @ref{BCS}, for each index in $D$, we can create a single basis vector.
In fact $T=\{\vect{A}_1, \vect{A}_3, \vect{A}_4, \vect{A}_6\}$ is a basis for $\csp{A}$.
In total the basis will have $4$ vectors, so the column space of $A$ will have dimension $4$ and we write $\rank{A}=4$.
@newline
@col
 By @ref{SSNS}, for each index in $F$, we can create a single basis vector.  In total the basis will have $3$ vectors, so the null space of $A$ will have dimension $3$ and we write $\nullity{A}=3$. In fact:
@newline
@col
 \[
R = \left\{\colvector{ 2 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0},
\colvector{ -4 \\ 0 \\ -3 \\ 1 \\ 1 \\ 0 \\ 0},
\colvector{-1 \\ 0 \\ 2 \\ 3 \\ 0 \\ -1 \\ 1}
\right\}
\]
is a basis for $\nsp{A}$. 
@endcol
@end
@newline
@slide
@thm
@title{Computing rank and nullity}
 Suppose $A \in M_{mn}$ and $A \rref B$.
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).
Then $\rank{A}=r$ and $\nullity{A}=n-r$. 
@end
@proof
@newcol
 Let $D=\{d_1, \ldots, d_r\}$ be the indexes of the pivot columns of $B$.
By @ref{BCS}, $\{\vect{A}_{d_1}, \ldots, \vect{A}_{d_r}\}$ is a basis for $\csp{A}$.
So $\rank{A}=r$.
@newlineBy @ref{SSNS}, each free variable corresponding to a single basis vector for the null space.
So $\nullity{A}$ is the number of free variables $=n-r$. 
@endcol
@end
@newline
@cor
@title{Dimension formulaRank-Nullity Theorem}
 Suppose $A\in M_{mn}$, then
\begin{align*}
\displaystyle r\left(A\right)+n\left(A\right)=n.
\end{align*} 
@end
@slide
@thm
 Let $A$ be a $m\times n$ matrix. Then
\begin{align*}
\displaystyle r\left(A\right)=r\left(A^{t}\right).
\end{align*} 
@newcol
 Equivalently
\begin{align*}
\displaystyle \dim\mathcal{C}\left(A\right)=\dim\mathcal{R}\!\left(A\right).
\end{align*} 
@endcol
@end
@proof
@newcol
 Let $A\xrightarrow{\text{RREF}}B$.
@newline
@col
 Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).
@newline
@col
 Then by the above discussion $r=r\left(A\right)$. By @ref{BRS}, the first $r$ columns of $B^{t}$ form a basis for $\mathcal{R}\!\left(A\right) = \mathcal{C}(A^t)$.
Hence $r=r\left(A^{t}\right)$. This completes the proof. 
@qed
@endcol
@end
 Let us take a look at the rank and nullity of a square matrix. 
@slide
@eg
 The matrix
\begin{align*}
\displaystyle E=\begin{bmatrix}0&amp;4&amp;-1&amp;2&amp;2&amp;3&amp;1\\
2&amp;-2&amp;1&amp;-1&amp;0&amp;-4&amp;-3\\
-2&amp;-3&amp;9&amp;-3&amp;9&amp;-1&amp;9\\
-3&amp;-4&amp;9&amp;4&amp;-1&amp;6&amp;-2\\
-3&amp;-4&amp;6&amp;-2&amp;5&amp;9&amp;-4\\
9&amp;-3&amp;8&amp;-2&amp;-4&amp;2&amp;4\\
8&amp;2&amp;2&amp;9&amp;3&amp;0&amp;9\end{bmatrix}
\end{align*} 
@newcol
 is row-equivalent to the matrix in reduced row-echelon form,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;\boxed{1}\end{bmatrix}
\end{align*} 
@col
 With $n=7$ columns and $r=7$ nonzero rows tells us the rank is $r\left(E\right)=7$ and the nullity is $n\left(E\right)=7-7=0$. 
@endcol
@end
 The value of either the nullity or the rank are enough to characterize a nonsingular matrix.
@newline
@slide
@thm
@title{Rank and Nullity of a Nonsingular Matrix}
@label{RNNM}
 Suppose that $A$ is a square matrix of size $n$. The following are equivalent. <ol class="ltx_enumerate">
<li class="ltx_item"> A is nonsingular. </li>
<li class="ltx_item"> The rank of $A$ is $n$, $r\left(A\right)=n$. </li>
<li class="ltx_item"> The nullity of $A$ is zero, $n\left(A\right)=0$. </li></ol>
@end
@proof
@skip
@newcol
 (1 $\Rightarrow$ 2) 
@newcol
 If $A$ is nonsingular then $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$.
@newline
@col
 If $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$, then the column space has dimension $n$, so the rank of $A$ is $n$. 
@endcol
@newline(2 $\Rightarrow$ 3) 
@newcol
 Suppose $r\left(A\right)=n$. Then the dimension formula gives
\begin{align*}
\displaystyle n\left(A\right)&amp;\displaystyle=n-r\left(A\right) \\
&amp;\displaystyle=n-n \\
&amp;\displaystyle=0
\end{align*} 
@endcol
@newline(3 $\Rightarrow$ 1) 
@newcol
 Suppose $n\left(A\right)=0$, so a basis for the null space of $A$ is the empty set. This implies that ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$ and hence $A$ is nonsingular. 
@endcol
@qed
@endcol
@end
 With a new equivalence for a nonsingular matrix, we can update our list of equivalences which now becomes a list requiring double digits to number. 
@slide
@thm
 Suppose that $A$ is a square matrix of size $n$. The following are equivalent. <ol class="ltx_enumerate">
<li class="ltx_item"> $A$ is nonsingular. </li>
<li class="ltx_item"> $A$ row-reduces to the identity matrix. </li>
<li class="ltx_item"> The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$. </li>
<li class="ltx_item"> The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$. </li>
<li class="ltx_item"> The columns of $A$ are a linearly independent set. </li>
<li class="ltx_item"> $A$ is invertible. </li>
<li class="ltx_item"> The column space of $A$ is ${\mathbb{R}}^{n}$, $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$. </li>
<li class="ltx_item"> The columns of $A$ are a basis for ${\mathbb{R}}^{n}$. </li>
<li class="ltx_item"> The rank of $A$ is $n$, $r\left(A\right)=n$. </li>
<li class="ltx_item"> The nullity of $A$ is zero, $n\left(A\right)=0$. </li></ol>
@end
@newline
@section{Bases and nonsingular matrices}
@slide
@thm
 Suppose that $A$ is a square matrix of size $m$.
@newlineThen, the columns of $A$ is a basis for ${\mathbb{R}}^{m}$ if and only if $A$ is nonsingular. 
@end
@proof
@newcol
 This is a direct consequence of the theorem  @ref{NME2}:
@newline
@col
 If columns of $A$ form a basis,
then in particular they are linearly independent.
So, item 5 of the theorem holds.
@newlineIt now follows from the theorem that item 1,
namely that $A$ is nonsingular, also holds.
@newline
@col
 Conversely, suppose $A$ is nonsingular.
@newline
@col
 Then, by Item 5 of @ref{NME2} the columns of $A$ are linearly independent,
and by Item 4 they span $\mathbb{R}^m$.
Hence, the columns of $A$ is a basis for $\mathbb{R}^m$. 
@qed
@endcol
@end
@slide
 In fact, we may further extend @ref{NME2} as follows: 
@thm
@title{Nonsingular Matrix Equivalences}
@label{NME25}
@newcol
 Suppose that $A$ is an $m\times m$ square matrix.
The following are equivalent: <ol class="ltx_enumerate">
<li class="ltx_item">
@col
 $A$ is nonsingular. </li>
<li class="ltx_item">
@col
 $A$ row-reduces to the identity matrix. </li>
<li class="ltx_item">
@col
 The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$. </li>
<li class="ltx_item">
@col
 The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$. </li>
<li class="ltx_item">
@col
 The columns of $A$ form a linearly independent set. </li>
<li class="ltx_item">
@col
 The columns of $A$ form a basis for $\mathbb{R}^m$. </li></ol>
@endcol
@end
@slide
@skip
@eg
 Consider $S^{\prime}=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\end{bmatrix}\right\}$.
@newlineLet
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}]=\begin{bmatrix}1&amp;1\\
0&amp;1\end{bmatrix}
\end{align*} 
@newcol
<strong>Exercise.</strong> The matrix $A$ is nonsingular.
@newline
@col
 Hence, $S^{\prime}$ is a basis for ${\mathbb{R}}^{2}$. 
@endcol
@end
@slide
@eg
 \begin{align*}
\displaystyle A=\begin{bmatrix}-7&amp;-6&amp;-12\\
5&amp;5&amp;7\\
1&amp;0&amp;4\end{bmatrix}.
\end{align*} 
@newcol
 It may be shown that $A$ is row equivalent to the $3\times 3$
identity matrix.
@newline
@col
 Hence $A$ is nonsingular,
so the columns of $A$ form a basis for ${\mathbb{R}}^{3}$. 
@endcol
@end
@section{
@newlineColumn Space of a Nonsingular Matrix}
@slide
@thm
@title{Column Space of a Nonsingular Matrix}
 Suppose $A$ is a square matrix of size $n$. Then $A$ is nonsingular if and only if $\mathcal{C}\left(A\right)={\mathbb{R}}^{n}$. 
@end
@proof
@newcol
 See @ref{thm:ANSRM}. 
@qed
@endcol
@end
@slide
@eg
 Let
\begin{align*}
\displaystyle A=\begin{bmatrix}0&amp;1&amp;2&amp;3\\
-1&amp;1&amp;2&amp;1\\
0&amp;1&amp;0&amp;2\\
1&amp;1&amp;1&amp;4\\
\end{bmatrix}.
\end{align*} 
@newcol
 We can show that $A$ is nonsingular as $A\xrightarrow{\text{RREF}}I_{4}$.
So $\mathcal{C}\left(A\right)={\mathbb{R}}^{4}$. 
@endcol
@end
@section{Linear relation of $P_n$ and $M_{mn}$}
@slide
@skip
<strong>You can skip this section. It is for math major only</strong>
@newlineIn this section, we discuss the linear relation of $P_{n}$ or $M_{mn}$ by using the techniques used for the vector space ${\mathbb{R}}^{k}$.
@newline
@newcol
 Let $V=P_{n}$ and $f_{1},\ldots,f_{m},g\in P_{n}$.
@newline
@col
 Write
@newline\begin{align*}
\displaystyle f_{i}(x)=a_{i0}+a_{i1}x+\cdots+a_{in}x^{n},
\end{align*}
\begin{align*}
\displaystyle g(x)=b_{0}+b_{1}x+\cdots+b_{n}x^{n}.
\end{align*} 
@col
 By comparing coefficients,
\begin{align*}
\displaystyle g(x)=\alpha_{1}f_{1}(x)+\alpha_{2}f_{2}(x)+\cdots+\alpha_{m}f_{m}(x)
\end{align*} 
@col
 if and only if
\begin{align*}
\displaystyle \alpha_{1}a_{10}+\alpha_{2}a_{20}+\cdots+\alpha_{m}a_{m0}=b_{0},
\end{align*}
\begin{align*}
\displaystyle \alpha_{1}a_{11}+\alpha_{2}a_{21}+\cdots+\alpha_{m}a_{m1}=b_{1},
\end{align*}
\begin{align*}
\displaystyle \vdots
\end{align*}
\begin{align*}
\displaystyle \alpha_{1}a_{1n}+\alpha_{2}a_{2n}+\cdots+\alpha_{m}a_{mn}=b_{n}
\end{align*} 
@col
 if and only if
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix}+\alpha_{2}\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix}+\cdots+\alpha_{m}\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*} 
@col
 The above motivates us to define
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix},\cdots,\mathbf{v}_{m}=\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix},\mathbf{u}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*} 
@col
 The entries of $\mathbf{v}_{i}$ are the coefficients of $f_{i}$.
@newline
@endcol
@slide
@skip
 We then have the following theorem: 
@thm
<ol class="ltx_enumerate">
<li class="ltx_item"> $\left\{f_{1},\ldots,f_{m}\right\}$ is linearly independent if and only if $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{m}\right\}$
is linearly independent. </li>
<li class="ltx_item"> $g$ is a linearly combination of $f_{1},\ldots,f_{m}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{m}$ </li></ol>
@end Problems regarding polynomials can therefore be transformed to problems regarding column vectors.
@newline
@newcol
 Similarly given $m\times n$ matrices $A_{1},\ldots,A_{k},B$. Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}
[A_{1}]_1\\
[A_{1}]_2\\
\vdots\\
[A_{1}]_n\end{bmatrix},
\mathbf{v}_{2}=\begin{bmatrix}
[A_{2}]_1\\
[A_{2}]_2\\
\vdots\\
[A_{2}]_{n}\\
\end{bmatrix},
\cdots,
\mathbf{u}=\begin{bmatrix}
[B]_1\\
[B]_2\\
\vdots\\
[B]_{n}\end{bmatrix}.
\end{align*}
We have the following: 
@col
@thm
<ol class="ltx_enumerate">
<li class="ltx_item"> $\left\{A_{1},\ldots,A_{k}\right\}$ is linearly independent if and only if $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$
is linearly independent. </li>
<li class="ltx_item"> $B$ is a linearly combination of $A_{1},\ldots,A_{k}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{k}$ </li></ol>
@end
@col
 Again, problems regarding polynomials can be transformed to problems regarding column vectors. 
@endcol
@slide
@skip
@eg
@enumerate
@item
 Determine if
\begin{align*}
\displaystyle A_{1}=\begin{bmatrix}1&amp;2\\
3&amp;4\end{bmatrix},A_{2}=\begin{bmatrix}1&amp;-1\\
5&amp;6\end{bmatrix},A_{3}=\begin{bmatrix}-2&amp;0\\
-3&amp;-4\end{bmatrix}
\end{align*}
is linearly independent or not. 
@item
@newcol
 Express
\begin{align*}
\displaystyle B=\begin{bmatrix}-3&amp;0\\
4&amp;1\end{bmatrix}
\end{align*}
as a linear combination of $A_{1},A_{2},A_{3}$. 
@endcol
@endenumerate
@end
@sol
@newcol
@enumerate
@item
@newcol
 Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
3\\
2\\
4\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
5\\
-1\\
6\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}-2\\
-3\\
0\\
-4\end{bmatrix},\mathbf{u}=\begin{bmatrix}-3\\
4\\
0\\
1\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}]=\begin{bmatrix}1&amp;1&amp;-2\\
3&amp;5&amp;-3\\
2&amp;-1&amp;0\\
4&amp;6&amp;-4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;1\\
0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*} 
@col
 Obviously the columns of the RREF is linearly independent, hence $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$
is linearly independent. Therefore $\left\{A_{1},A_{2},A_{3}\right\}$ is linearly independent. 
@endcol
@item
@newcol
 Next
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{b}]=\begin{bmatrix}1&amp;1&amp;-2&amp;-3\\
3&amp;5&amp;-3&amp;4\\
2&amp;-1&amp;0&amp;0\\
4&amp;6&amp;-5&amp;1\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;0&amp;1\\
0&amp;1&amp;0&amp;2\\
0&amp;0&amp;1&amp;3\\
0&amp;0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*} 
@newcol
 Then $\mathbf{u}=\mathbf{v}_{1}+2\mathbf{v}_{2}+3\mathbf{v}_{3}$. Hence $B=A_{1}+2A_{2}+3A_{3}$. 
@endcol
@endcol
@endenumerate
@endcol
@end
@slide
@skip
@eg
 Let $f_{1}(x)=1+x+x^{3}$, $f_{2}(x)=2+x+x^{2}$, $f_{3}(x)=4+3x+x^{2}+2x^{3}$, $f_{4}(x)=2x^{2}+x^{3}$, $f_{5}(x)=3+2x+3x^{2}+2x^{3}$.
@newlineFind a basis for $\left&lt; \left\{f_{1},f_{2},f_{3},f_{4},f_{5}\right\}\right&gt;$. 
@end
@sol
@newcol
 Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\\
1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}2\\
1\\
1\\
0\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}4\\
3\\
1\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}0\\
0\\
2\\
1\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}3\\
2\\
3\\
2\end{bmatrix}.
\end{align*} 
@newcol
 Then
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{v}_{4}|\mathbf{v}_{5}]=\begin{bmatrix}1&amp;2&amp;4&amp;0&amp;3\\
1&amp;1&amp;3&amp;0&amp;2\\
0&amp;1&amp;1&amp;2&amp;3\\
1&amp;0&amp;2&amp;1&amp;2\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&amp;0&amp;2&amp;0&amp;1\\
0&amp;1&amp;1&amp;0&amp;1\\
0&amp;0&amp;0&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;0\\
\end{bmatrix}.
\end{align*} 
@col
 Therefore $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{4}\right\}$ is a basis for $\left&lt; \left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4},\mathbf{v}_{5}\right\}\right&gt;$.
@newline
@col
 So $\left\{f_{1},f_{2},f_{4}\right\}$ is a basis for $\left&lt; \left\{f_{1},f_{2},f_{3},f_{4},f_{5}\right\}\right&gt;$. 
@qed
@endcol
@endcol
@end
<!--DELIMITER-->
