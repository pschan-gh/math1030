@course{MATH 1030}
@setchapter{1}
@chapter{Technique of solving system of linear equations}

<h5 class='notkw'>Reference:</h5>
@enumerate
@item
Gilbert Strang, <i>Linear Algebra and Its Applications</i>. Section 1.3.
@item
Robert A. Beezer, <i>A First Course in Linear Algebra</i>. Section <a target="_blank" href="http://linear.ups.edu/html/section-SSLE.html">Solving Systems of Linear Equations</a>.
@endenumerate


@section{Introduction}
@label{ex2}
In this section we give examples of systems of linear equations and solve one example.
@eg
Solve the system of equations:
\begin{eqnarray}
\label{eq:eg2-1}
2x_{1}+3x_{2}&=3\\
\label{eq:eg2-2}
x_{1}-x_{2}&=4\\
\end{eqnarray}
@end
@sol
@newcol
Adding Equation $\eqref{eq:eg2-1}$ to $-2$ $\times$
Equation $\eqref{eq:eg2-2}$ gives:
\[
5x_{2}=-5.
\]
So $x_{2}=-1$. Substituting $x_{2}=-1$ into Equation $\eqref{eq:eg2-1}$ gives
\[
2x_{1}+3(-1)=3
\]
Hence, $x_{1}=3$ and $x_{2}=-1$ is a solution.

@col
Indeed:
\[2(3)+3(-1)=3\]
\[(3)-(-1)=4.\]
In fact, it is the unique solution.
@endcol
@end
@slide
<strong>Main goal</strong>: One of the main goals of this course is to solve systems of linear equations with more variables and more equations.
@eg
\begin{align*}
\displaystyle x_{1}+2x_{2}+2x_{3}&=4\\
\displaystyle x_{1}+3x_{2}+3x_{3}&=5\\
\displaystyle 2x_{1}+6x_{2}+5x_{3}&=6
\end{align*}
@end
@eg
\begin{align*}
\displaystyle x_{1}+2x_{2}+x_{4}&=7\\
\displaystyle x_{1}+x_{2}+x_{3}-x_{4}&=3\\
\displaystyle 3x_{1}+x_{2}+5x_{3}-7x_{4}&=1
\end{align*}
@end
@section{System of linear equations of two unknowns}
You should all be very familiar with the procedure of solving a system equations of two unknowns.
@subsection{Substitution}
@eg
\begin{equation}\label{v2sube1}3x+4y=2\end{equation}
\begin{equation}\label{v2sube2}4x+5y=3\end{equation}
Use $\eqref{v2sube2}$, we can solve for $y$ in terms of $x$:
\begin{equation}\label{v2sube3}y=\frac{3}{5}-\frac{4}{5}x.\end{equation}
Substitution this into $\eqref{v2sube1}$:


\[3x+4(\frac{3}{5}-\frac{4}{5}x)=2\]
\[\frac{12}{5}-\frac{x}{5}=2\]
\[x=2.\]
Substituting $x=2$ into $\eqref{v2sube3}$, we can solve for $y$:
\[y=\frac{3}{5}-\frac{4}{5}\times 2=-1.\]
So the solution is $x=2$, $y=-1$.
@end


@remark @col
There are other ways to use substitution, for example
@enumerate
@item
Solve $x$ by $\eqref{v2sube2}$ in terms of $y$ and substitute it into $\eqref{v2sube1}$
@item
Solve $y$ by $\eqref{v2sube1}$ in terms of $x$ and substitute it into $\eqref{v2sube2}$
@item
Solve $x$ by $\eqref{v2sube1}$ in terms of $y$ and substitute it into $\eqref{v2sube2}$
@endenumerate
But you <strong>cannot</strong> get the solution by
@enumerate
@item
Solve $y$ by $\eqref{v2sube2}$ in terms of $x$ and substitute it into $\eqref{v2sube2}$ (No substitution back to the original equation)
@item
Solve $y$ by $\eqref{v2sube1}$ in terms of $x$ and substitute it into $\eqref{v2sube1}$

@item
Solve $x$ by $\eqref{v2sube2}$ in terms of $y$ and substitute it into $\eqref{v2sube2}$
@item
Solve $x$ by $\eqref{v2sube1}$ in terms of $y$ and substitute it into $\eqref{v2sube1}$
@endenumerate
@end

@subsection{Elimination}
@eg
Again, let’s solve the system of linear equations from the previous example.
\begin{equation}\label{v2elime1}3x+4y=2\end{equation}
\begin{equation}\label{v2elime2}4x+5y=3\end{equation}
Consider $\eqref{v2elime1}-\frac{3}{4}\eqref{v2elime2}$.


\begin{align*}
   & 3x & + & 4y & = & 2 \\
-)\quad & 3x & + & \frac{15}4 y & = & \frac{9}4 \\
\hline
     &   &  & \frac{1}{4} y &= &-\frac{1}{4}
\end{align*}
Thus $y=-1$. Substituting it into $\eqref{v2elime1}$:
\[3x+4(-1)=2\]
\[x=2.\]
So we obtain the solution $x=2$, $y=-1$.
@end


@remark
@enumerate
@item
The number $\frac{3}{4}$ is so chosen such that the coefficient of $x$ is eliminated.
@item
At some point, we still need to use substitution to get the solution.
@endenumerate
@end


@section{System of linear equations of three unknowns}
<strong>Cold joke</strong>
@col
There were two men trying to decide what to do for a living. They
went to see a counselor, and he decided that they had good problem
solving skills.
He tried a test to narrow the area of specialty. He put each man in a
room with a stove, a table, and a pot of water on the table. He said
”Boil the water”. Both men moved the pot from the table to the stove
and turned on the burner to boil the water. Next, he put them into a
room with a stove, a table, and a pot of water on the floor. Again,
he said ”Boil the water”. The first man put the pot on the stove and
turned on the burner. The counselor told him to be an Engineer,
because he could solve each problem individually. The second man
moved the pot from the floor to the table, and then moved the pot from
the table to the stove and turned on the burner. The counselor told
him to be a mathematician because he <strong>reduced the problem to a
previously solved problem</strong>.


@subsection{Substitution}
@eg
Solve the following system of linear equations
\begin{equation}\label{v3sube1}x+2y+2z=4\end{equation}
\begin{equation}\label{v3sube2}x+3y+3z=5\end{equation}
\begin{equation}\label{v3sube3}2x+6y+5z=6\end{equation}

@col
Find $x$ in terms of $y,z$ by $\eqref{v3sube1}$ :
\begin{equation}\label{v3sube4}x=4-2y-2z.\end{equation}
Substituting $\eqref{v3sube4}$ into $\eqref{v3sube2}$, we obtain
\[(4-2y-2z)+3y+3z=5\]
i.e.,
\begin{equation}
\label{v3sube5}y+z=1.
\end{equation}
@col
Substituting $\eqref{v3sube4}$ into $\eqref{v3sube3}$, we obtain
\[2(4-2y-2z)+6y+5z=6,\]
i.e.,
\begin{equation}\label{v3sube6}2y+z=-2\end{equation}
The equations are reduced to solving a linear system of equations with two unknowns:
\[
\begin{cases}
y+z=1\\
2y+z=-2
\end{cases}
\]
@col
Solve $y$ in terms of $z$ by $\eqref{v3sube5}$:
\begin{equation}\label{v3sube7}y=1-z\end{equation}
Then substitute $y=1-z$ into $\eqref{v3sube6}$:
\[2(1-z)+z=-2\]
i.e.
\[
z=4.
\]
@col
By $\eqref{v3sube7}$
\[y=1-z=-3.\]
Substitute $y=-3$, $z=4$ into $\eqref{v3sube4}$,
\[x=4-2y-2z=4-2\times(-3)-2\times 4=2.\]
Hence $x=2,y=-3,z=4$ is a solution.
@end
@subsection{Elimination}
Using substitution all the way to solve linear equations is not the best way. Instead, we can use elimination to simplify the system of linear equations first.
@eg
We solve the following system by a sequence of equation operations.
\[\displaystyle x+2y+2z=4 \tag{1}\]
\[\displaystyle x+3y+3z=5 \tag{2}\]
\[\displaystyle 2x+6y+5z=6 \tag{3}\]
@col
$-1$ times equation 1, add to equation 2:
\[\displaystyle x+2y+2z=4\]
\[\displaystyle 0x+1y+1z=1\]
\[\displaystyle 2x+6y+5z=6\]
@col
$-2$ times equation 1, add to equation 3:
\[\displaystyle x+2y+2z=4\]
\[\displaystyle 0x+1y+1z=1\]
\[\displaystyle 0x+2y+1z=-2\]
@col
$-2$ times equation 2, add to equation 3:
\[\displaystyle x+2y+2z=4\]
\[\displaystyle 0x+1y+1z=1\]
\[\displaystyle 0x+0y-1z=-4\]
@col
$-1$ times equation 3:
\[\displaystyle x+2y+2z=4\]
\[\displaystyle 0x+1y+1z=1\]
\[\displaystyle 0x+0y+1z=4\]
which can be written more clearly as:

@col
\[\displaystyle x+2y+2z=4\]
\[\displaystyle y+z=1\]
\[\displaystyle z=4\]
@col
The third equation requires that $z=4$ to be true. Making this substitution into equation 2 we arrive at $y=-3$, and finally, substituting these values of $y$ and $z$ into the first equation, we find that $x=2$.
@end
@remark
We can add several more eliminations to solve $x,y,z$ without substitution:
\[\displaystyle x+2y+2z=4\]
\[\displaystyle 0x+1y+1z=1\]
\[\displaystyle 0x+0y+1z=4\]
$-1$ times equation 3, add to equation 2 and $-2$ times equation 3, add to equation 1
\[\displaystyle x+2y+0z=-4\]
\[\displaystyle 0x+1y+0z=-3\]
\[\displaystyle 0x+0y+1z=4\]
$-2$ times equation 2, add to equation 1
\[\displaystyle x+0y+0z=2\]
\[\displaystyle 0x+1y+0z=-3\]
\[\displaystyle 0x+0y+1z=4\]
So $x=2,y=-3,z=4$ is a solution.
@end
@section{More Examples}
@eg
Solve:
\begin{align}
\displaystyle x_{1}-5x_{2}+3x_{3}&=1 \tag{1}\\
\displaystyle 2x_{1}-4x_{2}+x_{3}&=0 \tag{2}\\
\displaystyle x_{1}+x_{2}-2x_{3}&=-1 \tag{3}
\end{align}
@col
$-2$ times equation 1, add to equation 2:
\begin{align*}
x_1 - 5x_2 +  3x_3 &= 1 \\
0x_1  + 6x_2 -5 x_3 &= -2 \\
x_1 + x_2 -2x_3  &= -1
\end{align*}
@col
$-1$ times equation 1, add to equation 3:
\begin{align*}
x_1 - 5x_2 +  3x_3 &= 1 \\
0x_1+ 6x_2 -5 x_3 &= -2 \\
0x_1+ 6x_2 -5x_3  &= -2
\end{align*}
@col
$-1$ times equation 2, add to equation 3:
\begin{align*}
x_1 - 5x_2 +  3x_3 &= 1 \\
0x_1+ 6x_2 -5 x_3 &= -2 \\
0x_1+0x_2+0x_3 &= 0
\end{align*}
$\frac{5}{6}$ times equation 2, add to equation 1:
\begin{align*}
x_1 +0x_2 -\frac{7}{6}x_3 &= -\frac{2}{3} \\
0x_1+ 6x_2 -5 x_3 &= -2 \\
0x_1+0x_2+0x_3 &= 0
\end{align*}
@col
We can express $x_{1},x_{2}$ in terms of $x_{3}$:
\[\displaystyle x_{1}=-\frac{2}{3}+\frac{7}{6}x_{3}\]
\[\displaystyle x_{2}=-\frac{1}{3}+\frac{5}{6}x_{3}\]
@col
The solution set is:
\[\{(-\frac{2}{3}+\frac{7}{6}a,-\frac{1}{3}+\frac{5}{6}a,a)\,|\,a\text{ real numbers}.\}\]
@end
@slide
@eg
Solve:
\begin{align*}
x_1 - 5x_2 +  3x_3 &= 1 \\
2x_1 - 4x_2 + x_3 &= 0 \\
x_1 + x_2 -2x_3  &= -2
\end{align*}
@col
$-2$ times equation 1, add to equation 2:
\begin{align*}
x_1 - 5x_2 +  3x_3 &= 1 \\
0x_1+6x_2 -5 x_3 &= -2 \\
x_1 + x_2 -2x_3  &= -2
\end{align*}
@col
$-1$ times equation 1, add to equation 3:
\begin{align*}
x_1 - 5x_2 +  3x_3 &= 1 \\
0x_1+6x_2 -5 x_3 &= -2 \\
0x_1+6x_2 -5x_3  &= -3
\end{align*}
@col
$-1$ times equation 2, add to equation 3:
\begin{align*}
x_1 - 5x_2 +  3x_3 &= 1 \\
0x_1+6x_2 -5 x_3 &= -2 \\
0x_1+0x_2+0x_3 &= -1
\end{align*}
@col
The last equation, $0=-1$ has no solution. So the system of linear equations has no solution.
@end
@slide
@eg
Solve:
\begin{align*}
x_1+2x_2 +0x_3+ x_4&= 7\\
x_1+x_2+x_3-x_4&=3\\
3x_1+x_2+5x_3-7x_4&=1
\end{align*}
@col
$-1$ times equation 1, add to equation 2:
\begin{align*}
x_1+2x_2 +0x_3+ x_4&= 7\\
0x_1-x_2+x_3-2x_4&=-4\\
3x_1+x_2+5x_3-7x_4&=1
\end{align*}
@col
$-3$ times equation 1, add to equation 3:
\begin{align*}
x_1+2x_2 +0x_3+ x_4&= 7\\
0x_1-x_2+x_3-2x_4&=-4\\
0x_1-5x_2+5x_3-10x_4&=-20
\end{align*}
@col
$-5$ times equation 2, add to equation 3:
\begin{align*}
x_1+2x_2 +0x_3+ x_4&= 7\\
0x_1-x_2+x_3-2x_4&=-4\\
0x_1+0x_2+0x_3+0x_4&=0
\end{align*}
@col
$-1$ times equation 2:
\begin{align*}
x_1+2x_2 +0x_3+ x_4&= 7\\
0x_1+x_2-x_3+2x_4&=4\\
0x_1+0x_2+0x_3+0x_4&=0
\end{align*}
@col
$-2$ times equation 2, add to equation 1:
\begin{align*}
x_1+0x_2 +2x_3-3x_4&= -1\\
0x_1+x_2-x_3+2x_4&=4\\
0x_1+0x_2+0x_3+0x_4&=0
\end{align*}
which can be written more clearly as:

@col
\begin{align*}
x_1+2x_3 - 3x_4&= -1\\
x_2-x_3+2x_4&=4\\
0&=0
\end{align*}
@col
The last equation $0=0$ is always true, so we can ignore it and only consider the first two equations. We can analyze the second equation without consideration of the variable $x_{1}$. It would appear that there is considerable latitude in how we can choose $x_{2}$, $x_{3}$, $x_{4}$ and make this equation true. Let us choose $x_{3}$ and $x_{4}$ to be <strong>anything</strong> we please, say $x_{3}=a$ and $x_{4}=b$.

@col
Now we can take these arbitrary values for $x_{3}$ and $x_{4}$, substitute them in equation 1, to obtain
\[x_{1}+2a-3b=-1\]
\[x_{1}=-1-2a+3b\]
@col
Similarly, equation 2 becomes
\[x_{2}-a+2b=4\]
\[x_{2}=4+a-2b\]
@col
So our arbitrary choices of values for $x_{3}$ and $x_{4}$ ($a$ and $b$) translate into specific values of $x_{1}$ and $x_{2}$.

@col
Now we can easily and quickly find many more (infinitely more). Suppose we choose $a=5$ and $b=-2$, then we compute
\[\displaystyle x_{1}=-1-2(5)+3(-2)=-17\]
\[\displaystyle x_{2}=4+5-2(-2)=13\]
@col
and you can verify that $(x_{1},\,x_{2},\,x_{3},\,x_{4})=(-17,\,13,\,5,\,-2)$ makes all three equations true. The entire solution set is written as
\[\{(-1-2a+3b,\,4+a-2b,\,a,\,b)\,|\,a,b\text{ real numbers}\}.\]
@end
@slide
@eg
Solve the following system of linear equations:
\[
\begin{alignedat}{6}
& {} {} &  x_2 & {}+{} & x_3 & {}+{} & 2x_4   & {}+{} & 2x_5 &{}={} & 2 \\
x_1 & {}+{} &  2x_2 & {}+{} & 3x_3 & {}+{} & 2x_4 &  {}+{}&  3x_5&{}={} & 4 \\
-2x_1 & {}-{} &  x_2 & {}-{} & 3x_3 & {}+{} & 3x_4 & {}+{} &  x_5   &{}={} & 3
\end{alignedat}
\]
@col
Swap equation 1 and equation 2:
\[
\begin{alignedat}{6}
x_1 & {}+{} &  2x_2 & {}+{} & 3x_3 & {}+{} & 2x_4 &  {}+{}&  3x_5&{}={} & 4 \\
& {} {} &  x_2 & {}+{} & x_3 & {}+{} & 2x_4   & {}+{} & 2x_5 &{}={} & 2 \\
-2x_1 & {}-{} &  x_2 & {}-{} & 3x_3 & {}+{} & 3x_4 & {}+{} &  x_5   &{}={} & 3
\end{alignedat}
\]
@col
2 times equation 1, and add it to equation 3:
\[
\begin{alignedat}{6}
x_1 & {}+{} &  2x_2 & {}+{} & 3x_3 & {}+{} & 2x_4 &  {}+{}&  3x_5&{}={} & 4 \\
& {} {} &  x_2 & {}+{} & x_3 & {}+{} & 2x_4   & {}+{} & 2x_5 &{}={} & 2 \\
& {} {} & 3x_2 & {}+{} & 3x_3 & {}+{} & 7x_4 &  {}+{} &7x_5     &{}={} & 11
\end{alignedat}
\]
@col
-3 times equation 2 and add it to equation 3:
\[
\begin{alignedat}{6}
x_1 & {}+{} &  2x_2 & {}+{} & 3x_3 & {}+{} & 2x_4 &  {}+{}&  3x_5&{}={} & 4 \\
& {} {} &  x_2 & {}+{} & x_3 & {}+{} & 2x_4   & {}+{} & 2x_5 &{}={} & 2 \\
& {} {} &     & {}  {} &     & {} {} & x_4 &  {}+{}  & x_5 &{}={} & 5
\end{alignedat}
\]
@col
Now the system of linear equations looks like an ”inverted stair”.
We can then solve the system of linear equations by substitution.
(A better method well be given later.)

@col
By the last equation:
\[x_{4}=5-x_{5}.\]
Solve $x_{2}$ in terms of other variables by equation 2:
\[\displaystyle x_{2}=2-x_{3}-2x_{4}-2x_{5}\]
\[\displaystyle=2-x_{3}-2(5-x_{5})-2x_{5}\]
\[\displaystyle=-8-x_{3}\]
@col
Solve $x_{1}$ in terms of other variables by equation 1:
\[\displaystyle x_{1}=4-2x_{2}-3x_{3}-2x_{4}-3x_{5}\]
\[\displaystyle=4-2(-8-x_{3})-3x_{3}-2(5-x_{5})-3x_{5}\]
\[\displaystyle=10-x_{3}-x_{5}.\]
$x_{3},x_{5}$ can be taken as any values.

@col
Set $x_{3}=a$, $x_{5}=b$, the solution set can be given by
\[\{(10-a-b,-8-a,a,5-b,b)\,|\,a,b\text{ real numbers}\}.\]
@col
<strong>A better method:</strong>
Instead of subsitution, we could use more elimination:
\[
\begin{alignedat}{6}
x_1 & {}+{} &  2x_2 & {}+{} & 3x_3 & {}+{} & 2x_4 &  {}+{}&  3x_5&{}={} & 4 \\
& {} {} &  x_2 & {}+{} & x_3 & {}+{} & 2x_4   & {}+{} & 2x_5 &{}={} & 2 \\
& {} {} &     & {}  {} &     & {} {} & x_4 &  {}+{}  & x_5 &{}={} & 5
\end{alignedat}
\]
@col
-2 times equation 3 and add it to equation 2:
\[
\begin{alignedat}{6}
x_1 & {}+{} &  2x_2 & {}+{} & 3x_3 & {}+{} & 2x_4 &  {}+{}&  3x_5&{}={} & 4 \\
& {} {} &  x_2 & {}+{} & x_3 & {} {} &   & {} {} &  &{}={} & -8 \\
& {} {} &     & {}  {} &     & {} {} & x_4 &  {}+{}  & x_5 &{}={} & 5
\end{alignedat}
\]
@col
-2 times equation 3 and add it to equation 1:
\[
\begin{alignedat}{6}
x_1 & {}+{} &  2x_2 & {}+{} & 3x_3 & {}+{} &     &  {}+{}&  x_5&{}={} & -6 \\
& {} {} &  x_2 & {}+{} & x_3 & {} {} &   & {} {} &  &{}={} & -8 \\
& {} {} &     & {}  {} &     & {} {} & x_4 &  {}+{}  & x_5 &{}={} & 5
\end{alignedat}
\]
@col
-2 times equation 2 and add it to equation 1:
\[
\begin{alignedat}{6}
x_1 & {}+{} &      & {}+{} & x_3 & {} {} &     &  {}+{}&  x_5&{}={} &10 \\
& {} {} &  x_2 & {}+{} & x_3 & {} {} &   & {} {} &  &{}={} & -8 \\
& {} {} &     & {}  {} &     & {} {} & x_4 &  {}+{}  & x_5 &{}={} & 5
\end{alignedat}
\]
@col
Notice the following:
@enumerate
@item
@newcol
The system of equations looks like an ”inverted” stairs.
@endcol
@item
@newcol
The leftmost variables in the equations are $x_{1}$, $x_{2}$ and $x_{4}$.
@endcol
@item
@newcol
Only the first equation has variable $x_{1}$.
@endcol
@item
@newcol
Only the second equation has variable $x_{2}$.
@endcol
@item
@newcol
Only the third equation has variable $x_{4}$.
@endcol
@endenumerate
@col
Move $x_{3},x_{5}$ to another side.
\begin{align*}
 x_1 &= 10-x_3-x_5\\
 x_2 &= -8-x_3\\
 x_4 &= 5-x_5
\end{align*}
The right hand sides have $x_{3},x_{5}$ as variables only and $x_{3},x_{5}$ can be taken as any values.
Set $x_{3}=a$, $x_{5}=b$, the solution set can be given by
\[\{(10-a-b,-8-a,a,5-b,b)\,|\,a,b\text{ real numbers}\}.\]
@end
@section{Some Observations}
@ul
@li
Some systems of linear equations have exactly one solution.
@li
Some systems of linear equations have no solution.
@li
Some systems of linear equations have infinite many solutions. The solutions can be expressed in terms of 1 or 2 (or even more) variables.
@endul
@setchapter{2}
@chapter{Geometric interpretation of linear equations and vector spaces}
$
\newcommand{\R}{\mathbf{R}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\ol}{\overline}
\newcommand{\mat}[4]{\begin{bmatrix} {#1} & {#2} \\ {#3} & {#4}\end{bmatrix}}
\newcommand{\vvec}[2]{\begin{bmatrix}{#1} \\ {#2} \end{bmatrix}}
\newcommand{\hvec}[2]{\begin{bmatrix}{#1} & {#2} \end{bmatrix}}
\newcommand{\e}{\varepsilon}
$
@section{
Intersections between Lines and Planes
}
@subsection{
Two Dimensions
}
@eg
@label{ex:1}
Solve the equations:
\[
\begin{cases}
2x-3y=-5,\\ x+y=5.
\end{cases}
\]
@col
There are two variables and we can draw these equations in a (2-dimensional) plane.
Each (linear) equation represented by a straight line.
For the line that corresponds to $2x-3y=-5$, it passes through the point $(0,5/3)$ (take $x=0$ and one gets $y=5/3$) and another the point $(-5/2,0)$. Hence it is determined.
Similarly, the second straight line is the one passes through the points $(0,5)$ and $(5.0)$.
@figure
<iframe src="https://www.desmos.com/calculator/ivgmstarrz?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>
@caption{Intersection between lines}
@end
The solution of the two equations corresponds to the unique intersection $(2,3)$
between the two straight lines.


@slide
@eg
@label{ex:2}
The following system of equations has no solution.
\[
\begin{cases}
2x-3y=-5,\\ 2x-3y=0.
\end{cases}
\]
@col
@figure
<iframe src="https://www.desmos.com/calculator/fuhvpsxixz?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>
@caption{Parallel lines}
@end
This is because the corresponding lines are parallel to each other
and there is no intersections.
@end

@eg
@label{ex:3}
@newcol
How about the system of equations:
\[\begin{cases}
2x-3y=-5,\\2x-3y=-5.
\end{cases}\]
Here, there are infinite solutions:
For any real number $t$,
\[
(x,y)=(t,\frac{2t+5}{3})
\]
is a solution.
The two lines coincide and their intersection is just the whole line.
@endcol
@end


@subsection{
Three Dimensions
}

@eg
Solve the equations:
\[
\begin{cases}
2x+3y+3z=3,\\ 2x-y+0z=0.
\end{cases}
\]
@col
@figure
@label{fig:3d}
@caption{Intersection of planes}
<iframe style="width:100%;height:700px" frameborder="0" src="https://www.math.cuhk.edu.hk/~pschan/surfaceplot/index.php?sidebar=0&zscale=0&domain=2&numsamples=20&autozscale=1&showaxes=1&centeredaxes=1&xmin=-2&xmax=2&ymin=-2&ymax=2&zmin=-10&zmax=10&xticks=5&yticks=9&zticks=10&equations[0]='1%20-%20(2%2F3)x%20-%20y'&colors[0]='6cb8d9'&alphas[0]=0.9&equations[1]='s%2C%202s%2C%20t'&sdomain[1]='-1,1'&tdomain[1]='-2,3'&colors[1]='d998b7'&alphas[1]=0.9&rotationMatrix=[-0.35,-0.08,0.93,0,0.93,-0.18,0.33,0,0.14,0.99,0.14,0,0,0,0,1]&dimensions=[600,600]"/>
@end

@col
There are three variables and we work in 3-dimensional space.
Each (linear) equation corresponds to a plane.
For instance, the ones above correspond to the blue and red planes
in the figure above.

@col
Their intersection is the thick straight line,
where the points on it can be expressed as
\[
\left\{\left(\frac{3-3t}{8},\frac{3-3t}{4},t\right)\mid t\in\mathbb{R}\right\}.
\]
@end


@slide
@eg
The following system of equations has no solutions, since the corresponding planes are parallel.
\[\begin{cases}
2x-y+0z=2,\\ 2x-y+0z=0.
\end{cases}\]
@col
@figure
<iframe style="width:100%;height:700px" frameborder="0" src="https://www.math.cuhk.edu.hk/~pschan/surfaceplot/index.php?sidebar=0&zscale=0&domain=1&numsamples=20&autozscale=1&showaxes=1&centeredaxes=1&xmin=-1&xmax=1&ymin=-1&ymax=1&zmin=-10&zmax=10&xticks=5&yticks=9&zticks=10&equations[0]='s%2C%202s%2C%20t'&sdomain[0]='-1,1'&tdomain[0]='-1,1'&colors[0]='d998b7'&alphas[0]=0.9&equations[1]='s%2C%202s%20-%202%2C%20t'&sdomain[1]='-1,1'&tdomain[1]='-1,1'&colors[1]='1dd1d9'&alphas[1]=0.9&rotationMatrix=[-0.76,-0.4,0.52,0,0.66,-0.31,0.7,0,-0.12,0.87,0.49,0,0,0,0,1]&dimensions=[600,600]"/>
@caption{Parallel Planes}
@end
@end
@slide
@eg
@title{Singular Case}
Suppose that there are three (linear) equations with three variables
that corresponds to three distinguish planes.
If they do not share a unique intersection,
then either they share no intersections or they intersection is a line.
More precisely, there are five cases:
@ol
@li
All three planes are parallel to each other.
We shall illustrate this by the following picture.
@figure
@caption{}
<center>
<script type="text/tikz">
\begin{tikzpicture}[rotate=0,scale=1,arrow/.style={->,>=stealth,thick}]
\draw[red, thick](0,3)--(1,0)(1,3)--(2,0)(2,3)--(3,0);
\end{tikzpicture}
</script>
</center>
@end
@li
Only two planes are parallel to each other.
@figure
@caption{}
<center>
<script type="text/tikz">
\begin{tikzpicture}[rotate=0,scale=1,arrow/.style={->,>=stealth,thick}]
\draw[red, thick](0,3)--(1,0)   (1,3)--(2,0);
\draw[blue, thick](-1,0)--(3,3);
\end{tikzpicture}
</script>
</center>
@end
@li
The intersection of each pair of planes is a line and
three such lines are parallel to each other.
@figure
@caption{}
<center>
<script type="text/tikz">
\begin{tikzpicture}[rotate=0,scale=1,arrow/.style={->,>=stealth,thick}]
\draw[blue, thick](90+10:2)--(-30-10:2)
(90-10:2)--(210+10:2)
(-30+10:2)--(210-10:2);
\end{tikzpicture}
</script>
</center>
@end
@li
Their intersection is a line.
@figure
@caption{}
<center>
<script type="text/tikz">
\begin{tikzpicture}[rotate=0,scale=1,arrow/.style={->,>=stealth,thick}]
\draw[blue, thick](90+30:2)--(-30-30:2)
(90-30:2)--(210+30:2)
(-30+30:2)--(210-30:2);
\end{tikzpicture}
</script>
</center>
@end
@li
Their intersection is a point, e.g. the xy-plane, yz-plane and zx-plane intersect at $(0,0,0)$.
@figure
@caption{}
<center>
<script type="text/tikz">
\begin{tikzpicture}[scale=3,arrow/.style={->,>=stealth,thick}]
\filldraw[draw=red,fill=red!20]
(0,0,0)--(0,1.2,0)--(0,1.2,1.2)--(0,0,1.2)-- cycle;
\filldraw[draw=blue,fill=blue!20]
(1.7,0,0)--(1.7,1.2,0)--(0,1.2,0)--(0,0,0)-- cycle;
\filldraw[draw=yellow,fill=yellow!20]
(1.7,0,0)--(1.7,0,1.2)--(0,0,1.2)--(0,0,0)-- cycle;
\filldraw[draw=red,fill=red!20]
(0,0,0)--(0,1.2,0)--(0,1.2,1.2)--(0,0,1.2)-- cycle;
\draw[thin,->] (0,0,0) -- (1.7,0,0) node[anchor=north east]{$x$};
\draw[thin,->] (0,0,0) -- (0,1.2,0) node[anchor=north west]{$y$};
\draw[thin,->] (0,0,0) -- (0,0,1.2) node[anchor=south]{$z$}
(0,0,0)node[left]{$(0,0,0)$};
\end{tikzpicture}
</script>
</center>
@end
@endol
@end
@remark

<h5>Higher Dimensions</h5>
How does this row picture extend into $n$ dimensions? The $n$ equations will contain
$n$ unknowns. The first equation still determines a plane. It is no longer a two dimensional
plane in 3-space; somehow it has dimension $n-1$. It must be flat and
extremely thin within $n$-dimensional space, although it would look solid to us.


If time is the fourth dimension, then the plane t = 0 cuts through four-dimensional
space and produces the three-dimensional universe we live in (or rather, the universe as
it was at $t =0$). Another plane is $z=0$, which is also three-dimensional; it is the ordinary
$x-y$ plane taken over all time. Those three-dimensional planes will intersect! They share
the ordinary $x-y$ plane at $t = 0$. We are down to two dimensions, and the next plane
leaves a line. Finally a fourth plane leaves a single point. It is the intersection point of 4
planes in 4 dimensions, and it solves the 4 underlying equations.


I will be in trouble if that example from relativity goes any further. The point is that
linear algebra can operate with any number of equations. The first equation produces an
$(n-1)$-dimensional plane in $n$ dimensions, The second plane intersects it (we hope) in
a smaller set of dimension $n-2$. Assuming all goes well, every new plane (every new
equation) reduces the dimension by one. At the end, when all $n$ planes are accounted
for, the intersection has dimension zero. It is a point, it lies on all the planes, and its
coordinates satisfy all $n$ equations. It is the solution!
@end


@section{
Column Pictures: Vector Point of View
}
<strong>Skipped. Can be discussed after introducing column vectors.</strong>
@eg
Back to the equations in @ref{ex:1}, which can be written
in a single vector form equation:
\[
x\vvec{2}{1}+y\vvec{-3}{1}=\vvec{-5}{5}=b.
\]
@col
The problem expressed this way is to find the combination of the column vectors on the left side that
produces the vector $b$ on the right side.
Note that the vector $b$ is in fact identified with the pint whose coordinates are $-5$ and $5$.
In this case, we look for the combination of $\vvec{2}{1}$ (red vector)
and $\vvec{-3}{1}$ (blue vector) to produce $\vvec{-5}{5}$ (black vector),
cf. Figure  @ref{fig:vec}.

@figure
@label{fig:vec}
@caption{Combinations of vectors}
<div class='image'>
<img src='content/math1030/fig_vec_com.png'/>
</div>
@end
@end

@slide
@eg
@col
In  @ref{ex:2} and  @ref{ex:3},
the corresponding equations are
\[
x\vvec{2}{2}+y\vvec{-3}{-3}=\vvec{-5}{0}.
\]
and
\[
x\vvec{2}{2}+y\vvec{-3}{-3}=\vvec{-5}{-5},
\]
respectively.
The two vectors colinear.
When the target vector is not colinear with them, there is no solution ( @ref{ex:2})
and when the target vector is colinear with them,
there are infinitely many solutions (@ref{ex:3}).
@figure
@caption{}
<center>
<script type="text/tikz">
\begin{tikzpicture}[domain=0:4,scale=.8,arrow/.style={->,>=stealth,thick}]
\draw[ultra thick,->] (-3,-3)to(-5,-5) node[below] {(-5,-5)};
\draw[very thin,dashed,color=gray] (-5.5,-5.5) grid (3.5,3.5);
\draw[thin,->] (-5,0) -- (3,0) node[right] {$x$};
\draw[thin,->] (0,-5) -- (0,3) node[above] {$y$};
\draw[red,very thick,->] (0,0)to(2,2) node[above] {(2,2)};
\draw[blue,very thick,->] (0,0)to(-3,-3) node[right] {(-3,-3)};
\draw[ultra thick,->] (0,0)to(-5,0) node[above] {(-5,0)};
\end{tikzpicture}
</script>
</center>
@end

@end

@eg
@col
Let us check a 'trivial' 3-dimensional case.
\[x\begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix}+
y\begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}+
z\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}=
\begin{bmatrix}3 \\ 5 \\ 4\end{bmatrix}.
\]

@col
@figure
@label{fig:cube}
@caption{Vectors in 3 Dimensions}
<center>
<img src='content/math1030/fig_cube.png'/>
</center>
@end

<!--
<script type="text/tikz">
\begin{tikzpicture}[scale=2.7,arrow/.style={->,>=stealth,thick}]
\draw[thin,->] (0,0,0) -- (1.7,0,0) node[anchor=north east]{$x$};
\draw[thin,->] (0,0,0) -- (0,1.2,0) node[anchor=north west]{$y$};
\draw[thin,->] (0,0,0) -- (0,0,1.6) node[anchor=south]{$z$};
\draw[very thick,red,->](0,0,0)edge(.5,0,0)edge(0,.23,0)edge(0,0,.3);
\draw[ultra thick,blue,->](0,0,0)node{$\bullet$}to(1.5,1,1.5)node[above]{(3, 4, 5)};
\draw[thin,violet](1.5,1,1.5)edge(1.5,1,0)edge(0,1,1.5)edge(1.5, 0, 1.5);
\draw[thin,violet](1.5,1,0)edge(0,1,0)edge(1.5, 0, 0);
\draw[thin,violet](0,1,1.5)edge(0,1,0)edge(0, 0, 1.5);
\draw[thin,violet](1.5,0,1.5)edge(1.5,0,0)edge(0, 0, 1.5);
\draw[red](.5,0,0)node[above left]{(1, 0, 0)}(0, .25, 0)node[above right]{(0, 1, 0)}(0, 0, .3)node[left]{(0, 0, 1)};
\end{tikzpicture}
</script>
-->

@col
One sees that the solution is $x=3, y=4,z=5$,
which corresponds to the scaling of the red vectors so that
the sum of the rescaled vectors is the blue vector in @ref{fig:cube}.
More precisely, we have multiplication by scalars
\[
%    \text{\bf{}}:
3\begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix}=\begin{bmatrix}3 \\ 0 \\ 0\end{bmatrix},\quad
4\begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}=\begin{bmatrix}0 \\ 4 \\ 0\end{bmatrix},\quad
5\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}=\begin{bmatrix}0 \\ 0 \\ 5\end{bmatrix},
\]
and the vector addition
\[\begin{bmatrix}3 \\ 0 \\ 0\end{bmatrix}+
\begin{bmatrix}0 \\ 4 \\ 0\end{bmatrix}+
\begin{bmatrix}0 \\ 0 \\ 5\end{bmatrix}=
\begin{bmatrix}3 \\ 5 \\ 4\end{bmatrix}.
\]
@end

@eg
@col
Let us see a non-trivial case.
\[x\begin{bmatrix}2 \\ 4 \\ -2\end{bmatrix}+
y\begin{bmatrix}1 \\ -6 \\ 7\end{bmatrix}+
z\begin{bmatrix}1 \\ 0 \\ 2\end{bmatrix}=
\begin{bmatrix}5 \\ -2 \\ 9\end{bmatrix}.
\]
The corresponding vectors are pictured as follows:

@figure
@caption{}
@label{fig:cube_nontrivial}
<center>
@image{content/math1030/fig_cube_nontrivial.png}
<!--
<script type="text/tikz">
\begin{tikzpicture}[scale=.3,arrow/.style={->,>=stealth,thick}]
\draw[thin,->] (0,0,0) -- (14,0,0) node[anchor=north east]{$x$};
\draw[thin,->] (0,0,0) -- (0,12,0) node[anchor=north west]{$y$};
\draw[thin,->] (0,0,0) -- (0,0,8) node[anchor=south]{$z$};
\draw[red,thick,->](0,0,0)node{$\bullet$}to(2,4,-2)node[below]{(2, 4, -2)};
\draw[red,thick,->](0,0,0)node{$\bullet$}to(1,-6,-7)node[left]{(1, -6, -7)};
\draw[red,thick,->](0,0,0)node{$\bullet$}to(2,0,4)node[right]{(2, 0, 4)};
\draw[ultra thick,blue,->](0,0,0)node{$\bullet$}to(5,-2,9)node[above]{(5, -2, 9)};
\end{tikzpicture}
\caption{}
</script>
-->
</center>
@end
@end

@eg
@col
Suppose that there are three (linear) equations with three variables.
In the column picture, what it happens to the singular case.
So we have three vectors and try to combine them to produce vector $b$.


When the three column vectors are in the same plane $P$
any combination of them falls into $P$.
For instance, the vectors in the following equation are in the plane
$2x+3y+3z=0$.
\[x\begin{bmatrix}\frac{3}{2} \\ -1 \\ 0\end{bmatrix}+
y\begin{bmatrix}0 \\ 1 \\ -1\end{bmatrix}+
z\begin{bmatrix}\frac{3}{2} \\ 0 \\ -1\end{bmatrix}=
b.\]
Then there are two cases:

@itemize
@item
<strong>No solution:</strong>
When
$$b=b_0=\begin{bmatrix}0\\1\\1\end{bmatrix}$$
is not in $P$, then there is no solution.

@item
<strong>Infinite solution:</strong>
When $$b=b_\infty=\begin{bmatrix}3\\ -1\\ -1\end{bmatrix}$$
is in $P$, it may have infinite solution.
@enditemize
@end

@remark
@col
In general, if the $n$ planes have no point in common,
or infinitely many points, then the corresponding $n$
columns lie in the same plane.
@end
@setchapter{3}
@chapter{System of linear equations}
The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html Print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf


<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Sect SSLE (print version p7 - p14)
@section{
Introduction
}
@defn
@title{ (System of Linear Equations).}
A <b>system of linear equations</b> is a collection of $m$ equations in the variables $x_{1},\,x_{2},\,x_{3},\ldots,x_{n}$ of the form:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=b_1\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=b_2\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=b_3\\
&\vdots\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=b_m
\end{align*}
where $a_{ij}$, $b_{i}$ and $x_{j}$, $1\leq i\leq m$, $1\leq j\leq n$, are real numbers.
@end
@defn
$(s_{1},s_{2},\ldots,s_{n})$ is a <b>solution</b> of a system of linear equations in $n$ variables if every equation in the system is valid for $x_1 = s_1, x_2 = s_2, \ldots, x_n = s_n$.
The <b>solution set</b> of a linear system of equations is the set consisting of all solutions to the system, and nothing more.
@end
@slide
@eg
The following system of linear equations
\begin{align*}
x_1+2x_2+ x_4&= 7\\
x_1+x_2+x_3-x_4&=3\\
3x_1+x_2+5x_3-7x_4&=1
\end{align*}
can be rewritten as:

@newcol
\begin{align*}
1x_1+2x_2 + 0 x_3 + 1x_4&= 7\\
1x_1+1x_2+1x_3-1x_4&=3\\
3x_1+1x_2+5x_3-7x_4&=1
\end{align*}
So it is a system of linear equations, with $n=4$ variables and $m=3$ equations. Also,
\begin{align*}
a_{11}&=1 & a_{12}&=2 & a_{13}&=0 & a_{14}&=1 & b_{1}&=7\\
a_{21}&=1 & a_{22}&=1 & a_{23}&=1 & a_{24}&=-1 & b_{2}&=3\\
a_{31}&=3 & a_{32}&=1 & a_{33}&=5 & a_{34}&=-7 & b_{3}&=1
\end{align*}One solution is given by $x_{1}=-2$, $x_{2}=4$, $x_{3}=2$, $x_{4}=1$. In fact, from the previous lecture this system of equations has infinitely many solutions.

The solution set may be descrbied as follows:

@col
\[
\{(-1-2a+3b,\,4+a-2b,\,a,\,b)\,|\,a,b \in \mathbb{R}\}.
\]
@endcol
@end
@section{
Possibilities for Solution Sets
}
@eg
@enumerate
@item
The following system of linear equations has <b>only one solution</b>.
\begin{align*}
2x_1+3x_2&=3\\
x_1-x_2&=4
\end{align*}
The solution set is $(x_{1},x_{2})=(3,-1)$
@item
@newcol
The following system of linear equations has <b>infinite many solutions</b>.
\begin{align*}
2x_1+3x_2&=3\\
4x_1+6x_2&=6
\end{align*}
The solution set is $\{(x_{1},x_{2})=(t,\frac{3-2t}{3})\}$, where $t$ is any real number.
@endcol
@item
@newcol
The following system of linear equations has <b>no solutions</b>.
\begin{align*}
2x_1+3x_2&=3\\
4x_1+6x_2&=10
\end{align*}
The solution set is empty.
@endcol
@endenumerate
@end
@thm
@newcol
A system of linear equations can have (1) a unique solution or (2) infinitely many solutions
or (3) no solutions.
@endcol
@end
@remark
@newcol
For example it is impossible for a system of     linear equation to have exactly $2$ solutions.
@endcol
@end


@section{
Equivalent Systems and Equation Operations
}
@defn
@title{Equivalent Systems}
Two systems of linear equations are <b>equivalent</b> if their solution sets are equal.
@end
@defn
@title{Equation Operations}
@label{EO}
@newcol
Given a system of linear equations, the following three operations will transform the system into a different one, and each operation is known as an
<b>equation operation</b>:
@ol
@li
Swap the locations of two equations in the list of equations.
@li
Multiply each term of an equation by a nonzero quantity.
@li
Multiply each term of one equation by some quantity, and add these terms to a second equation, on both sides of the equality.
@endol
@endcol
@end
@eg
@newcol
The following two systems equations are equivalent:
\[\begin{cases}2x_{1}+3x_{2}=3\\
x_{1}-x_{2}=4\end{cases}\]
and
\[\begin{cases}5x_{2}=-5\\
x_{1}-x_{2}=4\end{cases}\]
@col
In fact, the second system of linear equations is obtained by applying operation 3 on equation 1 (namley, replace equation 1 with equation 1 - 2 $\times$ equation 2).
@endcol
@end
@thm
@title{Equation Operations Preserve Solution Sets}
@label{EOPSS}
If we apply one of the three equation operations of Definition          @ref{EO} to a system of linear equations, then the original system and the transformed system are equivalent.
@end
@proof
@title{(Sketch)}
@newcol
Let $S$ be a system of linear equations and $S^{\prime}$ be another system of linear equations obtained by applying equation operations on $S$.


Let $(x_{1},\ldots,x_{n})$ be a solution for $S$. Because $S^{\prime}$ is obtained by equation operations on $S$,


@col
it is obvious that $(x_{1},\ldots,x_{n})$ is also a solution for $S^{\prime}$. Conversely, suppose that $(x_{1},\ldots,x_{n})$ is a solution for $S^{\prime}$.
Note that the reverse of the equation operations are also equation operations.


@col
Reversing the equation operators on $S$, we can show that $S$ can be obtained by equation operations on $S^{\prime}$. So $(x_{1},\ldots,x_{n})$ is also a solution for $S$.


@col
Hence $S$ and $S^{\prime}$ have the same solution set.


@col
For details, See Beezer, Theorem EOPSS (Ver 3.5, print version p. 10).
@endcol
@end
@setchapter{4}
@chapter{Matrices}
The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.htmlPrint version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf

<br/>
<h5 class="notkw">Reference.</h5>
<ul>
<li>
Beezer, Ver 3.5 Subsection MVNSE (print version p17 - p21)
</li>
<li>
Strang, Sect 1.4
</li>
</ul>
@section{
Introduction
}
@itemize
@item
After solving a few systems of equations, you will recognize that it does not matter so much what we call our variables.
@item
A system in the variables $x_{1},\,x_{2},\,x_{3}$ would behave the same if we changed the names of the variables to $a,\,b,\,c$ and kept all the constants the same and in the same places.
@item
In this section, we will isolate the key bits of information about a system of equations into something called a <b>matrix</b>, and then use this matrix to systematically solve the equations. Along the way we will obtain one of our most important and useful computational tools.
@enditemize
@section{
Matrix and Vector Notation for Systems of Equations
}
@defn
@title{Matrix}
@label{M}
An $m\times n$ <b>matrix</b> is a rectangular layout of real numbers with $m$ rows and $n$ columns.
@itemize
@item
@newcol
Many people use large parentheses instead of brackets – the distinction is not important.
@endcol
@item
@newcol
Rows of a matrix are indexed from the top (with the first row at the top labeled "row 1"), and columns are indexed from the left (with the first column on left labeled "column 1").
@endcol
@item
@newcol
For a matrix $A$, the notation $\left[A\right]_{ij}$, or $A_{ij}$, $A_{i,j}$, refers to the number in row $i$ and column $j$ of $A$.
@endcol
@enditemize
@end
@eg
@newcol
\[B=\begin{bmatrix}-1&2&5&3\\
1&0&-6&1\\
-4&2&2&-2\end{bmatrix}\]
is a matrix with $m=3$ rows and $n=4$ columns. We can say that $\left[B\right]_{2,3}=-6$ while $\left[B\right]_{3,4}=-2$.
@endcol
@end
@slide
When we do equation operations on a system of equations, the names of the variables really are not very important. Whether we use $x_{1}$, $x_{2}$, $x_{3}$, or $a$, $b$, $c$, or $x$, $y$, $z$ does not matter so much. In this subsection we will describe some notation that will make it easier to describe linear systems, solve the systems and describe the solution sets.
@slide
@defn
@title{Column Vector}
@label{CV}
@itemize
@item
A <b>column vector</b> of <b>size</b> $m$ is an ordered list of $m$ numbers, which is written in order vertically from top to bottom. We often refer to a column vector as simply a <b>vector</b>.
@item
@newcol
In these notes,
a column vector are typically represented by a bold faced, lower-case Roman letter, e.g. $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$, $\mathbf{x}$, $\mathbf{y}$, $\mathbf{z}$, etc.
@endcol
@item
@newcol
Some authors prefer representing vectors with arrows, such as $\vec{u}$. Writing by hand, some like to put arrows on top of the symbol, or a tilde underneath the symbol, as in $\underset{\sim}{\textstyle u}$, or a line under the symbol, as $\underline{\textstyle u}$.
@endcol
@item
@newcol
To refer to $i$-th <b>entry</b> or <b>component</b> of a vector $\mathbf{v}$, we write $\left[\mathbf{v}\right]_{i}$ or $\mathbf{v}_i$.
@endcol
@enditemize
@end
@defn
@title{ (Zero Column Vector).}
@label{ZCV}
@col
The <b>zero vector</b> of size $m$ is the column vector of size $m$ where each entry is the number zero,
\[\mathbf{0}=\begin{bmatrix}0\\
0\\
0\\
\vdots\\
0\end{bmatrix}\]
or defined much more compactly, $\left[\mathbf{0}\right]_{i}=0$ for $1\leq i\leq m$.
@end
@section{
Partition of Matrices
}
Sometimes we use horizontal or vertical lines to visually divide a matrix into different areas. (Mathematically it is still the same object.)
@eg
@newcol
The matrix
\[
\left[\begin{array}[]{ccccc}
1&2&3&4&3.5\\
0&-1&1&1.1&1\\
3&5.8&1&0&-3\\
1&8&0&0&7\end{array}\right]
\]
is same as:
\[
\left[\begin{array}[]{cccc|c}1&2&3&4&3.5\\
0&-1&1&1.1&1\\
3&5.8&1&0&-3\\
1&8&0&0&7\end{array}\right]
\;,
\left[\begin{array}[]{c|c|c|c|c}1&2&3&4&3.5\\
0&-1&1&1.1&1\\
3&5.8&1&0&-3\\
1&8&0&0&7\end{array}\right],
\]
\[
\left[\begin{array}[]{ccccc}1&2&3&4&3.5\\
0&-1&1&1.1&1\\
\hline 3&5.8&1&0&-3\\
1&8&0&0&7\end{array}\right],
\;
\left[\begin{array}[]{cc|ccc}1&2&3&4&3.5\\
0&-1&1&1.1&1\\
\hline 3&5.8&1&0&-3\\
1&8&0&0&7\end{array}\right],
\]
\[
\left[\begin{array}[]{cc|ccc}1&2&3&4&3.5\\
0&-1&1&1.1&1\\
\hline 3&5.8&1&0&-3\\
\hline 1&8&0&0&7\end{array}\right]
\]
@endcol
@end
@eg
@col
One can also form <b>augmented matrices</b> as follows:

@col
\[A=\begin{bmatrix}1&2\\
3&4\\
5&6\\
7&8\end{bmatrix},\,\,\,\mathbf{u}=\begin{bmatrix}9\\
10\\
11\\
12\end{bmatrix}\,\,\,\mathbf{v}=\begin{bmatrix}13\\
14\\
15\\
16\end{bmatrix},\]
\[[A|\mathbf{u}]=\left[\begin{array}[]{cc|c}1&2&9\\
3&4&10\\
5&6&11\\
6&8&12\end{array}\right]=\left[\begin{array}[]{ccc}1&2&9\\
3&4&10\\
5&6&11\\
6&8&12\end{array}\right],\]
\[[A|\mathbf{u}|\mathbf{v}]=\left[\begin{array}[]{cc|c|c}1&2&9&13\\
3&4&10&14\\
5&6&11&15\\
6&8&12&15\end{array}\right]=\left[\begin{array}[]{cccc}1&2&9&13\\
3&4&10&14\\
5&6&11&15\\
6&8&12&15\end{array}\right],\]
@end
@eg
@col
\[A=\begin{bmatrix}1&2\\
3&4\end{bmatrix},\,\,B=\begin{bmatrix}5&6&7\\
8&9&10\end{bmatrix},\]
\[C=\begin{bmatrix}11&12\\
13&14\\
15&16\end{bmatrix},D=\begin{bmatrix}21&22&23\\
24&25&26\\
27&28&29\end{bmatrix}.\]
\[[A|B]=\left[\begin{array}[]{cc|ccc}1&2&5&6&7\\
3&4&8&9&10\end{array}\right]=\left[\begin{array}[]{ccccc}1&2&5&6&7\\
3&4&8&9&10\end{array}\right],\]
\[\left[\begin{array}[]{c|c}A&B\\
\hline C&D\end{array}\right]=\left[\begin{array}[]{cc|ccc}1&2&5&6&7\\
3&4&8&9&10\\
\hline 11&12&21&22&23\\
13&14&24&25&26\\
15&16&27&28&29\end{array}\right]=\left[\begin{array}[]{ccccc}1&2&5&6&7\\
3&4&8&9&10\\
11&12&21&22&23\\
13&14&24&25&26\\
15&16&27&28&29\end{array}\right]\]
@end
@section{
Matrix Representations of Linear Systems
}
The following definitions are stated in the context of
the following system of linear equations:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=b_1\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=b_2\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=b_3\\
\vdots&\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=b_m
\end{align*}
@defn
@title{Coefficient Matrix}
@label{CM}
@col
The <b>coefficient matrix</b> is the $m\times n$ matrix:
\[A=\begin{bmatrix}a_{11}&a_{12}&a_{13}&\dots&a_{1n}\\
a_{21}&a_{22}&a_{23}&\dots&a_{2n}\\
a_{31}&a_{32}&a_{33}&\dots&a_{3n}\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{m1}&a_{m2}&a_{m3}&\dots&a_{mn}\\
\end{bmatrix}\]
@end
@defn
@title{Vector of Constants}
@label{VOC}
@col
The <b>vector of constants</b> is the column vector of size $m$
\[\mathbf{b}=\begin{bmatrix}b_{1}\\
b_{2}\\
b_{3}\\
\vdots\\
b_{m}\\
\end{bmatrix}\]
@end
@defn
@title{Solution Vector}
@label{SOLV}
@col
The <b>solution vector</b> is the column vector of size $n$
\[\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
\vdots\\
x_{n}\\
\end{bmatrix}\]
@end
@defn
@title{Matrix Representation of a Linear System}
@label{MRLS}
@col
If $A$ is the coefficient matrix of a system of linear equations and $\mathbf{b}$ is the vector of constants, then we will write $\mathcal{LS}(A,\mathbf{b})$ as a shorthand expression for the system of linear equations, which we will refer to as the <b>matrix representation</b> of the linear system.
@end
@slide
@defn
@title{Augmented Matrix}
@label{AM}
Suppose we have a system of $m$ equations in $n$ variables, with coefficient matrix $A$ and vector of constants $\mathbf{b}$. Then the <b>augmented matrix</b> of the system of equations is the $m\times(n+1)$ matrix whose first $n$ columns are the columns of $A$ and whose last column ($n+1$) is the column vector $\mathbf{b}$. This matrix will be written as $\left[A|\mathbf{b}\right]$.
@end
@eg
@title{Notation for systems of linear equations}
@col
The system of linear equations
\[\displaystyle 2x_{1}+4x_{2}-3x_{3}+5x_{4}+x_{5}=9\]
\[\displaystyle 3x_{1}+x_{2}+\quad\quad x_{4}-3x_{5}=0\]
\[\displaystyle-2x_{1}+7x_{2}-5x_{3}+2x_{4}+2x_{5}=-3\]
has coefficient matrix:

@col
\[A=\left[\begin{array}[]{ccccc}2&4&-3&5&1\\
3&1&0&1&-3\\
-2&7&-5&2&2\end{array}\right]\]
and vector of constants:

@col
\[\mathbf{b}=\begin{bmatrix}9\\
0\\
-3\end{bmatrix}\]
and so will be referenced as $\mathcal{LS}(A, \mathbf{b})$.
The augmented matrix is
\[\left[A|\mathbf{b}\right]=\left[\begin{array}[]{ccccc|c}2&4&-3&5&1&9\\
3&1&0&1&-3&0\\
-2&7&-5&2&2&-3\end{array}\right]\]
@end
@section{
Row operations
}
An augmented matrix can be used to represent a system of linear equations and release us from writing out all the variables.
We have seen how certain operations we can perform on equations will preserve their solutions .
The next two definitions and the following theorem carry over these ideas to augmented matrices.
@defn
@title{Row Operations}
@label{RO}
The following three operations will transform an $m\times n$ matrix into a different matrix of the same size, and each is known as a <b>row operation</b>.
@enumerate
@item
@col
Swap the locations of two rows.
@item
@col
Multiply each entry of a single row by a nonzero quantity.
@item
@col
Multiply each entry of one row by some quantity, and add these values to the entries in the same columns of a second row. Leave the first row the same after this operation, but replace the second row by the new values.
@endenumerate
We will use a symbolic shorthand to describe these row operations:
@enumerate
@item
@col
$R_{i}\leftrightarrow R_{j}$: Swap the location of rows $i$ and $j$.
@item
@col
$\alpha R_{i}$: Multiply row $i$ by the nonzero scalar $\alpha$.
@item
@col
$\alpha R_{i}+R_{j}$: Multiply row $i$ by the scalar $\alpha$ and add to row $j$.
@endenumerate
@end
@defn
@title{Row-Equivalent Matrices}
@label{REM}
@col
Two matrices, $A$ and $B$, are <b>row-equivalent</b> if one can be obtained from the other by a sequence of row operations.
@end
@remark
@col
Notice that each of the three row operations is reversible, so we do not have to be careful about the distinction between $A$ is row-equivalent to $B$ and $B$ is row-equivalent to $A$.
@end
@slide
@eg
The matrices:
\begin{align*}
A=\begin{bmatrix}
2&-1&3&4\\
5&2&-2&3\\
1&1&0&6
\end{bmatrix}
&&
B=\begin{bmatrix}
1&1&0&6\\
3&0&-2&-9\\
2&-1&3&4
\end{bmatrix}
\end{align*}
are row-equivalent, as can be seen from:

@col
@steps
\begin{align*}
& \begin{bmatrix}
2&-1&3&4\\
5&2&-2&3\\
1&1&0&6
\end{bmatrix}
\\
&
\\
@nstep{\xrightarrow{\rowopswap{1}{3}}}
\quad&
@nstep{\begin{bmatrix}
1&1&0&6\\
5&2&-2&3\\
2&-1&3&4
\end{bmatrix}
}
\\
&
\\
@nstep{
\xrightarrow{\rowopadd{-2}{1}{2}}
}
\quad&
@nstep{
\begin{bmatrix}
1&1&0&6\\
3&0&-2&-9\\
2&-1&3&4
\end{bmatrix}
}
\end{align*}
@endsteps
In fact, any pair of these three matrices are row-equivalent.
@end
@slide
@thm
@title{Row-Equivalent Matrices represent Equivalent Systems}
@label{REMES}
Suppose that $A$ and $B$ are row-equivalent augmented matrices. Then the systems of linear equations that they represent are equivalent systems.
@end
@proof @col
See Beezer, Theorem REMES (Ver 3.5 print version p.20)
@end
@col
With this theorem, we now have a strategy for solving a system of linear equations:
@enumerate
@item
@col
Begin with a system of equations, represent the system by an augmented matrix.
@item
@col
perform row operations (which will preserve solutions for the system) to get a ”simpler” augmented matrix
@item
@col
convert back to a ”simpler” system of equations and then solve that system, knowing that its solutions are those of the original system.
@endenumerate
@slide
@eg
Solve:
\begin{align*}
x_1+2x_2+2x_3&=4\\
x_1+3x_2+3x_3&=5\\
2x_1+6x_2+5x_3&=6
\end{align*}
@col
Form the augmented matrix:
\begin{align*}
A=
\left[
\begin{array}{ccc|c}
1&2&2&4\\
1&3&3&5\\
2&6&5&6
\end{array}
\right]
\end{align*}
@col
then apply row operations:
@steps
\begin{align*}
\xrightarrow{\rowopadd{-1}{1}{2}}
\quad
&
@nstep{
\left[
\begin{array}{ccc|c}
1&2&2&4\\
0&1&1&1\\
2&6&5&6
\end{array}
\right]
}
\\
&
\\
@nstep{\xrightarrow{\rowopadd{-2}{1}{3}}}
\quad
&
@nstep{
\left[
\begin{array}{ccc|c}
1&2&2&4\\
0&1&1&1\\
0&2&1&-2
\end{array}
\right]
}
\\
&
\\
@nstep{
\xrightarrow{\rowopadd{-2}{2}{3}}
}
\quad
&
@nstep{
\left[
\begin{array}{ccc|c}
1&2&2&4\\
0&1&1&1\\
0&0&-1&-4
\end{array}
\right]
}
\\
&
\\
@nstep{
\xrightarrow{\rowopmult{-1}{3}}
}
\quad
&
@nstep{
\left[
\begin{array}{ccc|c}
1&2&2&4\\
0&1& 1&1\\
0&0&1&4
\end{array}
\right]
}
\end{align*}
@endsteps

@col
So the matrix
\[
\left[
\begin{array}{ccc|c}
1&2&2&4\\
0&1& 1&1\\
0&0&1&4
\end{array}
\right]
\]
is row equivalent to $A$.
By the previous theorem (@ref{REMES}), the system of equations below has the same solution set as the original system of equations:

@col
\begin{align*}
x_1+2x_2+2x_3&=4\\
x_2+ x_3&=1\\
x_3&=4
\end{align*}
@col
The third equation requires that $x_{3}=4$ to be true. Making this substitution into equation 2 we arrive at $x_{2}=-3$, and finally, substituting these values of $x_{2}$ and $x_{3}$ into the first equation, we find that $x_{1}=2$.
@end
@setchapter{5}
@chapter{More about matrices}


The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at @href{http://linear.ups.edu/download.html}.


The print version can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-print.pdf}.


<h5 class="notkw">Reference.</h5>
@itemize
@item
Beezer, Ver 3.5 Section Matrix Operations, Section Matrix Multiplication.
@item
Strang, Sect 1.4 and Sect 1.6.
@enditemize


Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf
(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section MO (p52-56), all. Section MM (p57-60), all except T12 and T35.
@section{
Matrix Equality, Addition, Scalar Multiplication
}
@label{MEASM}
Recall $M_{mn}$ is the set of $m\times n$ matrices with real entries. Throughout the section, unless otherwise stated,
\[A=\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}\end{bmatrix},\,\,B=\begin{bmatrix}b_{11}&b_{12}&\cdots&b_{1n}\\
b_{21}&b_{22}&\cdots&b_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
b_{m1}&b_{m2}&\cdots&b_{mn}\end{bmatrix}\]
@defn
@title{Matrix Equality}
@newcol
The $m\times n$ matrices $A$ and $B$ are <b>equal</b>, written $A=B$ provided: $\left[A\right]_{ij}=\left[B\right]_{ij}$ for all $1\leq i\leq m$, $1\leq j\leq n$, that is:
\[
a_{ij}=b_{ij}\quad\text{ for all } i,j.
\]
@endcol
@end
@defn
@title{Matrix Addition}
@label{MA}
@newcol
Given $m\times n$ matrices $A$ and $B$, define the <b>sum</b> of $A$ and $B$ as an $m\times n$ matrix, written $A+B$, according to
\[\displaystyle\left[A+B\right]_{ij}=\left[A\right]_{ij}+\left[B\right]_{ij}\]
i.e.,
\[A+B=\begin{bmatrix}a_{11}+b_{11}&a_{12}+b_{12}&\cdots&a_{1n}+b_{1n}\\
a_{21}+b_{21}&a_{22}+b_{22}&\cdots&a_{2n}+b_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}+b_{m1}&a_{m2}+b_{m2}&\cdots&a_{mn}+b_{mn}\end{bmatrix}\]
@endcol
@end
@slide
@eg
If
\[
\displaystyle A=\begin{bmatrix}2&-3&4\\
1&0&-7\end{bmatrix}
\quad
B=\begin{bmatrix}6&2&-4\\
3&5&2\end{bmatrix}
\]
then:

@newcol
\[\displaystyle A+B=\begin{bmatrix}2&-3&4\\
1&0&-7\end{bmatrix}+\begin{bmatrix}6&2&-4\\
3&5&2\end{bmatrix}\]
@col
\[\displaystyle=\begin{bmatrix}2+6&-3+2&4+(-4)\\
1+3&0+5&-7+2\end{bmatrix}=\begin{bmatrix}8&-1&0\\
4&5&-5\end{bmatrix}\]
@endcol
@end
@slide
@defn
@title{Matrix Scalar Multiplication}
Given $m\times n$ matrix $A$
and a scalar $\lambda\in{\mathbb{R}}$, the <b>scalar multiple</b> of $A$ by $\lambda$ is the $m\times n$ matrix, written $\lambda A$,
defined as follows:
\[
\displaystyle\left[\lambda A\right]_{ij}=\lambda\left[A\right]_{ij},
\]
i.e.,
\[\lambda A=\begin{bmatrix}\lambda a_{11}&\lambda a_{12}&\cdots&\lambda a_{1n}\\
\lambda a_{21}&\lambda a_{22}&\cdots&\lambda a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
\lambda a_{m1}&\lambda a_{m2}&\cdots&\lambda a_{mn}\end{bmatrix}.\]
@end
Notice again that we have yet another kind of multiplication, and it is again written putting two symbols side-by-side. Computationally, scalar matrix multiplication is very easy.
@eg
@newcol
If
\[A=\begin{bmatrix}2&8\\
-3&5\\
0&1\end{bmatrix}\]
and $\lambda=7$, then
\[\lambda A=7\begin{bmatrix}2&8\\
-3&5\\
0&1\end{bmatrix}=\begin{bmatrix}7(2)&7(8)\\
7(-3)&7(5)\\
7(0)&7(1)\end{bmatrix}=\begin{bmatrix}14&56\\
-21&35\\
0&7\end{bmatrix}.\]
@endcol
@end
@slide
@defn
@title{Zero Matrix}
The $m\times n$ <b>zero matrix</b> is written as ${\cal O}={\cal O}_{m\times n}$ and defined by $\left[{\cal O}\right]_{ij}=0$, for all $1\leq i\leq m$, $1\leq j\leq n$, i.e.


@newcol
\[{\cal O}={\cal O}_{m\times n}=\begin{bmatrix}0&0&\cdots&0\\
0&0&\cdots&0\\
\vdots&\vdots&\vdots&\vdots\\
0&0&\cdots&0\end{bmatrix}.\]
@endcol
@end


@defn
@title{Additive Inverse}
@newcol
The additive inverse of a matrix $A\in M_{mn}$, denoted by $-A$ is defined by $-A=(-1)A$, i.e.
\[-A=\begin{bmatrix}-a_{11}&-a_{12}&\cdots&-a_{1n}\\
-a_{21}&-a_{22}&\cdots&-a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
-a_{m1}&-a_{m2}&\cdots&-a_{mn}\end{bmatrix}.\]
@endcol
@end
@slide
Below are some obvious properties satisfied by the addition and scalar multiplication of matrices:
@enumerate
@item
<b>Associativity of Matrix Addition</b>
@newcol
For all $A,\,B,\,C\in M_{mn}$,
we have:
\[
A+\left(B+C\right)=\left(A+B\right)+C.
\]
@endcol
@item
<b>Commutativity of Matrix Addition</b>
@newcol
For all $A,\,B\in M_{mn}$, we have:
\[
A+B=B+A.
\]
@endcol
@item
<b>Additive Identity of Matrix Addition</b>
@newcol
$A+{\cal O}=A$ for all $A\in M_{mn}$.
@endcol
@item
<strong>Existence of</strong> <b>Additive Inverse</b>
@newcol
For any $m\times n$ matrix $A$, we have:
\[
A+(-A)={\cal O}_{m \times n}.
\]
@endcol
@item
<b>Associativity of Scalar Multiplication</b>
@newcol
For all $\alpha,\,\beta\in \mathbb{R}$
and $A\in M_{mn}$, we have:
\[
\alpha(\beta A)=(\alpha\beta)A.
\]
@endcol
@item
<b>Distributivity across Matrix Addition</b>
@newcol
For all $\alpha\in\mathbb{R}$ and $A,\,B\in M_{mn}$,
we have:
\[
\alpha(A+B)=\alpha A+\alpha B.
\]
@endcol
@item
<b>Distributivity across Scalar Addition</b>
@newcol
For all $\alpha,\,\beta\in \mathbb{R}$ and $A\in M_{mn}$,
we have:
\[
(\alpha+\beta)A=\alpha A+\beta A.
\]
@endcol
@item
<strong>Scalar Multiplication by $1 \in \mathbb{R}$</strong>
@newcol
For all $A\in M_{mn}$, we have  $1A=A$.
@endcol
@endenumerate


@eg
@newcol
As an example, we prove here property 7,
$(\alpha+\beta)A=\alpha A+\beta A$.
We need to establish the equality of two matrices.


@col
For any $i$ and $j$, $1\leq i\leq m$, $1\leq j\leq n$,


@col
@steps
\begin{align*}
\left[(\alpha+\beta)A\right]_{ij}
&= @nstep{(\alpha+\beta)\left[A\right]_{ij}}
\\
& @nstep{ = \alpha\left[A\right]_{ij}+\beta\left[A\right]_{ij}}
\\
& @nstep{ = \left[\alpha A\right]_{ij}+\left[\beta A\right]_{ij}}
\\
& @nstep{= \left[\alpha A+\beta A\right]_{ij}.}
\end{align*}
@endsteps
@col
Hence by the definition of equality of matrices, $(\alpha+\beta)A=\alpha A+\beta A$.
@endcol
@end
@section{
Transposes and Symmetric Matrices
}
@label{TSM}
We now describe one more common operation which can be performed on matrices.
Informally, to transpose a matrix is to build a new matrix by swapping its rows and columns.
@defn
@title{Transpose of a Matrix}
@label{TM}
Given an $m\times n$ matrix $A$, its <b>transpose</b> is the $n\times m$ matrix $A^{t}$ given by
\[\left[A^{t}\right]_{ij}=\left[A\right]_{ji},\quad 1\leq i\leq n,\,1\leq j\leq m,\]
i.e.

@newcol
\[
A^t=\begin{bmatrix}a_{11}&a_{12}&a_{13}&\dots&a_{1n}\\
a_{21}&a_{22}&a_{23}&\dots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{n1}&a_{n2}&a_{n3}&\dots&a_{nm}\\
\end{bmatrix}^{t}
\]
@col
\[
=\begin{bmatrix}a_{11}&a_{21}&a_{31}&\dots&a_{m1}\\
a_{12}&a_{22}&a_{32}&\dots&a_{m2}\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{1n}&a_{2n}&a_{3n}&\dots&a_{mn}\\
\end{bmatrix}.
\]
@endcol
@end
@eg
@newcol
Suppose:
\[
D=\begin{bmatrix}3&7&2&-3\\
-1&4&2&8\\
0&3&-2&5\end{bmatrix}.\]
Then,

@col
\[D^{t}=\begin{bmatrix}3&-1&0\\
7&4&3\\
2&2&-2\\
-3&8&5\end{bmatrix}.\]
@endcol
@end
@slide
@defn
@title{Symmetric Matrix}
@label{SYM}
A matrix $A$ is said to be <b>symmetric</b> if $A=A^{t}$, i.e.


@newcol
\[A=\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}\end{bmatrix}\]
with
\[a_{ij}=a_{ji}\text{ for all $i,j$}.\]
@endcol
@end
@eg
@newcol
The matrix:
\[E=\begin{bmatrix}2&3&-9&5&7\\
3&1&6&-2&-3\\
-9&6&0&-1&9\\
5&-2&-1&4&-8\\
7&-3&9&-8&-3\end{bmatrix}\]
is symmetric.
@endcol
@end
@thm
@title{Symmetric Matrices are Square}
@label{SMS}
Suppose that $A$ is a symmetric matrix. Then $A$ is square.
@end
@proof
@newcol
Suppose $A$ is a $n\times m$ matrix. Then $A^{t}$ is a $m\times n$ matrix.
In order for $A$ and $A^{t}$ to be equal, they must have the same dimension. Hence $n=m$.
@endcol
@end
@slide
@thm
@title{Transpose and Matrix Addition}
Suppose that $A$ and $B$ are $m\times n$ matrices. Then $(A+B)^{t}=A^{t}+B^{t}$.
@end
@proof
@newcol
For $1\leq i\leq n$, $1\leq j\leq m$,
@steps
\[
\begin{split}
\left[(A+B)^{t}\right]_{ij}&= @nstep{\left[A+B\right]_{ji}}\\
& @nstep{=\left[A\right]_{ji}+\left[B\right]_{ji}}\\
& @nstep{=\left[A^{t}\right]_{ij}+\left[B^{t}\right]_{ij}}\\
& @nstep{=\left[A^{t}+B^{t}\right]_{ij}}\\
\end{split}
\]
@endsteps


@col
Since the matrices $(A+B)^{t}$ and $A^{t}+B^{t}$ agree at each entry, they are equal.
@endcol
@end


@thm
@title{Transpose and Matrix Scalar Multiplication}
@newcol
Suppose that $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $A$ is an $m\times n$ matrix. Then $(\alpha A)^{t}=\alpha A^{t}$.
@endcol
@end
@proof
@newcol
For $1\leq i\leq m$, $1\leq j\leq n$,
\[\displaystyle\left[(\alpha A)^{t}\right]_{ji}=\left[\alpha A\right]_{ij}\]
\[\displaystyle=\alpha\left[A\right]_{ij}\]
\[\displaystyle=\alpha\left[A^{t}\right]_{ji}\]
\[\displaystyle=\left[\alpha A^{t}\right]_{ji}.\]
Since the matrices $(\alpha A)^{t}$ and $\alpha A^{t}$ agree at each entry, they are equal.
@endcol
@end
@thm
@title{Transpose of a Transpose}
@label{TT}
@newcol
Suppose that $A$ is an $m\times n$ matrix. Then $\left(A^{t}\right)^{t}=A$.
@endcol
@end
@proof
@newcol
For $1\leq i\leq m$, $1\leq j\leq n$,
\[\displaystyle\left[\left(A^{t}\right)^{t}\right]_{ij}=\left[A^{t}\right]_{ji}\]
\[\displaystyle=\left[A\right]_{ij}.\]
Since the matrices $\left(A^{t}\right)^{t}$ and $A$ agree at each entry, they are equal.
@endcol
@end
@section{
Matrix-Vector Product
}
@defn
@title{Matrix-Vector Product}
@label{MVP}
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$ and $\mathbf{u}$ is a vector of size $n$. Then the <b>matrix-vector product</b> of $A$ with $\mathbf{u}$ is the linear combination
\[A\mathbf{u}=\left[\mathbf{u}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{u}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{u}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{u}\right]_{n}\mathbf{A}_{n}\]
@end


@newcol
Note that an $m\times n$ matrix $A$
times a vector of size $n$ will create a column vector of size $m$.


In particular, if $A$ is (non-square) rectangular,
then the size of the vector changes.
@eg
Consider:
\[\displaystyle A=\begin{bmatrix}1&4&2&3&4\\
-3&2&0&1&-2\\
1&6&-3&-1&5\end{bmatrix},\;\mathbf{u}=\begin{bmatrix}2\\
1\\
-2\\
3\\
-1\end{bmatrix}\]
@newcol
Then:
\begin{align*}
A\mathbf{u}&=2\begin{bmatrix}1\\
-3\\
1\end{bmatrix}
+1\begin{bmatrix}4\\
2\\
6\end{bmatrix}
+(-2)\begin{bmatrix}2\\
0\\
-3\end{bmatrix}
+3\begin{bmatrix}3\\
1\\
-1\end{bmatrix}
+(-1)\begin{bmatrix}4\\
-2\\
5\end{bmatrix}
\\&
=\begin{bmatrix}7\\
1\\
6\end{bmatrix}.
\end{align*}
@endcol
@end
@endcol
@subsection{
Matrix Notation for Systems of Linear Equations
}
@thm
@title{Systems of Linear Equations as Matrix Multiplication}
@label{SLEMM}
The solution set to the linear system $\mathcal{LS}(A, \mathbf{b})$ is equal to the set of solutions $\mathbf{x}$ to the vector equation $A\mathbf{x}=\mathbf{b}$.
@end
@proof
@newcol
\[\displaystyle\mathbf{x}\text{ is a solution to } \mathcal{LS}(A,\mathbf{b})\]
\[\displaystyle\iff\left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n}=\mathbf{b}\]
\[\displaystyle\iff\mathbf{x}\text{ is a solution to }A\mathbf{x}=\mathbf{b}\]
@endcol
@end
@eg
@newcol
Consider the system of linear equations
\[\displaystyle 2x_{1}+4x_{2}-3x_{3}+5x_{4}+x_{5}=9\]
\[\displaystyle 3x_{1}+x_{2}+x_{4}-3x_{5}=0\]
\[\displaystyle-2x_{1}+7x_{2}-5x_{3}+2x_{4}+2x_{5}=-3\]
has coefficient matrix and vector of constants
\[\displaystyle A=\begin{bmatrix}2&4&-3&5&1\\
3&1&0&1&-3\\
-2&7&-5&2&2\end{bmatrix}\qquad\mathbf{b}=\begin{bmatrix}9\\
0\\
-3\end{bmatrix}\]
and so will be described compactly by the vector equation $A\mathbf{x}=\mathbf{b}$.
@endcol
@end
@slide
@thm
@title{Equality of Matrices and Matrix-Vector Products}
@label{EMMVP}
If $A$ and $B$ are $m\times n$ matrices such that $A\mathbf{x}=B\mathbf{x}$ for every $\mathbf{x}\in{\mathbb{R}}^{n}$,
then $A=B$.
@end
@proof
@newcol
Suppose $A\mathbf{x}=B\mathbf{x}$ for all $\mathbf{x}\in{\mathbb{R}}^{n}$.
Then, in particular this equality holds for the standard unit vectors, defined as follows:


For $1\leq j\leq n$, we define the <b>standard unit vector</b> $\mathbf{e}_{j}$ to be the column vector in ${\mathbb{R}}^{n}$ with the $j$-th entry equal $1$ and all other entries equal to zero.  For any $1 \leq i \leq m$ and $1 \leq j \leq n$, we have:


@col
\begin{align*}
\left[A\right]_{ij}
&=\left[A\mathbf{e}_{j}\right]_{i}\\
&=\left[B\mathbf{e}_{j}\right]_{i}\\
&=\left[B\right]_{ij}
\end{align*}
@col
Hence $A = B$.
@endcol
@end


@remark
@newcol
You might notice from studying the proof that the hypotheses of this theorem could be weakened i.e., made less restrictive). We need only suppose the equality of the matrix-vector products for the standard unit vectors or any other spanning set of ${\mathbb{R}}^{n}$. However, in practice, when we apply this theorem the stronger hypothesis will be in effect so this version of the theorem suffices for our purposes. (If we changed the statement of the theorem to have the less restrictive hypothesis, then we would call the theorem stronger.)
@endcol
@end
@section{
Matrix Multiplication
}
@defn
@title{Matrix Multiplication}
@label{NM}
Suppose $A$ is an $m\times n$ matrix and $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{p}$ are the columns of an $n\times p$ matrix $B$. Then the <b>matrix product</b> of $A$ with $B$ is the $m\times p$ matrix whose $i$th column is the matrix-vector product $A\mathbf{B}_{i}$. Symbolically,
\[AB=A\left[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}|\ldots|\mathbf{B}_{p}\right]=\left[A\mathbf{B}_{1}|A\mathbf{B}_{2}|A\mathbf{B}_{3}|\ldots|A\mathbf{B}_{p}\right].\]
@end
@eg
@newcol
Let:
\[\displaystyle A=\begin{bmatrix}1&2&-1&4&6\\
0&-4&1&2&3\\
-5&1&2&-3&4\end{bmatrix},\;
B=\begin{bmatrix}1&6&2&1\\
-1&4&3&2\\
1&1&2&3\\
6&4&-1&2\\
1&-2&3&0\end{bmatrix}.\]
@col
Then:
\[
\begin{split}
AB&=\left[A\begin{bmatrix}1\\
-1\\
1\\
6\\
1\end{bmatrix}\left\lvert A\begin{bmatrix}6\\
4\\
1\\
4\\
-2\end{bmatrix}\right.\left\lvert A\begin{bmatrix}2\\
3\\
2\\
-1\\
3\end{bmatrix}\right.\left\lvert A\begin{bmatrix}1\\
2\\
3\\
2\\
0\end{bmatrix}\right.\right]
\\
&
\\
&=\begin{bmatrix}28&17&20&10\\
20&-13&-3&-1\\
-18&-44&12&-3\end{bmatrix}.
\end{split}
\]
@endcol
@end
@remark
@newcol
Is this the definition of matrix multiplication you expected? Perhaps our previous operations for matrices caused you to think that we might multiply two matrices of the same size, entry-by-entry? Notice that our current definition uses matrices of different sizes (though the number of columns in the first must equal the number of rows in the second), and the result is of a third size.
Notice too that in the previous example we cannot even consider the product $BA$, since the sizes of the two matrices in this order are not compatible.
But it gets weirder than that. Many of your old ideas about multiplication will not apply to matrix multiplication, but some still will. So make no assumptions, and do not do anything until you have a theorem that says you can. Even if the sizes are right, matrix multiplication is not commutative – order matters.
@endcol
@end
@slide
@eg
This example demonstrates that
matrix multiplication is in general <strong>not</strong> commutative.


@newcol
Let:
\[\displaystyle A=\begin{bmatrix}1&3\\
-1&2\end{bmatrix},\;
B=\begin{bmatrix}4&0\\
5&1\end{bmatrix}.\]
@col
Then:
\[\displaystyle AB=\begin{bmatrix}19&3\\
6&2\end{bmatrix},\;
BA=\begin{bmatrix}4&12\\
4&17\end{bmatrix}\]
So, $AB\neq BA$.


@col
It should not be hard for you to construct other pairs of matrices that do not commute (try a couple of $3\times 3$’s). Can you find a pair of non-identical matrices that do commute?
@endcol
@end
@slide
@thm
@title{Entries of Matrix Products}
@label{EMP}
Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix. Then for $1\leq i\leq m$, $1\leq j\leq p$, the individual entries of $AB$ are given by:


@newcol
\begin{align*}
\left[AB\right]_{ij}
&=\left[A\right]_{i1}\left[B\right]_{1j}+\left[A\right]_{i2}\left[B\right]_{2j}+\left[A\right]_{i3}\left[B\right]_{3j}+ \cdots+\left[A\right]_{in}\left[B\right]_{nj}\\
&=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj}
\end{align*}
@endcol
@end
@remark
@newcol
In most books, this is used as the definition of $AB$.
@endcol
@end
@proof
@newcol
View the columns of $A$ as column vectors and denote them from left to right by:
$\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$.  Similarly, denote the columns of $B$ by: $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{p}$.


@col
Then, for $1\leq i\leq m$, $1\leq j\leq p$, we have:
\begin{align*}
\left[AB\right]_{ij}=\left[A\mathbf{B}_{j}\right]_{i}
&=\left[\left[\mathbf{B}_{j}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{B}_{j}\right]_{2}\mathbf{A}_{2}+\cdots+\left[\mathbf{B}_{j}\right]_{n}\mathbf{A}_{n}\right]_{i}\\
&=\left[\left[\mathbf{B}_{j}\right]_{1}\mathbf{A}_{1}\right]_{i}+
\left[\left[\mathbf{B}_{j}\right]_{2}\mathbf{A}_{2}\right]_{i}+\cdots+\left[\left[\mathbf{B}_{j}\right]_{n}\mathbf{A}_{n}\right]_{i}\\
&=\left[\mathbf{B}_{j}\right]_{1}\left[\mathbf{A}_{1}\right]_{i}+\left[\mathbf{B}_{j}\right]_{2}\left[\mathbf{A}_{2}\right]_{i}+\cdots+\left[\mathbf{B}_{j}\right]_{n}\left[\mathbf{A}_{n}\right]_{i}\\
&=\left[B\right]_{1j}\left[A\right]_{i1}+\left[B\right]_{2j}\left[A\right]_{i2}+\cdots+\left[B\right]_{nj}\left[A\right]_{in}\\
&=\left[A\right]_{i1}\left[B\right]_{1j}+\left[A\right]_{i2}\left[B\right]_{2j}+\cdots+\left[A\right]_{in}\left[B\right]_{nj}\\
&=\sum_{k=1}^{n}\left[A\right]_{ik}\left[B\right]_{kj}
\end{align*}
@endcol
@end
@eg
@title{Product of Two Matrices, Entry-by-Entry}
Consider the matrices:
\[\displaystyle A=\begin{bmatrix}1&2&-1&4&6\\
0&-4&1&2&3\\
-5&1&2&-3&4\end{bmatrix},\;
B=\begin{bmatrix}1&6&2&1\\
-1&4&3&2\\
1&1&2&3\\
6&4&-1&2\\
1&-2&3&0\end{bmatrix}\]
Suppose we just wanted the entry of $AB$ in the second row, third column:
\begin{align*}
\matrixentry{AB}{23}
=&
\matrixentry{A}{21}\matrixentry{B}{13}+
\matrixentry{A}{22}\matrixentry{B}{23}+
\matrixentry{A}{23}\matrixentry{B}{33}+
\matrixentry{A}{24}\matrixentry{B}{43}+
\matrixentry{A}{25}\matrixentry{B}{53}\\
=&(0)(2)+(-4)(3)+(1)(2)+(2)(-1)+(3)(3)=-3
\end{align*}
Notice how there are 5 terms in the sum, since 5 is the common dimension of the two matrices (column count for $A$, row count for $B$). In the conclusion of the above theorem, it would be the index $k$ that would run from 1 to 5 in this computation. Here is a bit more practice.
The entry of third row, first column:
\begin{align*}
\matrixentry{AB}{31}
=&
\matrixentry{A}{31}\matrixentry{B}{11}+
\matrixentry{A}{32}\matrixentry{B}{21}+
\matrixentry{A}{33}\matrixentry{B}{31}+
\matrixentry{A}{34}\matrixentry{B}{41}+
\matrixentry{A}{35}\matrixentry{B}{51}\\
=&(-5)(1)+(1)(-1)+(2)(1)+(-3)(6)+(4)(1)=-18
\end{align*}
Try to compute all the other entries.
@end
<h5 class="notkw">How to memorize the formula</h5>:
@newcol
To find the $(i,j)$-th entry of $AB$. (1) Find the $i$-th row of $A$ (simply called the row below)(2) Find the $j$-th column of $B$ (simply called the column below)(3) sum up the product the corresponding entries of the row and the column, i.e.
(entry 1 of the row $\times$ entry 1 of the column) + (entry 2 of the row $\times$ entry 2 of the column) + $\cdots$
@endcol
@eg
@newcol
Find the $(3,2)$ entry of $AB$ in the previous example.The $3$-rd row of $A$ is $\begin{bmatrix}-5&1&2&-3&4\end{bmatrix}$ The $2$-nd column of $B$ is $\begin{bmatrix}6\\
4\\
1\\
4\\
-2\end{bmatrix}$. Let’s do the multiplication:
row
-5
1
2
-3
4
column
6
4
1
4
-2
product
-30
4
2
-12
-8
The sum is
\[-30+4+2-12-8=-44.\]
@endcol
@end
@subsection{
Properties of Matrix Multiplication
}
In this subsection, we collect properties of matrix multiplication and its interaction with
the zero matrix, the identity matrix, matrix addition, scalar matrix multiplication and
the transpose.
@thm
@title{Matrix Multiplication and the Zero Matrix}
@label{MMZM}
@newcol
Suppose $A$ is an $m\times n$ matrix. Then
@enumerate
@item
$A{\cal O}_{n\times p}={\cal O}_{m\times p}$
@item
${\cal O}_{p\times m}A={\cal O}_{p\times n}$
@endenumerate
@endcol
@end
@proof
@newcol
We will prove (1) and leave (2) to you. Using the entry-by-entry definition of matrix multiplication, for $1\leq i\leq m$, $1\leq j\leq p$, we comoute
\[\displaystyle\left[A{\cal O}_{n\times p}\right]_{ij}=\sum_{k=1}^{n}\left[A\right]_{ik}\left[{\cal O}_{n\times p}\right]_{kj}\]
\[\displaystyle=\sum_{k=1}^{n}\left[A\right]_{ik}0\]
\[\displaystyle=\sum_{k=1}^{n}0\]
\[\displaystyle=0\]
\[\displaystyle=\left[{\cal O}_{m\times p}\right]_{ij}\]
So the matrices $A{\cal O}_{n\times p}$ and ${\cal O}_{m\times p}$ are equal.
@endcol
@end
@thm
@title{Matrix Multiplication and Identity Matrix}
@label{MMIM}
@newcol
Suppose that $A$ is an $m\times n$ matrix. Then
@enumerate
@item
$AI_{n}=A$


@item
$I_{m}A=A$
@endenumerate
@endcol
@end
@proof
@newcol
Again, we will prove (1) and leave (2) to you. Using the entry-by-entry definition of matrix multiplication, for $1\leq i\leq m$, $1\leq j\leq n$, we compute
\[\displaystyle\left[AI_{n}\right]_{ij}=\sum_{k=1}^{n}\left[A\right]_{ik}\left[I_{n}\right]_{kj}\]
\[\displaystyle=\left[A\right]_{ij}\left[I_{n}\right]_{jj}+\sum_{\begin{subarray}{c}k=1\\
k\neq j\end{subarray}}^{n}\left[A\right]_{ik}\left[I_{n}\right]_{kj}\]
\[\displaystyle=\left[A\right]_{ij}(1)+\sum_{k=1,k\neq j}^{n}\left[A\right]_{ik}(0)\]
\[\displaystyle=\left[A\right]_{ij}+\sum_{k=1,k\neq j}^{n}0\]
\[\displaystyle=\left[A\right]_{ij}.\]
So the matrices $A$ and $AI_{n}$ are equal entrywise. By the definition of matrix equality, they are equal matrices.
@endcol
@end
@remark
@newcol
It is the previous theorem that gives the identity matrix its name. It is a matrix that behaves with matrix multiplication like the scalar 1 does with scalar multiplication. To multiply by the identity matrix is to have no effect on the other matrix.
@endcol
@end
@slide
@thm
@title{Matrix Multiplication Distributes Across Addition}
@label{MMDAA}
Suppose that $A$ is an $m\times n$ matrix and $B$ and $C$ are $n\times p$ matrices and $D$ is a $p\times s$ matrix.
Then:
@enumerate
@item
$A(B+C)=AB+AC$
@item
$(B+C)D=BD+CD$
@endenumerate
@end
@proof
@newcol
We will do (1), you do (2). Entry-by-entry, for $1\leq i\leq m$, $1\leq j\leq p$,
\begin{align*}
\matrixentry{A(B+C)}{ij}
&=
\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B+C}{kj}
\\
&=\sum_{k=1}^{n}\matrixentry{A}{ik}(\matrixentry{B}{kj}+\matrixentry{C}{kj})
\\
&=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}+\matrixentry{A}{ik}\matrixentry{C}{kj}
\\
&=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}+\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{C}{kj}
\\
&=\matrixentry{AB}{ij}+\matrixentry{AC}{ij}
\\
&=\matrixentry{AB+AC}{ij}
\\
\end{align*}
So the matrices $A(B+C)$ and $AB+AC$ are equal, entry-by-entry.
Hence by the definition of matrix equality, we can say they are equal matrices.
@endcol
@end
@thm
@title{Matrix Multiplication and Scalar Matrix Multiplication}
@label{MMSMM}
@newcol
Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix. Let $\alpha$ be a scalar. Then $\alpha(AB)=(\alpha A)B=A(\alpha B)$.
@endcol
@end
@proof
@newcol
These are equalities of matrices. We will do the first one, the second is similar and will be good practice for you.
For $1\leq i\leq m$, $1\leq j\leq p$,
\begin{align*}
\matrixentry{\alpha(AB)}{ij}
&=\alpha\matrixentry{AB}{ij}
\\
&=\alpha\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}
\\
&=\sum_{k=1}^{n}\alpha\matrixentry{A}{ik}\matrixentry{B}{kj}
\\
&=\sum_{k=1}^{n}\matrixentry{\alpha A}{ik}\matrixentry{B}{kj}
\\
&=\matrixentry{(\alpha A)B}{ij}
\\
\end{align*}
So the matrices $\alpha(AB)$ and $(\alpha A)B$ are equal, entry-by-entry, and by the definition of matrix equality we can say they are equal matrices.
@endcol
@end
@slide
@thm
@title{Matrix Multiplication is Associative}
@label{MMA}
@newcol
Suppose $A$ is an $m\times n$ matrix, $B$ is an $n\times p$ matrix and $D$ is a $p\times s$ matrix. Then $A(BD)=(AB)D$.
@endcol
@end
@proof
@newcol
A matrix equality, so we will go entry-by-entry, no surprise there. For $1\leq i\leq m$, $1\leq j\leq s$,
\begin{align*}
\matrixentry{A(BD)}{ij}
&=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{BD}{kj}
\\
&=\sum_{k=1}^{n}\matrixentry{A}{ik}\left(\sum_{\ell=1}^{p}\matrixentry{B}{k\ell}\matrixentry{D}{\ell j}\right)
\\
&=\sum_{k=1}^{n}\sum_{\ell=1}^{p}\matrixentry{A}{ik}\matrixentry{B}{k\ell}\matrixentry{D}{\ell j}
\\
\end{align*}
@col
We can switch the order of the summation since these are finite sums.


@col
\begin{align*}
&=\sum_{\ell=1}^{p}\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{k\ell}\matrixentry{D}{\ell j}
\\
\end{align*}
@col
As $\matrixentry{D}{\ell j}$ does not depend on the index $k$, we can use distributivity to move it outside of the inner sum.


@col
\begin{align*}
&=\sum_{\ell=1}^{p}\matrixentry{D}{\ell j}\left(\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{k\ell}\right)
\\
&=\sum_{\ell=1}^{p}\matrixentry{D}{\ell j}\matrixentry{AB}{i\ell}
\\
&=\sum_{\ell=1}^{p}\matrixentry{AB}{i\ell}\matrixentry{D}{\ell j}
\\
&=\matrixentry{(AB)D}{ij}
\\
\end{align*}
@col
Hence, $(AB)D = A(BD)$.
@endcol
@end
@remark
@newcol
The above result says matrix multiplication is associative; it means we do not have to be careful about how we parenthesize an expression with just several matrices multiplied together. So this is where we draw the line on explaining every last detail in a proof. We will frequently add, remove, or rearrange parentheses with no comment.
@endcol
@end
@slide
@thm
@title{Matrix Multiplication and Transposes}
@label{MMT}
Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix. Then $(AB)^{t}=B^{t}A^{t}$.
@end
@proof
@newcol
Here we go again, entry-by-entry. For $1\leq i\leq m$, $1\leq j\leq p$,


@col
\begin{align*}
\matrixentry{\transpose{(AB)}}{ji}=&\matrixentry{AB}{ij}
\\
&=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}
\\
&=\sum_{k=1}^{n}\matrixentry{B}{kj}\matrixentry{A}{ik}
\\
&=\sum_{k=1}^{n}\matrixentry{\transpose{B}}{jk}\matrixentry{\transpose{A}}{ki}
\\
&=\matrixentry{\transpose{B}\transpose{A}}{ji}
\\
\end{align*}


@col
So, $(AB)^{t} = B^{t}A^{t}$.
@endcol
@end
@section{
Row Operations and Matrix Multiplication
}
In this section, we will discuss the relation between elementary row operations and matrix multiplication.


@defn
@title{Row Operations}
@label{RO}
@newcol
The following three operations, each of which is known as a <b>row operation</b>, will transform an $m\times n$ matrix into a different matrix of the same size.
@enumerate
@item
@newcol
Swap the locations of two rows.
@endcol
@item
@newcol
Multiply each entry of a single row by a nonzero quantity.
@endcol
@item
@newcol
Multiply each entry of one row by some quantity, and add these values to the entries in the same columns of a second row. Leave the first row the same after this operation, but replace the second row by the new values.
@endcol
@endenumerate
@endcol
@end
@thm
@label{thm:ROEM}
@newcol
Let $A\in M_{mn}$. Let $B$ be a matrix obtained by applying one of the above row operations on $A$.
Let $J$ be a matrix obtained by applying the same row operation on $I_{m}$. Then
\[JA=B.\]
@endcol
@end
@proof
@newcol
Exercise.
@endcol
@end
@slide
@eg
Let:
\[A=\begin{bmatrix}a_{11}&a_{12}&a_{13}&a_{14}&a_{15}\\
a_{21}&a_{22}&a_{23}&a_{24}&a_{25}\\
a_{31}&a_{32}&a_{33}&a_{34}&a_{35}\\
a_{41}&a_{42}&a_{43}&a_{44}&a_{45}\\
\end{bmatrix}\]
Consider the row operation $3R_{2}+R_{3}$.
\[A\xrightarrow{3R_{2}+R_{3}}B=\begin{bmatrix}a_{11}&a_{12}&a_{13}&a_{14}&a_{15}\\
a_{21}&a_{22}&a_{23}&a_{24}&a_{25}\\
3a_{21}+a_{31}&3a_{22}+a_{32}&3a_{23}+a_{33}&3a_{24}+a_{34}&3a_{25}+a_{35}\\
a_{41}&a_{42}&a_{43}&a_{44}&a_{45}\\
\end{bmatrix}\]
\[I_{4}\xrightarrow{3R_{2}+R_{3}}J=\begin{bmatrix}1&0&0&0\\
0&1&0&0\\
0&3&1&0\\
0&0&0&1\\
\end{bmatrix}\]
Then
\[JA=B.\]
@end
@setchapter{6}
@chapter{Reduced Row Echelon Forms}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Subsection RREF (print version p21 - p33) You can skip the proof of Thm REMEF on p.22 and Thm RREFU on p.24-27
<br/><br/>
<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf
Section SSLE (p.1-6) C30-34, C50, M30, T20. Sect RREF (p.6-13) C10-19, C31-33, M40 Part 1, T10, T11, T12.
@section{
Reduced Row Echelon Form
}
<strong>Terminology</strong>:
@ul
@li
<b>Zero row</b>: A row consisting only of $0$'s.
@li
<b>Leftmost nonzero entry of a row</b>: The first nonzero entry of a row.
@li
<b>Index of the leftmost nonzero entry of a row</b>: The column index of the first nonzero entry in the row.
@endul
<strong>Notation</strong>:
@col
Denote by $d_{i}$ the index of leftmost nonzero entry of row $i$.
@endcol
@slide
@eg
The underlined entries are the leftmost nonzero entry for each row.
\[\begin{bmatrix}0&\underline{1}&1&0&2\\
0&0&0&0&\underline{1}\\
0&0&0&\underline{1}&3\\
0&0&0&0&0\end{bmatrix}\]
@col
The index of the leftmost nonzero entry of row 1 is $d_{1}=2$. The index of the leftmost nonzero entry of row 2 is $d_{2}=5$. The index of the leftmost nonzero entry of row 3 is $d_{3}=4$. row 4 is a zero row.
@end
@eg
@newcol
The underlined entries are the leftmost nonzero entry for each row.
\[\begin{bmatrix}\underline{2}&0&1&2&3&4\\
0&\underline{1}&1&-1&0&3\\
0&0&0&0&\underline{1}&0\\
0&\underline{-1}&0&0&0&1\\
\end{bmatrix}\]
The index of the leftmost nonzero entry of row 1 is $d_{1}=1$. The index of the leftmost nonzero entry of row 2 is $d_{2}=2$. The index of the leftmost nonzero entry of row 3 is $d_{3}=5$. The index of the leftmost nonzero entry of row 3 is $d_{4}=2$.
@endcol
@end
@slide
A matrix is said to be in <b>reduced row echelon form</b> if it looks like this
( $*$ means an arbitary number)
\[\begin{bmatrix}1&*&\cdots&0&*&\cdots&0&*&\cdots\\
0&0&\cdots&1&*&\cdots&0&*&\cdots\\
0&0&\cdots&0&0&\cdots&1&*&\cdots\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
\end{bmatrix}\]
More precisely:
@ol
@li
It looks like an inverted staircase.
@li
Each new step down (i.e. up) gives a leading ”$1$”. Above it are $0$'s.
@li
The column that is at the edge a new step is call a <b>pivot column</b>.
@endol
@slide
@defn
@title{Reduced Row-Echelon Form}
@label{RREF}
A matrix is in <b>reduced row-echelon form</b> if it meets all of the following conditions:
@ol
@li
If there is a row where every entry is zero, then this row lies below any other row that contains a nonzero entry.
@li
The leftmost nonzero entry of a row is equal to 1.
@li
The leftmost nonzero entry of a row is the only nonzero entry in its column.
@li
If $i < j$ and both row $i$ and row $j$ are not zero rows,
then $d_{i} < d_{j}$, i.e.
$d_{1},d_{2},\ldots$ are in ascending order.
<br/>
In particular,
all matrix entries below a leftmost nonzero entry must be equal to zero.
@endol
@end
<strong>Terminology.</strong>
@col
A row of only zero entries is called a <b>zero row</b>.

@col
In the case of a matrix in reduced row-echelon form,
the leftmost nonzero entry of a nonzero row is a <b>leading 1</b>.

@col
A column containing a leading 1 will be called a <b>pivot column</b>.

@col
The number of nonzero rows will be denoted by $r$,
which is also equal to the number of leading 1’s and
the number of pivot columns.

@col
The set of column indices for the pivot columns will be denoted by:
\[
D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\},
\]
where:
\[
d_{1} < d_{2} < d_{3} < \cdots < d_{r}.
\]
@col
The columns that are not pivot columns will be denoted as:
\[
F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\},
\]
where:
\[
f_{1} < f_{2} < f_{3} < \cdots < f_{n-r}.
\]
@endcol
@end
@slide
@eg
The matrix below are in reduced row echelon from
@enumerate
@item
@newcol
\[\begin{bmatrix}1&3&0&0&1&0\\
0&0&1&3&4&0\\
0&0&0&0&0&1\\
0&0&0&0&0&0\end{bmatrix}\]
Column 1, 3, 6 are pivot columns, $r=3$, $D=\{1,3,6\}$, $d_{1}=1,d_{2}=3,d_{3}=6$,
$F=\{2,4,5\}$, $f_{1}=2,f_{2}=4,f_{3}=5$.
@endcol
@item
@newcol
\[\begin{bmatrix}1&0&5&3&0&0&5\\
0&1&3&6&0&0&6\\
0&0&0&0&1&0&7\\
0&0&0&0&0&1&3\end{bmatrix}\]
Column 1, 2, 5, 6 are pivot columns, $r=4$, $D=\{1,2,5,6\}$, $d_{1}=1,d_{2}=2,d_{3}=5,d_{4}=6$,
$F=\{3,4,7\}$, $f_{1}=3,f_{2}=4,f_{3}=7$.
@endcol
@item
@newcol
\[\begin{bmatrix}1&0&0\\
0&1&0\\
0&0&1\end{bmatrix}\]
Column 1, 2, 3 are pivot columns, $r=3$, $D=\{1,2,3\}$, $d_{1}=1,d_{2}=2,d_{3}=3$, $F=\emptyset$ (an empty set).
@endcol
@item
@newcol
\[\begin{bmatrix}0&1&1&0&0&1&0&9&6\\
0&0&0&0&1&1&0&8&8\\
0&0&0&0&0&0&1&3&4\\
0&0&0&0&0&0&0&0&0\end{bmatrix}\]
Column 2, 5, 7 are pivot columns. Note that column 3 is <strong>not</strong> a pivot column. $r=3$,
$D=\{2,5,7\}$, $d_{1}=2,d_{2}=5,d_{3}=7$,
$F=\{1,3,4,6,8,9\}$, $f_{1}=1,f_{2}=3,f_{3}=4,f_{4}=6,f_{5}=8,f_{6}=9$.
@endcol
@item
@newcol
The matrix $C$ is in reduced row-echelon form.
\[\displaystyle C=\begin{bmatrix}1&-3&0&6&0&0&-5&9\\
0&0&0&0&1&0&3&-7\\
0&0&0&0&0&1&7&3\\
0&0&0&0&0&0&0&0\\
0&0&0&0&0&0&0&0\end{bmatrix}\]
This matrix has two zero rows and three pivot columns.
So $r=3$. Columns 1, 5, and 6 are the pivot columns,
so $D=\{1,\,5,\,6\}$, $d_{1}=1,d_{2}=5,d_{3}=6$,
$F=\{2,\,3,\,4,\,7,\,8\}$, $f_{1}=2,f_{2}=3,f_{3}=4,f_{4}=7,f_{5}=8$.
@endcol
@endenumerate
@end
@slide
@eg
The following matrices are <b>not RREF</b>, explain why.
@enumerate
@item
@newcol
\[\begin{bmatrix}1&0&1&0&1&0\\
0&1&0&1&0&2\\
0&0&0&0&0&0\\
0&0&0&0&1&0\\
0&0&0&0&0&0\end{bmatrix}\]
It fails condition 1: row 3 is a zero row but row 4, which is under row 3, is not a zero row.
@endcol
@item
@newcol
The underline entries are the leftmost nonzero entry for each row.
\[\begin{bmatrix}\underline{1}&0&2&0\\
0&\underline{1}&3&0\\
0&0&0&\underline{3}\\
0&0&0&0\\
0&0&0&0\end{bmatrix}\]
It fails condition 2: the leftmost nonzero entry of row 3 is not 1.
@endcol
@item
@newcol
The underline entries are the leftmost nonzero entry for each row.
\[\begin{bmatrix}0&\underline{1}&0&0&1&2\\
0&0&\underline{1}&0&0&1\\
0&0&0&0&\underline{1}&3\\
0&0&0&0&0&0\end{bmatrix}\]
It fails condition 3: For row 3, the column consists the left most nonzero entry (i.e. column 5) has more than $1$ nonzero entries.
@endcol
@item
@newcol
The underline entries are the leftmost nonzero entry for each row.
\[\begin{bmatrix}\underline{1}&0&0&1&2\\
0&0&\underline{1}&0&1\\
0&\underline{1}&0&0&3\\
0&0&0&0&0\\
0&0&0&0&0\\
\end{bmatrix}\]
It fails condition 4: The index of the leftmost nonzero entry of row 1 is $d_{1}=1$. The index of the leftmost nonzero entry of row 2 is $d_{2}=3$. The index of the leftmost nonzero entry of row 3 is $d_{2}=2$. $2 < 3$ but $d_{2} > d_{3}$.
@endcol
@endenumerate
@end
@slide
@thm
@label{thm:REMEF}
@title{Row-Equivalent Matrix in Echelon Form}
Suppose $A$ is a matrix. Then there is a matrix $B$ such that:
@enumerate
@item
$A$ and $B$ are row-equivalent.
@item
$B$ is in reduced row-echelon form.
@endenumerate
@end
@proof
@newcol
(Feel free skip the proof and look through the next example.)

@col
Suppose that $A$ has $m$ rows and $n$ columns.
We will describe a process for converting $A$ into $B$ via row operations.
This procedure is known as <b>Gaussian elimination</b>
or sometimes called <b>Gauss-Jordan elimination</b>.
Tracing through this procedure will be easier if you recognize that
$i$ refers to a row that is being converted,
$j$ refers to a column that is being converted,
and $r$ keeps track of the number of nonzero rows.
@ol
@li
Set $j=0$ and $r=0$.
@li
Increase $j$ by 1. If $j$ now equals $n+1$, then stop.
@li
Examine the entries of $A$ in column $j$ located in rows $r+1$ through $m$. If all of these entries are zero, then go to Step 2.
@li
Choose a row from rows $r+1$ through $m$ with a nonzero entry in column $j$. Let $i$ denote the index for this row.
@li
Increase $r$ by 1.
@li
Use the first row operation to swap rows $i$ and $r$.
@li
Use the second row operation to convert the entry in row $r$ and column $j$ to a 1.
@li
Use the third row operation with row $r$ to convert every other entry of column $j$ to zero.
@li
Go to Step 2.
@endol
The result of this procedure is that the matrix $A$ is converted to a matrix in reduced row-echelon form, which we will refer to as $B$.
he matrix is only converted through row operations (Steps 6, 7, 8), so $A$ and $B$ are row-equivalent.
We need to now prove this claim by showing that the converted matrix has the requisite properties of Theorem                     @ref{RREF}.
We will skip the proof for now. See Beezer, Ver 3.5 (print version p23).
@endcol
@end
@slide
We will now run through some examples of using these definitions and theorems to solve some systems of equations. From now on, when we have a matrix in reduced row-echelon form, we will mark the leading 1’s with a small box.
@eg
Using the Gaussian elimination, find the RREF of
\[A=\begin{bmatrix}0&0&1&1&4\\
0&0&1&1&3\\
1&1&2&4&8\\
2&2&5&9&19\end{bmatrix}\]
@newcol
Set $r=0$.
Consider column 1 (set $j=1$), find a nonzero entry (underline below) in the column.
\[\begin{bmatrix}\underline{0}&0&1&1&4\\
\underline{0}&0&1&1&3\\
\underline{1}&1&2&4&8\\
\underline{2}&2&5&9&19\end{bmatrix}\]
@col
Move the nonzero entry to row 1 by swapping rows $R_{1}\leftrightarrow R_{i}$.

@col
If the entry at row 1, column 1 is nonzero, you don’t have to swap rows. But you can consider swap it with entry $=1$ or $-1$.

@col
In this example, for column 1, 3rd entry and 4th entry are nonzero, so we can use $R_{1}\leftrightarrow R_{3}$ or $R_{1}\leftrightarrow R_{4}$.

@col
There is nothing wrong about $R_{1}\leftrightarrow R_{4}$ but <strong>it is better to swap with the row with leading entry equal to $1$ or $-1$.</strong>

@col
So we use $R_{1}\leftrightarrow R_{3}$.
\[\xrightarrow{R_{1}\leftrightarrow R_{3}}\begin{bmatrix}\boxed{1}&1&2&4&8\\
0&0&1&1&3\\
0&0&1&1&4\\
2&2&5&9&19\end{bmatrix}\]
Also, at this point we increase $r$ to $r = 1$,
since we now know we have at least one nonzero row.

@col
If the boxed number is $1$, we are good to go.

@col
If the boxed number is not equal to $1$, say it is $a$, use $\frac{1}{a}R_{1}$ to convert it to $1$.

@col
Then use the boxed number to eliminate the nonzero entries above and below it by $\alpha R_{1}+R_{i}$.

@col
In our example, the boxed number is $1$, so we don’t have to do anything. Use $-2R_{1}+R_{4}$ and to remove the nonzero entries below it and above it. Since we are at the first row, so there is nothing above it).
\[\xrightarrow{-2R_{1}+R_{4}}\begin{bmatrix}\boxed{1}&1&2&4&8\\
0&0&1&1&3\\
0&0&1&1&4\\
0&0&1&1&3\end{bmatrix}\]
@col
Now we go back to Step 1 in the proof of @ref{thm:REMEF},
with column index $j$ increased to $2$ and $r = 1$.

<br/>
In fact, we may as well ignore row 1 and column 1,
and essentially apply the previous steps to the remaining $3 \times 4$ matrix:

@col
\[
\begin{bmatrix}*&*&*&*&*\\
*&\underline{0}&1&1&3\\
*&\underline{0}&1&1&4\\
*&\underline{0}&1&1&3\end{bmatrix}
\]
The entries of column 2 are underlined.

None of them are nonzero, so we move to next column.

@col
\[\begin{bmatrix}*&*&*&*&*\\
*&0&\underline{1}&1&3\\
*&0&\underline{1}&1&4\\
*&0&\underline{1}&1&3\end{bmatrix}\]
@col
Consider column 3.
That is, set $j=3$.
(Note that the number of nonzero rows is still $r = 1$.)

@col
All the entries of column 3 are equal to $1$.
So, we don’t need to do any swapping.
\[\begin{bmatrix}1&1&2&4&8\\
0&0&\boxed{1}&1&3\\
0&0&1&1&4\\
0&0&1&1&3\end{bmatrix}\]
Also, now we know there are at least 2 nonzero rows, so $r = 2$.
@col
Use the boxed number (the pivot) to eliminate the nonzero entries above and below it by $\alpha R_{2}+R_{i}$.
\[\xrightarrow{
\begin{split}
-2R_{2}+R_{1},\\
-1R_{2}+R_{3},\\
-1R_{2}+R_{4}
\end{split}
}
\begin{bmatrix}1&1&0&2&2\\
0&0&\boxed{1}&1&3\\
0&0&0&0&1\\
0&0&0&0&0\end{bmatrix}\]
@col
Now, we may ignore rows $2$ and $3$, and columns $1$ through $3$.
With $r = 2$, and the column index increased to $j = 4$, we repeat the whole
process.
\[\begin{bmatrix}*&*&*&*&*\\
*&*&*&*&*\\
*&*&*&\underline{0}&1\\
*&*&*&\underline{0}&0\end{bmatrix}\]
@col
All the underlined entries are zeros, so we move to the next row.
\[\begin{bmatrix}*&*&*&*&*\\
*&*&*&*&*\\
*&*&*&0&\underline{1}\\
*&*&*&0&\underline{0}\end{bmatrix}\]
@col
We can then use the boxed number to eliminate all the nonzero entries
above it and below it and get the RREF.
\[\begin{bmatrix}1&1&0&2&2\\
0&0&1&1&3\\
0&0&0&0&\boxed{1}\\
0&0&0&0&0\end{bmatrix}\]
\[\xrightarrow{-2R_{3}+R_{1},\;-3R_{3}+R_{2}}\begin{bmatrix}1&1&0&2&0\\
0&0&1&1&0\\
0&0&0&0&1\\
0&0&0&0&0\end{bmatrix}\]
@end
@slide
@eg
Using the Gaussian elimination, find the RREF of
\[A=\begin{bmatrix}0&0&2&2&6&2&3\\
2&4&1&3&7&3&-1\\
1&2&2&3&8&2&1\\
1&2&-1&0&-1&2&-1\\
\end{bmatrix}\]
@col
Set $r = 0$.
We first consider column 1 (set $j=1$).

Find a nonzero entry (underline below) in the column.
\[\begin{bmatrix}\underline{0}&0&2&2&6&2&3\\
\underline{2}&4&1&3&7&3&-1\\
\underline{1}&2&2&3&8&2&1\\
\underline{1}&2&-1&0&-1&2&-1\\
\end{bmatrix}\]
@col
Move the nonzero entry to row 1 by swapping rows $R_{1}\leftrightarrow R_{i}$.

@col
If the entry at row 1, column 1 is nonzero, you don’t have to swap rows. But you can consider swap it with entry $=1$ or $-1$.

@col
In this example, for column 1, 2nd entry, 3rd entry and 4th entry are nonzeros, so we can use $R_{1}\leftrightarrow R_{2}$, $R_{1}\leftrightarrow R_{3}$ or $R_{1}\leftrightarrow R_{4}$.

@col
There is nothing wrong about $R_{1}\leftrightarrow R_{2}$ but <strong>it is better to swap with the row with entry equal to $1$ or $-1$.</strong>

@col
So we use $R_{1}\leftrightarrow R_{3}$.
\[\xrightarrow{R_{1}\leftrightarrow R_{3}}\begin{bmatrix}\boxed{1}&2&2&3&8&2&1\\
2&4&1&3&7&3&-1\\
0&0&2&2&6&2&3\\
1&2&-1&0&-1&2&-1\\
\end{bmatrix}\]
@col
If the boxed number is $1$, we are good to go.

@col
If the boxed number is not equal to $1$, say it is $a$, use $\frac{1}{a}R_{1}$ to convert it to $1$.

@col
Then use the boxed number to eliminate the nonzero entries above and below it by $\alpha R_{1}+R_{i}$.

@col
In our example, the boxed number is $1$, so we don’t have to do anything. Use $-2R_{1}+R_{2}$ and $-1R_{1}+R_{4}$ to remove the nonzero entries below it and above it. Since we are at the first row, so there is nothing above it.
\[\xrightarrow{-2R_{1}+R_{2},\;-1R_{1}+R_{4}}\begin{bmatrix}1&2&2&3&8&2&1\\
0&0&-3&-3&-9&-1&-3\\
0&0&2&2&6&2&3\\
0&0&-3&-3&-9&0&-2\\
\end{bmatrix}\]
@col
Ignore the row 1 and col 1.  Consider column 2:
\[\begin{bmatrix}*&*&*&*&*&*&*\\
*&\underline{0}&-3&-3&-9&-1&-3\\
*&\underline{0}&2&2&6&2&3\\
*&\underline{0}&-3&-3&-9&0&-2\\
\end{bmatrix}\]
None of the entries of column 2 are nonzero.

@col
So, we consider the next column ($j = 3$, $r = 1$).
\[\begin{bmatrix}*&*&*&*&*&*&*\\
*&0&\underline{-3}&-3&-9&-1&-3\\
*&0&\underline{2}&2&6&2&3\\
*&0&\underline{-3}&-3&-9&0&-2\\
\end{bmatrix}\]

@col
Find a nonzero entry in column 3.
In this case, all the entries are nonzero,
so we may increase the number of nonzero rows to $r = 2$.

@col
There is no entry equal to $1$ or $-1$.
We don’t need to so any swapping.
\[\begin{bmatrix}*&*&*&*&*&*&*\\
*&0&\boxed{-3}&-3&-9&-1&-3\\
*&0&2&2&6&2&3\\
*&0&-3&-3&-9&0&-2\\
\end{bmatrix}\]
@col
Turn the boxed number into $1$ by $-\frac{1}{3}R_{2}$.
\[\xrightarrow{-\frac{1}{3}R_{2}}\begin{bmatrix}1&2&2&3&8&2&1\\
0&0&\boxed{1}&1&3&\frac{1}{3}&1\\
0&0&2&2&6&2&3\\
0&0&-3&-3&-9&0&-2\\
\end{bmatrix}\]
@col
We then use the boxed number to remove the nonzero entries about it and below it.
\[\xrightarrow{-2R_{2}+R_{1},\;-2R_{2}+R_{3},\;3R_{2}+R_{4}}
\begin{bmatrix}1&2&0&1&2&\frac{4}{3}&-1\\
0&0&1&1&3&\frac{1}{3}&1\\
0&0&0&0&0&\frac{4}{3}&1\\
0&0&0&0&0&1&1\\
\end{bmatrix}\]
@col
Now, ignore the first 2 rows and the first 3 columns.
\[\begin{bmatrix}*&*&*&*&*&*&*\\
*&*&*&*&*&*&*\\
*&*&*&\underline{0}&0&\frac{4}{3}&1\\
*&*&*&\underline{0}&0&1&1\\
\end{bmatrix}\]
Consider the column with index $j = 4$.

@col
All the entries in column 4 (underlined) are equal to zero.
So, we move to the next column, with index $j=5$.
\[\begin{bmatrix}*&*&*&*&*&*&*\\
*&*&*&*&*&*&*\\
*&*&*&0&\underline{0}&\frac{4}{3}&1\\
*&*&*&0&\underline{0}&1&1\\
\end{bmatrix}\]
@col
Again, all the entries in column 5 (underlined) are equal to zero.
So, we move to the next column, with index $j=6$.

@col
\[\begin{bmatrix}*&*&*&*&*&*&*\\
*&*&*&*&*&*&*\\
*&*&*&0&0&\underline{\frac{4}{3}}&1\\
*&*&*&0&0&\underline{1}&1\\
\end{bmatrix}\]
@col
We continue the process without detailed explanations:
\[\xrightarrow{R_{3}\leftrightarrow R_{4}}\begin{bmatrix}1&2&0&1&2&\frac{4}{3}&-1\\
0&0&1&1&3&\frac{1}{3}&1\\
0&0&0&0&0&1&1\\
0&0&0&0&0&\frac{4}{3}&1\\
\end{bmatrix}\]
@col
\[
\xrightarrow{
\begin{split}
-\frac{4}{3}R_{3}+R_{1},\\
-\frac{1}{3}R_{3}+R_{2},\\
-\frac{4}{3}R_{3}+R_{4}
\end{split}
}
\begin{bmatrix}1&2&0&1&2&0&-\frac{7}{3}\\
0&0&1&1&3&0&\frac{2}{3}\\
0&0&0&0&0&1&1\\
0&0&0&0&0&0&-\frac{1}{3}\\
\end{bmatrix}
\]
@col
\[\xrightarrow{-3R_{4}}\begin{bmatrix}1&2&0&1&2&0&-\frac{7}{3}\\
0&0&1&1&3&0&\frac{2}{3}\\
0&0&0&0&0&1&1\\
0&0&0&0&0&0&1\\
\end{bmatrix}\]
@col
\[\xrightarrow{
\begin{split}
\frac{7}{3}R_{4}+R_{1},\\
-\frac{2}{3}R_{4}+R_{2},\\
-1R_{4}+R_{3}
\end{split}
}
\begin{bmatrix}1&2&0&1&2&0&0\\
0&0&1&1&3&0&0\\
0&0&0&0&0&1&0\\
0&0&0&0&0&0&1\\
\end{bmatrix} = B\]
The matrix $B$ is the reduced row echelon form of $A$.
We write:
\[
A\xrightarrow{\text{RREF}}B.
\]
@end
@slide
@thm
@title{Reduced Row-Echelon Form is Unique}
@label{RREFU}
Suppose that $A$ is an $m\times n$ matrix and that $B$ and $C$ are $m\times n$ matrices that are row-equivalent to $A$ and in reduced row-echelon form. Then $B=C$.
@end
@proof
@newcol
See Beezer, Ver 3.5 (print version p24). We will prove it later. You can skip the proof for now.
@endcol
@end
@slide
@eg
Find the solutions to the following system of equations,
\[
\begin{split}
-7x_{1}-6x_{2}-12x_{3}&=-33\\
5x_{1}+5x_{2}+7x_{3}&=24\\
x_{1}+4x_{3}&=5
\end{split}
\]
@newcol
First, form the augmented matrix, is
\[\left[\begin{array}[]{ccc|c}-7&-6&-12&-33\\
5&5&7&24\\
1&0&4&5\\
\end{array}\right]\]
@col
and work to reduced row-echelon form, first with $j=1$,
\[\displaystyle\xrightarrow{R_{1}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|c}1&0&4&5\\
5&5&7&24\\
-7&-6&-12&-33\end{array}\right]\xrightarrow{-5R_{1}+R_{2}}
\left[
\begin{array}[
]{ccc|c}1&0&4&5\\
0&5&-13&-1\\
-7&-6&-12&-33
\end{array}
\right]
\]
@col
\[\displaystyle\xrightarrow{7R_{1}+R_{3}}\left[\begin{array}[]{ccc|c}\boxed{1}
&0&4&5\\
0&5&-13&-1\\
0&-6&16&2\end{array}\right]\]
@col
Now, with $j=2$,
\[\displaystyle\xrightarrow{\frac{1}{5}R_{2}}\left[\begin{array}[]{ccc|c}\boxed{1}&0&4&5\\
0&1&\frac{-13}{5}&\frac{-1}{5}\\
0&-6&16&2\end{array}\right]\xrightarrow{6R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}
\boxed{1}&0&4&5\\
0&\boxed{1}&\frac{-13}{5}&\frac{-1}{5}\\
0&0&\frac{2}{5}&\frac{4}{5}\end{array}\right]\]
@col
And finally, with $j=3$,
\[
\xrightarrow{\frac{5}{2}R_{3}}
\left[
\begin{array}[]{ccc|c}
\boxed{1}&0&4&5\\
0&\boxed{1}&\frac{-13}{5}&\frac{-1}{5}\\
0&0&1&2\end{array}
\right]
\xrightarrow{\frac{13}{5}R_{3}+R_{2}}
\left[
\begin{array}[]{ccc|c}\boxed{1}&0&4&5\\
0&\boxed{1}&0&5\\
0&0&1&2
\end{array}\right]
\]
@col
\[\displaystyle\xrightarrow{-4R_{3}+R_{1}}\left[\begin{array}[]{ccc|c}\boxed{1}&0&0&-3\\
0&\boxed{1}&0&5\\
0&0&\boxed{1}&2\end{array}\right]
\]
@col
This is now the augmented matrix of a very simple system of equations, namely $x_{1}=-3$, $x_{2}=5$, $x_{3}=2$, which has an obvious solution. Furthermore, we can see that this is the <strong>only</strong> solution to this system, so we have determined the entire solution set,

@col
\[\displaystyle S=\left\{\begin{bmatrix}-3\\
5\\
2\end{bmatrix}\right\}\]
@endcol
@end
@slide
@eg
Let us find the solutions to the following system of equations,
\[\begin{split}
x_{1}-x_{2}+2x_{3}&=1\\
2x_{1}+x_{2}+x_{3}&=8\\
x_{1}+x_{2}&=5\\
\end{split}\]
@newcol
First, form the augmented matrix,
\[\left[\begin{array}[]{ccc|c}1&-1&2&1\\
2&1&1&8\\
1&1&0&5\end{array}\right]\]
\[\displaystyle\xrightarrow{-2R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&-1&2&1\\
0&3&-3&6\\
1&1&0&5\end{array}\right]\xrightarrow{-1R_{1}+R_{3}}\left[\begin{array}[]{ccc|c}\boxed{1}&-1&2&1\\
0&3&-3&6\\
0&2&-2&4\end{array}\right]\]
@col
Now, with $j=2$,
\[\displaystyle\xrightarrow{\frac{1}{3}R_{2}}\left[\begin{array}[]{ccc|c}\boxed{1}&-1&2&1\\
0&1&-1&2\\
0&2&-2&4\end{array}\right]\xrightarrow{1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}\boxed{1}&0&1&3\\
0&1&-1&2\\
0&2&-2&4\end{array}\right]\]
\[\displaystyle\xrightarrow{-2R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}\boxed{1}&0&1&3\\
0&\boxed{1}&-1&2\\
0&0&0&0\end{array}\right]\]
@col
The system of equations represented by this augmented matrix needs to be considered a bit differently than the previous case.
First, the last row of the matrix is the equation $0=0$, which is always true, so it imposes no restrictions on our possible solutions and therefore we can safely ignore it as we analyze the other two equations. These equations are:

@col
\[\displaystyle x_{1}+x_{3}=3\]
\[\displaystyle x_{2}-x_{3}=2.\]
@col
While this system is fairly easy to solve, it also appears to have a multitude of solutions.
For example, choose $x_{3}=1$ and see that then $x_{1}=2$ and $x_{2}=3$ will together form a solution.
Or choose $x_{3}=0$, and then discover that $x_{1}=3$ and $x_{2}=2$ lead to a solution.
Try it yourself: pick any value of $x_{3}$ you please, and figure out what $x_{1}$ and $x_{2}$ should be to make the first and second equations (respectively) true. We’ll wait while you do that. Because of this behavior, we say that $x_{3}$ is a <b>free</b> or <b>independent</b> variable. But why do we vary $x_{3}$ and not some other variable? For now, notice that the third column of the augmented matrix is not a pivot column. With this idea, we can rearrange the two equations, solving each for the variable whose index is the same as the column index of a pivot column.
\[\displaystyle x_{1}=3-x_{3}\]
\[\displaystyle x_{2}=2+x_{3}\]
To write the set of solution vectors in set notation, we have:

@col
\[\displaystyle
S =
\left\{\left.\begin{bmatrix}3-x_{3}\\
2+x_{3}\\
x_{3}\end{bmatrix}\,\right|\,x_{3}\in\mathbb{R}\right\}
=
\left\{\left.\colvector{3\\2\\0} + x_3\colvector{-1\\1\\1}
\,\right|\,x_{3}\in\mathbb{R}\right\}
\]
We will learn more in the next lecture about systems with infinitely many solutions and how to express their solution sets.
@endcol
@end
@slide
@eg
Let us find the solutions to the following system of equations,
\begin{align*}
2x_{1}+x_{2}+7x_{3}-7x_{4} &= 2\\
-3x_{1}+4x_{2}-5x_{3}-6x_{4} &= 3\\
x_{1}+x_{2}+4x_{3}-5x_{4} &= 2
\end{align*}
@newcol
First, form the augmented matrix,
\[\left[\begin{array}[]{cccc|c}2&1&7&-7&2\\
-3&4&-5&-6&3\\
1&1&4&-5&2\\
\end{array}\right]\]
and work to reduced row-echelon form, first with $j=1$,
\[\displaystyle\xrightarrow{R_{1}\leftrightarrow R_{3}}\left[\begin{array}[]{
cccc|c}1&1&4&-5&2\\
-3&4&-5&-6&3\\
2&1&7&-7&2\end{array}\right]\xrightarrow{3R_{1}+R_{2}}\left[\begin{array}[]{
cccc|c}1&1&4&-5&2\\
0&7&7&-21&9\\
2&1&7&-7&2\end{array}\right]\]
\[\displaystyle\xrightarrow{-2R_{1}+R_{3}}\left[\begin{array}[]{cccc|c}\boxed{1}&1&4&-5&2\\
0&7&7&-21&9\\
0&-1&-1&3&-2\end{array}\right]\]
@col
Now, with $j=2$,
\[\displaystyle\xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{
cccc|c}\boxed{1}&1&4&-5&2\\
0&-1&-1&3&-2\\
0&7&7&-21&9\end{array}\right]\xrightarrow{-1R_{2}}\left[\begin{array}[]{cccc|c}\boxed{1}&1&4&-5&2\\
0&1&1&-3&2\\
0&7&7&-21&9\end{array}\right]\]
\[\displaystyle\xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&1&1&-3&2\\
0&7&7&-21&9\end{array}\right]\xrightarrow{-7R_{2}+R_{3}}\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&2\\
0&0&0&0&-5\end{array}\right]\]
@col
And finally, with $j=4$,
\[\displaystyle\xrightarrow{-\frac{1}{5}R_{3}}\left[\begin{array}[]{cccc|c}
\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&2\\
0&0&0&0&1\end{array}\right]\xrightarrow{-2R_{3}+R_{2}}\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&0\\
0&0&0&0&\boxed{1}\end{array}\right]\]
@col
The third equation will read $0=1$. This is patently false, all the time. No choice of values for our variables will ever make it true. We are done. Since we cannot even make the last equation true, we have no hope of making all of the equations simultaneously true. So this system has <b>no solutions</b>, and its solution set is the empty set, $\emptyset=\{\ \}$
Notice that we could have reached this conclusion sooner. After performing the row operation
$-7R_{2}+R_{3}$, we can see that the third equation reads $0=-5$, a false statement. Since the system represented by this matrix has no solutions, none of the systems represented has any solutions. However, for this example, we have chosen to bring the matrix all the way to reduced row-echelon form as practice.
The above three examples
illustrate the full range of possibilities for a system of linear equations –
no solutions, one solution, or infinitely many solutions.
In the next lecture we will examine these three scenarios more closely.
@endcol
@end
@setchapter{7}
@chapter{Types of Solution Sets}

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Subsection TTS (print version p35 - p41)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf
Section TSS (p.13 - 18) C21-28, T11, T20, T40, T41.
@section{
Introduction
}
We will now be more careful about analyzing the reduced row-echelon form derived from the augmented matrix of a system of linear equations.
In particular, we will see how to systematically handle the situation when we have infinitely many solutions to a system, and we will prove that every system of linear equations has either zero, one or infinitely many solutions.
With these tools, we will be able to routinely solve any linear system.
@section{
Consistent Systems
}
@label{CS}
A system of linear equations is <b>consistent</b>
if it has at least one solution. Otherwise, the system is called <b>inconsistent</b>.
@eg
@newcol
@enumerate
@item
The system of linear equations
\begin{align*}
2x_1 + 3x_2 &= 3 \\
x_1 - x_2 &= 4
\end{align*}
is consistent because it has solution $(x_{1},x_{2})=(1,-3)$.
@item
The system of linear equations
\begin{align*}
2x_1+3x_2&=3\\
4x_1+6x_2&=6
\end{align*}
is consistent because has infinite many solutions: $\{(t,\frac{3-2t}{3})\,|\,t\text{ real number}\}$.
@item
The system of linear equations
\begin{align*}
2x_1+3x_2&=3\\
4x_1+6x_2&=10
\end{align*}
is inconsistent because it has no solution.
@endenumerate
@endcol
@end
<strong>Notation Recall</strong>:
@newcol
Let $A$ be a reduced row echelon form.
@enumerate
@item
The number of non-zero rows is called the <b>rank</b> of $A$ and is denoted by $r$.
@item
The set of the column indexes for the pivot columns is denoted by
\[D=\{(d_{1},d_{2},d_{3},\ldots,d_{r})\,|\,d_{1} < d_{2} < d_{3} < \cdots < d_{r}.\}\]
@item
The set of column indexes that are not pivot columns is denoted by
\[F=\{(f_{1},f_{2},f_{3},\ldots,f_{n-r})\,|\,f_{1} < f_{2} < f_{3}< \cdots < f_{n-r}.\}\]
@endenumerate
@slide
@eg
@label{RREFN}
Reduced row-echelon form notation
For the $5\times 9$ matrix
\[\displaystyle B=\begin{bmatrix}\boxed{1}&5&0&0&2&8&0&5&-1\\
0&0&\boxed{1}&0&4&7&0&2&0\\
0&0&0&\boxed{1}&3&9&0&3&-6\\
0&0&0&0&0&0&\boxed{1}&4&2\\
0&0&0&0&0&0&0&0&0\end{bmatrix}\]
in reduced row-echelon form we have:

@col
\begin{align*}
r&=4\\
d_1&=1
&
d_2&=3
&
d_3&=4
&
d_4&=7\\
f_1&=2
&
f_2&=5
&
f_3&=6
&
f_4&=8
&
f_5&=9
\end{align*}
@col
Notice that the sets
\[\displaystyle D=\{d_{1},\,d_{2},\,d_{3},\,d_{4}\}=\{1,\,3,\,4,\,7\}\]
have nothing in common and together account for all of the columns of $B$.
@end
@slide
@eg
@label{ISSI}
Describe the infinite solution set of the following system of linear equations
with $m=4$ equations in $n=7$ variables.
\begin{align*}
x_1 +4x_2  - x_4  + 7x_6 - 9x_7 &= 3\\
2x_1 + 8x_2 - x_3 + 3x_4 + 9x_5 - 13x_6 + 7x_7 &= 9\\
 2x_3 -3x_4 -4x_5 +12x_6 -8x_7 &= 1\\
-x_1  - 4x_2 + 2x_3 +4x_4 + 8x_5 - 31x_6 + 37x_7 &= 4
\end{align*}
@col
This system has a $4\times 8$ augmented matrix
\[\left[\begin{array}[]{ccccccc|c}1&4&0&-1&0&7&-9&3\\
2&8&-1&3&9&-13&7&9\\
0&0&2&-3&-4&12&-8&1\\
-1&-4&2&4&8&-31&37&4\end{array}\right]\]
The matrix is row-equivalent to the following matrix reduced row-echelon form
(<strong>exercise</strong>: check this)

@col
\[\left[\begin{array}[]{ccccccc|c}\boxed{1}&4&0&0&2&1&-3&4\\
0&0&\boxed{1}&0&1&-3&5&2\\
0&0&0&\boxed{1}&2&-6&6&1\\
0&0&0&0&0&0&0&0\end{array}\right]\]
So we find that $r=3$ and
\[\displaystyle D=\{d_{1},\,d_{2},\,d_{3}\}=\{1,\,3,\,4\}\]
Let $i$ denote any one of the $r=3$ nonzero rows. Then the index $d_{i}$ is a pivot column. It will be easy in this case to use the equation represented by row $i$ to write an expression for the variable $x_{d_{i}}$. It will be a linear function of the variables $x_{f_{1}},\,x_{f_{2}},\,x_{f_{3}},\,x_{f_{4}}$

@col
\begin{align*}
(i=1)& & x_{d_1}&=x_1=4-4x_2-2x_5-x_6+3x_7\\
(i=2)& & x_{d_2}&=x_3=2-x_5+3x_6-5x_7\\
(i=3)& & x_{d_3}&=x_4=1-2x_5+6x_6-6x_7
\end{align*}
@col
Each element of the set $F=\{f_{1},\,f_{2},\,f_{3},\,f_{4},\,f_{5}\}=\{2,\,5,\,6,\,7,\,8\}$ is the index of a variable, except for $f_{5}=8$. We refer to $x_{f_{1}}=x_{2}$, $x_{f_{2}}=x_{5}$, $x_{f_{3}}=x_{6}$ and $x_{f_{4}}=x_{7}$ as <b>free</b> (or <b>independent</b>) variables since they are allowed to assume any possible combination of values that we can imagine and we can continue on to build a solution to the system by solving individual equations for the values of the other (<b>dependent</b>) variables.

@col
Each element of the set $D=\{d_{1},\,d_{2},\,d_{3}\}=\{1,\,3,\,4\}$ is the index of a variable. We refer to the variables $x_{d_{1}}=x_{1}$, $x_{d_{2}}=x_{3}$ and $x_{d_{3}}=x_{4}$ as <b>dependent</b> variables since they depend on the independent variables. More precisely, for each possible choice of values for the independent variables we get exactly one set of values for the dependent variables that combine to form a solution of the system.

@col
To express the solutions as a set, we write
\begin{equation}\left\{\left.\begin{bmatrix}
4-4x_{2}-2x_{5}-x_{6}+3x_{7}\\
x_{2}\\
2-x_{5}+3x_{6}-5x_{7}\\
1-2x_{5}+6x_{6}-6x_{7}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}\,\right|\,x_{2},\,x_{5},\,x_{6},\,x_{7}\in \mathbb{R}\right\}
\end{equation}
or equivalently:
\[
\left\{
\left.
\colvector{4\\0\\2\\1\\0\\0\\0}
+
x_2\colvector{-4\\1\\0\\0\\0\\0\\0}
+
x_5\colvector{-2\\ 0\\ -1\\ -2\\ 1\\ 0\\ 0}
+
x_6\colvector{-1\\ 0\\ 3\\ 6\\ 0\\ 1\\ 0}
+
x_7\colvector{3\\ 0\\ -5\\ -6\\ 0\\ 0\\ 1}
\,\right|\,
x_2, x_5, x_6, x_7 \in \mathbb{R}
\right\}
\]
@col
The condition that $x_{2},\,x_{5},\,x_{6},\,x_{7}$ are real numbers is how we specify that the variables $x_{2},\,x_{5},\,x_{6},\,x_{7}$ are <strong>free</strong> to assume any possible values.

@col
This systematic approach to solving a system of equations will allow us to create a precise description of the solution set for any consistent system once we have found the reduced row-echelon form of the augmented matrix. It will work just as well when the set of free variables is empty and we get just a single solution.
@end
@slide
@defn
@title{Independent and Dependent Variables}
@label{IDV}
Suppose $A$ is the augmented matrix of a consistent system of linear equations and $B$ is a row-equivalent matrix in reduced row-echelon form.
Suppose $j$ is the index of a pivot column of $B$. Then the variable $x_{j}$ is <strong>dependent</strong>.
A variable that is not dependent is called <strong>independent</strong> or <strong>free</strong>.
@keyword{dependent variable}
@keyword{independent variable}
@keyword{free variable}
@end
@eg
@label{FDV}
<b>Free and dependent variables</b>

@col
Consider the system of five equations in five variables,
\begin{align*}
 x_1  - x_2  -2 x_3 +  x_4 + 11 x_5 &= 13 \\
x_1 - x_2 +  x_3+  x_4 + 5 x_5 &= 16 \\
 2 x_1  -2 x_2       +  x_4 + 10 x_5 &= 21 \\
 2 x_1  -2 x_2  - x_3 + 3 x_4 + 20 x_5 &= 38 \\
 2 x_1  -2 x_ 2 +  x_3 +  x_4 + 8 x_ 5&= 22
\end{align*}
whose augmented matrix row-reduces to:

@col
\begin{equation}
    \left[\begin{array}[]{ccccc|c}\boxed{1}&-1&0&0&3&6\\
    0&0&\boxed{1}&0&-2&1\\
    0&0&0&\boxed{1}&4&9\\
    0&0&0&0&0&0\\
    0&0&0&0&0&0\end{array}\right]
\end{equation}
@col
Columns 1, 3 and 4 are pivot columns, so $D=\{1,\,3,\,4\}$. From this we know that the variables $x_{1}$, $x_{3}$ and $x_{4}$ will be dependent variables, and each of the $r=3$ nonzero rows of the row-reduced matrix will yield an expression for one of these three variables. The set $F$ is all the remaining column indices, $F=\{2,\,5,\,6\}$. The column index $6$ in $F$ means that the final column is not a pivot column, and thus the system is consistent (see the next theorem). The remaining indices in $F$ indicate free variables, so $x_{2}$ and $x_{5}$ (the remaining variables) are our free variables. The resulting three equations that describe our solution set are then,

@col
\begin{align*}
(x_{d_1}=x_1)& & x_1&=6+x_2-3x_5\\
(x_{d_2}=x_3)& & x_3&=1+2x_5\\
(x_{d_3}=x_4)& & x_4&=9-4x_5
\end{align*}
@col
Make sure you understand where these three equations came from, and notice how the location of the pivot columns determined the variables on the left-hand side of each equation. We can compactly describe the solution set as,
\begin{equation}
    S=\left\{\left.\begin{bmatrix}6+x_{2}-3x_{5}\\
    x_{2}\\
    1+2x_{5}\\
    9-4x_{5}\\
    x_{5}\end{bmatrix}\,\right|\,x_{2},\,x_{5}\text{ real numbers}\right\}
\end{equation}
@col
Notice how we express the freedom for $x_{2}$ and $x_{5}$: $x_{2},\,x_{5}\text{ real numbers}$.
@end
@slide
@thm
@title{Recognizing Consistency of a Linear System}
@label{RCLS}
Suppose $A$ is the augmented matrix of a system of linear equations with $n$ variables. Suppose also that $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ nonzero rows.
@ul
@li
Then the system of equations is <b>inconsistent</b> if and only if column $n+1$ (i.e., the last column) of $B$ is a pivot column.
@li
Equivalently a system is <b>consistent</b> if and only if column $n+1$ is not a pivot column of $B$.
@li
Another way of expressing the theorem is to say that
a system of linear equations is <b>consistent</b>
if and only if the last <b>non-zero row</b> is not $(0,0,\ldots,0,1)$.
@endul
@end
@proof
@col
(sketch, for details, see Beezer, Ver 3.5 print version p.38, proof of theorem RCLS, you can skip the proof in the textbook)
If the last column vector of $B$ is a pivot column, then $B$ is in the form of:

@col
\[\begin{bmatrix}
1 & \cdots & 0 & \cdots & 0 & \cdots & * & 0\\
  &        & 1 & \cdots & 0 & \cdots & * & 0\\
  &        &   &        & 1 & \cdots & * & 0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
0 & \cdots & 0 & \cdots & 0 & \cdots & 0 &1\\
0 & \cdots & 0 & \cdots & 0 & \cdots & 0 &0\\
&&&&&&&
\end{bmatrix}\]
For the system of linear equations with the above augmented matrix, the $r+1$-st equation (i.e. the last non-zero equation) is
\[0=1.\]
So the system of linear equations has no solution.
Conversely, if the last column vector is not a pivot column vector, then $B$ is in the form of:

@col
\[\begin{bmatrix}
1 & \cdots & 0 & \cdots & 0 & \cdots & 0 & \cdots\\
  &        & 1 & \cdots & 0 & \cdots & 0 & \cdots\\
  &        &   &        & 1 & \cdots & 0 & \cdots\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
0 & \cdots & 0 & \cdots & 0 & \cdots & 1 & \cdots\\
0 & \cdots & 0 & \cdots & 0 & \cdots & 0 &0\\
&&&&&&&
\end{bmatrix}\]
For the system of equations with the above augmented matrix,
we can move the variables corresponding to the non-pivot columns (i.e., $x_{f_{1}},x_{f_{2}},\ldots$) to the right hand side of the equations and therefore solve the equations.
Hence it is consistent.
Note that $x_{f_{1}},x_{f_{2}},\ldots$ are free variables.
@qed
@end
@slide
@eg
Determine if the following system of linear equation is consistent.
\begin{align*}
x_1+x_2+2 x_3+3 x_4+2 x_5+5 x_6 &=1 \\
2 x_1+2 x_2+3 x_3-x_4 &=1 \\
3 x_1+3 x_2+5 x_3+x_4+x_5-2 x_6 &=3 \\
x_4+x_5+7 x_6 &=0
\end{align*}
@col
The augmented matrix is
\[\left[\begin{array}[]{cccccc|c}1&1&2&3&2&5&1\\
2&2&3&-1&0&0&1\\
3&3&5&1&1&-2&3\\
0&0&0&1&1&7&-1\\
\end{array}\right]\]
@col
The reduced row echelon form is
\[\left[\begin{array}[]{cccccc|c}1&1&0&0&5&62&0\\
0&0&1&0&-3&-39&0\\
0&0&0&1&1&7&0\\
0&0&0&0&0&0&1\\
\end{array}\right]\]
@col
The last column is a pivot column. So the system is inconsistent.
@end
@eg
@col
Determine if the following system of linear equation is consistent.
\begin{align*}
x_1+x_2+2 x_3+3 x_4+2 x_5+5 x_6 &=1 \\
2 x_1+2 x_2+3 x_3-x_4 &=1 \\
3 x_1+3 x_2+5 x_3+x_4+x_5-2 x_6 &=3 \\
x_4+x_5+7 x_6 &=-1
\end{align*}
@col
The augmented matrix is
\[\left[\begin{array}[]{cccccc|c}1&1&2&3&2&5&1\\
2&2&3&-1&0&0&1\\
3&3&5&1&1&-2&3\\
0&0&0&1&1&7&-1\\
\end{array}\right]\]
@col
The reduced row echelon form is
\[\left[\begin{array}[]{cccccc|c}1&1&0&0&5&62&-12\\
0&0&1&0&-3&-39&8\\
0&0&0&1&1&7&-1\\
0&0&0&0&0&0&0\\
\end{array}\right]\]
@col
The last column is not a pivot column. So the system is consistent.
@end
@slide
@thm
@title{Consistent Systems, $r$ and $n$}
@label{CSRN}
Suppose $A$ is the augmented matrix of a consistent system of linear equations with $n$ variables. Suppose also that $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Then $r\leq n$. If $r=n$, then the system has a unique solution, and if $r < n$, then the system has infinitely many solutions.
@end
@proof
@col
This theorem contains three implications that we must establish.
Notice first that $B$ has $n+1$ columns, so there can be at most $n+1$ pivot columns, i.e., $r\leq n+1$.
If $r=n+1$, then every column of $B$ is a pivot column, and in particular, the last column is a pivot column.
So the previous theorem tells us that the system is inconsistent, contrary to our hypothesis. We are left with $r\leq n$.


When $r=n$, we find $n-r=0$ free variables (i.e., $F=\{n+1\}$) and the only solution is given by setting the $n$ variables to the the first $n$ entries of column $n+1$ of $B$.
When $r < n$, we have $n-r > 0$ free variables. Choose one free variable and set all the other free variables to zero. Now, set the chosen free variable to any fixed value. It is possible to then determine the values of the dependent variables to create a solution to the system. By setting the chosen free variable to different values, in this manner we can create infinitely many solutions.
@qed
@end
@section{
Free variables
}
@label{FV}
The next theorem simply states a conclusion from the final paragraph of the previous proof, allowing us to state explicitly the number of free variables for a consistent system.
@thm
@title{Free Variables for Consistent Systems}
@label{FVCS}
Suppose $A$ is the augmented matrix of a consistent system of linear equations with $n$ variables. Suppose also that $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ rows that are not completely zeros. Then the solution set can be described with $n-r$ free variables.
@end
@slide
@eg
@enumerate
@item
System of linear equations with $n=3$, $m=3$.

@col
\begin{align*}
x_1 -x_2 +2x_3 & =1\\
2x_1+ x_2 + x_3 & =8\\
x_1 + x_2 & =5
\end{align*}
Augmented matrix
\[\left[\begin{array}[]{ccc|c}1&-1&2&1\\
2&1&1&8\\
1&1&0&5\end{array}\right]\]
The reduced row echelon form of the augmented matrix.
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&1&3\\
0&\boxed{1}&-1&2\\
0&0&0&0\end{array}\right]\]
The last column is not a pivot column. So the system of linear equations is consistent. $r=2$, there are $3-2$ free variables. In fact $D=\{1,2\}$, $F=\{3\}$. $x_{1},x_{2}$ are dependent variables, $x_{3}$ is a free variables.
\begin{align*}
x_1 &= 3-x_3 \\
x_2 &= 2+x_3
\end{align*}
<hr/>
@item
System of linear equations with $n=3,m=3$.

@col
\begin{align*}
-7x_1 -6 x_2 - 12x_3 &=-33\\
 5x_1  + 5x_2 + 7x_3 &=24\\
 x_1 +4x_3 &=5
\end{align*}
Augmented matrix
\[\left[\begin{array}[]{ccc|c}-7&-6&-12&-33\\
5&5&7&24\\
1&0&4&5\end{array}\right]\]
The reduced row echelon form of the augmented matrix.
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&0&-3\\
0&\boxed{1}&0&5\\
0&0&\boxed{1}&2\end{array}\right]\]
The last column is not a pivot column. So the system of linear equations is consistent. $r=3$, there are $3-3=0$ free variables. So the solution is unique. In fact
In fact
\begin{align*}
x_1 &= -3 \\
x_2 &= 5 \\
x_3 &= 2
\end{align*}
<hr/>
@item
System of linear equations with $n=2$, $m=5$.

@col
\begin{align*}
2x_1  + 3x_2  &= 6 \\
-x_1 + 4x_2  &=  -14 \\
3x_1 +10x_2  &=  -2  \\
3x_1 - x_2  &=  20  \\
6x_1 + 9x_2  &=  18
\end{align*}
Augmented matrix
\[\left[\begin{array}[]{cc|c}2&3&6\\
-1&4&-14\\
3&10&-2\\
3&-1&20\\
6&9&18\end{array}\right]\]
The reduced row echelon form of the augmented matrix.
\[\left[\begin{array}[]{cc|c}\boxed{1}&0&6\\
0&\boxed{1}&-2\\
0&0&0\\
0&0&0\\
0&0&0\end{array}\right]\]
The last column is not a pivot column. So the system of linear equations is consistent. $r = 2$, there are $2-2=0$ free variables. So the solution is unique. In fact
\[\displaystyle x_{1}=6\]
\[\displaystyle x_{2}=-2\]
<hr/>
@item
System of linear equations with $n=4,m=3$.

@col
\begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &= 2 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &=  3 \\
x_1 +x_2 + 4x_3 - 5x_4 &=  2
\end{align*}
Augmented matrix
\[\left[\begin{array}[]{cccc|c}2&1&7&-7&2\\
-3&4&-5&-6&3\\
1&1&4&-5&2\end{array}\right]\]
The reduced row echelon form of the augmented matrix.
\[\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&0\\
0&0&0&0&\boxed{1}\end{array}\right]\]
The last column is a pivot column. Hence the system of linear equations is inconsistent. It has no solution.
<hr/>
@endenumerate
@end
@slide
@thm
@title{Possible Solution Sets for Linear Systems}
@label{PSSLS}
A system of linear equations has no solutions, a unique solution or infinitely many solutions.
@end
@proof
@col
If the system is inconsistent, that it has no solutions.Suppose the system is consistent. If it has $0$ free variable, it has a unique solution.
If it has $\geq 1$ free variables, it has infinite many solutions.
@qed
@end
@thm
@title{Consistent, More Variables than Equations, Infinite solutions}
@label{CMVEI}
@col
Suppose a consistent system of linear equations has $m$ equations in $n$ variables. If $n>m$, then the system has infinitely many solutions.
@end
@proof
@col
Suppose that the augmented matrix of the system of equations is row-equivalent to $B$, a matrix in reduced row-echelon form with $r$ nonzero rows.
Because $B$ has $m$ rows in total, the number of nonzero rows is less than or equal to $m$. In other words, $r\leq m$.
Follow this with the hypothesis that $n>m$ and we find that the system has a solution set described by at least one free variable because
\[n-r\geq n-m>0.\]
A consistent system with free variables will have an infinite number of solutions, as given by Theorem        @ref{CSRN}.
@qed
@end
<!-- @slide -->
<!-- @eg -->
<!-- \begin{align*} -->
<!-- 2x_1  + x_2 + 7x_3 - 7x_4 &= 8 \\ -->
<!-- -3x_1 + 4x_2 -5x_3 - 6x_4 &=  -12 \\ -->
<!-- x_1 +x_2 + 4x_3 - 5x_4 &=  4 -->
<!-- \end{align*} -->
<!-- Because $n=4$, $m=3$ and $n>m$. Hence it is consistent and has infinitely many solutions by the above theorem. -->
<!-- @end -->
@slide
These theorems give us the procedures and implications that allow us to completely solve any system of linear equations. The main computational tool is using row operations to convert an augmented matrix into reduced row-echelon form. Here is a broad outline of how we would instruct a computer to solve a system of linear equations.
<h5>Steps of Solving a System of Linear Equations.</h5>
@enumerate
@item
@col
Represent a system of linear equations in $n$ variables by an augmented matrix.
@item
@col
Convert the matrix to a row-equivalent matrix in reduced row-echelon form using the procedure given in lecture 3 Theorem 2.
Identify the location of the pivot columns, and the rank $r$.
@item
@col
If column $n+1$ is a pivot column, then the system is inconsistent.
@item
@col
If column $n+1$ is not a pivot column, there are two possibilities:
@enumerate
@item
$r=n$ and the solution is unique. It can be read off directly from the entries in rows 1 through $n$ of column $n+1$.
@item
$r < n$ and there are infinitely many solutions. we can describe the solution sets by the free variables.
@endenumerate
@endenumerate
@setchapter{8}
@chapter{Homogeneous Systems of Equations and Nonsingular Matrices}

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section HSE (print version p44 - p50)Section NM (print version p51 - p56)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf
@enumerate
@item
Section HSE (ex p.18-23) C21-C23, C25-C27, C30-C31, M50-M52, T10-T12, T20
@item
Section NM (ex p.23-27) C30-C33, C50, M30, M51-M52, T10, T12, T30, T31, T90.
@endenumerate
@section{
Solutions of Homogeneous Systems}
@label{SHS}
@defn
@title{Homogeneous System}
@label{HS}
A system of linear equations, $\mathcal{LS}(A, \mathbf{b})$ is <b>homogeneous</b>
if the vector of constants is the zero vector, in other words, if $\mathbf{b}=\mathbf{0}$, i.e.
\begin{align*}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n  &=0 \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n  &=0\\
\vdots &\\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n &= 0
\end{align*}
@end
@defn
@title{Homogeneous System corresponding to system of linear equation}
@col
The <b>homogeneous system</b> corresponding to $\linearsystem{A}{\vect{b}}$:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=b_1\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=b_2\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=b_3\\
\vdots&\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=b_m
\end{align*}
is $\linearsystem{A}{\vect{0}}$:
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=0\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=0\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=0\\
\vdots&\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=0
\end{align*}
@end
@slide
@eg
The following is a homogeneous system of linear equations:
\begin{align*}
x_1-2x_2+3x_3 - 4x_4 &= 0 \\
x_2-x_4 &=0 \\
x_1+3x_2 - 5x_3 + 5 x_4 &= 0
\end{align*}
It is the homogeneous system of linear equations corresponding to
\begin{align*}
x_1-2x_2+3x_3 - 4x_4 &= 1 \\
x_2-x_4 &=2 \\
x_1+3x_2 - 5x_3 + 5 x_4 &= 3
\end{align*}
@end
@slide
@thm
@title{Homogeneous Systems are Consistent}
@label{HSC}
Suppose that a system of linear equations is homogeneous.
Then the system is consistent. In fact $\mathbf{0}$ is a solution, i.e $x_{1}=x_{2}=\cdots=x_{n}=0$ is a solution. Such solution is called a <b>trivial solution</b>.
@end
@proof
@col
Set each variable of the system to zero. The left hand side of the all equations are zero, which are equal to the right hand side.
@qed
@end
@slide
@eg
@enumerate
@item
<br/>
\begin{align*}
-7x_1 -6 x_2 - 12x_3 &=0\\
 5x_1  + 5x_2 + 7x_3 &=0\\
 x_1 +4x_3 &=0
\end{align*}
The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&0&0\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&0\end{array}\right]\]
It has $n-r=3-3=0$ free variable. Hence it has only one solution.
@item
@col
\begin{align*}
x_1 -x_2 +2x_3 & =0\\
2x_1+ x_2 + x_3 & =0\\
x_1 + x_2 & =0
\end{align*}The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&1&0\\
0&\boxed{1}&-1&0\\
0&0&0&0\end{array}\right]\]
The system is consistent. It has $n-r=3-2=1$ free variable. The solution set is
\[S=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}\,\right|\,x_{1}=-x_{3},\,x_{2}=x_{3}\right\}=\left\{\left.\begin{bmatrix}-x_{3}\\
x_{3}\\
x_{3}\end{bmatrix}\,\right|\,x_{3}\text{ real number}\right\}\]
Geometrically, these are points in three dimensions that lie on a line through the origin.
@item
@col
\begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &= 0 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &=  0 \\
x_1 +x_2 + 4x_3 - 5x_4 &=  0
\end{align*}
The reduced row echelon form of the augmented matrix is
\[\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&0\\
0&0&0&0&0\end{array}\right]\]
The system is consistent. It has $n-r=4-2=2$ free variables. The solution set is
\[\displaystyle S=\left\{\left.\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{1}=-3x_{3}+2x_{4},\,x_{2}=-x_{3}+3x_{4}\right\}\]
\[\displaystyle=\left\{\left.\begin{bmatrix}-3x_{3}+2x_{4}\\
-x_{3}+3x_{4}\\
x_{3}\\
x_{4}\end{bmatrix}\,\right|\,x_{3},\,x_{4}\text{ real numbers}\right\}\]
@endenumerate
@end
@slide
Notice that when we do row operations on the augmented matrix of a homogeneous system of linear equations the last column of the matrix is all zeros. Any one of the three allowable row operations will convert zeros to zeros and thus, the final column of the matrix in reduced row-echelon form will also be all zeros. So in this case, we may be as likely to reference only the coefficient matrix and presume that we remember that the final column begins with zeros, and after any number of row operations is still zero.
@slide
@thm
@label{HMVEI}
Suppose that a homogeneous system of linear equations has $m$ equations and $n$ variables with $n>m$.
Then the system has infinitely many solutions.
@end
@proof
@col
The system is homogeneous, by theorem @ref{HSC} it is consistent.
Then the hypothesis that $n>m$, together with @ref{CMVEI}, gives infinitely many solutions.
@qed
@end
If $n=m$, then we can have a unique solution or infinitely many solutions (see the above examples).
@section{
Null Space of a Matrix}
@defn
@label{NSM}
The <b>null space</b> of a matrix $A$, denoted by ${\mathcal{N}}\!\left(A\right)$, is the set of all the vectors that are solutions to the homogeneous system $\linearsystem{A}{\mathbf{0}}$.
That is, if:
\[A=\begin{bmatrix}a_{11}&a_{12}&a_{13}&\dots&a_{1n}\\
a_{21}&a_{22}&a_{23}&\dots&a_{2n}\\
a_{31}&a_{32}&a_{33}&\dots&a_{3n}\\
\vdots&\\
a_{m1}&a_{m2}&a_{m3}&\dots&a_{mn}\\
\end{bmatrix}\]
then ${\mathcal{N}}\!\left(A\right)$ is the solution set of
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=0\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=0\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=0\\
\vdots&\\
a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=0
\end{align*}
@end
@slide
@eg
Suppose
\[A=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}\]
Then
\[\mathbf{x}=\begin{bmatrix}3\\
0\\
-5\\
-6\\
0\\
0\\
1\end{bmatrix}\qquad\qquad\mathbf{y}=\begin{bmatrix}-4\\
1\\
-3\\
-2\\
1\\
1\\
1\end{bmatrix}\]
are in ${\mathcal{N}}\!\left(A\right)$ as $A\mathbf{x}=\mathbf{0}$, $A\mathbf{y}=\mathbf{0}$.

@col
However, the vector
\[\mathbf{z}=\begin{bmatrix}1\\
0\\
0\\
0\\
0\\
0\\
2\end{bmatrix}\]
is not in ${\mathcal{N}}\!\left(A\right)$ as
\[A\mathbf{z}=\begin{bmatrix}-17\\
16\\
-16\\
73\end{bmatrix}\neq\mathbf{0}.\]
@end
@slide
@eg
@label{CNS1}
Let us compute the null space of
\[A=\begin{bmatrix}2&-1&7&-3&-8\\
1&0&2&4&9\\
2&2&-2&-1&8\end{bmatrix}\]
which we write as ${\mathcal{N}}\!\left(A\right)$. Translating @ref{NSM},
we simply desire to solve the homogeneous system $\linearsystem{A}{\mathbf{0}}$. So we row-reduce the augmented matrix to obtain:

@col
\[\left[\begin{array}[]{ccccc|c}\boxed{1}&0&2&0&1&0\\
0&\boxed{1}&-3&0&4&0\\
0&0&0&\boxed{1}&2&0\end{array}\right]\]
The variables (of the homogeneous system) $x_{3}$ and $x_{5}$ are free (since columns 1, 2 and 4 are pivot columns), so we arrange the equations represented by the matrix in reduced row-echelon form to:

@col
\begin{align*}
x_1&=-2x_3-x_5\\
x_2&=3x_3-4x_5\\
x_4&=-2x_5
\end{align*}

@col
So we can write the infinite solution set as sets using column vectors,
\[{\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}-2x_{3}-x_{5}\\
3x_{3}-4x_{5}\\
x_{3}\\
-2x_{5}\\
x_{5}\end{bmatrix}\,\right|\,x_{3},\,x_{5}\text{ real numbers}\right\}.\]
@end
@slide
@eg
@label{CNS2}
Let us compute the null space of
\[C=\begin{bmatrix}-4&6&1\\
-1&4&1\\
5&6&7\\
4&7&1\end{bmatrix}\]
which we write as ${\mathcal{N}}\!\left(C\right)$. Translating definition   @ref{NSM}, we simply desire to solve the homogeneous system
$\linearsystem{C}{\mathbf{0}}$.
So we row-reduce the augmented matrix to obtain
\[\left[\begin{array}[]{ccc|c}\boxed{1}&0&0&0\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&0\\
0&0&0&0\end{array}\right]\]
@col
There are no free variables in the homogeneous system represented by the row-reduced matrix, so there is only the trivial solution, the zero vector, $\mathbf{0}$. So we can write the (trivial) solution set as
\[{\mathcal{N}}\!\left(C\right)=\{\mathbf{0}\}=\left\{\begin{bmatrix}0\\
0\\
0\end{bmatrix}\right\}.\]
@end
@section{
Augmented matrix vs Coefficient Matrix}
The augmented matrix for the homogeneous of system of linear equations
$\linearsystem{A}{\mathbf{0}}$ is $[A|\mathbf{0}]$.
Any row operators on $[A|\mathbf{0}]$ will not change the last zero columns.
If
\[A\xrightarrow{\text{row operations}}B\]
then
\[[A|\mathbf{0}]\xrightarrow{\text{same row operations}}[B|\mathbf{0}].\]
@col
Therefore, for the homogeneous system of linear equations, we can replace the augmented matrix by the coefficient matrix. Just remember there is actually a zero column as the last column. For example:

@col
\begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &= 0 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &=  0 \\
x_1 +x_2 + 4x_3 - 5x_4 &=  0
\end{align*}
We can start with coefficient matrix:

@col
\[A=\begin{bmatrix}2&1&6&-7\\
-3&4&-5&-6\\
1&1&4&-5\end{bmatrix}\]
The RREF is:

@col
\[\left[\begin{array}[]{cccc}\boxed{1}&0&3&-2\\
0&\boxed{1}&1&-3\\
0&0&0&0\end{array}\right]\]
The corresponding augmented matrix is:

@col
\[\left[\begin{array}[]{cccc|c}\boxed{1}&0&3&-2&0\\
0&\boxed{1}&1&-3&0\\
0&0&0&0&0\end{array}\right]\]
The system is consistent. It has $n-r=4-2=2$ free variables.
The solution set is:

@col
\begin{align*}
S&=\setparts{\colvector{x_1\\x_2\\x_3\\x_4}}{x_1=-3x_3+2x_4,\,x_2=-x_3+3x_4}\\
 &=\setparts{\colvector{-3x_3+2x_4\\-x_3+3x_4\\x_3\\x_4}}{ x_3,\,x_4  \text{ real numbers}}
\end{align*}
@section{
Nonsingular Matrices}
In this section we specialize further and consider matrices with equal numbers of rows and columns,
which when considered as coefficient matrices lead to systems with equal numbers of equations and variables.
@defn
@title{Square Matrix}
@label{SQM}
A matrix with $m$ rows and $n$ columns is <b>square</b> if $m=n$.
In this case, we say the matrix has <b>size</b> $n$.
To emphasize the situation when a matrix is not square, we will call it <b>rectangular</b>.
@end
@defn
@title{Nonsingular Matrix}
@label{NM}
@col
Suppose $A$ is a square matrix.
Suppose further that the solution set to the homogeneous linear system of equations $\linearsystem{A}{\mathbf{0}}$ is $\{\mathbf{0}\}$, in other words, the system has only the trivial solution.
Then we say that $A$ is a <b>nonsingular</b> matrix. Otherwise we say $A$ is a <b>singular</b> matrix.
@end
@slide
@eg
@enumerate
@item
@col
Let
\[A=\begin{bmatrix}1&-1&2\\
2&1&1\\
1&1&0\end{bmatrix}.\]
By Example 2 Part 2, the system of linear equations $\linearsystem{A}{\mathbf{0}}$ has nontrivial solutions.
Hence $A$ is singular.
@item
@col
Let
\[A=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.\]
By example 2 part 1, the system of linear equations $\linearsystem{A}{\mathbf{0}}$ has only trivial solutions.
So it is nonsingular.
@endenumerate
@end
@slide
Recall:
@defn
@title{Identity Matrix}
@label{IM}
The $m\times m$ <b>identity matrix</b>, $I_{m}$, is defined by
\[\displaystyle\left[I_{m}\right]_{ij}=\begin{cases}1&i=j\\
0&i\neq j\end{cases}\quad\quad 1\leq i,\,j\leq m\]
i.e.
\[I_{m}=\begin{bmatrix}1&0&0&\cdots&0\\
0&1&0&\cdots&0\\
0&0&1&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&1\end{bmatrix}.\]
@end
@eg
@col
The $4\times 4$ identity matrix is:
\[I_{4}=\begin{bmatrix}1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\end{bmatrix}.\]
@end
@col
Notice that an identity matrix is square, and in reduced row-echelon form.
Also, every column is a pivot column,
and every possible pivot column appears once.
@slide
@thm
@title{Nonsingular Matrices Row Reduce to the Identity Matrix}
@label{NMRRI}
Suppose that $A$ is a square matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Then $A$ is nonsingular if and only if $B$ is the identity matrix.
@end
@proof

($\Leftarrow$)
@col
Suppose $B$ is the identity matrix. When the augmented matrix $\left[A|\mathbf{0}\right]$ is row-reduced, the result is $\left[B|\mathbf{0}\right]=\left[I_{n}|\mathbf{0}\right]$. The number of nonzero rows is equal to the number of variables in the linear system of equations $\linearsystem{A}{\mathbf{0}}$, so $n=r$ and has $n-r=0$ free variables.
Thus, the homogeneous system $\linearsystem{A}{\mathbf{0}}$ has just one solution, which must be the trivial solution.
This is exactly the definition of a nonsingular matrix.
@endcol

($\Rightarrow$)
@col
If $A$ is nonsingular, then the homogeneous system
$\linearsystem{A}{\mathbf{0}}$ has a unique solution,
and has no free variables in the description of the solution set.
The homogeneous system is consistent,
by Lecture 4 Theorem 4, the homogeneous system has $n-r$ free variables.
Thus, $n-r=0$, and so $n=r$. So $B$ has $n$ pivot columns among its total of $n$ columns. This is enough to force $B$ to be the $n\times n$ identity matrix $I_{n}$ (why?).
@qed
@endcol
@end
@slide
@eg
\[A=\begin{bmatrix}1&-1&2\\
2&1&1\\
1&1&0\end{bmatrix}\]
is row equivalent to the reduced row echelon form
\[B=\begin{bmatrix}1&0&1\\
0&1&-1\\
0&0&0\end{bmatrix}.\]
Since $B$ is not the $3\times 3$ identity matrix, the above theorem tells us that $A$ is a singular matrix.
@end
@eg
@col
\[A=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.\]
It is row-equivalent to the reduced row echelon form
\[\begin{bmatrix}1&0&0\\
0&1&0\\
0&0&1\end{bmatrix}\]
Since $B$ is the $3\times 3$ identity matrix, $A$ is a nonsingular matrix by the above theorem.
@end
@section{
Null Space of a Nonsingular Matrix}
@label{NSNM}
@thm
@title{Nonsingular Matrices have Trivial Null Spaces}
@label{NMTNS}
Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if the null space of $A$ is the set containing only the zero vector, i.e., ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
@end
@proof
@col
The null space of a square matrix, $A$,
is equal to the set of solutions to the homogeneous system,
$\linearsystem{A}{\mathbf{0}}$. A matrix is nonsingular if and only if the set of solutions to the homogeneous system,
$\linearsystem{A}{\mathbf{0}}$,
has only a trivial solution.
These two observations may be chained together to construct the two proofs necessary for each half of this theorem.
@qed
@end
@slide
@thm
@title{Nonsingular Matrices and Unique Solutions}
@label{NMUS}
Suppose that $A$ is a square matrix. $A$ is a nonsingular matrix if and only if the system $\linearsystem{A}{\mathbf{b}}$ has a unique solution for every choice of the constant vector $\mathbf{b}$.
@end
@proof

($\Rightarrow$)
@col
The hypothesis for this half of the proof is that the system
$\linearsystem{A}{\mathbf{b}}$
has a unique solution for every choice of the constant vector $\mathbf{b}$.
We will make a very specific choice for $\mathbf{b}$: $\mathbf{b}=\mathbf{0}$.
Then we know that the system $\linearsystem{A}{\mathbf{0}}$ has a unique solution.
But this is precisely the definition of what it means for $A$
to be nonsingular.
@endcol

($\Leftarrow$)
@col
We assume that $A$ is nonsingular of size $n\times n$,
so we know there is a sequence of row operations that will convert $A$
into the identity matrix $I_{n}$ (Theorem   @ref{NMRRI}).
Form the augmented matrix $A^{\prime}=\left[A|\mathbf{b}\right]$
and apply this same sequence of row operations to $A^{\prime}$.
The result will be the matrix $B^{\prime}=\left[I_{n}|\mathbf{c}\right]$,
which is in reduced row-echelon form with $r=n$.
Then the augmented matrix $B^{\prime}$ represents the (extremely simple)
system of equations $x_{i}=\left[\mathbf{c}\right]_{i}$, $1\leq i\leq n$.
The vector $\mathbf{c}$ is clearly a solution, so the system is consistent.
With a consistent system, we use Lecture 4 Theorem 4 to count free variables.
We find that there are $n-r=n-n=0$ free variables,
and so we therefore know that the solution is unique.
@qed
@endcol
@end
@slide
@thm
@title{Nonsingular Matrix Equivalences}
@label{NME1}
Suppose that $A$ is a square matrix. The following are equivalent.
@enumerate
@item
$A$ is nonsingular.
@item
$A$ row-reduces to the identity matrix.
@item
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
@item
The linear system $\linearsystem{A}{\mathbf{b}}$ has a unique solution for every possible choice of $\mathbf{b}$.
@endenumerate
@end
@proof
@col
The statement that $A$ is nonsingular is equivalent to each of the subsequent statements by, in turn,
theorems   @ref{NMRRI},   @ref{NMTNS},   @ref{NMUS}.
So the statement of this theorem is just a convenient way to organize all these results.
@qed
@end
@section{
Particular Solutions, Homogeneous Solutions}
@label{PSHS}
The next theorem tells us that in order to find all of the solutions to a linear system of equations, it is sufficient to find just one solution, and then find all of the solutions to the corresponding homogeneous system. This explains part of our interest in the null space, the set of all solutions to a homogeneous system.
@thm
@title{Particular Solution Plus Homogeneous Solutions}
@label{PSPHS}
Suppose that $\mathbf{w}$ is one solution to the linear system of equations
$\linearsystem{A}{\mathbf{b}}$. Then $\mathbf{y}$ is a solution to
$\linearsystem{A}{\mathbf{b}}$ if and only if $\mathbf{y}=\mathbf{w}+\mathbf{z}$ for some vector $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, i.e.
@enumerate
@item
If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$, then $\mathbf{y}-\mathbf{w}\in{\mathcal{N}}\!\left(A\right)$
@item
If $\mathbf{z} \in {\mathcal{N}}\!\left(A\right)$, then $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$
@endenumerate
In other words, there is a one-to-one correspondence between
\[\text{solution set of $A\mathbf{x}=\mathbf{b}$}\longleftrightarrow{\mathcal{N}}\!\left(A\right),\]
through
\[\mathbf{y}\rightarrow\mathbf{y}-\mathbf{w},\]
\[\mathbf{w}+\mathbf{z}\leftarrow\mathbf{z}.\]
@end
@proof
@col
Because $\mathbf{w}$ is one solution to the linear system of equations
$\linearsystem{A}{\mathbf{b}}$, $A\mathbf{w}=\mathbf{b}$.
@enumerate
@item
If $\mathbf{y}$ is a solution to $A\mathbf{x}=\mathbf{b}$,
then $A\mathbf{y}=\mathbf{b}$.
Hence $A(\mathbf{y}-\mathbf{w})=A\mathbf{y}-A\mathbf{w}=\mathbf{b}-\mathbf{b}
=\mathbf{0}$.
So $\mathbf{y}-\mathbf{w}\in{\mathcal{N}}\!\left(A\right)$.
@item
Suppose $\mathbf{z}\in{\mathcal{N}}\!\left(A\right)$, $A\mathbf{z}=\mathbf{0}$.
So $A(\mathbf{w}+\mathbf{z})=A\mathbf{w}+A\mathbf{z}=
\mathbf{b}+\mathbf{0}=\mathbf{b}$.
Hence $\mathbf{w}+\mathbf{z}$ is a solution of $A\mathbf{x}=\mathbf{b}$.
@endenumerate
@qed
@end
@slide
@eg
\begin{align*}
2x_1  + x_2 + 7x_3 - 7x_4 &= 8 \\
-3x_1 + 4x_2 -5x_3 - 6x_4 &=  -12 \\
x_1 +x_2 + 4x_3 - 5x_4 &=  4
\end{align*}
is a consistent system of equations with a nontrivial null space.
Let $A$ denote the coefficient matrix of this system.

Consider the following three solutions to the system:

@col
\begin{align*}
\vect{y}_1=\colvector{0\\1\\2\\1}&&
\vect{y}_2=\colvector{4\\0\\0\\0}&&
\vect{y}_3=\colvector{7\\8\\1\\3}
\end{align*}
Let $\mathbf{w}=\mathbf{y}_{1}$. Then,
\[
\mathbf{y}_{2}-\mathbf{w}=\begin{bmatrix}4\\
-1\\
-2\\
-1\end{bmatrix},
\quad
\mathbf{y}_{3}-\mathbf{w}=\begin{bmatrix}7\\
7\\
-1\\
2\end{bmatrix}
\]
are indeed elements in ${\mathcal{N}}\!\left(A\right)$ (check!).

@col
To find all the solutions, we may first work out
(using Gaussian elimination on $\left[A|\mathbf{0}\right]$, for example) that:
\[{\mathcal{N}}\!\left(A\right)=\left\{\left.x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}
\]
By the theorem, the solution set to the linear system is:

@col
\[\mathbf{w}+{\mathcal{N}}\!\left(A\right)=\left\{\left.\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}\,\right|\,x_{3},\,x_{4}\in{\mathbb{R}}^{\hbox{}}\right\}\]
@end
@setchapter{9}
@chapter{Vector space and subspace}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section VO (print version p57 - p63)Subsection VS, EVS (print version p197-203)
Strang, Section 2.1
<p/>
<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection VO (p.28-31) All questions.C10-C15, T05-T07, T13, T17, T18, T30-T32 Section VS (p.75-77) Replace $\mathbb{C}$ (the set of complex numbers) by $\mathbb{R}$ (the set of real numbers) M11, M12, M13, M14, M15, M20.


@section{
Vectors
}
<strong>Notation</strong>: ${\mathbb{R}}^{\hbox{}}$ is the set of real numbers.
If $X$ is a set, $x\in X$ means $x$ is an element of the set $X$.
@defn
@title{Vector Space of Column Vectors}
@label{VSCV}
The vector space ${\mathbb{R}}^{m}$ is the set of all column vectors of size $m$ with entries from the set of real numbers, ${\mathbb{R}}^{\hbox{}}$. ${\mathbb{R}}^{m}$ is also called the <b>Euclidean $m$-space</b>.
@end


@defn
@title{Column Vector Equality}
@label{CVE}
@newcol
Suppose that $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$. Then $\mathbf{u}$ and $\mathbf{v}$ are <b>equal</b>, written $\mathbf{u}=\mathbf{v}$ if
\begin{align*}
\displaystyle\left[\mathbf{u}\right]_{i}&=\left[\mathbf{v}\right]_{i}& 1\leq i\leq m
\end{align*}
That is,
\begin{align*}
\displaystyle \begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}=\begin{bmatrix}v_{1}\\
v_{2}\\
\vdots\\
v_{m}\end{bmatrix}
\end{align*}
if
\begin{align*}
\displaystyle u_{i}&=v_{i}& 1\leq i\leq m.
\end{align*}
@endcol
@end
@slide
@eg
The system of linear equations:
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&=24 \\
\displaystyle x_{1}+4x_{3}&=5
\end{align*}
can be rewritten as:


@newcol
\begin{align*}
\displaystyle \begin{bmatrix}-7x_{1}-6x_{2}-12x_{3}\\
5x_{1}+5x_{2}+7x_{3}\\
x_{1}+4x_{3}\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*}
@endcol
@end
@slide
@defn
@title{Column Vector Addition}
@label{CVA}
Suppose that $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$. The <b>sum</b> of $\mathbf{u}$ and $\mathbf{v}$ is the vector $\mathbf{u}+\mathbf{v}$ defined by
\begin{align*}
\displaystyle\left[\mathbf{u}+\mathbf{v}\right]_{i}&=\left[\mathbf{u}\right]_{i}+\left[\mathbf{v}\right]_{i}& 1\leq i\leq m.
\end{align*}
That is
\begin{align*}
\displaystyle \begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}+\begin{bmatrix}v_{1}\\
v_{2}\\
\vdots\\
v_{m}\end{bmatrix}=\begin{bmatrix}u_{1}+v_{1}\\
u_{2}+v_{2}\\
\vdots\\
u_{m}+v_{m}\end{bmatrix}.
\end{align*}
@end
@eg
<strong>Addition of two vectors in ${\mathbb{R}}^{4}$</strong>

@newcol
If
\begin{align*}
\displaystyle\mathbf{u}=\begin{bmatrix}2\\
-3\\
4\\
2\end{bmatrix}&
\quad&
\displaystyle\mathbf{v}=\begin{bmatrix}-1\\
5\\
2\\
-7\end{bmatrix}
\end{align*}
then
\begin{align*}
\displaystyle \mathbf{u}+\mathbf{v}=\begin{bmatrix}2\\
-3\\
4\\
2\end{bmatrix}+\begin{bmatrix}-1\\
5\\
2\\
-7\end{bmatrix}=\begin{bmatrix}2+(-1)\\
-3+5\\
4+2\\
2+(-7)\end{bmatrix}=\begin{bmatrix}1\\
2\\
6\\
-5\end{bmatrix}
\end{align*}
@endcol
@end
@slide
@defn
@title{Column Vector Scalar Multiplication}
@label{CVSM}
Suppose $\mathbf{u}\in{\mathbb{R}}^{m}$ and $\alpha\in{\mathbb{R}}^{\hbox{}}$, then the <b>scalar multiple</b> of $\mathbf{u}$ by $\alpha$ is the vector $\alpha\mathbf{u}$ defined by
\begin{align*}
\displaystyle\left[\alpha\mathbf{u}\right]_{i}&=\alpha\left[\mathbf{u}\right]_{i}& 1\leq i\leq m.
\end{align*}
That is
\begin{align*}
\displaystyle \alpha\begin{bmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{m}\end{bmatrix}=\begin{bmatrix}\alpha u_{1}\\
\alpha u_{2}\\
\vdots\\
\alpha u_{m}\end{bmatrix}.
\end{align*}
@end
@eg
@newcol
If
\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}3\\
1\\
-2\\
4\\
-1\end{bmatrix}
\end{align*}
and $\alpha=6$, then
\begin{align*}
\displaystyle \alpha\mathbf{u}=6\begin{bmatrix}3\\
1\\
-2\\
4\\
-1\end{bmatrix}=\begin{bmatrix}6(3)\\
6(1)\\
6(-2)\\
6(4)\\
6(-1)\end{bmatrix}=\begin{bmatrix}18\\
6\\
-12\\
24\\
-6\end{bmatrix}.
\end{align*}
@endcol
@end
@slide
@eg
The system of linear equations
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&=24 \\
\displaystyle x_{1}+4x_{3}&=5
\end{align*}
can be written as:


@newcol
\begin{align*}
\displaystyle x_{1}\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+x_{2}\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*}
@endcol
@end
@section{
Vector Space Properties
}
@label{VSP}
<strong>Warning</strong>: Read the statements of Theorem @ref{VSPCV} and skip the rest of this section <strong>unless you are/going to be</strong> a math major. The material skipped will not appear in the tests and the final exam.
With definitions of vector addition and scalar multiplication we can state, and prove, several properties of each operation, and some properties that involve their interplay. We now collect ten of them here for later reference.
@thm
@title{Vector Space Properties of Column Vectors}
@label{VSPCV}
Suppose that ${\mathbb{R}}^{m}$ is the set of column vectors of size $m$ with addition and scalar multiplication as defined in Definition @ref{CVA} and Definition @ref{CVSM}
@label{ACC}
@label{SCC}
@label{CC}
@label{AAC}
@label{ZC}
@label{AIC}
@label{SMAC}
@label{DVAC}
@label{DSAC}
@label{OC}
.
Then:
<ol class="ltx_enumerate">
<li class="ltx_item">
<b>ACC</b>
<em>Additive Closure, Column Vectors</em>


If $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\mathbf{v}\in{\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
<b>SCC</b>
<em>Scalar Closure, Column Vectors</em>


If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then $\alpha\mathbf{u}\in{\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
<b>CC</b>
<em>Commutativity, Column Vectors</em>


If $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$.
</li>
<li class="ltx_item">
<b>AAC</b>
<em>Additive Associativity, Column Vectors</em>


If $\mathbf{u},\,\mathbf{v},\,\mathbf{w}\in{\mathbb{R}}^{m}$, then $\mathbf{u}+\left(\mathbf{v}+\mathbf{w}\right)=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}$.
</li>
<li class="ltx_item">
<b>ZC</b>
<em>Zero Vector, Column Vectors</em>


There is a vector, $\mathbf{0}$, called the <b>zero vector</b>, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}\in{\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
<b>AIC</b>
<em>Additive Inverses, Column Vectors</em>


If $\mathbf{u}\in{\mathbb{R}}^{m}$, then there exists a vector $\mathbf{-u}\in{\mathbb{R}}^{m}$ so that $\mathbf{u}+(\mathbf{-u})=\mathbf{0}$.
</li>
<li class="ltx_item">
<b>SMAC</b>
<em>Scalar Multiplication Associativity, Column Vectors</em>


If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then $\alpha(\beta\mathbf{u})=(\alpha\beta)\mathbf{u}$.
</li>
<li class="ltx_item">
<b>DVAC</b>
<em>Distributivity across Vector Addition, Column Vectors</em>


If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u},\,\mathbf{v}\in{\mathbb{R}}^{m}$, then $\alpha(\mathbf{u}+\mathbf{v})=\alpha\mathbf{u}+\alpha\mathbf{v}$.
</li>
<li class="ltx_item">
<b>DSAC</b>
<em>Distributivity across Scalar Addition, Column Vectors</em>


If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in{\mathbb{R}}^{m}$, then
$(\alpha+\beta)\mathbf{u}=\alpha\mathbf{u}+\beta\mathbf{u}$.
</li>
<li class="ltx_item">
<b>OC</b>
<em>One, Column Vectors</em>


If $\mathbf{u}\in{\mathbb{R}}^{m}$, then $1\mathbf{u}=\mathbf{u}$.
</li>
</ol>
@end
@proof
While some of these properties seem very obvious, they all require proof. However, the proofs are not very interesting, and border on tedious.
We will prove one version of distributivity very carefully, and you can test your proof-building skills on some of the others. We need to establish an equality, so we will do so by beginning with one side of the equality, apply various definitions and theorems (listed to the right of each step) to massage the expression from the left into the expression on the right. Here we go with a proof of Property DSAC in @ref{VSPCV}.
For $1\leq i\leq m$,
\begin{align*}
\displaystyle\left[(\alpha+\beta)\mathbf{u}\right]_{i}&=(\alpha+\beta)\left[\mathbf{u}\right]_{i}&\text{definition} \\
&=\alpha\left[\mathbf{u}\right]_{i}+\beta\left[\mathbf{u}\right]_{i} \\
&=\left[\alpha\mathbf{u}\right]_{i}+\left[\beta\mathbf{u}\right]_{i}&\text{definition} \\
&=\left[\alpha\mathbf{u}+\beta\mathbf{u}\right]_{i}&\text{definition}
\end{align*}
Since the individual components of the vectors $(\alpha+\beta)\mathbf{u}$ and $\alpha\mathbf{u}+\beta\mathbf{u}$ are equal for all $i$, $1\leq i\leq m$, @ref{CVE} tells us the vectors are equal.
@qed
@end
Many of the conclusions of our theorems can be characterized as <b>identities</b>, especially when we are establishing basic properties of operations such as those in this section. Most of the properties listed in Theorem @ref{VSPCV} are examples. So some advice about the style we use for proving identities is appropriate right now.
Be careful with the notion of the vector $\mathbf{-u}$. This is a vector that we add to $\mathbf{u}$ so that the result is the particular vector $\mathbf{0}$. This is basically a property of vector addition. It happens that we can compute $\mathbf{-u}$ using the other operation, scalar multiplication. We can prove this directly by writing that
\begin{align*}
\displaystyle \left[\mathbf{-u}\right]_{i}=-\left[\mathbf{u}\right]_{i}=(-1)\left[\mathbf{u}\right]_{i}=\left[(-1)\mathbf{u}\right]_{i}
\end{align*}
We will see later how to derive this property as a <b>consequence</b> of several of the ten properties listed in Theorem @ref{VSPCV}.
Similarly, we will often write something you would immediately recognize as <b>vector subtraction</b>. This could be placed on a firm theoretical foundation – as you can do yourself with exercise T30.
A final note. @ref{VSPCV} Property <b>AAC</b> implies that we do not have to be careful about how we <em>parenthesize</em> the addition of vectors. In other words, there is nothing to be gained by writing
$\left(\mathbf{u}+\mathbf{v}\right)+\left(\mathbf{w}+\left(\mathbf{x}+\mathbf{y}\right)\right)$
rather than
$\mathbf{u}+\mathbf{v}+\mathbf{w}+\mathbf{x}+\mathbf{y}$, since we get the same result no matter which order we choose to perform the four additions. So we will not be careful about using parentheses this way.
@section{
Vector Space
}
<strong>For math major only. Non-math major can skip the rest of this section. The material will not appear in the midterms or final</strong> In this section we will give an abstract definition of vector space.


<strong>Why do we need the abstract definitions?</strong>
A lot of different algebraic objects (e.g. polynomials, matrices, sequences, functions) share similar properties
with the set of column vectors. We can use
the common properties to derive similar results.
we therefore don’t need to reproof and restate the results.


<strong>One stone, kill many birds</strong>.


@defn
@label{VS}
Suppose that $V$ is a set upon which we have defined two operations: (1) <b>vector addition</b>, which combines two elements of $V$ and is denoted by $+$, and (2) <b>scalar multiplication</b>, which combines a real number with an element of $V$ and is denoted by juxtaposition. Then $V$, along with the two operations, is a <b>vector space</b> over ${\mathbb{R}}^{\hbox{}}$ if the following ten properties hold.
<ol class="ltx_enumerate">
<li class="ltx_item">
<b>AC</b>
<em>Additive Closure</em>


If $\mathbf{u},\,\mathbf{v}\in V$, then $\mathbf{u}+\mathbf{v}\in V$.
</li>
<li class="ltx_item">
<b>SC</b>
<em>Scalar Closure</em>


If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then $\alpha\mathbf{u}\in V$.
</li>
<li class="ltx_item">
<b>C</b>
<em>Commutativity</em>


If $\mathbf{u},\,\mathbf{v}\in V$, then $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$.
</li>
<li class="ltx_item">
<b>AA</b>
<em>Additive Associativity</em>


If $\mathbf{u},\,\mathbf{v},\,\mathbf{w}\in V$, then $\mathbf{u}+\left(\mathbf{v}+\mathbf{w}\right)=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}$.
</li>
<li class="ltx_item">
<b>Z</b>
<em>Zero Vector</em>


There is a vector, $\mathbf{0}$, called the <b>zero vector</b>, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}\in V$.
</li>
<li class="ltx_item">
<b>AI</b>
<em>Additive Inverses</em>


If $\mathbf{u}\in V$, then there exists a vector $\mathbf{-u}\in V$ so that $\mathbf{u}+(\mathbf{-u})=\mathbf{0}$.
</li>
<li class="ltx_item">
<b>SMA</b>
<em>Scalar Multiplication Associativity</em>


If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then $\alpha(\beta\mathbf{u})=(\alpha\beta)\mathbf{u}$.
</li>
<li class="ltx_item">
<b>DVA</b>
<em>Distributivity across Vector Addition</em>


If $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u},\,\mathbf{v}\in V$, then $\alpha(\mathbf{u}+\mathbf{v})=\alpha\mathbf{u}+\alpha\mathbf{v}$.
</li>
<li class="ltx_item">
<b>DSA</b>
<em>Distributivity across Scalar Addition</em>


If $\alpha,\,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{u}\in V$, then
$(\alpha+\beta)\mathbf{u}=\alpha\mathbf{u}+\beta\mathbf{u}$.
</li>
<li class="ltx_item">
<b>O</b>
<em>One</em>


If $\mathbf{u}\in V$, then $1\mathbf{u}=\mathbf{u}$.
</li>
</ol>
The objects in $V$ are called <b>vectors</b>, no matter what else they might really be, simply by virtue of being elements of a vector space.
@end
@slide
@eg
<b>column vector space</b> The set of column vectors ${\mathbb{R}}^{n}$ is a vector space.
@end
@eg
<b>Row vector space</b>


@newcol
The set of row vector ($1\times n$ matrices), is a vector space with the following operations:
<ul class="ltx_itemize">
<li class="ltx_item">
Vector addition: $[a_{1}\,a_{2}\,\ldots\,a_{n}]+[a_{1}\,b_{2}\,\ldots\,b_{n}]=[a_{1}+b_{1}\,a_{2}+b_{2}\,\ldots\,a_{n}+b_{n}]$
</li>
<li class="ltx_item">
Scalar multiplication $\alpha[a_{1}\,a_{2}\,\ldots,a_{n}]=[\alpha a_{1}\,\alpha a_{2},\ldots,\alpha a_{n}]$
</li>
</ul>
@endcol
@end
@eg
<b>Matrices</b>


@newcol
The set of $m\times n$ matrices, denoted by $M_{mn}$, is a vector space with the following operations:
<ul class="ltx_itemize">
<li class="ltx_item">
Vector addition:
\begin{align*}
\displaystyle \begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}\end{bmatrix}+\begin{bmatrix}b_{11}&b_{12}&\cdots&b_{1n}\\
b_{21}&b_{22}&\cdots&b_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
b_{m1}&b_{m2}&\cdots&b_{mn}\end{bmatrix}=\begin{bmatrix}a_{11}+b_{11}&a_{12}+b_{12}&\cdots&a_{1n}+b_{1n}\\
a_{21}+b_{21}&a_{22}+b_{22}&\cdots&a_{2n}+b_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}+b_{m1}&a_{m2}+b_{m2}&\cdots&a_{mn}+b_{mn}\end{bmatrix}
\end{align*}
</li>
<li class="ltx_item">
Scalar multiplication
\begin{align*}
\displaystyle \alpha\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}\end{bmatrix}=\begin{bmatrix}\alpha a_{11}&\alpha a_{12}&\cdots&\alpha a_{1n}\\
\alpha a_{21}&\alpha a_{22}&\cdots&\alpha a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
\alpha a_{m1}&\alpha a_{m2}&\cdots&\alpha a_{mn}\end{bmatrix}
\end{align*}
</li>
</ul>
Property Z: The zero vector is
\begin{align*}
\displaystyle \begin{bmatrix}0&0&\cdots&0\\
0&0&\cdots&0\\
\vdots&\vdots&\vdots&\vdots\\
0&0&\cdots&0\end{bmatrix}
\end{align*}
Property AI:The inverse of
\begin{align*}
\displaystyle \begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}\end{bmatrix}
\end{align*}
is
\begin{align*}
\displaystyle \begin{bmatrix}-a_{11}&-a_{12}&\cdots&-a_{1n}\\
-a_{21}&-a_{22}&\cdots&-a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
-a_{m1}&-a_{m2}&\cdots&-a_{mn}\end{bmatrix}
\end{align*}
You can try proving all other properties.
@endcol
@end
@eg
<b>The vector space of polynomials, $P_{n}$</b>


@newcol
The set of all polynomials of degree $n$ or less in the variable $x$ with coefficients from ${\mathbb{R}}^{\hbox{}}$,
denoted by $P_{n}$ is a vector space.
<ul class="ltx_itemize">
<li class="ltx_item">
Vector Addition:
\begin{align*}
\displaystyle (a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n})+(b_{0}+b_{1}x+b_{2}x^{2}+\cdots+b_{n}x^{n})
\end{align*}
\begin{align*}
\displaystyle =(a_{0}+b_{0})+(a_{1}+b_{1})x+(a_{2}+b_{2})x^{2}+\cdots+(a_{n}+b_{n})x^{n}
\end{align*}
</li>
<li class="ltx_item">
Scalar Multiplication:
\begin{align*}
\displaystyle \alpha(a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n})=(\alpha a_{0})+(\alpha a_{1})x+(\alpha a_{2})x^{2}+\cdots+(\alpha a_{n})x^{n}
\end{align*}
</li>
</ul>
This set, with these operations, will fulfill the ten properties, though we will not work all the details here. However, we will make a few comments and prove one of the properties. First, the zero vector (property Z)
is what you might expect, and you can check that it has the required property.
\begin{align*}
\displaystyle \mathbf{0}=0+0x+0x^{2}+\cdots+0x^{n}
\end{align*}
The additive inverse (Property AI) is also no surprise, though consider how we have chosen to write it.
\begin{align*}
\displaystyle -\left(a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}\right)=(-a_{0})+(-a_{1})x+(-a_{2})x^{2}+\cdots+(-a_{n})x^{n}
\end{align*}
Now let us prove the associativity of vector addition (Property AA). This is a bit tedious, though necessary. Throughout, the plus sign ($+$) does triple-duty. You might ask yourself what each plus sign represents as you work through this proof.
\begin{align*}
\displaystyle\mathbf{u}+&(\mathbf{v}+\mathbf{w}) \\
&=(a_{0}+a_{1}x+\cdots+a_{n}x^{n})+\left((b_{0}+b_{1}x+\cdots+b_{n}x^{n})+(c_{0}+c_{1}x+\cdots+c_{n}x^{n})\right) \\
&=(a_{0}+a_{1}x+\cdots+a_{n}x^{n})+((b_{0}+c_{0})+(b_{1}+c_{1})x+\cdots+(b_{n}+c_{n})x^{n}) \\
&=(a_{0}+(b_{0}+c_{0}))+(a_{1}+(b_{1}+c_{1}))x+\cdots+(a_{n}+(b_{n}+c_{n}))x^{n} \\
&=((a_{0}+b_{0})+c_{0})+((a_{1}+b_{1})+c_{1})x+\cdots+((a_{n}+b_{n})+c_{n})x^{n} \\
&=((a_{0}+b_{0})+(a_{1}+b_{1})x+\cdots+(a_{n}+b_{n})x^{n})+(c_{0}+c_{1}x+\cdots+c_{n}x^{n}) \\
&=\left((a_{0}+a_{1}x+\cdots+a_{n}x^{n})+(b_{0}+b_{1}x+\cdots+b_{n}x^{n})\right)+(c_{0}+c_{1}x+\cdots+c_{n}x^{n}) \\
&=(\mathbf{u}+\mathbf{v})+\mathbf{w}
\end{align*}
You might try proving all the other properties.
@endcol
@end
@eg
<b>The vector space of functions</b>


@newcol
Let $F$ be the set of functions for ${\mathbb{R}}^{\hbox{}}$ to ${\mathbb{R}}^{\hbox{}}$
Equality: $f=g$ if and only if $f(x)=g(x)$ for all $x\in{\mathbb{R}}^{\hbox{}}$.
<ul class="ltx_itemize">
<li class="ltx_item">
Vector Addition: $f+g$ is the function with outputs defined by $(f+g)(x)=f(x)+g(x)$.
</li>
<li class="ltx_item">
Scalar Multiplication: $\alpha f$ is the function with outputs defined by $(\alpha f)(x)=\alpha f(x)$.
</li>
</ul>
The zero vector is a function $z$ whose definition is $z(x)=0$ for every input $x\in{\mathbb{R}}^{\hbox{}}$.
Try proving all the other properties.
@end
@eg
<b>The crazy vector space</b>

@col
Let $C=\left\{\left.(x_{1},\,x_{2})\,\right|\,x_{1},\,x_{2}\in{\mathbb{R}}^{\hbox{}}\right\}$.
<ol class="ltx_enumerate">
<li class="ltx_item">
Vector Addition: $(x_{1},\,x_{2})+(y_{1},\,y_{2})=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1)$.
</li>
<li class="ltx_item">
Scalar Multiplication: $\alpha(x_{1},\,x_{2})=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)$.
</li>
</ol>
I am free to define my set and my operations any way I please. They may not look natural, or even useful, but we will now verify that they provide us with another example of a vector space. We will check all it satisfies all the definition of vector spaces.
<ul class="ltx_itemize">
<li class="ltx_item">
<strong>Property AC, SC</strong>

The result of each operation is a pair of complex numbers, so these two closure properties are fulfilled.
</li>
<li class="ltx_item">
<strong>Property C</strong>
\begin{align*}
\displaystyle\mathbf{u}+\mathbf{v}&=(x_{1},\,x_{2})+(y_{1},\,y_{2})=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1) \\
&=(y_{1}+x_{1}+1,\,y_{2}+x_{2}+1)=(y_{1},\,y_{2})+(x_{1},\,x_{2}) \\
&=\mathbf{v}+\mathbf{u}
\end{align*}
</li>
<li class="ltx_item">
<strong>Property AA</strong>
\begin{align*}
\displaystyle\mathbf{u}+(\mathbf{v}+\mathbf{w})&=(x_{1},\,x_{2})+\left((y_{1},\,y_{2})+(z_{1},\,z_{2})\right) \\
&=(x_{1},\,x_{2})+(y_{1}+z_{1}+1,\,y_{2}+z_{2}+1) \\
&=(x_{1}+(y_{1}+z_{1}+1)+1,\,x_{2}+(y_{2}+z_{2}+1)+1) \\
&=(x_{1}+y_{1}+z_{1}+2,\,x_{2}+y_{2}+z_{2}+2) \\
&=((x_{1}+y_{1}+1)+z_{1}+1,\,(x_{2}+y_{2}+1)+z_{2}+1) \\
&=(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1)+(z_{1},\,z_{2}) \\
&=\left((x_{1},\,x_{2})+(y_{1},\,y_{2})\right)+(z_{1},\,z_{2}) \\
&=\left(\mathbf{u}+\mathbf{v}\right)+\mathbf{w}
\end{align*}
</li>
<li class="ltx_item">
<strong>Property Z</strong>

The zero vector is $\mathbf{0}=(-1,\,-1)$ (<strong>not</strong> $(0,0)$)
\begin{align*}
\displaystyle \mathbf{u}+\mathbf{0}=(x_{1},\,x_{2})+(-1,\,-1)=(x_{1}+(-1)+1,\,x_{2}+(-1)+1)=(x_{1},\,x_{2})=\mathbf{u}
\end{align*}
</li>
<li class="ltx_item">
<strong>Property AI</strong>

For each vector, $\mathbf{u}$, we must locate an additive inverse, $\mathbf{-u}$. Here it is, $-(x_{1},\,x_{2})=(-x_{1}-2,\,-x_{2}-2)$. As odd as it may look, I hope you are withholding judgment. Check:
\begin{align*}
\displaystyle\mathbf{u}+(\mathbf{-u})&=(x_{1},\,x_{2})+(-x_{1}-2,\,-x_{2}-2) \\
&=(x_{1}+(-x_{1}-2)+1,\,-x_{2}+(x_{2}-2)+1)=(-1,\,-1)=\mathbf{0}
\end{align*}
</li>
<li class="ltx_item">
<strong>Property SMA</strong>
\begin{align*}
\displaystyle\alpha(\beta\mathbf{u})&=\alpha(\beta(x_{1},\,x_{2})) \\
&=\alpha(\beta x_{1}+\beta-1,\,\beta x_{2}+\beta-1) \\
&=(\alpha(\beta x_{1}+\beta-1)+\alpha-1,\,\alpha(\beta x_{2}+\beta-1)+\alpha-1) \\
&=((\alpha\beta x_{1}+\alpha\beta-\alpha)+\alpha-1,\,(\alpha\beta x_{2}+\alpha\beta-\alpha)+\alpha-1) \\
&=(\alpha\beta x_{1}+\alpha\beta-1,\,\alpha\beta x_{2}+\alpha\beta-1) \\
&=(\alpha\beta)(x_{1},\,x_{2}) \\
&=(\alpha\beta)\mathbf{u}
\end{align*}
</li>
<li class="ltx_item">
<strong>Property DVA</strong>

If you have hung on so far, here is where it gets even wilder. In the next two properties we mix and mash the two operations.
\begin{align*}
\displaystyle\alpha(\mathbf{u}&+\mathbf{v}) \\
&=\alpha\left((x_{1},\,x_{2})+(y_{1},\,y_{2})\right) \\
&=\alpha(x_{1}+y_{1}+1,\,x_{2}+y_{2}+1) \\
&=(\alpha(x_{1}+y_{1}+1)+\alpha-1,\,\alpha(x_{2}+y_{2}+1)+\alpha-1) \\
&=(\alpha x_{1}+\alpha y_{1}+\alpha+\alpha-1,\,\alpha x_{2}+\alpha
y_{2}+\alpha+\alpha-1) \\
&=(\alpha x_{1}+\alpha-1+\alpha y_{1}+\alpha-1+1,\,\alpha x_{2}+\alpha-1+\alpha y_{2}+\alpha-1+1) \\
&=((\alpha x_{1}+\alpha-1)+(\alpha y_{1}+\alpha-1)+1,\,(\alpha x_{2}+\alpha-1)+(\alpha y_{2}+\alpha-1)+1) \\
&=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)+(\alpha y_{1}+\alpha-1,\,\alpha y_{2}+\alpha-1) \\
&=\alpha(x_{1},\,x_{2})+\alpha(y_{1},\,y_{2}) \\
&=\alpha\mathbf{u}+\alpha\mathbf{v}
\end{align*}
</li>
<li class="ltx_item">
<strong>Property DSA</strong>
\begin{align*}
\displaystyle(\alpha&+\beta)\mathbf{u} \\
&=(\alpha+\beta)(x_{1},\,x_{2}) \\
&=((\alpha+\beta)x_{1}+(\alpha+\beta)-1,\,(\alpha+\beta)x_{2}+(\alpha+\beta)-1) \\
&=(\alpha x_{1}+\beta x_{1}+\alpha+\beta-1,\,\alpha x_{2}+\beta x_{2}+\alpha+\beta-1) \\
&=(\alpha x_{1}+\alpha-1+\beta x_{1}+\beta-1+1,\,\alpha x_{2}+\alpha-1+\beta x_{2}+\beta-1+1) \\
&=((\alpha x_{1}+\alpha-1)+(\beta x_{1}+\beta-1)+1,\,(\alpha x_{2}+\alpha-1)+(\beta x_{2}+\beta-1)+1) \\
&=(\alpha x_{1}+\alpha-1,\,\alpha x_{2}+\alpha-1)+(\beta x_{1}+\beta-1,\,\beta x_{2}+\beta-1) \\
&=\alpha(x_{1},\,x_{2})+\beta(x_{1},\,x_{2}) \\
&=\alpha\mathbf{u}+\beta\mathbf{u}
\end{align*}
</li>
<li class="ltx_item">
<strong>Property O</strong>

After all that, this one is easy, but no less pleasing.
\begin{align*}
\displaystyle 1\mathbf{u}=1(x_{1},\,x_{2})=(x_{1}+1-1,\,x_{2}+1-1)=(x_{1},\,x_{2})=\mathbf{u}
\end{align*}
</li>
</ul>
That is it, $C$ is a vector space, as crazy as that may seem.
Notice that in the case of the zero vector and additive inverses, we only had to propose possibilities and then verify that they were the correct choices. You might try to discover how you would arrive at these choices, though you should understand why the process of discovering them is not a necessary component of the proof itself.
@endcol
@end
@section{
Basic Properties of Vector Spaces
}
<strong>For math major only. Non-math major can skip the rest of this section. The material will not appear in the midterms or final</strong>
@thm
@title{Cancellation Law for Vector Addition}
@label{cancell}
if $\mathbf{v}$, $\mathbf{u}$ and $\mathbf{w}$ are vectors in a vector space $V$ such that
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{u}+\mathbf{w},
\end{align*}
then $\mathbf{v}=\mathbf{u}$
@end
@proof
By Property AI, there exists a vector $-\mathbf{w}$ such that $\mathbf{w}+(-\mathbf{w})=\mathbf{0}$.
Thus,
\begin{align*}
\displaystyle (\mathbf{v}+\mathbf{w})+(-\mathbf{w})&=(\mathbf{u}+\mathbf{w})+(-\mathbf{w})\\
\displaystyle \mathbf{v}+(\mathbf{w}+(-\mathbf{w}))&=\mathbf{u}+(\mathbf{w}+(-\mathbf{w}))&\text{Property AA}\\
\displaystyle \mathbf{v}+\mathbf{0}&=\mathbf{u}+\mathbf{0}&\text{Property AI}\\
\displaystyle \mathbf{v}&=\mathbf{u}&\text{Property Z}.
\end{align*}


@qed
@end
@thm
@title{Uniqueness of the zero vector}
Let $V$ be a vector space.
The vector $\mathbf{0}$ described in Property Z is unique.
@end
@proof
Suppose both $\mathbf{0}_{1}$ and $\mathbf{0}_{2}$ satisfy the property described in Property Z.
Let $\mathbf{w}$ be an element in $V$.
\begin{align*}
\displaystyle \mathbf{0}_{1}+\mathbf{w}=\mathbf{w}=\mathbf{0}_{2}+\mathbf{w}\hskip 28.452756pt\text{Property Z}
\end{align*}
\begin{align*}
\displaystyle \mathbf{0}_{1}=\mathbf{0}_{2}\hskip 28.452756pt\text{by the previous theorem}
\end{align*}


@qed
@end
@thm
@title{Uniqueness of the additive inverse}
@label{invunique}
Let $V$ be a vector space and $\mathbf{v},\mathbf{u},\mathbf{w}$ are vectors of $V$. If
both $\mathbf{v}$ and $\mathbf{u}$ satisfies
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{0},
\end{align*}
\begin{align*}
\displaystyle \mathbf{u}+\mathbf{w}=\mathbf{0},
\end{align*}
i.e., both $\mathbf{u}$ and $\mathbf{v}$ are additive inverse of $\mathbf{w}$ in Property AI, then
\begin{align*}
\displaystyle \mathbf{v}=\mathbf{u}.
\end{align*}
This shows that the additive inverse is unique.
@end
@proof
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\mathbf{0}=\mathbf{u}+\mathbf{w}.
\end{align*}
By @ref{cancell},
\begin{align*}
\displaystyle \mathbf{v}=\mathbf{u}.
\end{align*}


@qed
@end
@thm
Let $V$ be a vector space, $\alpha$ a real number, $\mathbf{v}$ a vector in $V$. We have the following statement.
<ol class="ltx_enumerate">
<li class="ltx_item">
$0\mathbf{v}=\mathbf{0}$ .
</li>
<li class="ltx_item">
$a\mathbf{0}=\mathbf{0}$.
</li>
<li class="ltx_item">
$(-\alpha)\mathbf{v}=-(\alpha\mathbf{v})=\alpha(-\mathbf{v})$.
</li>
</ol>
@end
@proof
<ol class="ltx_enumerate">
<li class="ltx_item">
\begin{align*}
\displaystyle 0\mathbf{v}+0\mathbf{v}&=(0+0)\mathbf{v} & \text{Property DSA}\\
\displaystyle 0\mathbf{v}+0\mathbf{v}=0\mathbf{v}&=\mathbf{0}+0\mathbf{v} & \text{Property Z}
\end{align*}
Hence,
\[
\displaystyle 0\mathbf{v}=\mathbf{0},
\]
by @ref{cancell}.
</li>
<li class="ltx_item">
<br/>
\begin{align*}
\displaystyle\alpha\mathbf{0}+a\mathbf{0}&=\alpha(\mathbf{0}+\mathbf{0}) & \text{Property DVA} \\
&=\alpha\mathbf{0} & \text{Property Z} \\
&=\mathbf{0}+\alpha\mathbf{0} & \text{Property Z}
\end{align*}
By @ref{cancell},
\begin{align*}
\displaystyle \alpha\mathbf{0}=\mathbf{0}
\end{align*}
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle\alpha\mathbf{v}+(-\alpha)\mathbf{v}&=(\alpha+(-\alpha))\mathbf{v}&\text{Property DSA}. \\
&=0\mathbf{v}& \\
&=\mathbf{0}&\text{item 1}
\end{align*}
By Property AI and the uniqueness of the additive inverse (@ref{invunique}),
\begin{align*}
\displaystyle (-\alpha)\mathbf{v}=-\alpha\mathbf{v}.
\end{align*}
Next
\begin{align*}
\displaystyle\alpha\mathbf{v}+\alpha(-\mathbf{v})&=\alpha(\mathbf{v}+(-\mathbf{v})) & \text{Property DVA}. \\
&=\alpha\mathbf{0} & \text{Property AI} \\
&=\mathbf{0} & \text{item 2}
\end{align*}
By Property AI and the uniqueness of the additive inverse (@ref{invunique}),
\begin{align*}
\displaystyle \alpha(-\mathbf{v})=-\alpha\mathbf{v}.
\end{align*}
</li>
</ol>


@qed
@end
@section{
Subspaces
}
@defn
@label{subdef}
Let $V$ be vector space.
A subset $W$ of $V$ is said to be a <b>subspace</b> of $V$ if
<ol class="ltx_enumerate">
<li class="ltx_item">
$W$ is nonempty.
</li>
<li class="ltx_item">
For $\mathbf{v},\mathbf{w}\in W$, then $\mathbf{v}+\mathbf{w}\in W$.
</li>
<li class="ltx_item">
For $\alpha\in\mathbf{R}$, $\mathbf{v}\in W$, then $\alpha\mathbf{v}\in W$.
</li>
</ol>
@end
@slide
We will prove several theorem first before we give examples.
@prop
@label{0}
Let $V$ be a vector spaceand $W$ a subspace of $V$.
Then $\mathbf{0}$
is in $W$.
@end
@proof
@newcol
By @ref{subdef}, Condition 1, $W$ is nonempty. Let $\mathbf{w}\in W$.
By @ref{subdef}, Condition 3, with $\alpha=0$,
$0\mathbf{w}\in W$.
On the other hand, by @ref{1d82a47b16369aec09ebc36c7afbd598}, $0\mathbf{w} = \mathbf{0}$.
Hence, the zero vector $\mathbf{0}$ lies in $W$.
@qed
@endcol
@end
@thm
@label{subdef2}
@newcol
Let $V$ be a vector space
and $W$ a subset of $V$,
then $W$ is a subspace if and only if
<ol class="ltx_enumerate">
<li class="ltx_item">
$W$ is nonempty.
</li>
<li class="ltx_item">
For any $\alpha\in\mathbf{R}$, $\mathbf{v},\mathbf{w}\in W$, $\alpha\mathbf{v}+\mathbf{w}\in W$.
</li>
</ol>
@endcol
@end
@proof

($\Rightarrow$)
@newcol
By @ref{subdef}, Condition 1, $W$ is nonempty. Next, for $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W$.
By @ref{subdef}, Condition 3, $\alpha\mathbf{v}\in W$.
By @ref{subdef}, Condition 2, $\alpha\mathbf{v}+\mathbf{w}\in W$.
@endcol

($\Leftarrow$)
@newcol
By Condition 1, @ref{subdef}, Condition 1 is true.
Because $W$ is nonempty, let $\mathbf{x}\in W$. Let $\mathbf{v}=\mathbf{w}=\mathbf{x}$ and $\alpha=-1$.
Then by condition 2, $(-1)\mathbf{w}+\mathbf{w}=\mathbf{0}\in W.$

@col
Now we want to check @ref{subdef} Condition 2, suppose $\mathbf{v},\mathbf{w}\in W$.
In condition 2, let $\alpha=1$, then $\mathbf{v}+\mathbf{w}=\alpha\mathbf{v}+\mathbf{w}\in W$.

@col
Next we want to check @ref{subdef} Condition 3, suppose $\mathbf{v}\in W$, $\alpha\in{\mathbb{R}}^{\hbox{}}$.
Let $\mathbf{w}=\mathbf{0}$, then $\alpha\mathbf{v}=\alpha\mathbf{v}+\mathbf{w}\in W$.
@qed
@endcol
@end
@section{Examples}
@eg
Let $V = \mathbb{R}^3$.  Let:
\[
W = \left\{\left.\colvector{w_1\\0\\w_3} \;\right| \;w_1, w_3 \in \mathbb{R}\right\}.
\]
We now show that $W$ is a subspace of $V$:

@col
@ol
@li
Clearly, $\colvector{0\\0\\0}$ lies in $W$, hence $W$ is nonempty.
@li
Given any $\vec{v}$ and $\vec{w}$ in $W$, by definition of $W$ we have:
\[
\vec{v} = \colvector{v_1\\0\\v_3},\quad \vec{w} = \colvector{w_1\\0\\w_3},
\quad v_1,v_2,w_1,w_3 \in \mathbb{R}.
\]
Hence,
\[
\vec{v} + \vec{w} = \colvector{v_1 + w_1\\0 + 0\\v_3 + w_3}
= \colvector{v_1 + w_1\\0\\v_3 + w_3} \in W.
\]
@li
Given any $\alpha \in \mathbb{R}$ and $\vec{w}$ in $W$, by definition of $W$ we have:
\[
\vec{w} = \colvector{w_1\\0\\w_3}, \quad w_1,w_3 \in \mathbb{R}.
\]
Hence,
\[
\alpha\vec{w} = \colvector{\alpha w_1\\\alpha \cdot 0\\\alpha w_3}
= \colvector{\alpha w_1\\0\\\alpha w_3} \in W.
\]
@endol
@end
@eg
<strong>Skip for now, until you learn the definition of column space.</strong>

@col
Let $A\in M_{mn}$, then $\mathcal{C}\!\left(A\right)$ is a subspace of ${\mathbb{R}}^{m}$.
@end
@proof
@newcol
$\mathcal{C}\!\left(A\right)=\left<\{\mathbf{A}_{1},\ldots,\mathbf{A}_{n}\}\right>$. So by the previous theorem, $\mathcal{C}\!\left(A\right)$ is a subspace of ${\mathbb{R}}^{m}$. <b>Alternate proof</b>: Suppose For $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W=\mathcal{C}\!\left(A\right)$.
Recall
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\{A\mathbf{x}\,|\,x\in{\mathbb{R}}^{m}\}.
\end{align*}
Then there exist $\mathbf{x},\mathbf{y}$ such that $A\mathbf{x}=\mathbf{v}$, $A\mathbf{y}=\mathbf{w}$.
\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}=\alpha A\mathbf{x}+A\mathbf{y}=A(\alpha\mathbf{x}+\mathbf{y})\in\mathcal{C}\!\left(A\right).
\end{align*}
Thus by @ref{subdef2}, $W$ is a subspace.
@qed
@endcol
@end
@eg
<strong>For math majors only</strong>

@newcol
Let $S_{n}$ be the set of symmetric matrices of $M_{nn}$.
Then $S_{n}$ is a subspace of $M_{nn}$. Check that $W=S_{n}$ is a subspace: Because ${\cal O}\in W$, so $W$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $A,B\in W$. Then $A^{t}=A$, $B^{t}=B$.
\begin{align*}
\displaystyle (\alpha A+B)^{t}=\alpha A^{t}+B^{t}=\alpha A+B.
\end{align*}
Thus $\alpha A+B\in S_{n}$. Hence $S_{n}$ is a subspace by @ref{subdef2}.
@endcol
@end
@eg
<strong>For math majors only</strong>


@newcol
Let
\begin{align*}
\displaystyle F=\{f(x)\in P_{n}\,|\,f(1)=0\}.
\end{align*}
Then $F$ is a subspace of $P_{n}$ Because $0\in E$, so $E$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $f(x),g(x)\in E$. Then $f(1)=g(1)=0$.
Let $h=\alpha f+g$. Then
\begin{align*}
\displaystyle h(1)=\alpha f(1)+g(1)=\alpha 0+0=0.
\end{align*}
So $h\in F$. Hence $F$ is a subspace by @ref{subdef2}.
@endcol
@end
@eg
<strong>For math majors only</strong>

@newcol
Let
\begin{align*}
\displaystyle E=\{f(x)\in P_{n}\,|\,f(x)=f(-x)\}.
\end{align*}
Then $E$ is a subspace of $P_{n}$: Because $0\in E$, so $E$ is nonempty. Suppose $\alpha\in{\mathbb{R}}^{\hbox{}}$, $f(x),g(x)\in E$. Then $f(x)=f(-x)$, $g(x)=g(-x)$.
Let $h=\alpha f+g$. Then
\begin{align*}
\displaystyle h(-x)=\alpha f(-x)+g(-x)=\alpha f(x)+g(x)=h(x).
\end{align*}
So $h\in E$. Hence $E$ is a subspace.
@endcol
@end
@section{
Non-Examples
}
To show that $W$ is <strong>not</strong> a subspace of $V$, it suffices to show that it violates @ref{subdef} condition 1 or condition 2.
This can be done by finding counter examples to either condition. Usually before checking those conditions, we quickly check if $\mathbf{0}_{V}\in W$
(see @ref{0}).
@eg
@newcol
$V={\mathbb{R}}^{m}$, $W=\{\mathbf{v}\in V\,|\,[\mathbf{v}]_{1}=1\}$.

<strong>Method 1</strong> Obviously $\mathbf{0}$ is not in $W$. So by @ref{0}, $W$ is not a subspace.

<strong>Method 2</strong> For Suppose $\mathbf{v},\mathbf{w}\in W$.
Then $[\mathbf{v}+\mathbf{w}]_{1}=[\mathbf{v}]_{1}+[\mathbf{w}]_{1}=2$.
So $\mathbf{v}+\mathbf{w}\not\in W$. So $W$ violates @ref{subdef} condition 1 and hence not a subspace.
@endcol
@end
@eg
@newcol
$V={\mathbb{R}}^{m}$, $W=\{\mathbf{v}\in V\,|\,\sum_{i=1}^{n}[\mathbf{v}]_{i}=1\}$.

<strong>Method 1</strong> (the easiest method) Obviously $\mathbf{0}$ is not in $W$. So by @ref{0}, $W$ is not a subspace.

<strong>Method 2</strong> We will find an explicit counter example, let
\begin{align*}
\displaystyle \mathbf{v}=\mathbf{w}=\begin{bmatrix}1\\
0\\
\vdots\\
0\end{bmatrix}.
\end{align*}
Then both $\mathbf{v}$ and $\mathbf{w}$ are in $W$.
\begin{align*}
\displaystyle \mathbf{v}+\mathbf{w}=\begin{bmatrix}2\\
0\\
\vdots\\
0\end{bmatrix}.
\end{align*}
Obvious $\mathbf{v}+\mathbf{w}\notin W$.
Therefore $W$ violates @ref{subdef} condition 1 and hence not a subspace.
@endcol
@end
@eg
@newcol
$V=\mathbf{R}^{n}$, $W=\{\mathbf{v}\in V\,|\,[\mathbf{v}]_{1}\geq 0\}$. Let $\alpha=-1$ and
\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}1\\
0\end{bmatrix}.
\end{align*}
Then $[\alpha\mathbf{v}]_{1}=\alpha[\mathbf{v}]_{1}=\alpha=-1<0$.
So $\alpha\mathbf{v}\notin W$.
Thus $W$ violates @ref{subdef} condition 3 and hence not a subspace.
@endcol
@end
@eg
@newcol
$V={\mathbb{R}}^{2}$,
$W=\{\begin{bmatrix}v_{1}\\
v_{2}\end{bmatrix}\in V\,|\,v_{1}v_{2}\geq 0\}$. Let $\mathbf{v}=\begin{bmatrix}1\\
0\end{bmatrix}$ , $\mathbf{w}=\begin{bmatrix}0\\
-1\end{bmatrix}\in W$.
$\mathbf{v}+\mathbf{w}=\begin{bmatrix}1\\
-1\end{bmatrix}$. Because
$1\times(-1)=-1 < 0$. So $\mathbf{v}+\mathbf{w}\notin W$.
Thus $W$ violates @ref{subdef} condition 2 and hence not a subspace. In fact, we can show that $W$ satisfies @ref{subdef} condition 3 but fails condition 2.
@endcol
@end
@eg
<strong>For math majors only</strong>

@col
Let $V=P_{n}$. Let $G$ be the set of polynomial with degree exactly equal to $n$. Let $f(x)=x^{n}$, $g(x)=-x^{n}+1$. Both $f$ and $g$ have degree exactly equal to $n$.
But
\begin{align*}
\displaystyle f(x)+g(x)=1
\end{align*}
is a polynomial with degree $0$. So $f+g$ is not in $G$. Thus $W$ violates @ref{subdef} condition 2 and hence not a subspace.
@end
@setchapter{10}
@chapter{Linear Combinations}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section LC (print version p65 - p81)Strang: Section 2.3

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf

Section LC (p.32-33) C40, C41, M10, M11
@section{
Linear Combinations}
@label{LC}
@defn
@title{Linear Combination of Column Vectors}
@label{LCCV}
Given $n$ vectors $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}$ in ${\mathbb{R}}^{m}$
and $n$ scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\ldots,\,\alpha_{n}$,
their <b>linear combination</b> is the vector:
\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}
\end{align*}
in $\mathbb{R}^{m}$.
@end
@eg
@label{TLC}
<strong>Two linear combinations in ${\mathbb{R}}^{6}$</strong>

@col
Suppose that:
\[
\alpha_{1}=1 \quad \alpha_{2}=-4 \quad \alpha_{3}=2 \quad \alpha_{4}=-1
\]
and
\begin{align*}
\displaystyle\mathbf{u}_{1}&=\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}&\mathbf{u}_{2}&=\begin{bmatrix}6\\
3\\
0\\
-2\\
1\\
4\end{bmatrix}&\mathbf{u}_{3}&=\begin{bmatrix}-5\\
2\\
1\\
1\\
-3\\
0\end{bmatrix}&\mathbf{u}_{4}&=\begin{bmatrix}3\\
2\\
-5\\
7\\
1\\
3\end{bmatrix}.
\end{align*}
The resulting linear combination is:

@col
@steps
\begin{multline*}
\displaystyle\alpha_{1}\mathbf{u_{1}}+\alpha_{2}\mathbf{u_{2}}+\alpha_{3}\mathbf{u_{3}}+\alpha_{4}\mathbf{u_{4}}
\\
@nstep{=
(1)\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}+(-4)\begin{bmatrix}6\\
3\\
0\\
-2\\
1\\
4\end{bmatrix}+(2)\begin{bmatrix}-5\\
2\\
1\\
1\\
-3\\
0\end{bmatrix}+(-1)\begin{bmatrix}3\\
2\\
-5\\
7\\
1\\
3\end{bmatrix}
}
\\
@nstep{=
\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}+\begin{bmatrix}-24\\
-12\\
0\\
8\\
-4\\
-16\end{bmatrix}+\begin{bmatrix}-10\\
4\\
2\\
2\\
-6\\
0\end{bmatrix}+\begin{bmatrix}-3\\
-2\\
5\\
-7\\
-1\\
-3\end{bmatrix}
}
@nstep{=
\begin{bmatrix}-35\\
-6\\
4\\
4\\
-9\\
-10\end{bmatrix}
}
\end{multline*}
@endsteps

@col
A different linear combination, but with the same set of vectors, can be formed with different scalars. Taking
\[
\displaystyle\beta_{1}=3 \quad \beta_{2}=0 \quad \beta_{3}=5 \quad \beta_{4}=-1
\]
we can form the linear combination:
@steps
\begin{multline*}
\displaystyle\beta_{1}\mathbf{u_{1}}+\beta_{2}\mathbf{u_{2}}+\beta_{3}\mathbf{u_{3}}+\beta_{4}\mathbf{u_{4}}
\\
@nstep{
=(3)\begin{bmatrix}2\\
4\\
-3\\
1\\
2\\
9\end{bmatrix}+(0)\begin{bmatrix}6\\
3\\
0\\
-2\\
1\\
4\end{bmatrix}+(5)\begin{bmatrix}-5\\
2\\
1\\
1\\
-3\\
0\end{bmatrix}+(-1)\begin{bmatrix}3\\
2\\
-5\\
7\\
1\\
3\end{bmatrix}
}
\\
@nstep{
=\begin{bmatrix}6\\
12\\
-9\\
3\\
6\\
27\end{bmatrix}+\begin{bmatrix}0\\
0\\
0\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}-25\\
10\\
5\\
5\\
-15\\
0\end{bmatrix}+\begin{bmatrix}-3\\
-2\\
5\\
-7\\
-1\\
-3\end{bmatrix}
}
@nstep{
=\begin{bmatrix}-22\\
20\\
1\\
1\\
-10\\
24\end{bmatrix}.
}
\end{multline*}
@endsteps
@col
Notice how we could keep our set of vectors fixed but use a different set of scalars to construct different vectors.
Can you create the following vector $\mathbf{w}$ with a suitable choice of four scalars?
\begin{align*}
\displaystyle \mathbf{w}=\begin{bmatrix}13\\
15\\
5\\
-17\\
2\\
25\end{bmatrix}
\end{align*}
@col
Going further, can you create any possible vector from ${\mathbb{R}}^{6}$ by choosing the proper scalars?
@end
@slide
@eg
The system of linear equation:
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&=24 \\
\displaystyle x_{1}+4x_{3}&=5
\end{align*}

or equivalently
\begin{align*}
\displaystyle \begin{bmatrix}-7x_{1}-6x_{2}-12x_{3}\\
5x_{1}+5x_{2}+7x_{3}\\
x_{1}+4x_{3}\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix},
\end{align*}

can be rewritten as:

@col
\begin{align*}
\displaystyle \begin{bmatrix}-7x_{1}\\
5x_{1}\\
x_{1}\end{bmatrix}+\begin{bmatrix}-6x_{2}\\
5x_{2}\\
0x_{2}\end{bmatrix}+\begin{bmatrix}-12x_{3}\\
7x_{3}\\
4x_{3}\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}
\end{align*}
or:

@col
\begin{align*}
\displaystyle x_{1}\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+x_{2}\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}.
\end{align*}

The solution is:

@col
\[
x_{1}=-3 \quad x_{2}=5 \quad x_{3}=2.
\]
@col
So, in the context of this example, we can express the fact that these values of the variables are a solution by writing a linear combination:
\begin{align*}
\displaystyle (-3)\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+(5)\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+(2)\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}
\end{align*}
@col
Furthermore, these are the only three scalars that will accomplish this equality, since they come from a unique solution.

@col
Notice how the three vectors in this example are the columns of the coefficient matrix of the system of equations. This is our first hint of the important interplay between the vectors that form the columns of a matrix, and the matrix itself.
@end
@slide
@eg
The system of linear equations
\begin{align*}
\displaystyle x_{1}-x_{2}+2x_{3}&=1 \\
\displaystyle 2x_{1}+x_{2}+x_{3}&=8 \\
\displaystyle x_{1}+x_{2}&=5
\end{align*}
can be written as:

@col
\begin{align*}
\displaystyle \begin{bmatrix}x_{1}-x_{2}+2x_{3}\\
2x_{1}+x_{2}+x_{3}\\
x_{1}+x_{2}\end{bmatrix}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}.
\end{align*}
This vector equation is equivalent to:

@col
\begin{align*}
\displaystyle x_{1}\begin{bmatrix}1\\
2\\
1\end{bmatrix}+x_{2}\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+x_{3}\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}.
\end{align*}

@col
Row-reducing the augmented matrix for the system leads to the conclusion that the system is consistent and has free variables, hence infinitely many solutions. So for example, the two solutions:
\begin{align*}
x_{1}&=2 &\quad& x_{2}=3 &\quad& x_{3}&=1 \\
x_{1}&=3 &\quad& x_{2}=2 &\quad& x_{3}&=0
\end{align*}
can be used together to say that:

@col
\begin{align*}
\displaystyle (2)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(3)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}=(3)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(2)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(0)\begin{bmatrix}2\\
1\\
0\end{bmatrix}.
\end{align*}

@col
Ignore the middle of this equation, and move all the terms to the left-hand side:
\begin{align*}
\displaystyle (2)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(3)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
0\end{bmatrix}+(-3)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(-2)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(-0)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\end{bmatrix}.
\end{align*}

@col
Regrouping gives:
\begin{align*}
\displaystyle (-1)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(1)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\end{bmatrix}.
\end{align*}

@col
Notice that the three vectors on the left hand side are the columns of the coefficient matrix for the system of equations. This equality says that there is a linear combination of those columns that equals the vector of all zeros. Give it some thought, but this says that:
\begin{align*}
\displaystyle x_{1}=-1&\quad&x_{2}=1&\quad& x_{3}=1
\end{align*}
is a nontrivial solution to the homogeneous system of equations with the coefficient matrix of the original system. In particular, this demonstrates that this coefficient matrix is singular.
@end
@slide
@thm
@title{Solutions to Linear Systems are Linear Combinations}
@label{SLSLC}
Denote the columns of the $m\times n$ matrix $A$ as vectors $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$. Then $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution to the linear system of equations $\mathcal{LS}({A},{\mathbf{b}})$ if and only if $\mathbf{b}$ is equal to the linear combination of the columns of $A$ formed with the entries of $\mathbf{x}$,
\begin{align*}
\displaystyle \left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n}=\mathbf{b}
\end{align*}
@end
@proof
@col
If $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution of $\mathcal{LS}({A},{\mathbf{b}})$, then
\begin{align*}
\displaystyle \mathbf{b}=A\mathbf{x}=\left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n}.
\end{align*}
@col
Hence $\mathbf{b}$ is a linear combination of the columns of $A$.

@col
Conversely, if $\mathbf{b}$ is a linear combinations of the columns of $A$, say:
\begin{align*}
\displaystyle \mathbf{b}=\left[\mathbf{x}\right]_{1}\mathbf{A}_{1}+\left[\mathbf{x}\right]_{2}\mathbf{A}_{2}+\left[\mathbf{x}\right]_{3}\mathbf{A}_{3}+\cdots+\left[\mathbf{x}\right]_{n}\mathbf{A}_{n},
\end{align*}
@col
then
\begin{align*}
\displaystyle \mathbf{b}=A\mathbf{x}.
\end{align*}
@col
So $\mathbf{x}$ is a solution of $\mathcal{LS}({A},{\mathbf{b}})$.
@qed
@end
@slide
<strong>Computational question</strong>: Determine if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{n}$. That is, determine whether or not the system of linear equations:
\begin{align*}
\displaystyle x_{1}\mathbf{v}_{1}+\cdots+x_{n}\mathbf{v}_{n}=\mathbf{u}
\end{align*}
has a solution. The augmented matrix is:
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}|\mathbf{u}].
\end{align*}
Then we can solve the system of linear questions by the technique you learned.
@eg
@col
Let
\begin{align*}
\displaystyle \mathbf{u} =
\begin{bmatrix}1\\
1\\
3\end{bmatrix},\quad
\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
3\end{bmatrix},
\mathbf{v}_{2}=\begin{bmatrix}-1\\
1\\
0\end{bmatrix},
\mathbf{v}_{3}=\begin{bmatrix}2\\
1\\
3\end{bmatrix},
\mathbf{v}_{4}=\begin{bmatrix}-1\\
0\\
1\end{bmatrix}.
\end{align*}
<ol class="ltx_enumerate">
<li class="ltx_item" >

Determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$.
If yes, find the linear combination.
</li>
<li class="ltx_item" >

Determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4}\}$.
If yes, find the linear combination.
</li>
</ol>
@end
@sol
@col
<ol class="ltx_enumerate">
<li class="ltx_item" >
@col
To determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$,
we need to solve
\begin{align*}
\displaystyle x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3}=\mathbf{u},
\end{align*}
@col
i.e.
\begin{align*}
\displaystyle x_{1}-x_{2}+2x_{3}&=1 \\
\displaystyle 2x_{1}+x+2+x_{3}&=1 \\
\displaystyle 3x_{1}+3x_{3}&=3.
\end{align*}
@col
The augmented matrix is
\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}1&-1&2&1\\
2&1&1&1\\
3&0&3&3\end{array}\right]\xrightarrow{\operatorname{RREF}}\left[\begin{array}[]{ccc|c}1&0&1&0\\
0&1&-1&0\\
0&0&0&1\\
\end{array}\right].
\end{align*}
@col
Because the last column of the RREF is a pivot column, the system of linear equations is not solvable.
Hence $\mathbf{u}$ is not a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\}$.
</li>
<li class="ltx_item" >
@col
To determine if $\mathbf{u}$ is a linear combination of $\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4}\}$,
we need to solve
\begin{align*}
\displaystyle x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3}+x_{4}\mathbf{v}_{4}=\mathbf{u}.
\end{align*}
@col
The augmented matrix is:
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{v}_{4}|\mathbf{u}]=\left[\begin{array}[]{cccc|c}1&-1&2&-1&1\\
2&1&1&0&1\\
3&0&3&1&3\\
\end{array}\right].
\end{align*}
@col
Using the standard method, we find one solution (there are infinitely many):
\begin{align*}
\displaystyle x_{1}=\frac{5}{6}
&\quad&
x_{2}=-\frac{2}{3}
&\quad&
x_{3}=0
&\quad&
x_{4}=\frac{1}{2}
\end{align*}
@col
i.e.
\begin{align*}
\displaystyle \mathbf{u}=\frac{5}{6}\mathbf{v}_{1}-\frac{2}{3}\mathbf{v}_{2}+\frac{1}{2}\mathbf{v}_{4}.
\end{align*}
</li>
</ol>
@end
@section{
Vector Form of Solution Sets}
@label{VFSS}
@eg
Consider the linear system
\begin{align*}
\displaystyle 2x_{1}+x_{2}+7x_{3}-7x_{4}&=8 \\
\displaystyle-3x_{1}+4x_{2}-5x_{3}-6x_{4}&=-12 \\
\displaystyle x_{1}+x_{2}+4x_{3}-5x_{4}&=4.
\end{align*}
@col
Row-reducing the augmented matrix yields
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&3&-2&4\\
0&\boxed{1}&1&-3&0\\
0&0&0&0&0\end{bmatrix}
\end{align*}
@col
from which we see that there are $r=2$ pivot columns. Also, $D=\{1,\,2\}$, so that the dependent variables are $x_{1}$ and $x_{2}$, and $F=\{3,\,4,\,5\}$, so that the free variables are $x_{3}$ and $x_{4}$. We will express a generic solution for the system by two slightly different methods, though both arrive at the same conclusion.

Rearranging each equation represented in the row-reduced form of the augmented matrix by solving for the dependent variable in each row yields the vector equality,
\begin{align*}
\displaystyle\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}&=\begin{bmatrix}4-3x_{3}+2x_{4}\\
-x_{3}+3x_{4}\\
x_{3}\\
x_{4}\end{bmatrix}\\
\\
&=\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}-3x_{3}\\
-x_{3}\\
x_{3}\\
0\end{bmatrix}+\begin{bmatrix}2x_{4}\\
3x_{4}\\
0\\
x_{4}\end{bmatrix} \\
&=\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}.
\end{align*}

We will develop the same linear combination a bit quicker, using three steps. While the method above is instructive, the method below will be our preferred approach.

Step 1. Write the vector of variables as a fixed vector plus a linear combination of $n-r$ vectors, using the free variables as the scalars:
\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
\\
\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
\\
\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
\\
\end{bmatrix}
\end{align*}

Step 2. Use 0’s and 1’s to ensure equality for the entries of the vectors with indices in $F$ (corresponding to the free variables):
\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
0\\
1\end{bmatrix}.
\end{align*}

Step 3. For each dependent variable, use the augmented matrix to formulate an equation expressing the dependent variable as a constant plus a linear combination of the free variables. Convert this equation into entries of the vectors that ensure equality for each dependent variable, one at a time.
\begin{align*}
\displaystyle x_{1}=4-3x_{3}+2x_{4}&\Rightarrow&\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}4\\
\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
\\
0\\
1\end{bmatrix} \\
\displaystyle x_{2}=0-1x_{3}+3x_{4}&\Rightarrow&\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}+x_{4}\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}
\end{align*}

While this form is useful for quickly creating solutions, it is even better because it tells us exactly what every solution looks like. We know the solution set is infinite, which is pretty big, but now we can say that a solution is some multiple of $\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}$ plus a multiple of $\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}$ plus the fixed vector $\begin{bmatrix}4\\
0\\
0\\
0\end{bmatrix}$. Period. So it only takes us three vectors to describe the entire infinite solution set, provided we also agree on how to combine the three vectors into a linear combination.
@end
@eg
@col
Consider a linear system of $m=5$ equations in $n=7$ variables, having the augmented matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}2&1&-1&-2&2&1&5&21\\
1&1&-3&1&1&1&2&-5\\
1&2&-8&5&1&1&-6&-15\\
3&3&-9&3&6&5&2&-24\\
-2&-1&1&2&1&1&-9&-30\end{bmatrix}.
\end{align*}

Row-reducing we obtain the matrix
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&2&-3&0&0&9&15\\
0&\boxed{1}&-5&4&0&0&-8&-10\\
0&0&0&0&\boxed{1}&0&-6&11\\
0&0&0&0&0&\boxed{1}&7&-21\\
0&0&0&0&0&0&0&0\end{bmatrix}
\end{align*}

and we see that there are $r=4$ pivot columns. Also, $D=\{1,\,2,\,5,\,6\}$ so the dependent variables are $x_{1},\,x_{2},\,x_{5},$ and $x_{6}$. Similarly, $F=\{3,\,4,\,7,\,8\}$ and the $n-r=3$ free variables are $x_{3},\,x_{4}$ and $x_{7}$. We will express a generic solution for the system by two different methods: both a decomposition and a construction.

Rearranging each equation represented in the row-reduced echelon form of the augmented matrix by solving for the dependent variable in each row yields the vector equality
\begin{align*}
\displaystyle\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}&=\begin{bmatrix}15-2x_{3}+3x_{4}-9x_{7}\\
-10+5x_{3}-4x_{4}+8x_{7}\\
x_{3}\\
x_{4}\\
11+6x_{7}\\
-21-7x_{7}\\
x_{7}\end{bmatrix}. \\
\\
&=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+\begin{bmatrix}-2x_{3}\\
5x_{3}\\
x_{3}\\
0\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}3x_{4}\\
-4x_{4}\\
0\\
x_{4}\\
0\\
0\\
0\end{bmatrix}+\begin{bmatrix}-9x_{7}\\
8x_{7}\\
0\\
0\\
6x_{7}\\
-7x_{7}\\
x_{7}\end{bmatrix} \\
&=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}.
\end{align*}

We will now develop the same linear combination a bit quicker, using three steps. While the method above is instructive, the method below will be our preferred approach.

Step 1. Write the vector of variables as a fixed vector, plus a linear combination of $n-r$ vectors, using the free variables as the scalars:
\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}+x_{7}\begin{bmatrix}\\
\\
\\
\\
\\
\\
\end{bmatrix}
\end{align*}

Step 2. Use 0’s and 1’s to ensure equality for the entries of the vectors with indices in $F$ (corresponding to the free variables):
\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}\\
\\
0\\
0\\
\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}\\
\\
1\\
0\\
\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}\\
\\
0\\
1\\
\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}\\
\\
0\\
0\\
\\
\\
1\end{bmatrix}
\end{align*}

Step 3. For each dependent variable, use the augmented matrix to formulate an equation expressing the dependent variable as a constant plus multiples of the free variables. Convert this equation into entries of the vectors that ensure equality for each dependent variable, one at a time.
\begin{align*}
\displaystyle x_{1}&=15-2x_{3}+3x_{4}-9x_{7}\ \Rightarrow \\
&\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
\\
0\\
0\\
\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
\\
1\\
0\\
\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
\\
0\\
1\\
\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
\\
0\\
0\\
\\
\\
1\end{bmatrix} \\
\displaystyle x_{2}&=-10+5x_{3}-4x_{4}+8x_{7}\ \Rightarrow \\
&\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
\\
\\
1\end{bmatrix} \\
\displaystyle x_{5}&=11+6x_{7}\ \Rightarrow \\
&\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
\\
1\end{bmatrix} \\
\displaystyle x_{6}&=-21-7x_{7}\ \Rightarrow \\
&\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+x_{3}\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+x_{4}\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+x_{7}\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}
\end{align*}

This final form of a typical solution is especially pleasing and useful. For example, we can build solutions quickly by choosing values for our free variables, and then compute a linear combination. For example
\begin{align*}
\displaystyle x_{3}&=2,\,x_{4}=-4,\,x_{7}=3\quad\quad\Rightarrow \\
\displaystyle\mathbf{x}&=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+(2)\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+(-4)\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+(3)\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}=\begin{bmatrix}-28\\
40\\
2\\
-4\\
29\\
-42\\
3\end{bmatrix}
\end{align*}

or perhaps,
\begin{align*}
\displaystyle x_{3}&=5,\,x_{4}=2,\,x_{7}=1\quad\quad\Rightarrow \\
\displaystyle\mathbf{x}&=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+(5)\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+(2)\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+(1)\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}=\begin{bmatrix}2\\
15\\
5\\
2\\
17\\
-28\\
1\end{bmatrix}
\end{align*}

or even,
\begin{align*}
\displaystyle x_{3}&=0,\,x_{4}=0,\,x_{7}=0\quad\quad\Rightarrow \\
\displaystyle\mathbf{x}&=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\\
x_{6}\\
x_{7}\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}+(0)\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}+(0)\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}+(0)\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}.
\end{align*}

So we can compactly express all of the solutions to this linear system with just 4 fixed vectors, provided we agree how to combine them in a linear combinations to create solution vectors.

Suppose you were told that the vector $\mathbf{w}$ below was a solution to this system of equations. Could you turn the problem around and write $\mathbf{w}$ as a linear combination of the four vectors $\mathbf{c}$, $\mathbf{u}_{1}$, $\mathbf{u}_{2}$, $\mathbf{u}_{3}$?
\begin{align*}
\displaystyle\mathbf{w}&=\begin{bmatrix}100\\
-75\\
7\\
9\\
-37\\
35\\
-8\end{bmatrix}&\mathbf{c}&=\begin{bmatrix}15\\
-10\\
0\\
0\\
11\\
-21\\
0\end{bmatrix}&\mathbf{u}_{1}&=\begin{bmatrix}-2\\
5\\
1\\
0\\
0\\
0\\
0\end{bmatrix}&\mathbf{u}_{2}&=\begin{bmatrix}3\\
-4\\
0\\
1\\
0\\
0\\
0\end{bmatrix}&\mathbf{u}_{3}&=\begin{bmatrix}-9\\
8\\
0\\
0\\
6\\
-7\\
1\end{bmatrix}
\end{align*}
@end
@slide
@thm
@title{Vector Form of Solutions to Linear Systems}
@label{VFSLS}
Suppose that $\left[A|\mathbf{b}\right]$ is the augmented matrix for a consistent linear system $\mathcal{LS}({A},{\mathbf{b}})$ of $m$ equations in $n$ variables.
Let $B$ be a row-equivalent $m\times(n+1)$ matrix in reduced row-echelon form. Suppose that $B$ has $r$ pivot columns, with indices $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$, while the $n-r$ non-pivot columns have indices in $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r},\,n+1\}$. Define vectors $\mathbf{c}$, $\mathbf{u}_{j}$, $1\leq j\leq n-r$ of size $n$ by
\begin{align*}
\displaystyle\left[\mathbf{c}\right]_{i}&=\begin{cases}0&\text{if $i\in F$}\\
\left[B\right]_{k,n+1}&\text{if $i\in D$, $i=d_{k}$}\end{cases} \\
\displaystyle\left[\mathbf{u}_{j}\right]_{i}&=\begin{cases}1&\text{if $i\in F$, $i=f_{j}$}\\
0&\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&\text{if $i\in D$, $i=d_{k}$}\end{cases}.
\end{align*}

Then the set of solutions to the system of equations $\mathcal{LS}({A},{\mathbf{b}})$ is
\begin{align*}
\displaystyle S=\left\{\left.\mathbf{c}+\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n-r}\mathbf{u}_{n-r}\,\right|\,\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\ldots,\,\alpha_{n-r}\in{\mathbb{R}}^{\hbox{}}.\right\}
\end{align*}
@end
@proof
<strong>You can skip this proof for now, as long as you understand the examples</strong>
@col
First, the equation $\mathcal{LS}({A},{\mathbf{b}})$ is equivalent to the linear system of equations that has the matrix $B$ as its augmented matrix.
So we need only show that $S$ is the solution set for the system with $B$ as its augmented matrix. The conclusion of this theorem is that the solution set is equal to the set $S$.

We begin by showing that every element of $S$ is indeed a solution to the system. Let $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\ldots,\,\alpha_{n-r}$ be one choice of the scalars used to describe elements of $S$. So an arbitrary element of $S$, which we will consider as a proposed solution, is
\begin{align*}
\displaystyle \mathbf{x}=\mathbf{c}+\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n-r}\mathbf{u}_{n-r}.
\end{align*}

When $r+1\leq\ell\leq m$, row $\ell$ of the matrix $B$ is a zero row, so the equation represented by that row is always true, no matter which solution vector we propose. So concentrate on rows representing equations $1\leq\ell\leq r$. We evaluate equation $\ell$ of the system represented by $B$ with the proposed solution vector $\mathbf{x}$ and refer to the value of the left-hand side of the equation as $\beta_{\ell}$:
\begin{align*}
\displaystyle \beta_{\ell}=\left[B\right]_{\ell 1}\left[\mathbf{x}\right]_{1}+\left[B\right]_{\ell 2}\left[\mathbf{x}\right]_{2}+\left[B\right]_{\ell 3}\left[\mathbf{x}\right]_{3}+\cdots+\left[B\right]_{\ell n}\left[\mathbf{x}\right]_{n}
\end{align*}

Since $\left[B\right]_{\ell d_{i}}=0$ for all $1\leq i\leq r$, except that $\left[B\right]_{\ell d_{\ell}}=1$, we see that $\beta_{\ell}$ simplifies to
\begin{align*}
\displaystyle \beta_{\ell}=\left[\mathbf{x}\right]_{d_{\ell}}+\left[B\right]_{\ell f_{1}}\left[\mathbf{x}\right]_{f_{1}}+\left[B\right]_{\ell f_{2}}\left[\mathbf{x}\right]_{f_{2}}+\left[B\right]_{\ell f_{3}}\left[\mathbf{x}\right]_{f_{3}}+\cdots+\left[B\right]_{\ell f_{n-r}}\left[\mathbf{x}\right]_{f_{n-r}}.
\end{align*}

Notice that for $1\leq i\leq n-r$
\begin{align*}
\displaystyle\left[\mathbf{x}\right]_{f_{i}}&=\left[\mathbf{c}\right]_{f_{i}}+\alpha_{1}\left[\mathbf{u}_{1}\right]_{f_{i}}+\alpha_{2}\left[\mathbf{u}_{2}\right]_{f_{i}}+\cdots+\alpha_{i}\left[\mathbf{u}_{i}\right]_{f_{i}}+\cdots+\alpha_{n-r}\left[\mathbf{u_{n-r}}\right]_{f_{i}} \\
&=0+\alpha_{1}(0)+\alpha_{2}(0)+\cdots+\alpha_{i}(1)+\cdots+\alpha_{n-r}(0) \\
&=\alpha_{i}.
\end{align*}

So $\beta_{\ell}$ simplifies further, and we expand the first term
\begin{align*}
\displaystyle\beta_{\ell}&=\left[\mathbf{x}\right]_{d_{\ell}}+\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&=\left[\mathbf{c}+\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n-r}\mathbf{u}_{n-r}\right]_{d_{\ell}}+ \\
&\quad\quad\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&=\left[\mathbf{c}\right]_{d_{\ell}}+\alpha_{1}\left[\mathbf{u}_{1}\right]_{d_{\ell}}+\alpha_{2}\left[\mathbf{u}_{2}\right]_{d_{\ell}}+\alpha_{3}\left[\mathbf{u}_{3}\right]_{d_{\ell}}+\cdots+\alpha_{n-r}\left[\mathbf{u_{n-r}}\right]_{d_{\ell}}+ \\
&\quad\quad\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&=\left[B\right]_{\ell,{n+1}}+ \\
&\quad\quad\alpha_{1}(-\left[B\right]_{\ell f_{1}})+\alpha_{2}(-\left[B\right]_{\ell f_{2}})+\alpha_{3}(-\left[B\right]_{\ell f_{3}})+\cdots+\alpha_{n-r}(-\left[B\right]_{\ell f_{n-r}})+ \\
&\quad\quad\left[B\right]_{\ell f_{1}}\alpha_{1}+\left[B\right]_{\ell f_{2}}\alpha_{2}+\left[B\right]_{\ell f_{3}}\alpha_{3}+\cdots+\left[B\right]_{\ell f_{n-r}}\alpha_{n-r} \\
&=\left[B\right]_{\ell,{n+1}}.
\end{align*}

So $\beta_{\ell}$ began as the left-hand side of equation $\ell$ of the system represented by $B$ and we now know it equals $\left[B\right]_{\ell,{n+1}}$, the constant term for equation $\ell$ of this system. So the arbitrarily chosen vector from $S$ makes every equation of the system true, and therefore is a solution to the system. So all the elements of $S$ are solutions to the system.

For the second half of the proof, assume that $\mathbf{x}$ is a solution vector for the system having $B$ as its augmented matrix. For convenience and clarity, denote the entries of $\mathbf{x}$ by $x_{i}$. In other words, $x_{i}=\left[\mathbf{x}\right]_{i}$. We desire to show that this solution vector is also an element of the set $S$. Begin with the observation that the entries of a solution vector make equation $\ell$ of the system true for all $1\leq\ell\leq m$:


\begin{align*}
\displaystyle \left[B\right]_{\ell,1}x_{1}+\left[B\right]_{\ell,2}x_{2}+\left[B\right]_{\ell,3}x_{3}+\cdots+\left[B\right]_{\ell,n}x_{n}=\left[B\right]_{\ell,n+1}
\end{align*}

When $\ell\leq r$, the pivot columns of $B$ have zero entries in row $\ell$ with the exception of column $d_{\ell}$, which will contain a $1$. So for $1\leq\ell\leq r$, equation $\ell$ simplifies to
\begin{align*}
\displaystyle 1x_{d_{\ell}}+\left[B\right]_{\ell,f_{1}}x_{f_{1}}+\left[B\right]_{\ell,f_{2}}x_{f_{2}}+\left[B\right]_{\ell,f_{3}}x_{f_{3}}+\cdots+\left[B\right]_{\ell,f_{n-r}}x_{f_{n-r}}=\left[B\right]_{\ell,n+1}.
\end{align*}

This allows us to write,
\begin{align*}
\displaystyle\left[\mathbf{x}\right]_{d_{\ell}}&=x_{d_{\ell}} \\
&=\left[B\right]_{\ell,n+1}-\left[B\right]_{\ell,f_{1}}x_{f_{1}}-\left[B\right]_{\ell,f_{2}}x_{f_{2}}-\left[B\right]_{\ell,f_{3}}x_{f_{3}}-\cdots-\left[B\right]_{\ell,f_{n-r}}x_{f_{n-r}} \\
&=\left[\mathbf{c}\right]_{d_{\ell}}+x_{f_{1}}\left[\mathbf{u}_{1}\right]_{d_{\ell}}+x_{f_{2}}\left[\mathbf{u}_{2}\right]_{d_{\ell}}+x_{f_{3}}\left[\mathbf{u}_{3}\right]_{d_{\ell}}+\cdots+x_{f_{n-r}}\left[\mathbf{u}_{n-r}\right]_{d_{\ell}} \\
&=\left[\mathbf{c}+x_{f_{1}}\mathbf{u}_{1}+x_{f_{2}}\mathbf{u}_{2}+x_{f_{3}}\mathbf{u}_{3}+\cdots+x_{f_{n-r}}\mathbf{u}_{n-r}\right]_{d_{\ell}}.
\end{align*}

This tells us that the entries of the solution vector $\mathbf{x}$ corresponding to dependent variables (indices in $D$) are equal to those of a vector in the set $S$. We still need to check the other entries of the solution vector $\mathbf{x}$ corresponding to the free variables (indices in $F$) to see if they are equal to the entries of the same vector in the set $S$. To this end, suppose $i\in F$ and $i=f_{j}$. Then
\begin{align*}
\displaystyle\left[\mathbf{x}\right]_{i}&=x_{i}=x_{f_{j}} \\
&=0+0x_{f_{1}}+0x_{f_{2}}+0x_{f_{3}}+\cdots+0x_{f_{j-1}}+1x_{f_{j}}+0x_{f_{j+1}}+\cdots+0x_{f_{n-r}} \\
&=\left[\mathbf{c}\right]_{i}+x_{f_{1}}\left[\mathbf{u}_{1}\right]_{i}+x_{f_{2}}\left[\mathbf{u}_{2}\right]_{i}+x_{f_{3}}\left[\mathbf{u}_{3}\right]_{i}+\cdots+x_{f_{j}}\left[\mathbf{u}_{j}\right]_{i}+\cdots+x_{f_{n-r}}\left[\mathbf{u}_{n-r}\right]_{i} \\
&=\left[\mathbf{c}+x_{f_{1}}\mathbf{u}_{1}+x_{f_{2}}\mathbf{u}_{2}+\cdots+x_{f_{n-r}}\mathbf{u}_{n-r}\right]_{i}.
\end{align*}

So entries of
$\mathbf{x}$ and $\mathbf{c}+x_{f_{1}}\mathbf{u}_{1}+x_{f_{2}}\mathbf{u}_{2}+\cdots+x_{f_{n-r}}\mathbf{u}_{n-r}$
are equal and therefore they are equal vectors. Since $x_{f_{1}},\,x_{f_{2}},\,x_{f_{3}},\,\ldots,\,x_{f_{n-r}}$ are scalars, this shows us that $\mathbf{x}$ qualifies for membership in $S$. So the set $S$ contains all of the solutions to the system.
@qed
@end
@setchapter{11}
@chapter{Spanning Sets}


<h5 class="notkw">Reference.</h5>
<ul>
<li>
Beezer, Ver 3.5 Section SS (print version p83 - p94)
</li>
<li>
Strang, Sect 2.3
</li>
</ul>

<h5 class="notkw">Exercise.</h5>
<ul>
<li>
Exercises with solutions can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
Section SS (p.34-40) C40-45, C50, C60, M10, M11, M12. (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions) T10, T20, T21, T22.
</li>
<li>
Strang, Sect 2.3.
</li>
</ul>
@slide
In this section we will provide an extremely compact way to describe an infinite set of vectors, making use of linear combinations. This will give us a convenient way to describe the solution set of a linear system, the null space of a matrix, and many other sets of vectors.
@section{Span of a Set of Vectors}
@label{SSV}
<!--
We saw that the solution set of a homogeneous system can be described as all possible linear combinations of two particular sets of vectors. This is a useful way to construct or describe infinite sets of vectors, so we encapsulate the idea in a definition.
-->
@defn
@title{Span of a Set of Column Vectors}
@label{SSCV}
Given a set of vectors
\begin{align*}
\displaystyle S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\},
\end{align*}
their <b>span</b>, $\left< S \right>$, is the set of all linear combinations of $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}$.
Symbolically,
\begin{align*}
\displaystyle
\left< S \right>&=\left\{\left.\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}\,\right|\,\alpha_{i}\in{\mathbb{R}}^{\hbox{}},\,1\leq i\leq n\right\} \\
&=\left\{\left.\sum_{i=1}^{n}\alpha_{i}\mathbf{u}_{i}\,\right|\,\alpha_{i}\in{\mathbb{R}}^{\hbox{}},\,1\leq i\leq n\right\}
\end{align*}
@end
@slide
@thm
Let $S=\{\mathbf{u}_{1},\ldots,\mathbf{u}_{k}\}\subseteq V={\mathbb{R}}^{m}$.
Then $\left< S \right>$ is a subspace of $V$.
@end
@proof
@newcol
Obviously $\left< S \right>$ is nonempty. Let $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{w}\in W=\left< S \right>$.
Then there exists $\alpha_{1},\ldots,\alpha_{k},\beta_{1},\ldots,\beta_{k}$ such that
\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{k}\mathbf{u}_{k},
\end{align*}
\begin{align*}
\displaystyle \mathbf{w}=\beta_{1}\mathbf{u}_{1}+\cdots+\beta_{k}\mathbf{u}_{k}.
\end{align*}
Then
\begin{align*}
\displaystyle \alpha\mathbf{v}+\mathbf{w}=(\alpha\alpha_{1}+\beta_{1})\mathbf{u}_{1}+\cdots+(\alpha\alpha_{k}+\beta_{k})\mathbf{u}_{k}
\end{align*}
is in $\left< S \right>$.
Thus by @ref{b97161010780aa4b703cb89b1294f822}, $W$ is a subspace.
@qed
@endcol
@end
<strong>Main Questions.</strong>
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
Determine if a vector $\mathbf{v}$ is an element of $\left< S \right>$.
</li>
<li class="ltx_item">
Describe the set $\left< S \right>$.
</li>
<li class="ltx_item">
Is $\left< S \right>$ equal to ${\mathbb{R}}^{m}$?
</li>
</ol>
@slide
@eg
@label{ABS}
Consider the following set of 5 vectors, $S$, from ${\mathbb{R}}^{4}$:
\begin{align*}
\displaystyle S=\left\{\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix},\,\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix},\,\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix},\,\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}\right\}.
\end{align*}
@newcol
Consider the infinite set of vectors $\left< S \right>$ formed by all linear combinations of the elements of $S$. Here are four vectors which we definitely know are elements of $\left< S \right>$:
\begin{align*}
\displaystyle \mathbf{w}=(2)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(-1)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(2)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(3)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}-4\\
2\\
28\\
10\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle \mathbf{x}=(5)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(-6)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(-3)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(4)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(2)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}-26\\
-6\\
2\\
34\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle \mathbf{y}=(1)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(0)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(1)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(0)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(1)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}7\\
4\\
17\\
-4\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle \mathbf{z}=(0)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(0)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(0)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(0)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(0)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\\
0\end{bmatrix}.
\end{align*}
@col
<strong>Fundamental question:</strong> Determine if a given vector is an element of the set or not. Let us learn more about $\left< S \right>$ by investigating which vectors are elements of the set, and which are not.


First, is $\mathbf{u}=\begin{bmatrix}-15\\
-6\\
19\\
5\end{bmatrix}$ an element of $\left< S \right>$?

@col
In other words, are there scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$ such that:
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+\alpha_{2}\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+\alpha_{3}\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+\alpha_{4}\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+\alpha_{5}\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\mathbf{u}=\begin{bmatrix}-15\\
-6\\
19\\
5\end{bmatrix}.
\end{align*}
@col
Searching for such scalars is equivalent to finding a solution to the linear system of equations with augmented matrix:
\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&2&7&1&-1&-15\\
1&1&3&1&0&-6\\
3&2&5&-1&9&19\\
1&-1&-5&2&0&5\end{array}\right].
\end{align*}
@col
This matrix row-reduces to
\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}\boxed{1}&0&-1&0&3&10\\
0&\boxed{1}&4&0&-1&-9\\
0&0&0&\boxed{1}&-2&-7\\
0&0&0&0&0&0\end{array}\right].
\end{align*}
@col
At this point, we see that the system is consistent, so we know there is a solution for the five scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$. This is enough evidence for us to say that $\mathbf{u}\in\left< S \right>$.

Moreover, we can compute an actual solution, for example:

@col
\begin{align*}
\displaystyle\alpha_{1}&=2&\alpha_{2}&=1&\alpha_{3}&=-2&\alpha_{4}&=-3&\alpha_{5}&=2.
\end{align*}
This particular solution allows us to write
\begin{align*}
\displaystyle (2)\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+(1)\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+(-2)\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+(-3)\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+(2)\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\mathbf{u}=\begin{bmatrix}-15\\
-6\\
19\\
5\end{bmatrix}
\end{align*}
making it even more obvious that $\mathbf{u}\in\left< S \right>$.

@col
We now determine if $\mathbf{v}=\begin{bmatrix}3\\
1\\
2\\
-1\end{bmatrix}$ an element of $\left< S \right>$.

@col
We want to know if there are scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$ such that:
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}1\\
1\\
3\\
1\end{bmatrix}+\alpha_{2}\begin{bmatrix}2\\
1\\
2\\
-1\end{bmatrix}+\alpha_{3}\begin{bmatrix}7\\
3\\
5\\
-5\end{bmatrix}+\alpha_{4}\begin{bmatrix}1\\
1\\
-1\\
2\end{bmatrix}+\alpha_{5}\begin{bmatrix}-1\\
0\\
9\\
0\end{bmatrix}=\mathbf{v}=\begin{bmatrix}3\\
1\\
2\\
-1\end{bmatrix}
\end{align*}
@col
Again, this is equivalent to finding a solution to the linear system of equations with augmented matrix:
\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&2&7&1&-1&3\\
1&1&3&1&0&1\\
3&2&5&-1&9&2\\
1&-1&-5&2&0&-1\end{array}\right].
\end{align*}
This matrix row-reduces to
\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}\boxed{1}&0&-1&0&3&0\\
0&\boxed{1}&4&0&-1&0\\
0&0&0&\boxed{1}&-2&0\\
0&0&0&0&0&\boxed{1}\end{array}\right].
\end{align*}
@col
At this point, we see that the system is inconsistent, so we know there is no solution for the five scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4},\,\alpha_{5}$. This is enough evidence for us to say that $\mathbf{v}\not\in\left< S \right>$. End of story.
@endcol
@end
@slide
From the previous example, we have the following theorem:
@thm
Suppose that $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}$ are in $\mathbf{R}^{m}$.
Let $A$ be the $m\times n$ matrix whose $i$-th column is $\mathbf{u}_{i}$.
Then $\mathbf{v}\in\left< S \right>$ is and only if $\mathcal{LS}({A},{\mathbf{v}})$ is consistent.
@end
@slide
@eg
<strong>Computational Technique.</strong>

Given $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ in ${\mathbb{R}}^{m}$, determine if
\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{m}\end{bmatrix}\in\left< S \right>.
\end{align*}
@end
@sol
@enumerate
@item
@col
To determine if $\mathbf{v}\in\left< S \right>$, we need to find $\alpha_{1},\ldots,\alpha_{n}$ such that
\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{v}.
\end{align*}
@item
@col
This is equivalent to solving the system of linear equations $\mathcal{LS}({A},{\mathbf{v}})$,
where $A$ is the $m\times n$ matrix whose $i$-th column is $\mathbf{u}_{i}$.
@item
@col
Row-reduce the augmented matrix $[A|\mathbf{v}]$ to an RREF $B$.
@enumerate
@item
@col
If the last column of $B$ is a pivot column, then the system is inconsistent and $\mathbf{v}\notin\left< S \right>$.
@item
@col
If the last column of $B$ is not a pivot column, then the system is consistent and $\mathbf{v}\in\left< S \right>$.
@endenumerate
@endenumerate
@end
@slide
@eg
Following the previous example, determine if:
\[
\mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{4}\end{bmatrix}\in\left< S \right>.
\]
@col
Applying Gauss-Jordan elimination to the augmented matrix
\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&2&7&1&-1&v_{1}\\
1&1&3&1&0&v_{2}\\
3&2&5&-1&9&v_{3}\\
1&-1&-5&2&0&v_{4}\end{array}\right],
\end{align*}
we obtain

@col
\begin{align*}
\displaystyle \left[\begin{array}[]{ccccc|c}1&0&-1&0&3&-3v_{1}+5v_{2}-v_{4}\\
0&1&4&0&-1&v_{1}-v_{2}\\
0&0&0&1&-2&2v_{1}-3v_{2}+v_{4}\\
0&0&0&0&0&9v_{1}-16v_{2}+v_{3}+4v_{4}\\
\end{array}\right].
\end{align*}
@col
If $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$, then the last column is not a pivot column and the above is an RREF. In this case, $\mathbf{v}\in\left< S \right>$.

@col
If instead $9v_{1}-16v_{2}+v_{3}+4v_{4}\neq 0$, then the above matrix is not an RREF. Hence, the corresponding system of linear equations is inconsistent
and thus $\mathbf{v}\not\in\left< S \right>$.

@col
We therefore conclude that $\mathbf{v}\in\left< S \right>$ if and only if:
\[
9v_{1}-16v_{2}+v_{3}+4v_{4}=0.
\]
@end
@slide
@eg
@label{SCAA}
Consider:
\begin{align*}
\displaystyle S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3}\}=\left\{\begin{bmatrix}1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
1\end{bmatrix},\,\begin{bmatrix}2\\
1\\
0\end{bmatrix}\right\}
\end{align*}
and consider the infinite set $\left< S \right>$.


<!-- First, as an example, note that
\begin{align*}
\displaystyle \mathbf{v}=(5)\begin{bmatrix}1\\
2\\
1\end{bmatrix}+(-3)\begin{bmatrix}-1\\
1\\
1\end{bmatrix}+(7)\begin{bmatrix}2\\
1\\
0\end{bmatrix}=\begin{bmatrix}22\\
14\\
2\end{bmatrix}
\end{align*}
is in $\left< S \right>$, since it is a linear combination of $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3}$. There is nothing magical about the scalars $\alpha_{1}=5,\,\alpha_{2}=-3,\,\alpha_{3}=7$; they can be chosen arbitrarily. So repeat this part of the example yourself, using different values of $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$. What happens if you choose all three scalars to be zero?


So we know how to quickly construct sample elements of the set $\left< S \right>$. A slightly different question arises when you are handed a vector of the correct size and asked if it is an element of $\left< S \right>$. For example, is -->

Does $\mathbf{w}=\begin{bmatrix}1\\
8\\
5\end{bmatrix}$ lie in $\left< S \right>$?

@col
To answer this question, we will look for scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ such that
\begin{align*}
\displaystyle\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}&=\mathbf{w}.
\end{align*}
This is equivalent to solving the system of linear equations
\begin{align*}
\displaystyle\alpha_{1}-\alpha_{2}+2\alpha_{3}&=1 \\
\displaystyle 2\alpha_{1}+\alpha_{2}+\alpha_{3}&=8 \\
\displaystyle\alpha_{1}+\alpha_{2}&=5.
\end{align*}
Building the augmented matrix for this linear system, and row-reducing, gives:

@col
\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&0&1&3\\
0&\boxed{1}&-1&2\\
0&0&0&0\end{array}\right].
\end{align*}
This system has infinitely many solutions (there is a free variable in $x_{3}$), but all we need is one solution vector. The solution,
\begin{align*}
\displaystyle\alpha_{1}&=2&\alpha_{2}&=3&\alpha_{3}&=1
\end{align*}
tells us that:

@col
\begin{align*}
\displaystyle (2)\mathbf{u}_{1}+(3)\mathbf{u}_{2}+(1)\mathbf{u}_{3}=\mathbf{w}.
\end{align*}
@col
So we are convinced that $\mathbf{w}$ really is in $\left< S \right>$.

@col
Notice that there is an infinite number of ways to answer this question affirmatively. We could choose a different solution, this time choosing the free variable to be zero,
\begin{align*}
\displaystyle\alpha_{1}&=3&\alpha_{2}&=2&\alpha_{3}&=0,
\end{align*}
showing that
\begin{align*}
\displaystyle (3)\mathbf{u}_{1}+(2)\mathbf{u}_{2}+(0)\mathbf{u}_{3}=\mathbf{w}.
\end{align*}
Verifying the arithmetic in this second solution will make it obvious that $\mathbf{w}$ is in this span. And of course, we now realize that there are an <em>infinite</em> number of ways to realize $\mathbf{w}$ as element of $\left< S \right>$.

@col
Let us ask the same type of question again, but this time with: $\mathbf{y}=\begin{bmatrix}2\\
4\\
3\end{bmatrix}$.

Is $\mathbf{y}\in\left< S \right>$?

@col
So we will look for scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ such that
\begin{align*}
\displaystyle\alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}&=\mathbf{y}.
\end{align*}
This is equivalent to finding solutions to the system of equations
\begin{align*}
\displaystyle\alpha_{1}-\alpha_{2}+2\alpha_{3}&=2 \\
\displaystyle 2\alpha_{1}+\alpha_{2}+\alpha_{3}&=4 \\
\displaystyle\alpha_{1}+\alpha_{2}&=3,
\end{align*}
Building the augmented matrix for this linear system and row-reducing gives
\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&0&1&0\\
0&\boxed{1}&-1&0\\
0&0&0&\boxed{1}\end{array}\right].
\end{align*}
This system is inconsistent because the last column is a pivot column.
So there are no scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ that will create a linear combination of $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3}$ that equals $\mathbf{y}$. More precisely, $\mathbf{y}\not\in\left< S \right>$.


There are three things to observe in this example.
<ol class="ltx_enumerate">
<li class="ltx_item">
It is easy to construct vectors in $\left< S \right>$.
</li>
<li class="ltx_item">
It is possible that some vectors are in $\left< S \right>$ (such as $\mathbf{w}$), while others are not (such as $\mathbf{y}$).
</li>
<li class="ltx_item">
Deciding if a given vector is in $\left< S \right>$ leads to a linear system of equations and asking if the system is consistent.
</li>
</ol>
@end
@slide
@eg
Let
\begin{align*}
\displaystyle R=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3}\}=\left\{\begin{bmatrix}-7\\
5\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
5\\
0\end{bmatrix},\,\begin{bmatrix}-12\\
7\\
4\end{bmatrix}\right\}
\end{align*}
and consider its span $\left< R \right>$.
<!--
First, as an example, note that
\begin{align*}
\displaystyle \mathbf{x}=(2)\begin{bmatrix}-7\\
5\\
1\end{bmatrix}+(4)\begin{bmatrix}-6\\
5\\
0\end{bmatrix}+(-3)\begin{bmatrix}-12\\
7\\
4\end{bmatrix}=\begin{bmatrix}-2\\
9\\
-10\end{bmatrix}
\end{align*}
is in $\left< R \right>$, since it is a linear combination of $\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3}$. In other words, $\mathbf{x}\in\left< R \right>$. Try some different values of $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ yourself, and see what vectors you can create as elements of $\left< R \right>$.


Now ask if a given vector is an element of $\left< R \right>$. For example, is -->

@col
Does the vector$\mathbf{z}=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}$ lie in $\left< R \right>$?

@col
To answer this question, we will look for scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ so that
\begin{align*}
\displaystyle\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\mathbf{v}_{3}&=\mathbf{z}.
\end{align*}
This is equivalent to finding solutions to the following system of linear equations:
\begin{align*}
\displaystyle-7\alpha_{1}-6\alpha_{2}-12\alpha_{3}&=-33 \\
\displaystyle 5\alpha_{1}+5\alpha_{2}+7\alpha_{3}&=24 \\
\displaystyle\alpha_{1}+4\alpha_{3}&=5.
\end{align*}
Building the augmented matrix for this linear system and row-reducing gives
\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&0&0&-3\\
0&\boxed{1}&0&5\\
0&0&\boxed{1}&2\end{array}\right].
\end{align*}
This system has a unique solution,
\begin{align*}
\displaystyle\alpha_{1}=-3\quad\displaystyle\alpha_{2}=5\quad\displaystyle\alpha_{3}=2
\end{align*}
telling us that
\begin{align*}
\displaystyle (-3)\mathbf{v}_{1}+(5)\mathbf{v}_{2}+(2)\mathbf{v}_{3}=\mathbf{z}.
\end{align*}
So we are convinced that $\mathbf{z}$ really is in $\left< R \right>$. Notice that in this case we have only one way to answer the question affirmatively, since the solution is unique.

@col
Let us ask about another vector.
Let $\mathbf{x}=\begin{bmatrix}-7\\
8\\
-3\end{bmatrix}$.
Is $\mathbf{x}$ a vector in $\left< R \right>$?

@col
In other words, are there scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ so that:
\begin{align*}
\displaystyle\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\mathbf{v}_{3}&=\mathbf{x}
\end{align*}
This is equivalent to finding the solutions to the system of equations

\begin{align*}
\displaystyle-7\alpha_{1}-6\alpha_{2}-12\alpha_{3}&=-7 \\
\displaystyle 5\alpha_{1}+5\alpha_{2}+7\alpha_{3}&=8 \\
\displaystyle\alpha_{1}+4\alpha_{3}&=-3.
\end{align*}
Building the augmented matrix for this linear system and row-reducing gives
\begin{align*}
\displaystyle \left[\begin{array}[]{ccc|c}\boxed{1}&0&0&1\\
0&\boxed{1}&0&2\\
0&0&\boxed{1}&-1\end{array}\right].
\end{align*}
This system has a unique solution,
\begin{align*}
\displaystyle\alpha_{1}=1\quad\displaystyle\alpha_{2}=2\quad\displaystyle\alpha_{3}=-1
\end{align*}
telling us that
\begin{align*}
\displaystyle (1)\mathbf{v}_{1}+(2)\mathbf{v}_{2}+(-1)\mathbf{v}_{3}=\mathbf{x}.
\end{align*}
So we are convinced that $\mathbf{x}$ really is in $\left< R \right>$. Notice that in this case we again have only one way to answer the question affirmatively since the solution is again unique.


We could continue to test other vectors for membership in $\left< R \right>$, but there is no point. A question about membership in $\left< R \right>$ inevitably leads to a system of three equations in the three variables $\alpha_{1},\,\alpha_{2},\,\alpha_{3}$ with a coefficient matrix whose columns are the vectors $\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3}$. This particular coefficient matrix is nonsingular, so by @ref{NME1} the system is guaranteed to have a solution. (This solution is unique, but that is not critical here.) So no matter which vector we might have chosen for $\mathbf{z}$, we are certain to discover that it was an element of $\left< R \right>$.

<strong>Conclusion</strong>: Every vector of size 3 is in $\left< R \right>$, or $\left< R \right>={\mathbb{R}}^{3}$.
@end
The example above inspires the following result:
@thm
@label{thm:ANSRM}
@col
Given $m$ vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{m}\}$ in ${\mathbb{R}}^{m}$, let $A$ be the $m\times m$ square matrix whose $i$-th column is $\mathbf{u}_{i}$. Then $A$ is non-singular if and only if $\left< S \right>={\mathbb{R}}^{m}$.
@end
@proof
@newcol
($\Rightarrow$)
@newcol
If $A$ is non-singular, then for every $\mathbf{b}\in{\mathbb{R}}^{m}$ the equation $\mathcal{LS}({A},{\mathbf{b}})$ is consistent
by @ref{b25e99edb5cb21a0655d0d961da21e47}. Hence $\mathbf{b}\in\left< S \right>$. So ${\mathbb{R}}^{m}=\left< S \right>$.
@endcol

($\Leftarrow$)
@newcol
This part is difficult; we will only sketch the idea.
Let the RREF of $A$ be $B$. Suppose that $A$ is singular. Then $B\neq I_{m}$ and the last row of $B$ is a zero row. So there exists $\mathbf{b}\in{\mathbb{R}}^{m}$ such that $\mathcal{LS}({B},{\mathbf{b}})$ is inconsistent.
(e.g. $\mathbf{b}=\begin{bmatrix}0\\
\vdots\\
0\\
1\end{bmatrix}$).
Hence there exists $\mathbf{c}\in{\mathbb{R}}^{m}$ such that $\mathcal{LS}({A},{\mathbf{c}})$ is inconsistent (why?).
Thus $\mathbf{c}\notin\left< S \right>$. So $\left< S \right>\neq{\mathbb{R}}^{m}$. This completes the proof.
@endcol
@qed
@endcol
@end
@section{
Spanning Sets of Null Spaces
}

The following theorem describe how to express a null space as $\left< S \right>$.
@thm
@title{Spanning Sets for Null Spaces}
@label{SSNS}
Suppose that $A$ is an $m\times n$ matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Suppose that $B$ has $r$ pivot columns, with indices given by $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$, while the $n-r$ non-pivot columns have indices $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$,
\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&\text{if $i\in F$, $i=f_{j}$}\\
0&\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}
Then the null space of $A$ is given by
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left<\left\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\right\}\right>
\end{align*}
@end
@proof
@newcol
The can be seen by moving the free variables to another side. For details. See Beezer p88. Don’t memorize this theorem. Instead, study the examples below.
@qed
@endcol
@end
@slide
@eg
<b>Spanning set of a null space</b>

Find a set of vectors, $S$, so that the null space of the matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}1&3&3&-1&-5\\
2&5&7&1&1\\
1&1&5&1&5\\
-1&-4&-2&0&4\end{bmatrix}
\end{align*}
is the span of $S$, that is, $\left< S \right>={\mathcal{N}}\!\left(A\right)$.
<hr/>
@col
The null space of $A$ is the set of all solutions to the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$.
Begin by row-reducing $A$. The result is
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&6&0&4\\
0&\boxed{1}&-1&0&-2\\
0&0&0&\boxed{1}&3\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
We have $D=\{1,\,2,\,4\}$ and $F=\{3,\,5\}$. Hence $x_{3}$ and $x_{5}$ are free variables and we can interpret each nonzero row as an expression for the dependent variables $x_{1}$, $x_{2}$, $x_{4}$ (respectively) in the free variables $x_{3}$ and $x_{5}$. With this we can write the vector form of a solution vector as
\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}\end{bmatrix}=\begin{bmatrix}-6x_{3}-4x_{5}\\
x_{3}+2x_{5}\\
x_{3}\\
-3x_{5}\\
x_{5}\end{bmatrix}=x_{3}\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix}+x_{5}\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}.
\end{align*}
Then, in the notation of the above theorem, we have
\begin{align*}
\displaystyle\mathbf{z}_{1}&=\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix}&\mathbf{z}_{2}&=\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}
\end{align*}
and
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)
=\left<
\left\{
\mathbf{z}_{1},\,\mathbf{z}_{2}
\right\}
\right>
=\left<
\left
\{
\begin{bmatrix}-6\\
1\\
1\\
0\\
0\end{bmatrix},\,\begin{bmatrix}-4\\
2\\
0\\
-3\\
1\end{bmatrix}
\right\}
\right>.
\end{align*}
@end
@eg
@col
Consider the matrix:
\begin{align*}
\displaystyle A=\begin{bmatrix}2&1&5&1&5&1\\
1&1&3&1&6&-1\\
-1&1&-1&0&4&-3\\
-3&2&-4&-4&-7&0\\
3&-1&5&2&2&3\end{bmatrix}.
\end{align*}
Row-reducing $A$ gives the matrix
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&2&0&-1&2\\
0&\boxed{1}&1&0&3&-1\\
0&0&0&\boxed{1}&4&-2\\
0&0&0&0&0&0\\
0&0&0&0&0&0\end{bmatrix}.
\end{align*}
First, the non-pivot columns have indices $F=\{3,\,5,\,6\}$, so we will construct the $n-r=6-3=3$ vectors with a pattern of zeros and ones dictated by the indices in $F$. This is the realization of the first two lines of the three-case definition of the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$.
\begin{align*}
\displaystyle\mathbf{z}_{1}&=\begin{bmatrix}\\
\\
1\\
\\
0\\
0\end{bmatrix}&\mathbf{z}_{2}&=\begin{bmatrix}\\
\\
0\\
\\
1\\
0\end{bmatrix}&\mathbf{z}_{3}&=\begin{bmatrix}\\
\\
0\\
\\
0\\
1\end{bmatrix}.
\end{align*}
Each of these vectors arises due to the presence of a non-pivot column. The remaining entries of each vector are the entries of the non-pivot column, negated, and distributed into the empty slots in order (these slots have indices in the set $D$, so also refer to pivot columns). This is the realization of the third line of the three-case definition of the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$.
\begin{align*}
\displaystyle\mathbf{z}_{1}&=\begin{bmatrix}-2\\
-1\\
1\\
0\\
0\\
0\end{bmatrix}&\mathbf{z}_{2}&=\begin{bmatrix}1\\
-3\\
0\\
-4\\
1\\
0\end{bmatrix}&\mathbf{z}_{3}&=\begin{bmatrix}-2\\
1\\
0\\
2\\
0\\
1\end{bmatrix}.
\end{align*}
So we have
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)=\left<
\left\{
\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3}
\right\}
\right>
=
\left\langle
\left\{\begin{bmatrix}-2\\
-1\\
1\\
0\\
0\\
0\end{bmatrix},\,\begin{bmatrix}1\\
-3\\
0\\
-4\\
1\\
0\end{bmatrix},\,\begin{bmatrix}-2\\
1\\
0\\
2\\
0\\
1\end{bmatrix}
\right\}
\right\rangle.
\end{align*}
@end
@section{Obtain Same Span Using Fewer Vectors}
@eg
Begin with the following set of four vectors of size $3$:
\begin{align*}
\displaystyle T
=\left\{\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3},\,\mathbf{w}_{4}\right\}
=\left\{\begin{bmatrix}2\\
-3\\
1\end{bmatrix},\,\begin{bmatrix}1\\
4\\
1\end{bmatrix},\,\begin{bmatrix}7\\
-5\\
4\end{bmatrix},\,\begin{bmatrix}-7\\
-6\\
-5\end{bmatrix}
\right\}.
\end{align*}
@col
Let:
\begin{align*}
\displaystyle D=\begin{bmatrix}2&1&7&-7\\
-3&4&-5&-6\\
1&1&4&-5\end{bmatrix}
\end{align*}
and consider the infinite set $W=\left< T \right>$. Check that the vector
\begin{align*}
\displaystyle \mathbf{z}_{2}=\begin{bmatrix}2\\
3\\
0\\
1\end{bmatrix}
\end{align*}
is a solution to the homogeneous system $\mathcal{LS}({D},{\mathbf{0}})$

@col
We can write the linear combination,
\begin{align*}
\displaystyle 2\mathbf{w}_{1}+3\mathbf{w}_{2}+0\mathbf{w}_{3}+1\mathbf{w}_{4}=\mathbf{0}
\end{align*}
which we can solve for $\mathbf{w}_{4}$ as
\begin{align*}
\displaystyle \mathbf{w}_{4}=(-2)\mathbf{w}_{1}+(-3)\mathbf{w}_{2}.
\end{align*}
@col
This equation says that whenever we encounter the vector $\mathbf{w}_{4}$, we can replace it with a specific linear combination of the vectors $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$. So using $\mathbf{w}_{4}$ in the set $T$, along with $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$, is excessive. An example of what we mean here can be illustrated by the computation:

@col
\begin{align*}
\displaystyle 5\mathbf{w}_{1}&+(-4)\mathbf{w}_{2}+6\mathbf{w}_{3}+(-3)\mathbf{w}_{4} \\
&=5\mathbf{w}_{1}+(-4)\mathbf{w}_{2}+6\mathbf{w}_{3}+(-3)\left((-2)\mathbf{w}_{1}+(-3)\mathbf{w}_{2}\right) \\
&=5\mathbf{w}_{1}+(-4)\mathbf{w}_{2}+6\mathbf{w}_{3}+\left(6\mathbf{w}_{1}+9\mathbf{w}_{2}\right) \\
&=11\mathbf{w}_{1}+5\mathbf{w}_{2}+6\mathbf{w}_{3}.
\end{align*}
@col
So, what began as a linear combination of the vectors $\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3},\,\mathbf{w}_{4}$ has been reduced to a linear combination of the vectors $\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3}$.

Hence:
@col
\begin{align*}
\displaystyle W=\left<\left\{\mathbf{w}_{1},\,\mathbf{w}_{2},\,\mathbf{w}_{3}\right\}\right>,
\end{align*}
and the span of our set of vectors, $W$, has not changed, but we have described it by the span of a set of three vectors, rather than four. Furthermore, we can achieve yet another, similar, reduction.

@col
Check that the vector
\begin{align*}
\displaystyle \mathbf{z}_{1}=\begin{bmatrix}-3\\
-1\\
1\\
0\end{bmatrix}
\end{align*}
is a solution to the homogeneous system $\mathcal{LS}({D},{\mathbf{0}})$.

@col
We can write the linear combination,
\begin{align*}
\displaystyle (-3)\mathbf{w}_{1}+(-1)\mathbf{w}_{2}+1\mathbf{w}_{3}=\mathbf{0}
\end{align*}
which we can solve for $\mathbf{w}_{3}$ as
\begin{align*}
\displaystyle \mathbf{w}_{3}=3\mathbf{w}_{1}+1\mathbf{w}_{2}.
\end{align*}
@col
This equation says that whenever we encounter the vector $\mathbf{w}_{3}$, we can replace it with a specific linear combination of the vectors $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$. So, as before, the vector $\mathbf{w}_{3}$ is not needed in the description of $W$, provided we have $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$ available. In particular, a careful proof would show that
\begin{align*}
\displaystyle W=\left<\left\{\mathbf{w}_{1},\,\mathbf{w}_{2}\right\}\right>
\end{align*}
@col
So $W$ began life as the span of a set of four vectors. We have now shown (utilizing solutions to a homogeneous system) that $W$ can also be described as the span of a set of just two vectors. Convince yourself that we cannot go any further. In other words, it is not possible to dismiss either $\mathbf{w}_{1}$ or $\mathbf{w}_{2}$ in a similar fashion and winnow the set down to just one vector.

@col
What was it about the original set of four vectors that allowed us to declare certain vectors as surplus? And just which vectors were we able to dismiss? And why did we have to stop once we had two vectors remaining? The answers to these questions motivate <b>linear independence</b>, our next section and next definition, and so are worth considering carefully now.
@end
@setchapter{12}
@chapter{Linear Independence}
<h5 class='notkw'>Reference.</h5>
<ul>
	<li>
		Beezer, Ver 3.5 Section LI (print version p95 - p104)
	</li>
	<li>
		Strang, Section 2.3
	</li>
</ul>
<h5 class='notkw'>Exercise.</h5>
<ul>
	<li>
		Exercises with solutions can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
		Section LI (p.40-48) (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions) C20-25, C30-33, C60, M20, M21, M50, M51, T10-13, T15, T20, T50.
	</li>
	<li>
		Strang, Section 2.3
	</li>
</ul>
@slide
<b>Linear independence</b> is one of the most fundamental conceptual ideas in linear algebra, along with the notion of <strong>span</strong>. So this lecture, and the subsequent one, will explore this new idea.
@section{
Linearly Independent Sets of Vectors}
@label{LISV}
@defn
@title{Relation of Linear Dependence}
@label{RLDCV}
Given a set of vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$, an equality of the form
\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{0}
\end{align*}
is a <b>relation of linear dependence</b> on $S$. If this equality is formed in a trivial fashion, i.e., $\alpha_{i}=0$, $1\leq i\leq n$, then we say that it is the <b>trivial relation of linear dependence</b> on $S$.
@end
@defn
@title{Linear Independence}
@label{LICV}
@col
The set of vectors $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ is <b>linearly dependent</b> if there is a relation of linear dependence on $S$ that is not trivial. In the case where the only relation of linear dependence on $S$ is the trivial one, then $S$ is a <b>linearly independent</b> set of vectors.
@end
@remark
@col
In short, a set of vectors $\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}$ is linearly independent if and only if the only solution to:
\[
x_1 \mathbf{u}_1 + x_2 \mathbf{u}_2 + \cdots + x_n \mathbf{u}_n = \mathbf{0}
\]
is:
\[
x_1 = x_2 = \cdots = x_n = 0.
\]
@end
@slide
@thm
@title{Linearly Independent Vectors and Homogeneous Systems}
@label{LIVHS}
Suppose that $S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\}\subseteq{\mathbb{R}}^{m}$ is a set of vectors and that $A$ is the $m\times n$ matrix whose columns are the vectors in $S$. Then $S$ is a linearly independent set if and only if the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution.
@end
@proof

($\Leftarrow$) @col
Suppose that $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution. Since it is a homogeneous system, this solution must be the trivial solution, $\mathbf{x}=\mathbf{0}$. This means that the only relation of linear dependence on $S$ is the trivial one. So $S$ is linearly independent.
@endcol

($\Rightarrow$) @col
We will prove the converse. Suppose that $\mathcal{LS}({A},{\mathbf{0}})$ does not have a unique solution. Since it is a homogeneous system, it is consistent. And so must have infinitely many solutions. One of these infinitely many solutions must be nontrivial (in fact, almost all of them are); choose one. This nontrivial solution will give a nontrivial relation of linear dependence on $S$. We therefore conclude that $S$ is a linearly dependent set.
@endcol
@qed
@end
@slide
Since the above theorem is an "if-and-only-if" statement, we can use it to determine the linear independence or dependence of any set of column vectors, just by creating a matrix and analyzing its row-reduced echelon form. Let us illustrate this with two more examples.
@slide
@eg
@label{LDS}
<b>Linearly dependent set in ${\mathbb{R}}^{5}$</b>

Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$:
\begin{align*}
\displaystyle S=
\left\{
\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}
\right\}
\end{align*}
@col
To determine linear independence, we first form an arbitrary relation of linear dependence,
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+\alpha_{2}\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+\alpha_{3}\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+\alpha_{4}\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}=\mathbf{0}
\end{align*}
We know that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$ is a solution to this equation, but that is of no interest whatsoever. That is always the case, no matter what four vectors we might have chosen. We are curious to know if there are other, nontrivial, solutions.

In other words, are there nontrivial solutions to the homogeneous linear system
$\mathcal{LS}(A, \mathbf{0})$, where the columns of $A$ consist of the vectors in $S$.

@col
Row-reducing the matrix $A$ gives:
\begin{align*}
\displaystyle A=\begin{bmatrix}2&1&2&-6\\
-1&2&1&7\\
3&-1&-3&-1\\
1&5&6&0\\
2&2&1&1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&-2\\
0&\boxed{1}&0&4\\
0&0&\boxed{1}&-3\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
We could solve the corresponding homogeneous system completely, but for this example all we need is one nontrivial solution. Setting the lone free variable to any nonzero value, such as $x_{4}=1$, yields the nontrivial solution:
\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}2\\
-4\\
3\\
1\end{bmatrix}.
\end{align*}
@col
Hence,
\begin{align*}
\displaystyle 2\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+(-4)\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+3\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+1\begin{bmatrix}-6\\
7\\
-1\\
0\\
1\end{bmatrix}=\mathbf{0}.
\end{align*}
This is a relation of linear dependence on $S$ that is not trivial, so we conclude that $S$ is <strong>linearly dependent</strong>.
@end
@eg
@label{LIS}
@col
<b>Linearly independent set in ${\mathbb{R}}^{5}$</b>

Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$:
\begin{align*}
\displaystyle T=
\left\{
\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix},\,\begin{bmatrix}-6\\
7\\
-1\\
1\\
1\end{bmatrix}
\right\}.
\end{align*}
To determine linear independence we first form an arbitrary relation of linear dependence,
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}2\\
-1\\
3\\
1\\
2\end{bmatrix}+\alpha_{2}\begin{bmatrix}1\\
2\\
-1\\
5\\
2\end{bmatrix}+\alpha_{3}\begin{bmatrix}2\\
1\\
-3\\
6\\
1\end{bmatrix}+\alpha_{4}\begin{bmatrix}-6\\
7\\
-1\\
1\\
1\end{bmatrix}=\mathbf{0}.
\end{align*}
@col
We want to know if there are solutions to the equation above besides the trivial one: $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$.

Row-reducing the associated matrix gives:
\begin{align*}
\displaystyle B=\begin{bmatrix}2&1&2&-6\\
-1&2&1&7\\
3&-1&-3&-1\\
1&5&6&1\\
2&2&1&1\end{bmatrix}&\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&0\\
0&0&0&\boxed{1}\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
From the form of this matrix, we see that there are no free variables. Hence the associated homogeneous linear system has only the trivial solution. So we now know that there is but one way to combine the four vectors of $T$ into a relation of linear dependence, and that this one way is the easy and obvious way. Hence, the set $T$ is <strong>linearly independent</strong>.
@end

@subsection{More Examples}
@eg
@label{LIHS}
<b>Linearly independent</b>

@col
Is the set of vectors:
\begin{align*}
\displaystyle S=
\left\{
\begin{bmatrix}
2\\
-1\\
3\\
4\\
2
\end{bmatrix},
\,
\begin{bmatrix}
6\\
2\\
-1\\
3\\
4
\end{bmatrix},
\,
\begin{bmatrix}
4\\
3\\
-4\\
5\\
1
\end{bmatrix}
\right\}
\end{align*}
linearly independent or linearly dependent?
@end
@sol
@col
The above theorem suggests that we study the matrix $A$ whose columns are the vectors in $S$. Specifically, we are interested in the size of the solution set of the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Row-reducing $A$ gives:

@col
\begin{align*}
\displaystyle A=\begin{bmatrix}2&6&4\\
-1&2&3\\
3&-1&-4\\
4&3&5\\
2&4&1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0\\
0&\boxed{1}&0\\
0&0&\boxed{1}\\
0&0&0\\
0&0&0\end{bmatrix}.
\end{align*}
@col
We have $r=3$, so there are $n-r=3-3=0$ free variables. Hence $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution. By the above theorem, the set $S$ is linearly independent.
@endcol
@end

@slide
@eg
@label{LDHS}
<b>Linearly dependent</b>

@col
Is the set of vectors:
\begin{align*}
\displaystyle S=
\left\{\begin{bmatrix}2\\
-1\\
3\\
4\\
2\end{bmatrix},\,\begin{bmatrix}6\\
2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}4\\
3\\
-4\\
-1\\
2\end{bmatrix}
\right\}
\end{align*}
linearly independent or linearly dependent?
@end
@sol

@col
Theorem @ref{LIVHS} suggests that we study the matrix $A$ whose columns are the vectors in $S$. Specifically, we are interested in the size of the solution set of the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Row-reducing $A$ gives
\begin{align*}
\displaystyle A=\begin{bmatrix}2&6&4\\
-1&2&3\\
3&-1&-4\\
4&3&-1\\
2&4&2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&-1\\
0&\boxed{1}&1\\
0&0&0\\
0&0&0\\
0&0&0\end{bmatrix}.
\end{align*}
We have $r=2$, so there are $n-r=3-2=1$ free variables. Hence $\mathcal{LS}({A},{\mathbf{0}})$ has infinitely many solutions. By
Theorem @ref{LIVHS}, the set $S$ is linearly dependent.
@qed
@end

@slide
Theorem @ref{LIVHS} gives us a straightforward way to determine if a set of vectors is linearly independent or dependent.

Review the previous two examples. They are very similar, differing only in the last two slots of the third vector. This resulted in slightly different matrices when row-reduced, and different values of $r$, the number of nonzero rows. Notice, too, that we are less interested in the actual solution set, and more interested in its form or size. These observations allow us to make a slight improvement on Theorem @ref{LIVHS}.

@section{Relation between Linear Independence and the Number of Pivot Columns}
@thm
@title{Linearly Independent Vectors, $r$ and $n$}
@label{LIVRN}
Suppose that
\begin{align*}
\displaystyle S=\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\}\subseteq{\mathbb{R}}^{m}
\end{align*}
is a set of vectors and that $A$ is the $m\times n$ matrix whose columns are the vectors in $S$. Let $B$ be a matrix in reduced row-echelon form that is row-equivalent to $A$ and let $r$ denote the number of pivot columns in $B$. Then $S$ is linearly independent if and only if $n=r$.
@end
@proof
@col
Theorem @ref{LIVHS} says the linear independence of $S$ is equivalent to the homogeneous linear system $\mathcal{LS}({A},{\mathbf{0}})$ having a unique solution. Since the zero vector is a solution of $\mathcal{LS}({A},{\mathbf{0}})$,
$\mathcal{LS}({A},{\mathbf{0}})$ is consistent. We can therefore can apply @ref{CSRN} to see that the solution is unique exactly when $n=r$.
@qed
@end
@slide
Here is an example of the most straightforward way to determine if a set of column vectors is linearly independent or linearly dependent. While this method can be quick and easy, do not forget the logical progression from the definition of linear independence through homogeneous system of equations which makes it possible.
@eg
<b>Linear dependence, $r$ and $n$</b>

@col
Is the set of vectors:
\begin{align*}
\displaystyle S=
\left\{\begin{bmatrix}2\\
-1\\
3\\
1\\
0\\
3\end{bmatrix},\,\begin{bmatrix}9\\
-6\\
-2\\
3\\
2\\
1\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
0\\
0\\
1\end{bmatrix},\,\begin{bmatrix}-3\\
1\\
4\\
2\\
1\\
2\end{bmatrix},\,\begin{bmatrix}6\\
-2\\
1\\
4\\
3\\
2\end{bmatrix}\right\}
\end{align*}
linearly independent or linearly dependent?
@end
@sol
@col
Theorem @ref{LIVRN} suggests that we take the vectors of $S$ as the columns of a matrix and then analyze its reduced row-echelon form:
\begin{align*}
\displaystyle \begin{bmatrix}2&9&1&-3&6\\
-1&-6&1&1&-2\\
3&-2&1&4&1\\
1&3&0&2&4\\
0&2&0&1&3\\
3&1&1&2&2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&-1\\
0&\boxed{1}&0&0&1\\
0&0&\boxed{1}&0&2\\
0&0&0&\boxed{1}&1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
Now we need only compute that
$r=4<5=n$
to recognize, via Theorem @ref{LIVRN}, that $S$ is a linearly dependent set. Boom!
@qed
@end
@slide
@eg
<b>Large linearly dependent set in ${\mathbb{R}}^{4}$</b>

@col
Consider the set of $n=9$ vectors from ${\mathbb{R}}^{4}$,
\begin{align*}
\displaystyle R=\left\{\begin{bmatrix}-1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}7\\
1\\
-3\\
6\end{bmatrix},\,\begin{bmatrix}1\\
2\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}0\\
4\\
2\\
9\end{bmatrix},\,\begin{bmatrix}5\\
-2\\
4\\
3\end{bmatrix},\,\begin{bmatrix}2\\
1\\
-6\\
4\end{bmatrix},\,\begin{bmatrix}3\\
0\\
-3\\
1\end{bmatrix},\,\begin{bmatrix}1\\
1\\
5\\
3\end{bmatrix},\,\begin{bmatrix}-6\\
-1\\
1\\
1\end{bmatrix}\right\}.
\end{align*}
@col
To employ Theorem @ref{LIVHS}, we form a $4\times 9$ matrix $C$ whose columns are the vectors in $R$:
\begin{align*}
\displaystyle C=\begin{bmatrix}-1&7&1&0&5&2&3&1&-6\\
3&1&2&4&-2&1&0&1&-1\\
1&-3&-1&2&4&-6&-3&5&1\\
2&6&-2&9&3&4&1&3&1\end{bmatrix}.
\end{align*}
@col
To determine if the homogeneous system $\mathcal{LS}({C},{\mathbf{0}})$ has a unique solution or not, we would normally row-reduce this matrix. But in this particular example, we can do better:

@col
Since the system is homogeneous with $n=9$ variables in $m=4$ equations,
and $n > m$, there are infinitely many solutions. Since there is not a unique solution, Theorem @ref{LIVHS} says the set $R$ is linearly dependent.
@end
@slide
The following theorem generalizes the previous example.
@thm
@title{More Vectors than Size implies Linear Dependence}
@label{MVSLD}
Suppose that $S=\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\}\subseteq{\mathbb{R}}^{m}$ and $n>m$. Then $S$ is a linearly dependent set.
@end
@proof
@col
Form the $m\times n$ matrix $A$ whose columns are $\mathbf{u}_{i}$, $1\leq i\leq n$. Consider the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. By @ref{HMVEI} this system has infinitely many solutions. Since the system does not have a unique solution, Theorem @ref{LIVHS} says the columns of $A$ form a linearly dependent set, as desired.
@qed
@end
@section{
Linear Independence and Nonsingular Matrices}
@label{LINM}
We will now specialize to sets of $n$ vectors in ${\mathbb{R}}^{n}$.
<!--
This will put Theorem @ref{MVSLD} off-limits, while Theorem @ref{LIVHS} will involve square matrices.
-->
@eg
<b>Linearly dependent columns</b> Do the columns of the matrix
\begin{align*}
\displaystyle \begin{bmatrix}1&-1&2\\
2&1&1\\
1&1&0\end{bmatrix}
\end{align*}
form a linearly independent or dependent set?
@end
@sol
@col
We can show that $A$ is singular.
According to the definition of nonsingular matrices, the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$ has infinitely many solutions. So, by Theorem @ref{LIVHS}, the columns of $A$ form a linearly dependent set.
@qed
@end
@eg
<b>Linearly independent columns</b>

@col
Do the columns of this matrix
\begin{align*}
\displaystyle B=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}
\end{align*}
form a linearly independent or dependent set?
@end
@sol
@col
We can show that $B$ is nonsingular. According to the definition of nonsingular matrices, the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution. So, by Theorem @ref{LIVHS}, the columns of $B$ form a linearly independent set.
@qed
@end
@slide
That the previous two examples have opposite properties for the columns of their coefficient matrices is no accident. Here is the theorem, and then we will update our equivalences for nonsingular matrices.
@thm
@title{Nonsingular Matrices have Linearly Independent Columns}
@label{NMLIC}
Suppose that $A$ is a <em>square</em> matrix. Then $A$ is nonsingular if and only if the columns of $A$ form a linearly independent set.
@end
@proof
@col
This is a proof where we can chain together equivalences, rather than proving the two halves separately.
\begin{align*}
A \text{ nonsingular} &\iff\text{$\mathcal{LS}({A},{\mathbf{0}})$ has a unique solution} \\
&\iff\text{columns of $A$ are linearly independent}
\end{align*}
@qed
@end
@slide
Here is the update to @ref{NME1}
@thm
@title{Nonsingular Matrix Equivalences, Round 2}
@label{NME2}
Suppose that $A$ is a square matrix. The following are equivalent.
<ol class="ltx_enumerate">
<li class="ltx_item">
@col
$A$ is nonsingular.
</li>
<li class="ltx_item" >
@col
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item" >
@col
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
</li>
<li class="ltx_item" >
@col
The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>
<li class="ltx_item" >
@col
The columns of $A$ form a linearly independent set.
</li>
</ol>
@end
@proof
@col
This follows directly from @ref{NMLIC} and @ref{NME2}.
@qed
@end
@section{
Null Spaces, Spans, Linear Independence}

In this section, we will find a linearly independent set that spans a null space.
Recall that, by @ref{SSNS},
there exists a particular set of $n-r$
vectors that could be used to span the null space of a matrix.

@eg
@label{LINSB}
<b>Linear independence of null space basis</b>
Suppose that we are interested in the null space of a $3\times 7$ matrix $A$ which row-reduces to
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&-2&4&0&3&9\\
0&\boxed{1}&5&6&0&7&1\\
0&0&0&0&\boxed{1}&8&-5\end{bmatrix}.
\end{align*}
@col
Then $F=\{3,\,4,\,6,\,7\}$ is the set of indices for our four free variables that would be used in a description of the solution set for the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Applying @ref{HMVEI}, we can begin to construct a set of four vectors whose span is the null space of $A$, a set of vectors we will refer to as $T$.
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(A\right)
=\left< T\right>
&=\left< \{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\mathbf{z}_{4}\}\right>\\
&=\left< \left\{\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix},\,\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix}\right\}\right>
\end{align*}
@col
So far, we have constructed as much of these individual vectors as we can, based just on the knowledge of the contents of the set $F$. This has allowed us to determine the entries in slots 3, 4, 6 and 7, while we have left slots 1, 2 and 5 blank. Without doing any more, let us ask if $T$ is linearly independent? Begin with a relation of linear dependence on $T$, and see what we can learn about the scalars.

@col
\begin{align*}
\displaystyle\mathbf{0}&=\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\alpha_{4}\mathbf{z}_{4} \\
\displaystyle\begin{bmatrix}0\\
0\\
0\\
0\\
0\\
0\\
0\end{bmatrix}&=\alpha_{1}\begin{bmatrix}\\
\\
1\\
0\\
\\
0\\
0\end{bmatrix}+\alpha_{2}\begin{bmatrix}\\
\\
0\\
1\\
\\
0\\
0\end{bmatrix}+\alpha_{3}\begin{bmatrix}\\
\\
0\\
0\\
\\
1\\
0\end{bmatrix}+\alpha_{4}\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
1\end{bmatrix} \\
&=\begin{bmatrix}\\
\\
\alpha_{1}\\
0\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
\alpha_{2}\\
\\
0\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
\alpha_{3}\\
0\end{bmatrix}+\begin{bmatrix}\\
\\
0\\
0\\
\\
0\\
\alpha_{4}\end{bmatrix}=\begin{bmatrix}\\
\\
\alpha_{1}\\
\alpha_{2}\\
\\
\alpha_{3}\\
\alpha_{4}\end{bmatrix}
\end{align*}
@col
Applying the equalities of vectors, we see that $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0$. So the only relation of linear dependence on the set $T$ is the trivial one. By the definition of linear independence, the set $T$ is linearly independent. The important feature of this example is how the <b>pattern of zeros and ones</b> in the four vectors led to the conclusion of linear independence.
@end
@slide
<!--
The proof of Theorem @ref{BNS} is quite straightforward, and relies on the <b>pattern of zeros and ones</b> that arise in the vectors $\mathbf{z}_{i}$, $1\leq i\leq n-r$, in the entries that arise with the locations of the non-pivot columns.
-->
@thm
@title{Basis for Null Spaces}
@label{BNS}
Suppose that $A$ is an $m\times n$ matrix, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\}$ and $F=\{f_{1},\,f_{2},\,f_{3},\,\ldots,\,f_{n-r}\}$ be the sets of column indices of $B$ which are and are not, respectively, pivot columns. Construct the $n-r$ vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$ of size $n$ as
\begin{align*}
\displaystyle \left[\mathbf{z}_{j}\right]_{i}=\begin{cases}1&\text{if $i\in F$, $i=f_{j}$}\\
0&\text{if $i\in F$, $i\neq f_{j}$}\\
-\left[B\right]_{k,f_{j}}&\text{if $i\in D$, $i=d_{k}$}\end{cases}
\end{align*}
(In fact $\mathbf{z}_{j}$ corresponding to the solution $x_{f_{j}}=1$ and $x_{f_{k}}=0$ for $k\neq j$.) Define the set $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$. Then
<ol class="ltx_enumerate">
<li class="ltx_item" >
${\mathcal{N}}\!\left(A\right)=\left< S\right>$.
</li>
<li class="ltx_item" >
$S$ is a linearly independent set.
</li>
</ol>
@end
@proof
@col
Study the above example. You can skip the proof for now. Notice first that the vectors $\mathbf{z}_{j}$, $1\leq j\leq n-r$, are the same as the $n-r$ vectors defined in @ref{SSNS}. Also, the hypotheses of @ref{SSNS} are the same as the hypotheses of the theorem we are currently proving. So @ref{SSNS} tells us that ${\mathcal{N}}\!\left(A\right)=\left< S\right>$. That was the easy half, but the second part is not much harder. What is new here is the claim that $S$ is a linearly independent set.

To prove the linear independence of a set, we need to start with a relation of linear dependence and somehow conclude that the scalars involved must all be zero, i.e., that the relation of linear dependence is trivial. So, we start with and equation of the form
\begin{align*}
\displaystyle \alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}=\mathbf{0}.
\end{align*}
For each $j$, $1\leq j\leq n-r$, consider the equality of the individual entries of the vectors on both sides of this equality in position $f_{j}$:
\begin{align*}
\displaystyle 0&=\left[\mathbf{0}\right]_{f_{j}} \\
&=\left[\alpha_{1}\mathbf{z}_{1}+\alpha_{2}\mathbf{z}_{2}+\alpha_{3}\mathbf{z}_{3}+\cdots+\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&=\left[\alpha_{1}\mathbf{z}_{1}\right]_{f_{j}}+\left[\alpha_{2}\mathbf{z}_{2}\right]_{f_{j}}+\left[\alpha_{3}\mathbf{z}_{3}\right]_{f_{j}}+\cdots+\left[\alpha_{n-r}\mathbf{z}_{n-r}\right]_{f_{j}} \\
&=\alpha_{1}\left[\mathbf{z}_{1}\right]_{f_{j}}+\alpha_{2}\left[\mathbf{z}_{2}\right]_{f_{j}}+\alpha_{3}\left[\mathbf{z}_{3}\right]_{f_{j}}+\cdots+ \\
&\quad\quad\alpha_{j-1}\left[\mathbf{z}_{j-1}\right]_{f_{j}}+\alpha_{j}\left[\mathbf{z}_{j}\right]_{f_{j}}+\alpha_{j+1}\left[\mathbf{z}_{j+1}\right]_{f_{j}}+\cdots+ \\
&\quad\quad\alpha_{n-r}\left[\mathbf{z}_{n-r}\right]_{f_{j}} \\
&=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+ \\
&\quad\quad\alpha_{j-1}(0)+\alpha_{j}(1)+\alpha_{j+1}(0)+\cdots+\alpha_{n-r}(0)& \text{ definition of } \mathbf{z}_{j} \\
&=\alpha_{j}
\end{align*}
So for all $j$, $1\leq j\leq n-r$, we have $\alpha_{j}=0$. Hence, the only relation of linear dependence on $S=\{\mathbf{z}_{1},\,\mathbf{z}_{2},\,\mathbf{z}_{3},\,\ldots,\,\mathbf{z}_{n-r}\}$ is the trivial one. By the definition of linear independence, the set is linearly independent, as desired.
@qed
@end
@slide
@eg
Find the null space of the matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}-2&-1&-2&-4&4\\
-6&-5&-4&-4&6\\
10&7&7&10&-13\\
-7&-5&-6&-9&10\\
-4&-3&-4&-6&6\\
\end{bmatrix}.
\end{align*}
@end
@sol
@col
The RREF of $A$ is:
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&0&1&-2\\
0&\boxed{1}&0&-2&2\\
0&0&\boxed{1}&2&-1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
The free variables are $x_{4}$ and $x_{5}$.

Setting $x_{4}=1$ and $x_{5}=0$ gives:

@col
\begin{align*}
\displaystyle \mathbf{z}_{1}=\begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix}.
\end{align*}
@col
Setting instead $x_{4}=0$ and $x_{5}=1$ gives
\begin{align*}
\displaystyle \mathbf{z}_{2}=\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}.
\end{align*}
Hence
\begin{align*}
\displaystyle {\mathcal{N}}\!\left(L\right)=\left< \begin{bmatrix}-1\\
2\\
-2\\
1\\
0\end{bmatrix},\begin{bmatrix}2\\
-2\\
1\\
0\\
1\end{bmatrix}\right>.
\end{align*}
@qed
@setchapter{13}
@chapter{Linear Dependence and Span}
<h5 class="notkw">Reference.</h5>
<ul>
<li>
Beezer, Ver 3.5 Section LDS (print version p105 - p113)
</li>
<li>
Strang, Sect 2.3
</li>
</ul>
<h5 class="notkw">Exercise</h5>
<ul>
<li>
Exercises with solutions can be downloaded at
@href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
Section LI (p.48-51) (Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$ in the following questions)
C20, C40, C50, C51, C52, C55, C70, M10, T40.
</li>
<li>
Strang, Sect 2.3
</li>
</ul>
@section{Linearly Dependent Sets and Spans}
If we use a linearly dependent set to construct a span, then we can always create the same infinite set by starting with a set that is one vector smaller in size. We will illustrate this behaviour in @ref{RSC5}. However, this will not be possible if we build a span from a linearly independent set. So, in a certain sense, using a linearly independent set to formulate a span is the best possible way – there are no any extra vectors being used to build up all the necessary linear combinations. OK, here is the theorem, and then the example.
@slide
@thm
@title{Dependency in Linearly Dependent Sets}
@label{DLDS}
Suppose that $S=\left\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{n}\right\}$ is a set of vectors. Then $S$ is a linearly dependent set if and only if there is an index $t$, $1\leq t\leq n$, such that $\mathbf{u_{t}}$ is a linear combination of the vectors $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{t-1},\,\mathbf{u}_{t+1},\,\ldots,\,\mathbf{u}_{n}$.
@end
@proof

($\Rightarrow$)
@newcol
Suppose that $S$ is linearly dependent. Then there exists a nontrivial relation of linear dependence (Lecture 12 Definition 1). That is, there are scalars, $\alpha_{i}$, $1\leq i\leq n$, not all of which are zero, such that
\begin{align*}
\displaystyle \alpha_{1}\mathbf{u}_{1}+\alpha_{2}\mathbf{u}_{2}+\alpha_{3}\mathbf{u}_{3}+\cdots+\alpha_{n}\mathbf{u}_{n}=\mathbf{0}.
\end{align*}
Suppose that $\alpha_{t}$ is nonzero. Then,
\begin{align*}
\displaystyle\mathbf{u}_{t}&=\frac{-1}{\alpha_{t}}\left(-\alpha_{t}\mathbf{u}_{t}\right) \\
&=\frac{-1}{\alpha_{t}}\left(\alpha_{1}\mathbf{u}_{1}+\cdots+\alpha_{t-1}\mathbf{u}_{t-1}+\alpha_{t+1}\mathbf{u}_{t+1}+\cdots+\alpha_{n}\mathbf{u}_{n}\right) \\
&=\frac{-\alpha_{1}}{\alpha_{t}}\mathbf{u}_{1}+\cdots+\frac{-\alpha_{t-1}}{\alpha_{t}}\mathbf{u}_{t-1}+\frac{-\alpha_{t+1}}{\alpha_{t}}\mathbf{u}_{t+1}+\cdots+\frac{-\alpha_{n}}{\alpha_{t}}\mathbf{u}_{n}.
\end{align*}
Since $\frac{\alpha_{i}}{\alpha_{t}}$ is again a scalar, we have expressed $\mathbf{u}_{t}$ as a linear combination of the other elements of $S$.
@endcol

($\Leftarrow$)
@newcol
Assume that the vector $\mathbf{u}_{t}$ is a linear combination of the other vectors in $S$. Write such a linear combination as
\begin{align*}
\displaystyle\mathbf{u_{t}}&=\beta_{1}\mathbf{u}_{1}+\beta_{2}\mathbf{u}_{2}+\cdots+\beta_{t-1}\mathbf{u}_{t-1}+\beta_{t+1}\mathbf{u}_{t+1}+\cdots+\beta_{n}\mathbf{u}_{n}.
\end{align*}
@col
Then we have
\begin{align*}
\displaystyle\beta_{1}\mathbf{u}_{1}&+\cdots+\beta_{t-1}\mathbf{u}_{t-1}+(-1)\mathbf{u}_{t}+\beta_{t+1}\mathbf{u}_{t+1}+\cdots+\beta_{n}\mathbf{u}_{n} \\
&=\mathbf{u}_{t}+(-1)\mathbf{u}_{t} \\
&=\left(1+\left(-1\right)\right)\mathbf{u}_{t} \\
&=0\mathbf{u}_{t} \\
&=\mathbf{0}.
\end{align*}
@col
So the scalars $\beta_{1},\,\beta_{2},\,\beta_{3},\,\ldots,\,\beta_{t-1},\,\beta_{t}=-1,\beta_{t+1},\,\,\ldots,\,\beta_{n}$ provide a nontrivial relation of linear dependence of the vectors in $S$, thus establishing that $S$ is a linearly dependent set.
@endcol
@qed
@end
@col
This theorem can be used, sometimes repeatedly, to whittle down the size of a set of vectors used in a span construction.
In the next example we will examine some of the subtleties.
@slide
@eg
@label{RSC5}
<b>Reducing the generating set of a span in ${\mathbb{R}}^{5}$</b>

Consider the following set of $n=4$ vectors in ${\mathbb{R}}^{5}$,
\begin{align*}
\displaystyle R=
\left\{
\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\mathbf{v}_{4}\right\}
=\left\{\begin{bmatrix}1\\
2\\
-1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}2\\
1\\
3\\
1\\
2\end{bmatrix},\,\begin{bmatrix}0\\
-7\\
6\\
-11\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
1\\
2\\
1\\
6\end{bmatrix}\right\}.\\
\end{align*}
Define $V=\left<  R\right>$.



@col
We form a $5\times 4$ matrix, $D$, and row-reduce it to understand the solutions to the homogeneous system $\mathcal{LS}({D},{\mathbf{0}})$:
\begin{align*}
\displaystyle D=\begin{bmatrix}1&2&0&4\\
2&1&-7&1\\
-1&3&6&2\\
3&1&-11&1\\
2&2&-2&6\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&4\\
0&\boxed{1}&0&0\\
0&0&\boxed{1}&1\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
We can find infinitely many solutions to the system $\mathcal{LS}({D},{\mathbf{0}})$, most of which are nontrivial. Choose any nontrivial solution to build a nontrivial relation of linear dependence on $R$. Let us begin with $x_{4}=1$, to find the solution
\begin{align*}
\displaystyle \begin{bmatrix}-4\\
0\\
-1\\
1\end{bmatrix}.
\end{align*}
@col
The corresponding relation of linear dependence is
\begin{align*}
\displaystyle (-4)\mathbf{v}_{1}+0\mathbf{v}_{2}+(-1)\mathbf{v}_{3}+1\mathbf{v}_{4}=\mathbf{0}.
\end{align*}
@col
The theorem above guarantees that we can solve this relation of linear dependence for some vector in $R$, but the choice of which one is up to us. Notice however that $\mathbf{v}_{2}$ has a zero coefficient. In this case, we cannot choose to solve for $\mathbf{v}_{2}$. Maybe some other relation of linear dependence would produce a nonzero coefficient for $\mathbf{v}_{2}$ if we just had to solve for this vector. Unfortunately, this example has been engineered to always produce a zero coefficient here, as you can see from solving the homogeneous system. Every solution has $x_{2}=0$!




OK, if we are convinced that we cannot solve for $\mathbf{v}_{2}$, let us instead solve for $\mathbf{v}_{3}$:
\begin{align*}
\displaystyle \mathbf{v}_{3}=(-4)\mathbf{v}_{1}+0\mathbf{v}_{2}+1\mathbf{v}_{4}=(-4)\mathbf{v}_{1}+1\mathbf{v}_{4}
\end{align*}
@col
We claim that this particular equation will allow us to write
\begin{align*}
\displaystyle V=\left< R\right>=\left< \left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\mathbf{v}_{4}\right\}\right>=\left< \left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{4}\right\}\right>,
\end{align*}
in essence declaring $\mathbf{v}_{3}$ as surplus for the task of building $V$ as a span of $R$. This claim is an equality of two sets. Let $R^{\prime}=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{4}\right\}$ and $V^{\prime}=\left< R^{\prime}\right>$. We want to show that $V=V^{\prime}$.


First show that $V^{\prime}\subseteq V$. Since every vector of $R^{\prime}$ is in $R$, any vector we can construct in $V^{\prime}$ as a linear combination of vectors from $R^{\prime}$ can also be constructed as a vector in $V$ by the same linear combination of the same vectors in $R$. That was easy, now turn it around.

@col
Next show that $V\subseteq V^{\prime}$. Choose any $\mathbf{v}$ from $V$. So there are scalars $\alpha_{1},\,\alpha_{2},\,\alpha_{3},\,\alpha_{4}$ such that
\begin{align*}
\displaystyle\mathbf{v}&=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\mathbf{v}_{3}+\alpha_{4}\mathbf{v}_{4} \\
&=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\alpha_{3}\left((-4)\mathbf{v}_{1}+1\mathbf{v}_{4}\right)+\alpha_{4}\mathbf{v}_{4} \\
&=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\left((-4\alpha_{3})\mathbf{v}_{1}+\alpha_{3}\mathbf{v}_{4}\right)+\alpha_{4}\mathbf{v}_{4} \\
&=\left(\alpha_{1}-4\alpha_{3}\right)\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\left(\alpha_{3}+\alpha_{4}\right)\mathbf{v}_{4}.
\end{align*}
@col
This equation says that $\mathbf{v}$ can be written as a linear combination of the vectors in $R^{\prime}$ and hence qualifies for membership in $V^{\prime}$. So $V\subseteq V^{\prime}$ and we have established that $V=V^{\prime}$.


If $R^{\prime}$ was also linearly dependent (in fact, it is not), we could reduce the set $R^{\prime}$ even further. Notice that we could have chosen to eliminate any one of $\mathbf{v}_{1}$, $\mathbf{v}_{3}$ or $\mathbf{v}_{4}$, but somehow $\mathbf{v}_{2}$ is essential to the creation of $V$ since it cannot be replaced by any linear combination of $\mathbf{v}_{1}$, $\mathbf{v}_{3}$ or $\mathbf{v}_{4}$.
@end
@section{
Casting Out Vectors}
@label{COV}
In @ref{RSC5} we used four vectors to create a span. With a relation of linear dependence in hand, we were able to toss out one of these four vectors and create the same span from a subset of just three of the original set of four vectors. We did have to take some care as to just which vector we tossed out. In the next example, we will be more methodical about just how we choose to eliminate vectors from a linearly dependent set while preserving a span.
@slide
@label{slide:canonicalbasis}
In $\mathbb{R}^m$,
for $i = 1, 2, \ldots, m$, let:
\[
\vec{e}_1 = \colvector{1\\0\\0\\\vdots\\0},\;
\vec{e}_2 = \colvector{0\\1\\0\\\vdots\\0},
\;\cdots\;,\;
\vec{e}_m = \colvector{0\\0\\0\\\vdots\\1}.
\]
That is:
\[
\left[\vec{e}_m\right]_j = \begin{cases}
1 & \text{ if } j = m;\\
0 & \text{ otherwise.}
\end{cases}
\]
@col
Observe that every vector $\vec{v} = \colvector{v_1\\v_2\\ \vdots \\ v_m} \in \mathbb{R}^m$
lies in the span of $\{\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_m\}$, since:

@col
\[
\colvector{v_1\\v_2\\ \vdots \\ v_m} = v_1 \colvector{1\\0\\0\\\vdots\\0}
+ v_2 \colvector{0\\1\\0\\\vdots\\0} + \cdots + v_m \colvector{0\\0\\0\\\vdots\\1}.
\]
@col
Moreover, for any positive integer $r < n$,
and any vector $\vec{v} \in \mathbb{R}^m$ of the form:
\[
\vec{v} = \colvector{v_1\\v_2\\ \vdots \\ v_r \\ 0 \\0 \\ \vdots \\0},
\]

@col
we have:
\[
\vec{v} \in \left\langle \{\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_r\}\right\rangle.
\]

@col
<strong>Exercise.</strong>
Notice also that the vectors: $\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_r$ are linearly independent.
@endcol
@slide
@eg
@label{eg:COV}
<b>Casting out vectors</b>
@col
Consider now the following set $S$ of $n = 7$ vectors in ${\mathbb{R}}^{4}$:
\begin{align*}
\displaystyle S=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\,\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\,\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\,\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\,\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\,\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\right\}.
\end{align*}
@col
By @ref{8fc8ebb550bd42bcda3fb034a92efbd1},
the set $S$ is obviously linearly dependent, since we have $n = 7$ vectors in ${\mathbb{R}}^{4}$. So, we can slim down $S$ some and express the subspace
$\left\langle S \right\rangle$ as the span of a smaller set of vectors.

We would like to know:

<strong>
What's the smallest subset $S'$ of $S$ such that
$\left\langle S' \right\rangle = \left\langle S \right\rangle$?
</strong>

@col
Consider the matrix $A$ whose columns consist of the vectors in $S$:
\begin{align*}
\displaystyle A=\left[\mathbf{A}_{1}|\mathbf{A}_{2}|\mathbf{A}_{3}|\ldots|\mathbf{A}_{7}\right]=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}.
\end{align*}
The matrix $A$ is row-equivalent to the RREF matrix:
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}.
\end{align*}
In this case, the rank (i.e. the number of non-zero rows) of $B$ is $r = 3$.
The pivot columns of $B$ are $\mathbf{B}_1, \mathbf{B}_3, \mathbf{B}_4$.

@col
<ol class="ltx_enumerate">
<li class="ltx_item">
<strong>
$\displaystyle \mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$ are linearly independent.
</strong>
@col
The pivot columns of $B$ are precisely the vectors:
\begin{align*}
\mathbf{B}_1 &= \vec{e}_1, & \mathbf{B}_3 &= \vec{e}_2, & \mathbf{B}_4 &= \vec{e}_3.
\end{align*}
In particular, they are linearly independent.

@col
We claim that the corresponding columns of $A$
(namely $\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$) are also linearly independent.
The reason is as follows:

@col
The augmented matrix:
\[
B' = [\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4] =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
\]
is an RREF matrix which is row-equivalent to the augmented matrix:
\[
A' = [\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4].
\]
It now follows from @ref{60dd89e0db5ce8bae68e89cd2a2330de}
that the columns of $A'$ are linearly independent.
</li>
<li class="ltx_item">
<strong>
The span of $S$ is equal to
$\displaystyle
\left\langle\{\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4\}\right\rangle.
$
</strong>
@col
First, notice that every column of $B$ is a vector in $\mathbb{R}^4$.
Moreover, since the rank of $B$ is $r = 3$,
the $4$-th entry of each such column vector is zero.

@col
By the observations made earlier, we have:
\[
\mathbf{B}_i \in \left\langle \underbrace{\mathbf{B}_1}_{\vec{e}_1},
\underbrace{\mathbf{B}_3}_{\vec{e}_2}, \underbrace{\mathbf{B}_4}_{\vec{e}_3}
\right\rangle
\]
for $i = 1, 2, \ldots, 7$.

@col
In other words, for any $1 \leq i \leq 7$, the vector equation:
\[
[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\vec{x} = \mathbf{B}_i
\]
has a solution $\vec{x} \in \mathbb{R}^3$.
<!-- or equivalently the linear system:
\[
\mathcal{LS}(B', \mathbf{B}_i)
\]
is consistent.
-->

@col
On the other hand, the augmented matrix $[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4| \mathbf{B}_i]$ is row-equivalent to
$[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4| \mathbf{A}_i]$, which implies that any solution to
$[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\vec{x} = \mathbf{B}_i$
is also a solution to
$[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4]\vec{x} = \mathbf{A}_i$.

@col
For example, we have:
\[
[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\colvector{2\\1\\2}
=
2\mathbf{B}_1 + \mathbf{B}_3 + 2 \mathbf{B}_4
= \colvector{2\\1\\2\\0}
= \mathbf{B}_5
\]
which implies that:
\[
\vec{x} = \colvector{2\\1\\2}
\]
is a solution to
$[\mathbf{B}_1 | \mathbf{B}_3 | \mathbf{B}_4]\vec{x} = \mathbf{B}_5$
and hence also a solution to
$[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4]\vec{x} = \mathbf{A}_5$.
Indeed:

@col
\[
[\mathbf{A}_1 | \mathbf{A}_3 | \mathbf{A}_4]\colvector{2\\1\\2} =
2\mathbf{A}_1 + \mathbf{A}_3 + 2 \mathbf{A}_4 = \colvector{0\\9\\-4\\8}
= \mathbf{A}_5
\]
In particular, $\mathbf{A}_5$ lies in the span of
$\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$.

@col
It now follows, since every $\mathbf{B}_i$ is in the span of the
$\mathbf{B}_1, \mathbf{B}_3, \mathbf{B}_4$, that
every $\mathbf{A}_i$ lies in the span of $\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4$.

@col
Hence,
the span of $S$ is equal to the span of the vectors:
\[
\mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_4.
\]
</li>
</ol>
@end
@slide
The previous example motivates the following fundamental theorem:
@thm
@title{Basis of a Span}
@label{BS}
Suppose that $S=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{n}\right\}$ is a set of column vectors. Define $W=\left< S\right>$ and let $A$ be the matrix whose columns are the vectors from $S$. Let $B$ be the reduced row-echelon form of $A$, with $D=\left\{{d}_{1},\,{d}_{2},\,{d}_{3},\,\ldots,\,{d}_{r}\right\}$ the set of indices for the pivot columns of $B$. Then
<ol class="ltx_enumerate">
<li class="ltx_item">
$T=\left\{\mathbf{v}_{d_{1}},\,\mathbf{v}_{d_{2}},\,\mathbf{v}_{d_{3}},\,\ldots\,\mathbf{v}_{d_{r}}\right\}$ is a linearly independent set.
</li>
<li class="ltx_item">
$W=\left< T\right>$.
</li>
</ol>
@end
@proof
<strong>Try to understand the example and skip the proof for now</strong>

@col
To prove that $T$ is linearly independent, begin with a relation of linear dependence on $T$,
\begin{align*}
\displaystyle \mathbf{0}=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\alpha_{3}\mathbf{v}_{d_{3}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}.
\end{align*}
@col
We will try to conclude that the only possibility for the scalars $\alpha_{i}$ is that they are all zero.
Denote the non-pivot columns of $B$ by $F=\left\{{f}_{1},\,{f}_{2},\,{f}_{3},\,\ldots,\,{f}_{n-r}\right\}$. Then we can preserve the equality by adding a big fat zero to the linear combination:
\begin{align*}
\displaystyle \mathbf{0}=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\alpha_{3}\mathbf{v}_{d_{3}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+0\mathbf{v}_{f_{1}}+0\mathbf{v}_{f_{2}}+0\mathbf{v}_{f_{3}}+\ldots+0\mathbf{v}_{f_{n-r}}.
\end{align*}
@col
The scalars in this linear combination (suitably reordered) are a solution to the homogeneous system $\mathcal{LS}({A},{\mathbf{0}})$. Notice that this is the solution obtained by setting each free variable to zero. In the case of a homogeneous system, we see that if all of the free variables are set to zero, then the resulting solution vector is trivial (all zeros). So it must be that $\alpha_{i}=0$, $1\leq i\leq r$. This implies, by the definition of linear independence, that $T$ is a linearly independent set.


@col
The second conclusion of this theorem is an equality of sets. Since $T$ is a subset of $S$, any linear combination of elements of the set $T$ can also be viewed as a linear combination of elements of the set $S$. So $\left< T\right>\subseteq\left< S\right>=W$. It remains to prove that $W=\left< S\right>\subseteq\left< T\right>$.


For each $k$, $1\leq k\leq n-r$, form a solution $\mathbf{x}$ to $\mathcal{LS}({A},{\mathbf{0}})$ by setting the free variables as follows:
\begin{align*}
\displaystyle x_{f_{1}}&=0& x_{f_{2}}&=0& x_{f_{3}}&=0&\ldots& x_{f_{k}}&=1&\ldots& x_{f_{n-r}}&=0.
\end{align*}
@col
The remainder of this solution vector is given by
\begin{align*}
\displaystyle x_{d_{1}}&=-\left[B\right]_{1,f_{k}}& x_{d_{2}}&=-\left[B\right]_{2,f_{k}}& x_{d_{3}}&=-\left[B\right]_{3,f_{k}}&\ldots& x_{d_{r}}&=-\left[B\right]_{r,f_{k}}.
\end{align*}
From this solution, we obtain a relation of linear dependence on the columns of $A$,
\begin{align*}
\displaystyle -\left[B\right]_{1,f_{k}}\mathbf{v}_{d_{1}}-\left[B\right]_{2,f_{k}}\mathbf{v}_{d_{2}}-\left[B\right]_{3,f_{k}}\mathbf{v}_{d_{3}}-\ldots-\left[B\right]_{r,f_{k}}\mathbf{v}_{d_{r}}+1\mathbf{v}_{f_{k}}=\mathbf{0},
\end{align*}
which can be arranged to the equality
\begin{align*}
\displaystyle \mathbf{v}_{f_{k}}=\left[B\right]_{1,f_{k}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{k}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{k}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{k}}\mathbf{v}_{d_{r}}.
\end{align*}
Now, suppose we take an arbitrary element $\mathbf{w}$ of $W=\left< S\right>$ and write it as a linear combination of the elements of $S$, but with the terms organized according to the indices in $D$ and $F$:
\begin{align*}
\displaystyle\mathbf{w}&=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+\beta_{1}\mathbf{v}_{f_{1}}+\beta_{2}\mathbf{v}_{f_{2}}+\ldots+\beta_{n-r}\mathbf{v}_{f_{n-r}}
\end{align*}
From the above, we can replace each $\mathbf{v}_{f_{j}}$ by a linear combination of the $\mathbf{v}_{d_{i}}$:
\begin{align*}
\displaystyle\mathbf{w}&=\alpha_{1}\mathbf{v}_{d_{1}}+\alpha_{2}\mathbf{v}_{d_{2}}+\ldots+\alpha_{r}\mathbf{v}_{d_{r}}+ \\
&\beta_{1}\left(\left[B\right]_{1,f_{1}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{1}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{1}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{1}}\mathbf{v}_{d_{r}}\right)+ \\
&\beta_{2}\left(\left[B\right]_{1,f_{2}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{2}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{2}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{2}}\mathbf{v}_{d_{r}}\right)+ \\
&\quad\quad\vdots \\
&\beta_{n-r}\left(\left[B\right]_{1,f_{n-r}}\mathbf{v}_{d_{1}}+\left[B\right]_{2,f_{n-r}}\mathbf{v}_{d_{2}}+\left[B\right]_{3,f_{n-r}}\mathbf{v}_{d_{3}}+\ldots+\left[B\right]_{r,f_{n-r}}\mathbf{v}_{d_{r}}\right) \\
\displaystyle=&\ \left(\alpha_{1}+\beta_{1}\left[B\right]_{1,f_{1}}+\beta_{2}\left[B\right]_{1,f_{2}}+\beta_{3}\left[B\right]_{1,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{1,f_{n-r}}\right)\mathbf{v}_{d_{1}}+ \\
&\left(\alpha_{2}+\beta_{1}\left[B\right]_{2,f_{1}}+\beta_{2}\left[B\right]_{2,f_{2}}+\beta_{3}\left[B\right]_{2,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{2,f_{n-r}}\right)\mathbf{v}_{d_{2}}+ \\
&\quad\quad\vdots \\
&\left(\alpha_{r}+\beta_{1}\left[B\right]_{r,f_{1}}+\beta_{2}\left[B\right]_{r,f_{2}}+\beta_{3}\left[B\right]_{r,f_{3}}+\ldots+\beta_{n-r}\left[B\right]_{r,f_{n-r}}\right)\mathbf{v}_{d_{r}}.
\end{align*}
@col
This mess expresses the vector $\mathbf{w}$ as a linear combination of the vectors in
\begin{align*}
\displaystyle T=\left\{\mathbf{v}_{d_{1}},\,\mathbf{v}_{d_{2}},\,\mathbf{v}_{d_{3}},\,\ldots\,\mathbf{v}_{d_{r}}\right\},
\end{align*}
thus saying that $\mathbf{w}\in\left< T\right>$. Therefore, $W=\left< S\right>\subseteq\left< T\right>$.
@qed
@end

<!--
@slide
In @ref{eg:COV}, we tossed-out vectors one at a time. But in each instance, we rewrote the offending vector as a linear combination of those vectors with the column indices of the pivot columns of the reduced row-echelon form of the matrix of columns. In the proof of Theorem   @ref{BS}, we accomplish this reduction in one big step. In @ref{eg:COV} we arrived at a linearly independent set at exactly the same moment that we ran out of free variables to exploit. This was not a coincidence; it is the substance of our conclusion of linear independence in Theorem   @ref{BS}.
-->

@slide
Here is a straightforward application of Theorem @ref{BS}.
@eg
<b>Reducing the generating set of a span in ${\mathbb{R}}^{4}$</b>

@col
Begin with a set of five vectors in ${\mathbb{R}}^{4}$,
\begin{align*}
\displaystyle S=\left\{\begin{bmatrix}1\\
1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}2\\
2\\
4\\
2\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}7\\
1\\
-1\\
4\end{bmatrix},\,\begin{bmatrix}0\\
2\\
5\\
1\end{bmatrix}\right\}
\end{align*}
and let $W=\left< S\right>$.

@col
To arrive at a (smaller) linearly independent set, follow the procedure described in Theorem   @ref{BS}. Place the vectors from $S$ into a matrix as columns, and row-reduce:
\begin{align*}
\displaystyle \begin{bmatrix}1&2&2&7&0\\
1&2&0&1&2\\
2&4&-1&-1&5\\
1&2&1&4&1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&2&0&1&2\\
0&0&\boxed{1}&3&-1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
Columns 1 and 3 are the pivot columns ($D=\left\{1,\,3\right\}$). So the set
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1\\
1\\
2\\
1\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix}\right\}
\end{align*}
is linearly independent and $\left< T\right>=\left< S\right>=W$. Boom!


Since the reduced row-echelon form of a matrix is unique, the procedure of Theorem   @ref{BS} leads us to a unique set $T$. However, there is a wide variety of possibilities for sets $T$ that are linearly independent and which can be employed in a span to create $W$. Without proof, we list two other possibilities:
\begin{align*}
\displaystyle T^{\prime}&=\left\{\begin{bmatrix}2\\
2\\
4\\
2\end{bmatrix},\,\begin{bmatrix}2\\
0\\
-1\\
1\end{bmatrix}\right\} \\
\displaystyle T^{*}&=\left\{\begin{bmatrix}3\\
1\\
1\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
3\\
0\end{bmatrix}\right\}.
\end{align*}
Can you prove that $T^{\prime}$ and $T^{*}$ are linearly independent sets and that $W=\left< S\right>=\left< T^{\prime}\right>=\left< T^{*}\right>$?
@end
@slide
@eg
<b>Reworking elements of a span</b>

Begin with a set of five vectors in ${\mathbb{R}}^{4}$
\begin{align*}
\displaystyle R=\left\{\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix},\,\begin{bmatrix}-8\\
-1\\
-9\\
-4\end{bmatrix},\,\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}-10\\
-1\\
-1\\
4\end{bmatrix}\right\}.
\end{align*}
@col
It is easy to create elements of $X=\left< R\right>$ – we will create one at random,
\begin{align*}
\displaystyle \mathbf{y}=6\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix}+(-7)\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix}+1\begin{bmatrix}-8\\
-1\\
-9\\
-4\end{bmatrix}+6\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}+2\begin{bmatrix}-10\\
-1\\
-1\\
4\end{bmatrix}=\begin{bmatrix}9\\
2\\
1\\
-3\end{bmatrix}.
\end{align*}
@col
We know we can replace $R$ by a smaller set, that will create the same span. Here goes:
\begin{align*}
\displaystyle \begin{bmatrix}2&-1&-8&3&-10\\
1&1&-1&1&-1\\
3&0&-9&-1&-1\\
2&1&-4&-2&4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&-3&0&-1\\
0&\boxed{1}&2&0&2\\
0&0&0&\boxed{1}&-2\\
0&0&0&0&0\end{bmatrix}.
\end{align*}
@col
So, if we collect the first, second and fourth vectors from $R$,


\begin{align*}
\displaystyle P=\left\{\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix},\,\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix},\,\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}\right\}
\end{align*}
@col
then $P$ is linearly independent and $\left< P\right>=\left< R\right>=X$ by Theorem   @ref{BS}. Since we built $\mathbf{y}$ as an element of $\left< R\right>$ it must also be an element of $\left< P\right>$. Can we write $\mathbf{y}$ as a linear combination of just the three vectors in $P$? The answer is, of course, yes. But let us compute an explicit linear combination just for fun. We can get such a linear combination by solving a system of equations with the column vectors of $R$ as the columns of a coefficient matrix, and $\mathbf{y}$ as the vector of constants.



@col
We employ an augmented matrix to solve this system:
\begin{align*}
\displaystyle \begin{bmatrix}2&-1&3&9\\
1&1&1&2\\
3&0&-1&1\\
2&1&-2&-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&1\\
0&\boxed{1}&0&-1\\
0&0&\boxed{1}&2\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
So we see, as expected, that
\begin{align*}
\displaystyle 1\begin{bmatrix}2\\
1\\
3\\
2\end{bmatrix}+(-1)\begin{bmatrix}-1\\
1\\
0\\
1\end{bmatrix}+2\begin{bmatrix}3\\
1\\
-1\\
-2\end{bmatrix}=\begin{bmatrix}9\\
2\\
1\\
-3\end{bmatrix}=\mathbf{y}.
\end{align*}
A key feature of this example is that the linear combination that expresses $\mathbf{y}$ as a linear combination of the vectors in $P$ is unique. This is a consequence of the linear independence of $P$. The linearly independent set $P$ is smaller than $R$, but still just (barely) big enough to create elements of the set $X=\left< R\right>$. There are many, many ways to write $\mathbf{y}$ as a linear combination of the five vectors in $R$ (the appropriate system of equations to verify this claim yields two free variables in the description of the solution set), yet there is precisely one way to write $\mathbf{y}$ as a linear combination of the three vectors in $P$.
@end
@section{
Uniqueness of RREF}
<strong>Math Major only. You can skip this section. Similar concept appears in the classworks. </strong>
@eg
<b>Entries of RREF $B$ gives relationship of columns of $A$</b>

@col
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&1&8&1&17\\
1&2&2&13&3&37\\
1&2&0&3&-2&-10\\
\end{bmatrix}.
\end{align*}
@col
Then $A$ can be row reduced to


\begin{align*}
\displaystyle B=\begin{bmatrix}1&2&0&3&0&4\\
0&0&1&5&0&6\\
0&0&0&0&1&7\end{bmatrix}.
\end{align*}
Let $\mathbf{A}_{i}$ (resp. $\mathbf{B}_{i}$) be the $i$-th column of $A$ (resp. $B$) for $i=1,\ldots,6$.
By the equivalence of system of linear equation $\mathcal{LS}({A},{\mathbf{0}})$ and $\mathcal{LS}({B},{\mathbf{0}})$, we have
\begin{align}
\displaystyle x_{1}\mathbf{A}_{1}+x_{2}\mathbf{A}_{2}+\cdots+x_{6}\mathbf{A}_{6}=\mathbf{0}&
\end{align}
if and only if
\begin{align}
\label{eq:B}
\displaystyle x_{1}\mathbf{B}_{1}+x_{2}\mathbf{B}_{2}+\cdots+x_{6}\mathbf{B}_{6}=\mathbf{0}.&
\end{align}
@col
<strong>Step 1</strong>
@newcol
First of all, if $(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},0,0,0,0,0)$ is a solution of \eqref{eq:B}, then
\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}=\mathbf{0}.
\end{align*}
@col
So $x_{1}$ is zero. This is equivalent to
\begin{align*}
\displaystyle x_{1}\mathbf{A}_{1}=\mathbf{0}.
\end{align*}
It has only the trivial solution, i.e. $\left\{\mathbf{A}_{1}\right\}$ is linearly independent. Hence $d_{1}=1$ is a pivot column.
@endcol

<strong>Step 2</strong>
@newcol
Let’s move to $x_{2}$. Suppose that
$(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},x_{2},0,0,0,0)$.

@col
Then
\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}+x_{2}\mathbf{B}_{1}=\mathbf{0}
\end{align*}
has nontrivial solution. Say $(x_{1},x_{2})=(-2,1)$.

@col
These can also be seen as
\begin{align*}
\displaystyle -2\mathbf{A}_{1}+\mathbf{A}_{2}=\mathbf{0}
\end{align*}
or equivalently
\begin{align*}
\displaystyle \mathbf{A}_{2}=2\mathbf{A}_{1}.
\end{align*}
@endcol

<strong>Step 3</strong>
@newcol
Consider $x_{3}$. Let $(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(x_{1},0,x_{3},0,0,0)$. Then
\begin{align*}
\displaystyle x_{1}\mathbf{B}_{1}+x_{3}\mathbf{B}_{3}=\mathbf{0}
\end{align*}
has only trivial solution. Equivalently $\left\{\mathbf{A}_{1},\mathbf{A}_{3}\right\}$ is linearly independent.Column $3$ of $B$ is a pivot column.
@endcol

<strong>Step 4</strong>
@newcol
Consder
\begin{align*}
\displaystyle \mathbf{B}_{4}=3\mathbf{B}_{1}+5\mathbf{B}_{3},
\end{align*}
or equivalently
\begin{align*}
\displaystyle \mathbf{A}_{4}=3\mathbf{A}_{1}+5\mathbf{A}_{3}.
\end{align*}
@col
The relation of columns of $A$ gives the entries of the column 4 of $B$.
@endcol

<strong>Step 5</strong>
@newcol
$\mathbf{B}_{5}$ is not in span of $\mathbf{B}_{1}$ and $\mathbf{B}_{3}$. Equivalently $\mathbf{A}_{5}$ is not in span of $\mathbf{A}_{1}$ and $\mathbf{A}_{3}$. Column $5$ of $B$ is a pivot column.
@endcol

<strong>Step 6</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{B}_{6}=4\mathbf{B}_{1}+6\mathbf{B}_{3}+7\mathbf{B}_{5}.
\end{align*}
Equivalently
\begin{align*}
\displaystyle \mathbf{A}_{6}=4\mathbf{A}_{1}+6\mathbf{A}_{3}+7\mathbf{A}_{5}.
\end{align*}
@col
The relation of columns of $A$ gives the entries of the column 6 of $B$.
@end

@eg
<b>Relationship of columns of $A$ determine entries of $B$</b>

@col
Row reduce
\begin{align*}
\displaystyle A=\begin{bmatrix}1&1&3&1&0&0&4\\
2&1&5&1&1&2&7\\
1&-1&1&2&1&-3&10\\
1&3&5&1&-1&1&1\\
\end{bmatrix}
\end{align*}
to a RREF $B$ by the above technique. Let $\mathbf{A}_{i}$ (resp. $\mathbf{B}_{i}$) be the $i$-th column of $A$ (resp. $B$) for $i=1,\ldots,7$.

@col
<strong>Step 1</strong>
@newcol
$\mathbf{A}_{1}$ is nonzero column. So the index $d_{1}=1$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{1}=\begin{bmatrix}1\\
0\\
0\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 2</strong>
@newcol
$\mathbf{A}_{2}$ is not in $\left< \left\{\mathbf{A}_{d_{1}}\right\}\right>$. So the index $d_{2}=2$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{2}=\begin{bmatrix}0\\
1\\
0\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 3</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{A}_{3}=2\mathbf{A}_{d_{1}}+\mathbf{A}_{d_{2}}.
\end{align*}
@col
So we have
\begin{align*}
\displaystyle \mathbf{B}_{3}=2\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}=\begin{bmatrix}2\\
1\\
0\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 4</strong>
@newcol
$\mathbf{A}_{4}$ is not in $\left< \left\{\mathbf{A}_{d_{1}},\mathbf{A}_{d_{2}}\right\}\right>$.

@col
So the index $d_{3}=4$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{4}=\begin{bmatrix}0\\
0\\
1\\
0\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 5</strong>
@newcol
$\mathbf{A}_{5}$ is not in $\left< \left\{\mathbf{A}_{d_{1}},\mathbf{A}_{d_{3}},\mathbf{A}_{d_{3}}\right\}\right>$.

@col
So the index $d_{4}=5$ corresponds to a pivot column. We have
\begin{align*}
\displaystyle \mathbf{B}_{5}=\begin{bmatrix}0\\
0\\
0\\
1\end{bmatrix}.
\end{align*}
@endcol

<strong>Step 6</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{A}_{6}=\mathbf{A}_{d_{1}}+\mathbf{A}_{d_{2}}-2\mathbf{A}_{d_{3}}+\mathbf{A}_{d_{4}}.
\end{align*}
@col
So, we have
\begin{align*}
\displaystyle \mathbf{B}_{6}=\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}-2\mathbf{B}_{d_{3}}+\mathbf{B}_{d_{4}}=\begin{bmatrix}1\\
1\\
-2\\
1\end{bmatrix}
\end{align*}
@endcol

<strong>Step 7</strong>
@newcol
Consider
\begin{align*}
\displaystyle \mathbf{A}_{7}=2\mathbf{A}_{d_{1}}-\mathbf{A}_{d_{2}}+3\mathbf{A}_{d_{3}}+\mathbf{A}_{d_{4}}.
\end{align*}
@col
So, we have
\begin{align*}
\displaystyle \mathbf{B}_{7}=\mathbf{B}_{d_{1}}+\mathbf{B}_{d_{2}}-2\mathbf{B}_{d_{3}}+\mathbf{B}_{d_{4}}=\begin{bmatrix}2\\
-1\\
3\\
1\end{bmatrix}
\end{align*}
@col
Hence the RREF of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&0&2&0&0&1&2\\
0&1&1&0&0&1&-1\\
0&0&0&1&0&-2&3\\
0&0&0&0&1&1&1\\
\end{bmatrix}.
\end{align*}
@endcol

@col
<strong>Important remark</strong>: from the above computation, the entries of $B$ are uniquely determined by $A$.

@col
So the RREF $B$ is unique.

@end
@setchapter{14}
@chapter{Column and Row Space}
<h5 class="notkw">Reference.</h5>
<ul>
<li>
Beezer, Ver 3.5 Section CRS (print version p167-178)
</li>
</ul>

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at
@href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)
Section CRS p.66-71 C20, C30-C35, M10, M20, M21, T40, T41, T45.
@section{Column Spaces and Systems of Equations}
@slide
@defn
@title{Column Space of a Matrix}
@label{CSM}
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$. Then the <b>column space</b> of $A$, written $\mathcal{C}\!\left(A\right)$, is the subset of ${\mathbb{R}}^{m}$ containing all linear combinations of the columns of $A$,
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\left< \left\{\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}\right\}\right>
\end{align*}
@end
@slide
@thm
@title{Column Spaces and Consistent Systems}
@label{CSCS}
Suppose $A$ is an $m\times n$ matrix and $\mathbf{b}$ is a vector of size $m$.
Then $\mathbf{b}\in\mathcal{C}\!\left(A\right)$ if and only if $\mathcal{LS}({A},{\mathbf{b}})$ is consistent.
@end
@proof
@col
($\Rightarrow$) Suppose $\mathbf{b}\in\mathcal{C}\!\left(A\right)$. Then we can write $\mathbf{b}$ as some linear combination of the columns of $A$. Then by @ref{RCLS} we can use the scalars from this linear combination to form a solution to $\mathcal{LS}({A},{\mathbf{b}})$, so this system is consistent.


($\Leftarrow$) If $\mathcal{LS}({A},{\mathbf{b}})$ is consistent, there is a solution that may be used with @ref{RCLS} to write $\mathbf{b}$ as a linear combination of the columns of $A$. This qualifies $\mathbf{b}$ for membership in $\mathcal{C}\!\left(A\right)$.
@qed
@end
This theorem tells us that asking if the system $\mathcal{LS}({A},{\mathbf{b}})$ is consistent is exactly the same question as asking if $\mathbf{b}$ is in the column space of $A$. Or equivalently, it tells us that the column space of the matrix $A$ is precisely those vectors of constants, $\mathbf{b}$, that can be paired with $A$ to create a system of linear equations $\mathcal{LS}({A},{\mathbf{b}})$ that is consistent.

@col
We can form the chain of equivalences
\begin{align*}
\displaystyle\mathbf{b}\in\mathcal{C}\!\left(A\right)\iff\mathcal{LS}({A},{\mathbf{b}})\text{ is consistent}\iff A\mathbf{x}=\mathbf{b}\text{ for some }\mathbf{x}
\end{align*}
@col
Thus, an alternative (and popular) definition of the column space of an $m\times n$ matrix $A$ is
\begin{align*}
\displaystyle\mathcal{C}\!\left(A\right)&=\left\{\left.\mathbf{y}\in{\mathbb{R}}^{m}\,\right|\,\mathbf{y}=A\mathbf{x}\text{ for some }\mathbf{x}\in{\mathbb{R}}^{n}\right\}=\left\{\left.A\mathbf{x}\,\right|\,\mathbf{x}\in{\mathbb{R}}^{n}\right\}\subseteq{\mathbb{R}}^{m}
\end{align*}
@slide
@eg
Consider the column space of the $3\times 4$ matrix $A$,
\begin{align*}
\displaystyle A=\begin{bmatrix}3&2&1&-4\\
-1&1&-2&3\\
2&-4&6&-8\end{bmatrix}
\end{align*}
@col
Show that $\mathbf{v}=\begin{bmatrix}18\\
-6\\
12\end{bmatrix}$ is in the column space of $A$, $\mathbf{v}\in\mathcal{C}\!\left(A\right)$.
The above theorem says that we need to check the consistency of $\mathcal{LS}({A},{v})$.
From the augmented matrix and row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}3&2&1&-4&18\\
-1&1&-2&3&-6\\
2&-4&6&-8&12\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&1&-2&6\\
0&\boxed{1}&-1&1&0\\
0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Since the last column is not a pivot column, so the system is consistent and hence $v\in\mathcal{C}\!\left(A\right)$.
In fact, we have
\begin{align*}
\displaystyle \mathbf{v}=6\mathbf{A}_{1}.
\end{align*}
@col
Next we show that $\mathbf{w}=\begin{bmatrix}2\\
1\\
-3\end{bmatrix}$ is not in the column space of $A$, $\mathbf{w}\not\in\mathcal{C}\!\left(A\right)$.
The above theorem says that we need to check the consistency of $\mathcal{LS}({A},{v})$.
From the augmented matrix and row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}3&2&1&-4&2\\
-1&1&-2&3&1\\
2&-4&6&-8&-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&1&-2&0\\
0&\boxed{1}&-1&1&0\\
0&0&0&0&\boxed{1}\end{bmatrix}
\end{align*}
@col
Since the final column is a pivot column, the system is inconsistent and therefore $\mathbf{w}\not\in\mathcal{C}\!\left(A\right)$.
@end
The next two examples illustrate the main idea of describing $\mathcal{C}\!\left(A\right)$.
@slide
@eg
<b>Describe $\mathcal{C}\!\left(A\right)$ as a null space</b>

Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&7&1&-1\\
1&1&3&1&0\\
3&2&5&-1&9\\
1&-1&-5&2&0\end{bmatrix}.
\end{align*}
@col
Find $\mathcal{C}\!\left(A\right)$. Let’s determine if $\mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{4}\end{bmatrix}\in\left< S\right>$.

@col
Applying Gauss-Jordan elimination to the augmented matrix
\begin{align*}
\displaystyle \begin{bmatrix}1&2&7&1&-1&v_{1}\\
1&1&3&1&0&v_{2}\\
3&2&5&-1&9&v_{3}\\
1&-1&-5&2&0&v_{4}\end{bmatrix},
\end{align*}
@col
we obtain
\begin{align*}
\displaystyle \begin{bmatrix}1&0&-1&0&3&-3v_{1}+5v_{2}-v_{4}\\
0&1&4&0&-1&v_{1}-v_{2}\\
0&0&0&1&-2&2v_{1}-3v_{2}+v_{4}\\
0&0&0&0&0&9v_{1}-16v_{2}+v_{3}+4v_{4}\\
\end{bmatrix}
\end{align*}
@col
If $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$, the above is a RREF. The last column is not a pivot columns. So $\mathbf{v}\in\left< S\right>$. If $9v_{1}-16v_{2}+v_{3}+4v_{4}\neq 0$, the equation corresponding to the last row is
\begin{align*}
\displaystyle 9v_{1}-16v_{2}+v_{3}+4v_{4}=0.
\end{align*}
@col
So the corresponding system of linear equations is inconsistent. So $\mathbf{v}\in\left< S\right>$. Hence $\mathbf{v}\in\left< S\right>$ if and only if $9v_{1}-16v_{2}+v_{3}+4v_{4}=0$. Therefore
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)={\mathcal{N}}\!\left([9\,\,-16\,\,1\,\,4]\right).
\end{align*}
@end
@slide
@eg
<b>Describe $\mathcal{C}\!\left(A\right)$ by basis</b>

Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix},
\end{align*}
@col
find $\mathcal{C}\!\left(A\right)$.
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}B=\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}.
\end{align*}
@col
The indexes of the pivot columns are $D=\left\{1,3,4\right\}$.
Hence $\mathcal{C}\!\left(A\right)=\left< A\right>=\left< \left\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\right\}\right>$.
@end
@section{
Column Space Spanned by Original Columns}
@label{CSSOC}
So we have a foolproof, automated procedure for determining membership in $\mathcal{C}\!\left(A\right)$. While this works just fine a vector at a time, we would like to have a more useful description of the set $\mathcal{C}\!\left(A\right)$ as a whole. The next example will preview the first of two fundamental results about the column space of a matrix.
@slide
@eg
Consider the $5\times 7$ matrix $A$,
\begin{align*}
\displaystyle \begin{bmatrix}2&4&1&-1&1&4&4\\
1&2&1&0&2&4&7\\
0&0&1&4&1&8&7\\
1&2&-1&2&1&9&6\\
-2&-4&1&3&-1&-2&-2\end{bmatrix}
\end{align*}
@col
The column space of $A$ is
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\left< \left\{\begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
2\\
0\\
2\\
-4\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix},\,\begin{bmatrix}4\\
4\\
8\\
9\\
-2\end{bmatrix},\,\begin{bmatrix}4\\
7\\
7\\
6\\
-2\end{bmatrix}\right\}\right>
\end{align*}
@col
While this is a concise description of an infinite set, we might be able to describe the span with fewer than seven vectors. Now we row-reduce,
\begin{align*}
\displaystyle \begin{bmatrix}2&4&1&-1&1&4&4\\
1&2&1&0&2&4&7\\
0&0&1&4&1&8&7\\
1&2&-1&2&1&9&6\\
-2&-4&1&3&-1&-2&-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&2&0&0&0&3&1\\
0&0&\boxed{1}&0&0&-1&0\\
0&0&0&\boxed{1}&0&2&1\\
0&0&0&0&\boxed{1}&1&3\\
0&0&0&0&0&0&0\end{bmatrix}
\end{align*}
@col
The pivot columns are $D=\left\{1,\,3,\,4,\,5\right\}$, so we can create the set
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}2\\
1\\
0\\
1\\
-2\end{bmatrix},\,\begin{bmatrix}1\\
1\\
1\\
-1\\
1\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
4\\
2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
2\\
1\\
1\\
-1\end{bmatrix}\right\}
\end{align*}
@col
and know that $\mathcal{C}\!\left(A\right)=\left< T\right>$ and $T$ is a linearly independent set of columns from the set of columns of $A$.
@end
@slide
The following theorem is a direct consequence of @ref{BS}:
@thm
@title{Basis of the Column Space}
@label{BCS}
@col
Suppose that $A$ is an $m\times n$ matrix with columns $\mathbf{A}_{1},\,\mathbf{A}_{2},\,\mathbf{A}_{3},\,\ldots,\,\mathbf{A}_{n}$, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ pivot columns. Let $D=\left\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\right\}$ be the set of indices for the pivot columns of $B$.
Let $T=\left\{\mathbf{A}_{d_{1}},\,\mathbf{A}_{d_{2}},\,\mathbf{A}_{d_{3}},\,\ldots,\,\mathbf{A}_{d_{r}}\right\}$. Then
<ol class="ltx_enumerate">
<li class="ltx_item">
$T$ is a linearly independent set.
</li>
<li class="ltx_item">
$\mathcal{C}\!\left(A\right)=\left< T\right>$.
</li>
</ol>
@end
@section{
Column Space of a Nonsingular Matrix}
@slide
@thm
@title{Column Space of a Nonsingular Matrix}
Suppose $A$ is a square matrix of size $n$. Then $A$ is nonsingular if and only if $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$.
@end
@proof
@col
See @ref{thm:ANSRM}.
@qed
@end
@slide
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}0&1&2&3\\
-1&1&2&1\\
0&1&0&2\\
1&1&1&4\\
\end{bmatrix}.
\end{align*}
@col
We can show that $A$ is nonsingular as $A\xrightarrow{\text{RREF}}I_{4}$.
So $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{4}$.
@end
@section{
Row Space of a Matrix}
<!--
@slide
@def
Let $n$ be a positive integer.
A <b>row vector</b> or size $n$ is an array of real numbers of the form:
\[
\left(v_1, v_2, \ldots, v_n\right),\quad v_i \in \mathbb{R}.
\]
In the context of this course, it is perfectly reasonable to view a row vector
as simply a $1 \times n$ matrix.

@col
In fact,
the standard laws of addition and scalar multiplication on row vectors
of size $n$ are those defined for $1 \times n$ matrices (see @ref{MEASM}).
That is:
\begin{multline*}
\left(v_1 , v_2 , \ldots , v_n\right) + \left(w_1, w_2, \ldots, w_n\right)
= \left(v_1 + w_1, v_2 + w_2, \ldots, v_n + w_n\right),\\
 \quad v_i, w_i \in \mathbb{R};
\end{multline*}
\[
\alpha\left(v_1, v_2, \ldots, v_n\right)
=
\left(\alpha v_1, \alpha v_2, \ldots, \alpha v_n\right), \quad v_i, \alpha \in \mathbb{R}.
\]
@col
For any fixed positive integer $n$,
by @ref{fb1ac45c60c8c59135e7173658dededc} the set of all row vectors of size $n$
is a vector space under the laws of addition and scalar multiplication defined above.
@end
@slide
-->
@defn
@title{Row Space of a Matrix}
Suppose $A$ is an $m\times n$ matrix.
The <b>row space</b> of $A$,
$\mathcal{R}\!\left(A\right)$ is column space $\mathcal{C}\left(A^t\right)$ of $A^t$.
@end

Informally, the row space is the set of all linear combinations of the rows of $A$. However, we write the rows as column vectors, thus the necessity of using the transpose to make the rows into columns. Additionally, with the row space defined in terms of the column space, all of the previous results of this section can be applied to row spaces.

@col
Notice that if $A$ is a rectangular $m\times n$ matrix, then $\mathcal{C}\!\left(A\right)\subseteq{\mathbb{R}}^{m}$, while $\mathcal{R}\!\left(A\right)\subseteq{\mathbb{R}}^{n}$ and the two sets are not comparable since they do not even hold objects of the same type. However, when $A$ is square of size $n$, both $\mathcal{C}\!\left(A\right)$ and $\mathcal{R}\!\left(A\right)$ are subsets of ${\mathbb{R}}^{n}$, though usually the sets will not be equal.
@slide
@eg
Find $\mathcal{R}\!\left(A\right)$ for
\begin{align*}
\displaystyle A=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}.
\end{align*}
@col
To build the row space, we transpose the matrix,
\begin{align*}
\displaystyle A^{t}=\begin{bmatrix}1&2&0&-1\\
4&8&0&-4\\
0&-1&2&2\\
-1&3&-3&4\\
0&9&-4&8\\
7&-13&12&-31\\
-9&7&-8&37\end{bmatrix}
\end{align*}
@col
Then the columns of this matrix are used in a span to build the row space,
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\left< \left\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix},\,\begin{bmatrix}-1\\
-4\\
2\\
4\\
8\\
-31\\
37\end{bmatrix}\right\}\right>.
\end{align*}
@col
First, row-reduce $A^{t}$,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&0&-\frac{31}{7}\\
0&\boxed{1}&0&\frac{12}{7}\\
0&0&\boxed{1}&\frac{13}{7}\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
Since the pivot columns have indices $D=\left\{1,\,2,\,3\right\}$, the column space of $A^{t}$ can be spanned by just the first three columns of $A^{t}$,
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\left< \left\{\begin{bmatrix}1\\
4\\
0\\
-1\\
0\\
7\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
8\\
-1\\
3\\
9\\
-13\\
7\end{bmatrix},\,\begin{bmatrix}0\\
0\\
2\\
-3\\
-4\\
12\\
-8\end{bmatrix}\right\}\right>.
\end{align*}
@end
@slide
@thm
@title{Row-Equivalent Matrices have Equal Row Spaces}
@label{REMRS}
Suppose $A$ and $B$ are row-equivalent matrices. Then $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$.
@end
@proof
@col
Observe that if $B$ is obtained from $A$ via a row operation of the type
$R_i \leftrightarrow R_j$,
then the rows of $B$ are the same as the rows of $A$,
and hence the columns of $B^t$ are still the same as the columns of $A^t$,
only with the order changed.  Hence,
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) = \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\]

@col
If $B$ is obtained from $A$ via a row operation of the type $\alpha R_i$ ($\alpha \neq 0$),
then the $i$-th column of $B^t$ is equal to $\alpha$ times the $i$-th column of $A^t$,
and the other columns remain the same as those of $A^t$ with the corresponding indices.

In paricular, the $i$-th column of $B^t$ is a linear combination of the columns of $A^t$.

@col
Hence, the columns of $B^t$ all lie in $\mathcal{C}(A^t)$, which in turn implies that:
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) \subseteq \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\]
@col
On the other hand, if $B$ is obtained from $A$ via $\alpha R_i$, then $A$ is obtained
from $B$ via $\left(\frac{1}{\alpha}\right) R_i$.  So, by the same argument as before we have:
\[
\mathcal{R}(A) = \mathcal{C}\left(A^t\right) \subseteq \mathcal{C}\left(B^t\right) = \mathcal{R}(B).
\]
Hence, $\mathcal{R}(B) = \mathcal{R}(A)$.

@col
If $B$ is obtained from $A$ via a row operation of the type $\alpha R_i + R_j$,
then:
\[
\left[B^t\right]_j = \alpha\left[A^t\right]_i + \left[A^t\right]_j,
\]
and the other columns of $B^t$ remain the same as those of $A^t$
with the corresponding indices.

In paricular, the $i$-th column of $B^t$ is a linear combination of the columns of $A^t$.

@col
Hence, the columns of $B^t$ all lie in $\mathcal{C}(A^t)$, which in turn implies that:
\[
\mathcal{R}(B) = \mathcal{C}\left(B^t\right) \subseteq \mathcal{C}\left(A^t\right) = \mathcal{R}(A).
\]
@col
On the other hand, if $B$ is obtained from $A$ via $\alpha R_i + R_j$, then $A$ is obtained
from $B$ via $(-\alpha) R_i + R_j$.  So, by the same argument as before we have:
\[
\mathcal{R}(A) = \mathcal{C}\left(A^t\right) \subseteq \mathcal{C}(B^t) = \mathcal{R}(B).
\]
Hence, $\mathcal{R}(B) = \mathcal{R}(A)$.

@col
We now see that the row space of a matrix remains unchanged after any application
of a row operation.

Hence, $\mathcal{R}(B) = \mathcal{R}(A)$ if $B$ is row-equivalent to $A$,
since by the definition of row-equivalence (@ref{dacc247eacab6b0b430f64f7de449ebe}) $B$ is obtained by $A$ via a series
of row operations.
@end
<!--
@proof
@col
Two matrices are row-equivalent if one can be obtained from another by a sequence of (possibly many) row operations. We will prove the theorem for two matrices that differ by a single row operation, and then this result can be applied repeatedly to get the full statement of the theorem. The row spaces of $A$ and $B$ are spans of the columns of their transposes. For each row operation we perform on a matrix, we can define an analogous operation on the columns. Perhaps we should call these <b>column operations</b>. Instead, we will still call them row operations, but we will apply them to the columns of the transposes.

@col
Refer to the columns of $A^{t}$ and $B^{t}$ as $\mathbf{A}_{i}$ and $\mathbf{B}_{i}$, $1\leq i\leq m$. The row operation that switches rows will just switch columns of the transposed matrices. This will have no effect on the possible linear combinations formed by the columns.

@col
Suppose that $B^{t}$ is formed from $A^{t}$ by multiplying column $\mathbf{A}_{t}$ by $\alpha\neq 0$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for all $i\neq t$. We need to establish that two sets are equal, $\mathcal{C}\!\left(A^{t}\right)=\mathcal{C}\!\left(B^{t}\right)$. We will take a generic element of one and show that it is contained in the other.
\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&\beta_{2}\mathbf{B}_{2}+\beta_{3}\mathbf{B}_{3}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\beta_{3}\mathbf{A}_{3}+\cdots+\left(\alpha\beta_{t}\right)\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}
@col
says that $\mathcal{C}\!\left(B^{t}\right)\subseteq\mathcal{C}\!\left(A^{t}\right)$. Similarly,
\begin{align*}
\displaystyle\gamma_{1}\mathbf{A}_{1}+&\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\left(\frac{\gamma_{t}}{\alpha}\alpha\right)\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\gamma_{3}\mathbf{A}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\left(\alpha\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\gamma_{3}\mathbf{B}_{3}+\cdots+\frac{\gamma_{t}}{\alpha}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}
@col
says that $\mathcal{C}\!\left(A^{t}\right)\subseteq\mathcal{C}\!\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\mathcal{C}\!\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the second type is performed.

@col
Suppose now that $B^{t}$ is formed from $A^{t}$ by replacing $\mathbf{A}_{t}$ with $\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$ for some $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $s\neq t$. In other words, $\mathbf{B}_{t}=\alpha\mathbf{A}_{s}+\mathbf{A}_{t}$, and $\mathbf{B}_{i}=\mathbf{A}_{i}$ for $i\neq t$.
\begin{align*}
\displaystyle\beta_{1}\mathbf{B}_{1}+&\beta_{2}\mathbf{B}_{2}+\cdots+\beta_{s}\mathbf{B}_{s}+\cdots+\beta_{t}\mathbf{B}_{t}+\cdots+\beta_{m}\mathbf{B}_{m} \\
&=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\beta_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\beta_{m}\mathbf{A}_{m} \\
&=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\cdots+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\beta_{s}\mathbf{A}_{s}+\left(\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m} \\
&=\beta_{1}\mathbf{A}_{1}+\beta_{2}\mathbf{A}_{2}+\cdots+\left(\beta_{s}+\beta_{t}\alpha\right)\mathbf{A}_{s}+\cdots+\beta_{t}\mathbf{A}_{t}+\cdots+\beta_{m}\mathbf{A}_{m}
\end{align*}
@col
says that $\mathcal{C}\!\left(B^{t}\right)\subseteq\mathcal{C}\!\left(A^{t}\right)$. Similarly,
\begin{align*}
\displaystyle\gamma_{1}&\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\gamma_{s}\mathbf{A}_{s}+\cdots+\left(-\alpha\gamma_{t}\mathbf{A}_{s}+\alpha\gamma_{t}\mathbf{A}_{s}\right)+\gamma_{t}\mathbf{A}_{t}+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&=\gamma_{1}\mathbf{A}_{1}+\gamma_{2}\mathbf{A}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{A}_{s}+\cdots+\gamma_{t}\left(\alpha\mathbf{A}_{s}+\mathbf{A}_{t}\right)+\cdots+\gamma_{m}\mathbf{A}_{m} \\
&=\gamma_{1}\mathbf{B}_{1}+\gamma_{2}\mathbf{B}_{2}+\cdots+\left(-\alpha\gamma_{t}+\gamma_{s}\right)\mathbf{B}_{s}+\cdots+\gamma_{t}\mathbf{B}_{t}+\cdots+\gamma_{m}\mathbf{B}_{m}
\end{align*}
@col
says that $\mathcal{C}\!\left(A^{t}\right)\subseteq\mathcal{C}\!\left(B^{t}\right)$. So $\mathcal{R}\!\left(A\right)=\mathcal{C}\!\left(A^{t}\right)=\mathcal{C}\!\left(B^{t}\right)=\mathcal{R}\!\left(B\right)$ when a single row operation of the third type is performed.

@col
So the row space of a matrix is preserved by each row operation, and hence row spaces of row-equivalent matrices are equal sets.
@qed
@end
-->
@slide
@eg
<b>Row spaces of two row-equivalent matrices</b>

The matrices
\begin{align*}
\displaystyle A&=\begin{bmatrix}2&-1&3&4\\
5&2&-2&3\\
1&1&0&6\end{bmatrix}& B&=\begin{bmatrix}1&1&0&6\\
3&0&-2&-9\\
2&-1&3&4\end{bmatrix}
\end{align*}
@col
are row-equivalent via a sequence of two row operations.

Hence by the above theorem
\begin{align*}
\displaystyle \mathcal{R}\!\left(A\right)=\left< \left\{\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix},\,\begin{bmatrix}5\\
2\\
-2\\
3\end{bmatrix},\,\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}1\\
1\\
0\\
6\end{bmatrix},\,\begin{bmatrix}3\\
0\\
-2\\
-9\end{bmatrix},\,\begin{bmatrix}2\\
-1\\
3\\
4\end{bmatrix}\right\}\right>=\mathcal{R}\!\left(B\right)
\end{align*}
@end
@slide
@thm
@title{Basis for the Row Space}
@label{BRS}
Suppose that $A$ is a matrix and $B$ is a row-equivalent matrix in reduced row-echelon form. Let $S$ be the set of nonzero columns of $B^{t}$. Then
<ol class="ltx_enumerate">
<li class="ltx_item">
$\mathcal{R}\!\left(A\right)=\left< S\right>$.
</li>
<li class="ltx_item">
$S$ is a linearly independent set.
</li>
</ol>
@end
@proof
@col
From Theorem    @ref{REMRS}. we know that $\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)$. If $B$ has any zero rows, these are columns of $B^{t}$ that are the zero vector. We can safely toss out the zero vector in the span construction, since it can be recreated from the nonzero vectors by a linear combination where all the scalars are zero. So $\mathcal{R}\!\left(A\right)=\left< S\right>$.

@col
Suppose $B$ has $r$ nonzero rows and let $D=\left\{d_{1},\,d_{2},\,d_{3},\,\ldots,\,d_{r}\right\}$ denote the indices of the pivot columns of $B$. Denote the $r$ column vectors of $B^{t}$, the vectors in $S$, as $\mathbf{B}_{1},\,\mathbf{B}_{2},\,\mathbf{B}_{3},\,\ldots,\,\mathbf{B}_{r}$. To show that $S$ is linearly independent, start with a relation of linear dependence
\begin{align*}
\displaystyle \alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}=\mathbf{0}
\end{align*}
@col
Now consider this vector equality in location $d_{i}$. Since $B$ is in reduced row-echelon form, the entries of column $d_{i}$ of $B$ are all zero, except for a leading 1 in row $i$. Thus, in $B^{t}$, row $d_{i}$ is all zeros, excepting a 1 in column $i$. So, for $1\leq i\leq r$,
\begin{align*}
\displaystyle 0&=\left[\mathbf{0}\right]_{d_{i}} \\
&=\left[\alpha_{1}\mathbf{B}_{1}+\alpha_{2}\mathbf{B}_{2}+\alpha_{3}\mathbf{B}_{3}+\cdots+\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&=\left[\alpha_{1}\mathbf{B}_{1}\right]_{d_{i}}+\left[\alpha_{2}\mathbf{B}_{2}\right]_{d_{i}}+\left[\alpha_{3}\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\left[\alpha_{r}\mathbf{B}_{r}\right]_{d_{i}} \\
&=\alpha_{1}\left[\mathbf{B}_{1}\right]_{d_{i}}+\alpha_{2}\left[\mathbf{B}_{2}\right]_{d_{i}}+\alpha_{3}\left[\mathbf{B}_{3}\right]_{d_{i}}+\cdots+\alpha_{r}\left[\mathbf{B}_{r}\right]_{d_{i}} \\
&=\alpha_{1}(0)+\alpha_{2}(0)+\alpha_{3}(0)+\cdots+\alpha_{i}(1)+\cdots+\alpha_{r}(0) \\
&=\alpha_{i}
\end{align*}
@col
So we conclude that $\alpha_{i}=0$ for all $1\leq i\leq r$, establishing the linear independence of $S$.
@qed
@end
@slide
@eg
<b>Improving a span</b>

Suppose in the course of analyzing a matrix (its column space, its null space, its ...) we encounter the following set of vectors, described by a span
\begin{align*}
\displaystyle X=\left< \left\{\begin{bmatrix}1\\
2\\
1\\
6\\
6\end{bmatrix},\,\begin{bmatrix}3\\
-1\\
2\\
-1\\
6\end{bmatrix},\,\begin{bmatrix}1\\
-1\\
0\\
-1\\
-2\end{bmatrix},\,\begin{bmatrix}-3\\
2\\
-3\\
6\\
-10\end{bmatrix}\right\}\right>
\end{align*}
@col
Let $A$ be the matrix whose rows are the vectors in $X$, so by design $X=\mathcal{R}\!\left(A\right)$,
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&1&6&6\\
3&-1&2&-1&6\\
1&-1&0&-1&-2\\
-3&2&-3&6&-10\end{bmatrix}
\end{align*}
@col
Row-reduce $A$ to form a row-equivalent matrix in reduced row-echelon form,
\begin{align*}
\displaystyle B=\begin{bmatrix}\boxed{1}&0&0&2&-1\\
0&\boxed{1}&0&3&1\\
0&0&\boxed{1}&-2&5\\
0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Then the above theorem says we can grab the nonzero columns of $B^{t}$ and write
\begin{align*}
\displaystyle X=\mathcal{R}\!\left(A\right)=\mathcal{R}\!\left(B\right)=\left< \left\{\begin{bmatrix}1\\
0\\
0\\
2\\
-1\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
3\\
1\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
-2\\
5\end{bmatrix}\right\}\right>
\end{align*}
@col
These three vectors provide a much-improved description of $X$. There are fewer vectors, and the pattern of zeros and ones in the first three entries makes it easier to determine membership in $X$.
@end
@slide
@thm
@title{Column Space, Row Space, Transpose}
@label{CSRST}
Suppose $A$ is a matrix. Then $\mathcal{C}\!\left(A\right)=\mathcal{R}\!\left(A^{t}\right)$.
@end
@proof
@col
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\mathcal{C}\!\left(\left(A^{t}\right)^{t}\right)=\mathcal{R}\!\left(A^{t}\right)
\end{align*}
@qed
@end
@slide
@eg
<b>Column space from row operations</b>

Find the column space of $A$ in @ref{cb807061818215f8c700c6df6edc1229}.

<strong>Method 1</strong>
@col
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Let
\begin{align*}
\displaystyle T=\left\{\mathbf{A}_{1},\mathbf{A}_{3},\mathbf{A}_{4}\right\}=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\right\}.
\end{align*}
@col
Then $T$ is linear independent and $\mathcal{C}\!\left(A\right)=\left< T\right>$.
@endcol

<strong>Method 2</strong>
@col
The transpose of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&2&0&-1\\
4&8&0&-4\\
0&-1&2&2\\
-1&3&-3&4\\
0&9&-4&8\\
7&-13&12&-31\\
-9&7&-8&37\end{bmatrix}.
\end{align*}
@col
Row-reduced this becomes,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&0&-\frac{31}{7}\\
0&\boxed{1}&0&\frac{12}{7}\\
0&0&\boxed{1}&\frac{13}{7}\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
Now, using Theorem    @ref{CSRST} and Theorem    @ref{BRS},
\begin{align*}
\displaystyle \mathcal{C}\!\left(A\right)=\mathcal{R}\!\left(A^{t}\right)=\left< \left\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\right\}\right>.
\end{align*}
@col
This is a very nice description of the column space. Fewer vectors than the 7 involved in the definition, and the pattern of the zeros and ones in the first 3 slots can be used to advantage. For example,
let’s check if
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}
\end{align*}
is in $\mathcal{C}\!\left(A\right)$ or not.

@col
If it is, then
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=x\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+y\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+z\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}=\begin{bmatrix}x\\
y\\
z\\
-\frac{31}{7}x+\frac{12}{7}y+\frac{13}{7}z\end{bmatrix}.
\end{align*}
@col
From the first three coordinate $x=3,y=9,z=1$. Let’s check the last coordinate:
\begin{align*}
\displaystyle -\frac{31}{7}\times 3+\frac{12}{7}\times 9+\frac{13}{7}\times 1=4.
\end{align*}
@col
So
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}3\\
9\\
1\\
4\end{bmatrix}=3\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix}+9\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix}+1\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}
\end{align*}
@col
and hence $\mathbf{b}\in\mathcal{C}\!\left(A\right)$.
@end

@remark
@col
Both methods describe algorithms to find bases
(i.e., linear independent set the generate the column space)
for the column space.
Here are the differences.
<ol class="ltx_enumerate">
<li class="ltx_item">
In method 1, we find a subset of columns that forms a basis.
However in method 2, the basis is not a subset of columns.
</li>
<li class="ltx_item">
Given a vector $\mathbf{b}\in\mathcal{C}\!\left(A\right)$, it is easier to express it as a linear combination of the basis given by method 2.
</li>
</ol>
@end
@setchapter{15}
@chapter{Basis}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section B (print version p233-238), Section D (print version p245-253)

<h5 class="notkw">Exercise.</h5>
<ul>
<li>
Exercises with solutions can be downloaded at
@href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)

Section B p.88-92 C10, C11, C12, M20 Section D
p.92-96 C21, C23, C30, C31, C35, C36, C37, M20, M21.
</li>
</ul>

@section{Basis}
@defn
Let $V$ be a vector space. Then a subset $S$ of $V$ is said to be a <b>basis</b> for $V$ if
<ol class="ltx_enumerate">
<li class="ltx_item">
$S$ is linearly independent.
</li>
<li class="ltx_item">
$\left< S\right>=V$, i.e. $S$ spans $V$.
</li>
</ol>
@end
@remark
@col
Most of the time $V$ is a subspace of ${\mathbb{R}}^{m}$. Occasionally $V$ is assumed to be a subspace of $M_{mn}$ or $P_{n}$. It does not hurt to assume $V$ is a subspace of ${\mathbb{R}}^{m}$.
@end
@slide
@eg
Let $V={\mathbb{R}}^{m}$, then $B=\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}$ is a basis for $V$.
(recall all the entries of $\mathbf{e}_{i}$ is zero, except the $i$-th entry being $1$).

@col
It is called the <b>standard basis</b>: Obviously $B$ is linearly independent. Also, for any $\mathbf{v}\in V$, $\mathbf{v}=[\mathbf{v}]_{1}\mathbf{e}_{1}+\cdots+[\mathbf{v}]_{m}\mathbf{e}_{m}\in\left< B\right>$. So $\left< B\right>=V$.
@end
@slide
@eg
<strong>Math major only</strong>

@col
Consider $V=M_{22}$. Let:
\begin{align*}
\displaystyle B_{11}&=\begin{bmatrix}1&0\\
0&0\end{bmatrix},
&
B_{12}&=\begin{bmatrix}0&1\\
0&0\end{bmatrix},
\end{align*}

\begin{align*}
\displaystyle B_{21}&=\begin{bmatrix}0&0\\
1&0\end{bmatrix},
&
B_{22}&=\begin{bmatrix}0&0\\
0&1\end{bmatrix},
\end{align*}
@col
Then $B=\left\{B_{11},B_{12},B_{21},B_{22}\right\}$ is a basis for $V$.

@col
Check: Obviously $B$ is linearly independent (exercise). Also for any $A\in V$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
c&d\end{bmatrix}=aB_{11}+bB_{12}+cB_{21}+dB_{22}.
\end{align*}
@col
So $\left< B\right>=M_{22}$.
@end

@ex
<strong>Math major only</strong>

@col
Let $V=M_{mn}$.

For $1\leq i\leq m$, $1\leq j\leq n$,
let $B_{ij}$ be the $m\times n$ matrix with $(i,j)$-th entry equal to $1$ and all other entries equal to $0$.

Then $\left\{ B_{ij} | 1\leq i \leq m, 1\leq j\leq n\right\}$ is a basis for $V$.
@end

@eg
<strong>Math major only</strong>

@col
Let $V=P_{n}$. Then $1,x,x^{2},\ldots,x^{n}$ is a basis. It is easy to show that $S=\left\{1,x,x^{2},\ldots,x^{n}\right\}$ is linearly independent. Also any polynomial
\begin{align*}
\displaystyle f(x)=a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{n}x^{n}
\end{align*}
@col
is a linear combinations of $S$.


@end
@slide
@eg
A vector space can have different bases.

Consider the vector space $V={\mathbb{R}}^{2}$.

Then,
\[
S=\left\{\mathbf{e}_{1},\mathbf{e}_{2}\right\}
\]
is a basis for $V$, and:

@col
\[
S^{\prime}=\left\{\begin{bmatrix}1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\end{bmatrix}\right\}
\]
is also a basis.
@end
@section{Bases for spans of column vectors}

Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}$ be a subset of ${\mathbb{R}}^{m}$
Recall from lecture 14,
that there are several methods to find a subset
$T\subseteq \langle S \rangle$.

@col
Such that (i) $T$ is linearly independent (ii) $\left< T\right>=\left< S\right>$. (In other words $T$ is a basis for $\left< S\right>$.)

<strong>Method 1</strong>
@newcol
Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]\xrightarrow{\text{RREF}}B$.

@col
Suppose $D=\left\{d_{1},\ldots,d_{r}\right\}$ be the indexes of the pivot columns of $B$.

@col
Let $T=\left\{\mathbf{v}_{d_{1}},\ldots,\mathbf{v}_{d_{r}}\right\}$. Then $T$ is a basis for $\left< S\right>=\mathcal{C}\!\left(A\right)$
@endcol

<strong>Method 2</strong>
@newcol
Let $A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{n}]$. Suppose $A^{t}\xrightarrow{\text{RREF}}B$.

@col
Let $T$ be the nonzero columns of $B^{t}$. Then $T$ is a basis for $\left< S\right>=\mathcal{C}\!\left(A\right)$

This is an example from Lecture 14.
@slide
@eg
<b>Column space from row operations</b>

Let
\begin{align*}
\displaystyle S=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}4\\
8\\
0\\
-4\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}0\\
9\\
-4\\
8\end{bmatrix},\mathbf{v}_{6}=\begin{bmatrix}7\\
-13\\
12\\
-31\end{bmatrix},\mathbf{v}_{7}=\begin{bmatrix}-9\\
7\\
-8\\
37\end{bmatrix}\right\}.
\end{align*}
@col
Find a basis for $\left< S\right>$.
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\cdots|\mathbf{v}_{7}]=\begin{bmatrix}1&4&0&-1&0&7&-9\\
2&8&-1&3&9&-13&7\\
0&0&2&-3&-4&12&-8\\
-1&-4&2&4&8&-31&37\end{bmatrix}.
\end{align*}

<strong>Method 1</strong>
@newcol
\begin{align*}
\displaystyle A\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&4&0&0&2&1&-3\\
0&0&\boxed{1}&0&1&-3&5\\
0&0&0&\boxed{1}&2&-6&6\\
0&0&0&0&0&0&0\end{bmatrix}
\end{align*}
@col
Let
\begin{align*}
\displaystyle T=\left\{\mathbf{v}_{1},\mathbf{v}_{3},\mathbf{v}_{4}\right\}=\left\{\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix},\begin{bmatrix}0\\
-1\\
2\\
2\end{bmatrix},\begin{bmatrix}-1\\
3\\
-3\\
4\end{bmatrix}\right\}.
\end{align*}
@col
Then $T$ is a basis for $\left< S\right>=\mathcal{C}\!\left(A\right)$.
@endcol

<strong>Method 2</strong>
@newcol
The transpose of $A$ is
\begin{align*}
\displaystyle \begin{bmatrix}1&2&0&-1\\
4&8&0&-4\\
0&-1&2&2\\
-1&3&-3&4\\
0&9&-4&8\\
7&-13&12&-31\\
-9&7&-8&37\end{bmatrix}.
\end{align*}
@col
Row-reduced this becomes,
\begin{align*}
\displaystyle D=\begin{bmatrix}\boxed{1}&0&0&-\frac{31}{7}\\
0&\boxed{1}&0&\frac{12}{7}\\
0&0&\boxed{1}&\frac{13}{7}\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0\end{bmatrix}.
\end{align*}
@col
Then we can take
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1\\
0\\
0\\
-\frac{31}{7}\end{bmatrix},\,\begin{bmatrix}0\\
1\\
0\\
\frac{12}{7}\end{bmatrix},\,\begin{bmatrix}0\\
0\\
1\\
\frac{13}{7}\end{bmatrix}\right\}.
\end{align*}


$T$ is a basis for $\mathcal{C}\!\left(A\right)=\left< S\right>$.
@end
@slide
@thm
@label{basisexists}
Let $S$ be a finite subset of ${\mathbb{R}}^{m}$.
Then, a basis for $\left< S\right>$ exists.

@col
In fact, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left< S\right>$ (see @ref{BCS}).
@end
@section{Bases and nonsingular matrices}
@slide
@thm
Suppose that $A$ is a square matrix of size $m$.

Then, the columns of $A$ is a basis for ${\mathbb{R}}^{m}$ if and only if $A$ is nonsingular.
@end
@proof
@col
This is a direct consequence of the theorem  @ref{NME2}:

@col
If columns of $A$ form a basis,
then in particular they are linearly independent.
So, item 5 of the theorem holds.

It now follows from the theorem that item 1,
namely that $A$ is nonsingular, also holds.

@col
Conversely, suppose $A$ is nonsingular.

@col
Then, by Item 5 of @ref{NME2} the columns of $A$ are linearly independent,
and by Item 4 they span $\mathbb{R}^m$.
Hence, the columns of $A$ is a basis for $\mathbb{R}^m$.
@qed
@end
@slide
In fact, we may further extend @ref{NME2} as follows:
@thm
@title{Nonsingular Matrix Equivalences}
@label{NME3}
@col
Suppose that $A$ is an $m\times m$ square matrix.
The following are equivalent:
<ol class="ltx_enumerate">
<li class="ltx_item">
@col
$A$ is nonsingular.
</li>
<li class="ltx_item" >
@col
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item" >
@col
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
</li>
<li class="ltx_item" >
@col
The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>
<li class="ltx_item" >
@col
The columns of $A$ form a linearly independent set.
</li>
<li class="ltx_item" >
@col
The columns of $A$ form a basis for $\mathbb{R}^m$.
</li>
</ol>
@end
@slide
@eg
Consider $S^{\prime}=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\end{bmatrix}\right\}$.

Let
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}]=\begin{bmatrix}1&1\\
0&1\end{bmatrix}
\end{align*}
@col
<strong>Exercise.</strong> The matrix $A$ is nonsingular.

@col
Hence, $S^{\prime}$ is a basis for ${\mathbb{R}}^{2}$.
@end
@slide
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.
\end{align*}
@col
It may be shown that $A$ is row equivalent to the $3\times 3$
identity matrix.

@col
Hence $A$ is nonsingular,
so the columns of $A$ form a basis for ${\mathbb{R}}^{3}$.
@end
@section{Dimension}

@defn
@title{Dimension}
Let $V$ be a vector space.

Suppose a finite set of vectors $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$ is a basis for $V$.

Then, we say that $V$ is a <b>finite dimensional vector space</b>.

The number $t$ (namely the number of vectors in the basis)
is called the <b>dimension</b> of $V$.
@end
@remark
@col
It is a non-trivial fact that the dimension is well-defined, i.e.,
If both $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{t}\right\}$
and $\left\{\mathbf{u}_{1},\ldots,\mathbf{u}_{s}\right\}$
are bases for $V$,
then $s=t$.
@slide
@thm
@label{SSLD}
Suppose that $S=\left\{\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{t}\right\}$ is a finite set of vectors which spans the vector space $V$. Then any set of $t+1$ or more vectors from $V$ is linearly dependent.
@end
@proof
@col
Let $A = [\vect{v}_1 | \vect{v}_2 | \cdots | \vect{v}_t ]$.
Since $S$ is a basis, for every $\vect{u}_i$ ($1\leq i \leq m$)
there exists
$\vect{w}_i \in \mathbb{R}^t$ such that:
\[
A\vect{w}_i = \vect{u}_i.
\]

@col
Now, consider the matrix:
\[
B = [\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m].
\]
This is a $t \times m$ matrix.
In particular, it has more columns than rows,
due to the assumption that $m > t$.

@col
Hence, the homogeneous linear system $\mathcal{LS}(B, \vect{0})$
has a non-trivial solution $\vect{x} \in \mathbb{R}^m$.  That is:
\[
B\vect{x} = \vect{0}.
\]

@col
The above equation implies that:
\[
A\left(B\vect{x}\right) = A\vect{0} = \vect{0}.
\]
@col
By the associativity of matrix multiplication, we have:
\[
A\left(B\vect{x}\right) = \left(AB\right)\vect{x}.
\]
@col
On the other hand:
@steps
\[
\begin{split}
AB &= A[\vect{w}_1 | \vect{w}_2 | \cdots | \vect{w}_m]
\\&
@nstep{=[A\vect{w}_1 | A\vect{w}_2 | \cdots | A\vect{w}_m]}
\\&
@nstep{ = [\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]}
\end{split}
\]
@endsteps
@col
Hence,
\[
\left(AB\right)\vect{x} = \vect{0}
\]
is equivalent to:
\[
[\vect{u}_1 | \vect{u}_2 | \cdots | \vect{u}_m]\vect{x} = \vect{0}
\]
@col
which is in turn equivalent to:
\[
x_1 \vect{u}_1 + x_2 \vect{u}_2 + \cdots x_m \vect{u}_m = \vect{0}.
\]
Since, $\vect{x}$ is not the zero vector,
not all the $x_i$'s are equal to zero.
We conclude that the vectors
$\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_m$ are linearly dependent.
@qed
@end

<!--
@proof
@of{SSLD}
@col
We want to prove that any set of $t+1$ or more vectors from $V$ is linearly dependent. So we will begin with a totally arbitrary set of vectors from $V$, $R=\left\{\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{m}\right\}$, where $m>t$. We will now construct a nontrivial relation of linear dependence on $R$.

@col
Each vector $\mathbf{u}_{1},\,\mathbf{u}_{2},\,\mathbf{u}_{3},\,\ldots,\,\mathbf{u}_{m}$ can be written as a linear combination of the vectors $\mathbf{v}_{1},\,\mathbf{v}_{2},\,\mathbf{v}_{3},\,\ldots,\,\mathbf{v}_{t}$ since $S$ is a spanning set of $V$. This means there exist scalars $a_{ij}$, $1\leq i\leq t$, $1\leq j\leq m$, so that
\begin{align*}
\displaystyle\mathbf{u}_{1}&=a_{11}\mathbf{v}_{1}+a_{21}\mathbf{v}_{2}+a_{31}\mathbf{v}_{3}+\cdots+a_{t1}\mathbf{v}_{t} \\
\displaystyle\mathbf{u}_{2}&=a_{12}\mathbf{v}_{1}+a_{22}\mathbf{v}_{2}+a_{32}\mathbf{v}_{3}+\cdots+a_{t2}\mathbf{v}_{t} \\
\displaystyle\mathbf{u}_{3}&=a_{13}\mathbf{v}_{1}+a_{23}\mathbf{v}_{2}+a_{33}\mathbf{v}_{3}+\cdots+a_{t3}\mathbf{v}_{t} \\
&\quad\quad\vdots \\
\displaystyle\mathbf{u}_{m}&=a_{1m}\mathbf{v}_{1}+a_{2m}\mathbf{v}_{2}+a_{3m}\mathbf{v}_{3}+\cdots+a_{tm}\mathbf{v}_{t}
\end{align*}
@col
Now we form, unmotivated, the homogeneous system of $t$ equations in the $m$ variables, $x_{1},\,x_{2},\,x_{3},\,\ldots,\,x_{m}$, where the coefficients are the just-discovered scalars $a_{ij}$,
\begin{align*}
\displaystyle a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+\cdots+a_{1m}x_{m}&=0 \\
\displaystyle a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+\cdots+a_{2m}x_{m}&=0 \\
\displaystyle a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+\cdots+a_{3m}x_{m}&=0 \\
\displaystyle\vdots& \\
\displaystyle a_{t1}x_{1}+a_{t2}x_{2}+a_{t3}x_{3}+\cdots+a_{tm}x_{m}&=0
\end{align*}
@col
This is a homogeneous system with more variables than equations (our hypothesis is expressed as $m>t$), so there are infinitely many solutions. Choose a nontrivial solution and denote it by $x_{1}=c_{1},\,x_{2}=c_{2},\,x_{3}=c_{3},\,\ldots,\,x_{m}=c_{m}$. As a solution to the homogeneous system, we then have
\begin{align*}
\displaystyle a_{11}c_{1}+a_{12}c_{2}+a_{13}c_{3}+\cdots+a_{1m}c_{m}&=0 \\
\displaystyle a_{21}c_{1}+a_{22}c_{2}+a_{23}c_{3}+\cdots+a_{2m}c_{m}&=0 \\
\displaystyle a_{31}c_{1}+a_{32}c_{2}+a_{33}c_{3}+\cdots+a_{3m}c_{m}&=0 \\
\displaystyle\vdots& \\
\displaystyle a_{t1}c_{1}+a_{t2}c_{2}+a_{t3}c_{3}+\cdots+a_{tm}c_{m}&=0
\end{align*}
@col
As a collection of nontrivial scalars, $c_{1},\,c_{2},\,c_{3},\,\dots,\,c_{m}$ will provide the nontrivial relation of linear dependence we desire,
\begin{multline*}
\displaystyle c_{1}\mathbf{u}_{1}+c_{2}\mathbf{u}_{2}+c_{3}\mathbf{u}_{3}+\cdots+c_{m}\mathbf{u}_{m} \\
\displaystyle=c_{1}\left(a_{11}\mathbf{v}_{1}+a_{21}\mathbf{v}_{2}+a_{31}\mathbf{v}_{3}+\cdots+a_{t1}\mathbf{v}_{t}\right) \\
\displaystyle\quad\quad+c_{2}\left(a_{12}\mathbf{v}_{1}+a_{22}\mathbf{v}_{2}+a_{32}\mathbf{v}_{3}+\cdots+a_{t2}\mathbf{v}_{t}\right) \\
\displaystyle\quad\quad+c_{3}\left(a_{13}\mathbf{v}_{1}+a_{23}\mathbf{v}_{2}+a_{33}\mathbf{v}_{3}+\cdots+a_{t3}\mathbf{v}_{t}\right) \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+c_{m}\left(a_{1m}\mathbf{v}_{1}+a_{2m}\mathbf{v}_{2}+a_{3m}\mathbf{v}_{3}+\cdots+a_{tm}\mathbf{v}_{t}\right) \\
\displaystyle=c_{1}a_{11}\mathbf{v}_{1}+c_{1}a_{21}\mathbf{v}_{2}+c_{1}a_{31}\mathbf{v}_{3}+\cdots+c_{1}a_{t1}\mathbf{v}_{t} \\
\displaystyle\quad\quad+c_{2}a_{12}\mathbf{v}_{1}+c_{2}a_{22}\mathbf{v}_{2}+c_{2}a_{32}\mathbf{v}_{3}+\cdots+c_{2}a_{t2}\mathbf{v}_{t} \\
\displaystyle\quad\quad+c_{3}a_{13}\mathbf{v}_{1}+c_{3}a_{23}\mathbf{v}_{2}+c_{3}a_{33}\mathbf{v}_{3}+\cdots+c_{3}a_{t3}\mathbf{v}_{t} \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+c_{m}a_{1m}\mathbf{v}_{1}+c_{m}a_{2m}\mathbf{v}_{2}+c_{m}a_{3m}\mathbf{v}_{3}+\cdots+c_{m}a_{tm}\mathbf{v}_{t} \\
\displaystyle=\left(c_{1}a_{11}+c_{2}a_{12}+c_{3}a_{13}+\cdots+c_{m}a_{1m}\right)\mathbf{v}_{1} \\
\displaystyle\quad\quad+\left(c_{1}a_{21}+c_{2}a_{22}+c_{3}a_{23}+\cdots+c_{m}a_{2m}\right)\mathbf{v}_{2} \\
\displaystyle\quad\quad+\left(c_{1}a_{31}+c_{2}a_{32}+c_{3}a_{33}+\cdots+c_{m}a_{3m}\right)\mathbf{v}_{3} \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+\left(c_{1}a_{t1}+c_{2}a_{t2}+c_{3}a_{t3}+\cdots+c_{m}a_{tm}\right)\mathbf{v}_{t} \\
\displaystyle=\left(a_{11}c_{1}+a_{12}c_{2}+a_{13}c_{3}+\cdots+a_{1m}c_{m}\right)\mathbf{v}_{1} \\
\displaystyle\quad\quad+\left(a_{21}c_{1}+a_{22}c_{2}+a_{23}c_{3}+\cdots+a_{2m}c_{m}\right)\mathbf{v}_{2} \\
\displaystyle\quad\quad+\left(a_{31}c_{1}+a_{32}c_{2}+a_{33}c_{3}+\cdots+a_{3m}c_{m}\right)\mathbf{v}_{3} \\
\displaystyle\quad\quad\quad\quad\vdots \\
\displaystyle\quad\quad+\left(a_{t1}c_{1}+a_{t2}c_{2}+a_{t3}c_{3}+\cdots+a_{tm}c_{m}\right)\mathbf{v}_{t} \\
\displaystyle=0\mathbf{v}_{1}+0\mathbf{v}_{2}+0\mathbf{v}_{3}+\cdots+0\mathbf{v}_{t} \\
\displaystyle=\mathbf{0}+\mathbf{0}+\mathbf{0}+\cdots+\mathbf{0} \\
\displaystyle=\mathbf{0}
\end{multline*}
@col
That does it. $R$ has been undeniably shown to be a linearly dependent set.
@qed
@end
-->
@slide
@thm
Suppose that $V$ is a vector space with a finite basis $B$ and a second basis $C$.

@col
Then $B$ and $C$ have the same size.
@end
@proof
@col
Denote the size of $B$ by $t$. If $C$ has $\geq t+1$ vectors,
then by the previous theorem, $C$ is linearly dependent,
in contradiction to the condition that $C$ is a basis.

@col
By the same reasoning, the linearly independent set $B$ must also not
have more vectors than $C$.

@col
So, $B$ and $C$ have the same number of vectors.
@qed
@end
@remark
@col
The above theorem shows that the dimension is well-defined. No matter which basis we choose, the size is always the same.
@end
@slide
@eg
It follows from @ref{d848ac74d9e041feff51c974a9969985} that:
\[
\dim{\mathbb{R}}^{m}=m.
\]
@end
@slide
@eg
<strong>Math major only</strong>


$\dim M_{mn}=mn$. See example 3.
@end

@eg
<strong>Math major only</strong>


$\dim P_{n}=n+1$. See example 4.
@end

@eg
<strong>Math major only</strong>

Let $S_{2}$ be the set of $2\times 2$ symmetric matrices. For $A\in S_{2}$,
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
b&c\end{bmatrix}=a\begin{bmatrix}1&0\\
0&0\end{bmatrix}+b\begin{bmatrix}0&1\\
1&0\end{bmatrix}+c\begin{bmatrix}0&0\\
0&1\end{bmatrix}
\end{align*}
@col
We can show that:
\begin{align*}
\displaystyle T=\left\{\begin{bmatrix}1&0\\
0&0\end{bmatrix},\begin{bmatrix}0&1\\
1&0\end{bmatrix},\begin{bmatrix}0&0\\
0&1\end{bmatrix}\right\}
\end{align*}
@col
is a basis for $S_{2}$. Hence $\dim S_{2}=3$.
@end

@eg
<strong>Math major only</strong>

Let $P$ be the set of all real polynomials. As $\left\{1,x,x^{2},x^{3},\ldots\right\}$ is linearly independent,
so $\dim P$ does not exists (or we can write $\dim P=\infty$).
@end
@slide
@lemma
Let $V$ be a vector space and $\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\in V$.

@col
Suppose
$S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent and $\mathbf{u}\notin\left< S\right>$. Then
$S^{\prime}=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k},\mathbf{u}\right\}$ is linearly independent.
@end
@proof
@col
Let the relation of linear dependence of $S^{\prime}$ be
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}+\alpha\mathbf{u}=\mathbf{0}.
\end{align*}
@col
Suppose $\alpha\neq 0$, then
\begin{align*}
\displaystyle \mathbf{u}=-\frac{\alpha_{1}}{\alpha}\mathbf{v}_{1}-\cdots-\frac{\alpha_{k}}{\alpha}\mathbf{v}_{k}\in\left< S\right>.
\end{align*}
@col
Contradiction.

@col
So $\alpha=0$, then
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\mathbf{0}.
\end{align*}
@col
By the linear independence of $S$, $\alpha_{i}=0$ for all $i$. Hence the above relation of dependence of $S^{\prime}$ is trivial.
@qed
@end
@slide
@thm
@label{basisexists2}
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.

@col
There exists a basis for $V$.
@end
@proof
@col
If $V$ is the zero vector space, i.e. $V=\left\{\mathbf{0}\right\}$. Then the theorem is trivial. Suppose $V$ is not the zero vector space, let $\mathbf{v}_{1}$ be a nonzero vector in $V$. If $V=\left< \left\{\mathbf{v}_{1}\right\}\right>$, we can take $S=\left\{\mathbf{v}_{1}\right\}$. Then obviously $\left\{\mathbf{v}_{1}\right\}$ is linearly independent and hence $S$ is a basis for $V$.

@col
Otherwise, let $\mathbf{v}_{2}\in V$ but not in $\left< \left\{\mathbf{v}_{1}\right\}\right>$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$ is linearly independent. If $V=\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}\right>$, we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}$.

@col
So $S$ is a basis for $V$.

@col
Otherwise, let $\mathbf{v}_{3}\in V$ but not in $\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2}\right\}\right>$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$ is linearly independent. Repeat the above process, inductive we can define $\mathbf{v}_{k+1}$ as following: If $V=\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right>$,
we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, $S$ is a basis for $V$.

@col
Otherwise defined $\mathbf{v}_{k+1}\not\in\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right>$.

@col
By the previous lemma, $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k+1}\right\}$ is linearly independent.

@col
If the process stops, say at step $k$, i.e., $V=\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}\right>$.

@col
Then we can take $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$.

@col
Because $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}\right\}$ is linearly independent, it is a basis for $V$.

@col
This completes the proof.

@col
Otherwise, the process continues infinitely, in particular, we can take $k=m+1$ and
$V\neq\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}\right>$ and
$\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ is linearly independent.

@col
Since $\left\langle\left\{\mathbf{e}_{1},\ldots,\mathbf{e}_{m}\right\}\right\rangle = \mathbb{R}^m$,
by @ref{SSLD} the vectors $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{m+1}\right\}$ are linearly dependent. Contradiction.
@qed
@end
@slide
@prop
@label{basisexistsprop}
Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}\subseteq{\mathbb{R}}^{m}$. Then
\begin{align*}
\displaystyle \dim\left< S\right>\leq n.
\end{align*}
@end
@proof
@col
By Theorem    @ref{basisexists}, there exists a subset $T$ of $S$ such that $T$ is a basis for $\left< S\right>$.
\begin{align*}
\displaystyle \dim\left< S\right>
=
\text{number of vectors in $T$}
\leq
\text{number of vectors in $S$}=n.
\end{align*}
@qed
@end
@remark
@col
Both @ref{basisexists} and @ref{basisexistsprop} is valid
if we replace ${\mathbb{R}}^{m}$ by $P_{n}$, $M_{mn}$
or any finite dimensional vector space.
@end

@slide
@thm
Suppose a vector space $V$ has dimension $n$.
Then, any linearly independent set with $n$ vectors in $V$
is a basis for $V$.
@end
@thm
Suppose a vector space $V$ has dimension $n$.
Suppose $S$ is a set of $n$ vectors in $V$ which spans $V$
(That is, $\left\langle S \right\rangle = V$).

Then, $S$ is a basis for $V$.
@end

@section{Rank and nullity of a matrix}
@defn
@title{Nullity of a matrix}
@label{NOM}
Suppose that $A \in M_{mn}$. Then the <b>nullity</b> of $A$ is the dimension of the null space of $A$,
$\nullity{A}=\dim(\nsp{A})$.
@end

@defn
@title{Rank of a matrix}
@label{ROM}
Suppose that $A \in M_{mn}$. Then the <b>rank</b> of $A$ is the dimension of the column space of $A$,
$\rank{A}=\dim(\csp{A})$.
@end

@slide
@eg
<b>Rank and nullity of a matrix</b>

Let us compute the rank and nullity of
\[
A=\begin{bmatrix}
2 & -4 & -1 & 3 & 2 & 1 & -4\\
1 & -2 & 0 & 0 & 4 & 0 & 1\\
-2 & 4 & 1 & 0 & -5 & -4 & -8\\
1 & -2 & 1 & 1 & 6 & 1 & -3\\
2 & -4 & -1 & 1 & 4 & -2 & -1\\
-1 & 2 & 3 & -1 & 6 & 3 & -1
\end{bmatrix}
\]
@col
To do this, we will first row-reduce the matrix since that will help us determine bases for the null space and column space.
\[
\begin{bmatrix}
\leading{1} & -2 & 0 & 0 & 4 & 0 & 1\\
0 & 0 & \leading{1} & 0 & 3 & 0 & -2\\
0 & 0 & 0 & \leading{1} & -1 & 0 & -3\\
0 & 0 & 0 & 0 & 0 & \leading{1} & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
@col
From this row-equivalent matrix in reduced row-echelon form we record $D=\set{1,\,3,\,4,\,6}$ and $F=\set{2,\,5,\,7}$.

@col
By @ref{BCS}, for each index in $D$, we can create a single basis vector.
In fact $T=\{\vect{A}_1, \vect{A}_3, \vect{A}_4, \vect{A}_6\}$ is a basis for $\csp{A}$.
In total the basis will have $4$ vectors, so the column space of $A$ will have dimension $4$ and we write $\rank{A}=4$.

@col
By @ref{SSNS}, for each index in $F$, we can create a single basis vector.  In total the basis will have $3$ vectors, so the null space of $A$ will have dimension $3$ and we write $\nullity{A}=3$. In fact:

@col
\[
R = \left\{
\colvector{ 2 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0},
\colvector{ -4 \\ 0 \\ -3 \\ 1 \\ 1 \\ 0 \\ 0},
\colvector{-1 \\ 0 \\ 2 \\ 3 \\ 0 \\ -1 \\ 1}
\right\}
\]
is a basis for $\nsp{A}$.
@end

@slide
@thm
@title{Computing rank and nullity}
Suppose $A \in M_{mn}$ and $A \rref B$.
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).
Then $\rank{A}=r$ and $\nullity{A}=n-r$.
@end
@proof
@col
Let $D=\{d_1, \ldots, d_r\}$ be the indexes of the pivot columns of $B$.
By @ref{BCS}, $\{\vect{A}_{d_1}, \ldots, \vect{A}_{d_r}\}$ is a basis for $\csp{A}$.
So $\rank{A}=r$.

By @ref{SSNS}, each free variable corresponding to a single basis vector for the null space.
So $\nullity{A}$ is the number of free variables $=n-r$.
@end

@cor
@title{Dimension formula}
Suppose $A\in M_{mn}$, then
\begin{align*}
\displaystyle r\left(A\right)+n\left(A\right)=n.
\end{align*}
@end
@slide
@thm
Let $A$ be a $m\times n$ matrix. Then
\begin{align*}
\displaystyle r\left(A\right)=r\left(A^{t}\right).
\end{align*}
@col
Equivalently
\begin{align*}
\displaystyle \dim\mathcal{C}\!\left(A\right)=\dim\mathcal{R}\!\left(A\right).
\end{align*}
@end
@proof
@col
Let $A\xrightarrow{\text{RREF}}B$.

@col
Let $r$ denote the number of pivot columns ($=$ number of nonzero rows).

@col
Then by the above discussion $r=r\left(A\right)$. By @ref{BRS}, the first $r$ columns of $B^{t}$ form a basis for $\mathcal{R}\!\left(A\right) = \mathcal{C}(A^t)$.
Hence $r=r\left(A^{t}\right)$. This completes the proof.
@qed
@end
Let us take a look at the rank and nullity of a square matrix.
@slide
@eg
The matrix
\begin{align*}
\displaystyle E=\begin{bmatrix}0&4&-1&2&2&3&1\\
2&-2&1&-1&0&-4&-3\\
-2&-3&9&-3&9&-1&9\\
-3&-4&9&4&-1&6&-2\\
-3&-4&6&-2&5&9&-4\\
9&-3&8&-2&-4&2&4\\
8&2&2&9&3&0&9\end{bmatrix}
\end{align*}
@col
is row-equivalent to the matrix in reduced row-echelon form,
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&0&0&0&0&0\\
0&\boxed{1}&0&0&0&0&0\\
0&0&\boxed{1}&0&0&0&0\\
0&0&0&\boxed{1}&0&0&0\\
0&0&0&0&\boxed{1}&0&0\\
0&0&0&0&0&\boxed{1}&0\\
0&0&0&0&0&0&\boxed{1}\end{bmatrix}
\end{align*}
@col
With $n=7$ columns and $r=7$ nonzero rows tells us the rank is $r\left(E\right)=7$ and the nullity is $n\left(E\right)=7-7=0$.
@end
The value of either the nullity or the rank are enough to characterize a nonsingular matrix.


@slide
@thm
@title{Rank and Nullity of a Nonsingular Matrix}
@label{RNNM}
Suppose that $A$ is a square matrix of size $n$. The following are equivalent.
<ol class="ltx_enumerate">
<li class="ltx_item">
A is nonsingular.
</li>
<li class="ltx_item">
The rank of $A$ is $n$, $r\left(A\right)=n$.
</li>
<li class="ltx_item">
The nullity of $A$ is zero, $n\left(A\right)=0$.
</li>
</ol>
@end
@proof
@col
(1 $\Rightarrow$ 2)
@newcol
If $A$ is nonsingular then $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$.

@col
If $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$, then the column space has dimension $n$, so the rank of $A$ is $n$.
@endcol

(2 $\Rightarrow$ 3)
@newcol
Suppose $r\left(A\right)=n$. Then the dimension formula gives
\begin{align*}
\displaystyle n\left(A\right)&=n-r\left(A\right) \\
&=n-n \\
&=0
\end{align*}
@endcol

(3 $\Rightarrow$ 1)
@newcol
Suppose $n\left(A\right)=0$, so a basis for the null space of $A$ is the empty set. This implies that ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$ and hence $A$ is nonsingular.
@endcol
@qed
@end
With a new equivalence for a nonsingular matrix, we can update our list of equivalences which now becomes a list requiring double digits to number.
@slide
@thm
Suppose that $A$ is a square matrix of size $n$. The following are equivalent.
<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is nonsingular.
</li>
<li class="ltx_item">
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item">
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\left\{\mathbf{0}\right\}$.
</li>
<li class="ltx_item">
The linear system $\mathcal{LS}({A},{\mathbf{b}})$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>
<li class="ltx_item">
The columns of $A$ are a linearly independent set.
</li>
<li class="ltx_item">
$A$ is invertible.
</li>
<li class="ltx_item">
The column space of $A$ is ${\mathbb{R}}^{n}$, $\mathcal{C}\!\left(A\right)={\mathbb{R}}^{n}$.
</li>
<li class="ltx_item">
The columns of $A$ are a basis for ${\mathbb{R}}^{n}$.
</li>
<li class="ltx_item">
The rank of $A$ is $n$, $r\left(A\right)=n$.
</li>
<li class="ltx_item">
The nullity of $A$ is zero, $n\left(A\right)=0$.
</li>
</ol>
@end
@section{Linear relation of $P_n$ and $M_{mn}$}
<strong>You can skip this section. It is for math major only</strong>

@col
In this section, we discuss the linear relation of $P_{n}$ or $M_{mn}$ by using the techniques used for the vector space ${\mathbb{R}}^{k}$.

@col
Let $V=P_{n}$ and $f_{1},\ldots,f_{m},g\in P_{n}$.

@col
Write


\begin{align*}
\displaystyle f_{i}(x)=a_{i0}+a_{i1}x+\cdots+a_{in}x^{n},
\end{align*}
\begin{align*}
\displaystyle g(x)=b_{0}+b_{1}x+\cdots+b_{n}x^{n}.
\end{align*}
@col
By comparing coefficients,
\begin{align*}
\displaystyle g(x)=\alpha_{1}f_{1}(x)+\alpha_{2}f_{2}(x)+\cdots+\alpha_{m}f_{m}(x)
\end{align*}
@col
if and only if
\begin{align*}
\displaystyle \alpha_{1}a_{10}+\alpha_{2}a_{20}+\cdots+\alpha_{m}a_{m0}=b_{0},
\end{align*}
\begin{align*}
\displaystyle \alpha_{1}a_{11}+\alpha_{2}a_{21}+\cdots+\alpha_{m}a_{m1}=b_{1},
\end{align*}
\begin{align*}
\displaystyle \vdots
\end{align*}
\begin{align*}
\displaystyle \alpha_{1}a_{1n}+\alpha_{2}a_{2n}+\cdots+\alpha_{m}a_{mn}=b_{n}
\end{align*}
@col
if and only if
\begin{align*}
\displaystyle \alpha_{1}\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix}+\alpha_{2}\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix}+\cdots+\alpha_{m}\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*}
@col
The above motivates us to define
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}a_{10}\\
a_{11}\\
\vdots\\
a_{1n}\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}a_{20}\\
a_{21}\\
\vdots\\
a_{2n}\end{bmatrix},\cdots,\mathbf{v}_{m}=\begin{bmatrix}a_{m0}\\
a_{m1}\\
\vdots\\
a_{mn}\end{bmatrix},\mathbf{u}=\begin{bmatrix}b_{0}\\
b_{1}\\
\vdots\\
b_{n}\end{bmatrix}.
\end{align*}
@col
The entries of $\mathbf{v}_{i}$ are the coefficients of $f_{i}$.

@slide
We then have the following theorem:
@thm
<ol class="ltx_enumerate">
<li class="ltx_item">
$\left\{f_{1},\ldots,f_{m}\right\}$ is linearly independent if and only if $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{m}\right\}$
is linearly independent.
</li>
<li class="ltx_item">
$g$ is a linearly combination of $f_{1},\ldots,f_{m}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{m}$
</li>
</ol>
@end
Problems regarding polynomials can therefore be transformed to problems regarding column vectors.

@col
Similarly given $m\times n$ matrix $A_{1},\ldots,A_{k},B$. Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}[A_{1}]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}[A_{2}]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix},\cdots,\mathbf{u}=\begin{bmatrix}[B]_{11}\\
\vdots\\
_{m1}\\
_{12}\\
\vdots\\
_{m2}\\
\vdots\\
_{1n}\\
\vdots\\
_{mn}\end{bmatrix}.
\end{align*}
We have the following:
@col
@thm
<ol class="ltx_enumerate">
<li class="ltx_item">
$\left\{A_{1},\ldots,A_{k}\right\}$ is linearly independent if and only if $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$
is linearly independent.
</li>
<li class="ltx_item">
$B$ is a linearly combination of $A_{1},\ldots,A_{k}$
if and only if $\mathbf{u}$ is a linear combination of $\mathbf{v}_{1},\ldots,\mathbf{v}_{k}$
</li>
</ol>
@end
@col
Again, problems regarding polynomials can be transformed to problems regarding column vectors.
@slide
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
Determine if
\begin{align*}
\displaystyle A_{1}=\begin{bmatrix}1&2\\
3&4\end{bmatrix},A_{2}=\begin{bmatrix}1&-1\\
5&6\end{bmatrix},A_{3}=\begin{bmatrix}-2&0\\
-3&-4\end{bmatrix}
\end{align*}
is linearly independent or not.
</li>
<li class="ltx_item">
Express
\begin{align*}
\displaystyle B=\begin{bmatrix}-3&0\\
4&1\end{bmatrix}
\end{align*}
@col
as a linear combination of $A_{1},A_{2},A_{3}$.
</li>
</ol>
@end
@sol
<ol class="ltx_enumerate">
<li class="ltx_item">
Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
3\\
2\\
4\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
5\\
-1\\
6\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}-2\\
-3\\
0\\
-4\end{bmatrix},\mathbf{u}=\begin{bmatrix}-3\\
4\\
0\\
1\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}]=\begin{bmatrix}1&1&-2\\
3&5&-3\\
2&-1&0\\
4&6&-4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&0\\
0&1&0\\
0&0&1\\
0&0&0\\
\end{bmatrix}.
\end{align*}
@col
Obviously the columns of the RREF is linearly independent, hence $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$
is linearly independent. Therefore $\left\{A_{1},A_{2},A_{3}\right\}$ is linearly independent.
</li>
<li class="ltx_item">
Next
\begin{align*}
\displaystyle [\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{b}]=\begin{bmatrix}1&1&-2&-3\\
3&5&-3&4\\
2&-1&0&0\\
4&6&-5&1\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&0&1\\
0&1&0&2\\
0&0&1&3\\
0&0&0&0\\
\end{bmatrix}.
\end{align*}
@col
Then $\mathbf{u}=\mathbf{v}_{1}+2\mathbf{v}_{2}+3\mathbf{v}_{3}$. Hence $B=A_{1}+2A_{2}+3A_{3}$.
</li>
</ol>
@qed
@end
@slide
@eg
Let $f_{1}(x)=1+x+x^{3}$, $f_{2}(x)=2+x+x^{2}$, $f_{3}(x)=4+3x+x^{2}+2x^{3}$, $f_{4}(x)=2x^{2}+x^{3}$, $f_{5}(x)=3+2x+3x^{2}+2x^{3}$.

@col
Find a basis for $\left< \left\{f_{1},f_{2},f_{3},f_{4},f_{5}\right\}\right>$.
@end
@sol
Let
\begin{align*}
\displaystyle \mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\\
1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}2\\
1\\
1\\
0\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}4\\
3\\
1\\
2\end{bmatrix},\mathbf{v}_{4}=\begin{bmatrix}0\\
0\\
2\\
1\end{bmatrix},\mathbf{v}_{5}=\begin{bmatrix}3\\
2\\
3\\
2\end{bmatrix}.
\end{align*}
@col
Then
\begin{align*}
\displaystyle A=[\mathbf{v}_{1}|\mathbf{v}_{2}|\mathbf{v}_{3}|\mathbf{v}_{4}|\mathbf{v}_{5}]=\begin{bmatrix}1&2&4&0&3\\
1&1&3&0&2\\
0&1&1&2&3\\
1&0&2&1&2\\
\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&2&0&1\\
0&1&1&0&1\\
0&0&0&1&1\\
0&0&0&0&0\\
\end{bmatrix}.
\end{align*}
@col
Therefore $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{4}\right\}$ is a basis for $\left< \left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3},\mathbf{v}_{4},\mathbf{v}_{5}\right\}\right>$.

@col
So $\left\{f_{1},f_{2},f_{4}\right\}$ is a basis for $\left< \left\{f_{1},f_{2},f_{3},f_{4},f_{5}\right\}\right>$.
@qed
@end
@setchapter{16}
@chapter{Linear transformations and change of basis}

<b>Skip this lecture</b>

<b>Warning</b>: the note is for reference only. It may contain typos. Read at your own risk. The lecture is based on Beezer, A first course in Linear algebra. Ver 3.5 Downloadable at http://linear.ups.edu/download.html.The print version can be downloaded at http://linear.ups.edu/download/fcla-3.50-print.pdf.

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section VR.

@section{Matrices as linear transformation}

Let $A\in M_{mn}$, $\mathbf{v}\in{\mathbb{R}}^{n}$. Then $A\mathbf{v}\in{\mathbb{R}}^{m}$. We can define define a map, denoted by $L_{A}$ (stand for left multiplying), defined as
\begin{align*}
\displaystyle L_{A}:{\mathbb{R}}^{n}\rightarrow{\mathbb{R}}^{m}.
\end{align*}
\begin{align*}
\displaystyle L_{A}(\mathbf{v})=A\mathbf{v}.
\end{align*}
@slide
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&3\\
4&5&6\end{bmatrix}.
\end{align*}
@newcol
Then $L_{A}$ is a map from ${\mathbb{R}}^{3}\rightarrow{\mathbb{R}}^{2}$. Define by
\begin{align*}
\displaystyle L_{A}(\begin{bmatrix}x\\
y\\
z\end{bmatrix})=\begin{bmatrix}x+2y+3z\\
4x+5y+6z\end{bmatrix}.
\end{align*}
@endcol
@end
@newcol
The map $L_{A}$ has the following properties:

@endcol
@slide
@prop
For $\mathbf{v},\mathbf{u}\in{\mathbb{R}}^{n}$, $\alpha\in{\mathbb{R}}^{\hbox{}}$

<ol class="ltx_enumerate">
<li class="ltx_item">
$L_{A}(\mathbf{v}+\mathbf{u})=L_{A}(\mathbf{v})+L_{A}(\mathbf{u})$
</li>
<li class="ltx_item">
$L_{A}(\alpha\mathbf{v})=\alpha L_{A}(\mathbf{v})$.
</li>

</ol>
@end
@proof
@newcol
<ol class="ltx_enumerate">
<li class="ltx_item">
$L_{A}(\mathbf{v}+\mathbf{u})=A(\mathbf{v}+\mathbf{u})=A\mathbf{v}+A\mathbf{u}=L_{A}(\mathbf{v})+L_{A}(\mathbf{u})$
</li>
<li class="ltx_item">
$L_{A}(\alpha\mathbf{v})=A(\alpha\mathbf{v})=\alpha A(\mathbf{v})=\alpha L_{A}(\mathbf{v})$.
</li>

</ol>

@qed
@endcol
@end
@slide
@defn
Let $V,W$ be vector spaces (you can think of $V$ as subspace of ${\mathbb{R}}^{a}$ and $W$ as subspace of ${\mathbb{R}}^{b}$),
$T:V\rightarrow W$.
Then $T$ is said to be a linear transformation if for $\alpha\in{\mathbb{R}}^{\hbox{}}$, $\mathbf{v},\mathbf{u}\in W$.

<ol class="ltx_enumerate">
<li class="ltx_item">
$T(\mathbf{v}+\mathbf{u})=T(\mathbf{v})+T(\mathbf{u})$
</li>
<li class="ltx_item">
$T(\alpha\mathbf{v})=\alpha T(\mathbf{v})$.
</li>

</ol>
@end
@section{Geometric interpretation of some linear transformations}

In below we regard $\begin{bmatrix}x\\
y\end{bmatrix}$ as the point $(x,y)$ in the $xy$-plane.
@slide
@eg
<b>Rotation</b>

@newcol
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}\cos\theta&-\sin\theta\\
\sin\theta&\cos\theta\end{bmatrix}.
\end{align*}
@col
Then $L_{A}$ is $\theta$ degree anticlockwise rotation.
@endcol
@end
@eg
<b>Reflection</b>

@newcol
Let $c$ be a real number,
\begin{align*}
\displaystyle A=\begin{bmatrix}1&0\\
0&-1\end{bmatrix}\,\,\,(\text{resp.}A=\begin{bmatrix}-1&0\\
0&1\end{bmatrix})
\end{align*}
@col
Then $L_{A}$ is a reflection through the $x$-axis (resp. $y$-axis).
@endcol
@end
@eg
<b>Stretching</b>

@newcol
Let $c$ be real number, then
\begin{align*}
\displaystyle A=\begin{bmatrix}k&0\\
0&1\end{bmatrix}\,\,\,(\text{resp.}A=\begin{bmatrix}1&0\\
0&k\end{bmatrix})
\end{align*}
@col
Then $L_{A}$ is a stretching of $x$-axis (resp. $y$-axis) by a factor of $k$.
@endcol
@end
@section{Coordinate vector and change of basis}

Let $V$ be a vector space(you can think of it as a subspace of ${\mathbb{R}}^{n}$). Let $B=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ be a basis for $V$. Recall the following theorem

@slide
@thm
Let $\mathbf{v}\in V$ and if
\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\,\,\alpha_{i}\in{\mathbb{R}}^{\hbox{}}&
\end{align*}
then $\alpha_{1},\cdots,\alpha_{k}$ are unique, i.e.
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\beta_{1}\mathbf{v}_{1}+\cdots+\beta_{k}\mathbf{v}_{k},&
\end{align*}
then
\begin{align*}
\displaystyle \alpha_{i}=\beta_{i},\,\,\,i=1,\ldots,k.
\end{align*}
@end
@proof
@newcol
If (  @ref{vlin2}) is true, then
\begin{align*}
\displaystyle (\alpha_{1}-\beta_{1})\mathbf{v}_{1}+\cdots+(\alpha_{k}-\beta_{k})\mathbf{v}_{k}=\mathbf{0}.
\end{align*}
@col
Because $S$ is a basis, it is linearly independent. Therefore
\begin{align*}
\displaystyle \alpha_{i}-\beta_{i}=0.
\end{align*}
@col
Hence
\begin{align*}
\displaystyle \alpha_{i}=\beta_{i}.
\end{align*}
@qed
@endcol
@end
@newcol
By the theorem, using notation in (  @ref{vlin}), we can define
\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\begin{bmatrix}\alpha_{1}\\
\alpha_{2}\\
\vdots\\
\alpha_{k}\end{bmatrix}.
\end{align*}
@endcol
@slide
@eg
Let $V={\mathbb{R}}^{3}$, $B=\{\mathbf{e}_{1},\mathbf{e}_{2},\mathbf{e}_{3}\}$.
Then
\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}x\\
y\\
z\end{bmatrix}=x\mathbf{e}_{1}+y\mathbf{e}_{2}+z\mathbf{e}_{3}.
\end{align*}
@newcol
Hence
\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\begin{bmatrix}x\\
y\\
z\end{bmatrix}.
\end{align*}
@endcol
@end
@eg
Generally if $V={\mathbb{R}}^{n}$, $B=\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}$, then
\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\mathbf{v}.
\end{align*}
@end
@eg
Let $V={\mathbb{R}}^{3}$,
\begin{align*}
\displaystyle B=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}1\\
2\\
3\end{bmatrix}\}.
\end{align*}
@newcol
We can show that $B$ is linearly independent (exercise). Hence $B$ is a basis for $V$.
Let
\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}3\\
4\\
3\end{bmatrix}.
\end{align*}
@col
Find $\rho_{B}(\mathbf{u})$. To find $\rho_{B}(\mathbf{u})$, we compute the coefficient
\begin{align*}
\displaystyle \mathbf{u}=x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3},
\end{align*}
i.e.
\begin{align*}
\displaystyle x_{1}+x_{2}+x_{3}&=3 \\
\displaystyle x_{1}+x+2+2x_{3}&=4 \\
\displaystyle x+2+3x_{3}&=3
\end{align*}
@col
Solve the system of linear equations, we have $x_{1}=2$, $x_{2}=0$, $x_{3}=1$. Hence
\begin{align*}
\displaystyle \rho_{B}(\mathbf{u})=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}=\begin{bmatrix}2\\
0\\
1\end{bmatrix}.
\end{align*}
@endcol
@end
@eg
Let
\begin{align*}
\displaystyle B=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
2\\
3\\
4\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
-1\\
1\\
-1\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}-1\\
1\\
2\\
2\end{bmatrix}.\}.
\end{align*}
@newcol
Let
\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}0\\
3\\
11\\
8\end{bmatrix}.
\end{align*}
<ol class="ltx_enumerate">
<li class="ltx_item">
Show that $\mathbf{u}\in\left<B\right>$.
</li>
<li class="ltx_item">
Hence find $\rho_{B}(\mathbf{u})$.
</li>

</ol>

@col
Find the solution of $\mathbf{u}=x_{1}\mathbf{v}_{1}+x_{2}\mathbf{v}_{2}+x_{3}\mathbf{v}_{3}$:
\begin{align*}
\displaystyle x_{1}+x_{2}-x_{3}&=0 \\
\displaystyle 2x_{1}-x_{2}+x_{3}&=3 \\
\displaystyle 3x_{1}+x_{2}+2x_{3}&=11 \\
\displaystyle 4x_{1}-x_{2}+2x_{3}&=8
\end{align*}
@col
We find solution $x_{1}=1,x_{2}=2,x_{3}=3$. Hence
\begin{align*}
\displaystyle \rho_{B}(\mathbf{u})=\begin{bmatrix}1\\
2\\
3\end{bmatrix}.
\end{align*}
@endcol
@end
@section{Change of basis}

Let $V$ be a vector space(you can think of it as a subspace of ${\mathbb{R}}^{n}$). Let $B=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\}$ be a basis for $V$
and $B^{\prime}=\{\mathbf{v}^{\prime}_{1},\ldots,\mathbf{v}^{\prime}_{k}\}$ be another basis for $V$. For $\mathbf{v}\in V$. We want to find the relation between $\rho_{B}(\mathbf{v})$ and $\rho_{B^{\prime}}(\mathbf{v})$.

@newcol
Let
\begin{align*}
\displaystyle \mathbf{v}_{i}=a_{1i}\mathbf{v}^{\prime}_{1}+a_{2i}\mathbf{v}^{\prime}_{2}+\cdots+a_{ki}\mathbf{v}^{\prime}_{k},&
\end{align*}
i.e.
\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v}_{i})=\begin{bmatrix}a_{1i}\\
a_{2i}\\
\vdots\\
a_{ki}\end{bmatrix}.
\end{align*}
@col
Define the <b>change-of-basis matrix from $S$ to $S^{\prime}$</b> as
\begin{align*}
\displaystyle C_{{B},{B^{\prime}}}=[\rho_{B^{\prime}}(\mathbf{v}_{1})|\rho_{B^{\prime}}(\mathbf{v}_{2})|\cdots|\rho_{B^{\prime}}(\mathbf{v}_{k})]=\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1k}\\
a_{21}&a_{22}&\cdots&a_{2k}\\
\vdots&\vdots&\vdots&\vdots\\
a_{k1}&a_{k2}&\cdots&a_{kk}\end{bmatrix}.
\end{align*}
@endcol
@slide
@thm
For $\mathbf{v}\in V$,
\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v})=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{v}).
\end{align*}
@end
@proof
@newcol
Let
\begin{align*}
\displaystyle \rho_{B}(\mathbf{v})=\begin{bmatrix}\alpha_{1}\\
\alpha_{2}\\
\vdots\\
\alpha_{k}\end{bmatrix},\,\rho_{B^{\prime}}(\mathbf{v})=\begin{bmatrix}\alpha^{\prime}_{1}\\
\alpha^{\prime}_{2}\\
\vdots\\
\alpha^{\prime}_{k}\end{bmatrix},
\end{align*}
i.e.
\begin{align*}
\displaystyle\mathbf{v}&=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\cdots+\alpha_{k}\mathbf{v}_{k} \\
&=\alpha^{\prime}_{1}\mathbf{v}^{\prime}_{1}+\alpha^{\prime}_{2}\mathbf{v}^{\prime}_{2}+\cdots+\alpha^{\prime}_{k}\mathbf{v}^{\prime}_{k}.
\end{align*}
@col
By   @ref{vv'},
\begin{align*}
\displaystyle\mathbf{v}&=\alpha_{1}\mathbf{v}_{1}+\alpha_{2}\mathbf{v}_{2}+\cdots+\alpha_{k}\mathbf{v}_{k} \\
&=\alpha_{1}(a_{11}\mathbf{v}^{\prime}_{1}+a_{21}\mathbf{v}^{\prime}_{2}+\cdots+a_{k1}\mathbf{v}^{\prime}_{k}) \\
&+\alpha_{2}(a_{12}\mathbf{v}^{\prime}_{1}+a_{22}\mathbf{v}^{\prime}_{2}+\cdots+a_{k2}\mathbf{v}^{\prime}_{k}) \\
&+\cdots \\
&+\alpha_{2}(a_{1k}\mathbf{v}^{\prime}_{1}+a_{2k}\mathbf{v}^{\prime}_{2}+\cdots+a_{kk}\mathbf{v}^{\prime}_{k}) \\
&=(a_{11}\alpha_{1}+a_{12}\alpha_{2}+\cdots+a_{1k}\alpha_{k})\mathbf{v}^{\prime}_{1} \\
&+(a_{21}\alpha_{1}+a_{22}\alpha_{2}+\cdots+a_{2k}\alpha_{k})\mathbf{v}^{\prime}_{2} \\
&+\cdots \\
&+(a_{k1}\alpha_{1}+a_{k2}\alpha_{2}+\cdots+a_{kk}\alpha_{k})\mathbf{v}^{\prime}_{2}
\end{align*}
@col
Thus
\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v})=\begin{bmatrix}\alpha^{\prime}_{1}\\
\alpha^{\prime}_{2}\\
\vdots\\
\alpha^{\prime}_{k}\end{bmatrix}=\begin{bmatrix}a_{11}\alpha_{1}+a_{12}\alpha_{2}+\cdots+a_{1k}\alpha_{k}\\
a_{21}\alpha_{1}+a_{22}\alpha_{2}+\cdots+a_{2k}\alpha_{k}\\
\vdots\\
a_{k1}\alpha_{1}+a_{k2}\alpha_{2}+\cdots+a_{kk}\alpha_{k}\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle =\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1k}\\
a_{21}&a_{22}&\cdots&a_{2k}\\
\vdots&\vdots&\vdots&\vdots\\
a_{k1}&a_{k2}&\cdots&a_{kk}\end{bmatrix}\begin{bmatrix}\alpha_{1}\\
\alpha_{2}\\
\vdots\\
\alpha_{k}\end{bmatrix}=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{v})
\end{align*}
@qed
@endcol
@end
@slide
@eg
In example 7,
let $V={\mathbb{R}}^{3}$,
\begin{align*}
\displaystyle B=\{\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
0\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}1\\
2\\
3\end{bmatrix}\},
\end{align*}
\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}3\\
4\\
3\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle \rho_{B}(\mathbf{u})=\begin{bmatrix}2\\
0\\
1\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle B^{\prime}=\{\mathbf{v}^{\prime}_{1}=\begin{bmatrix}0\\
0\\
1\end{bmatrix},\mathbf{v}^{\prime}_{2}=\begin{bmatrix}1\\
0\\
1\end{bmatrix},\mathbf{v}^{\prime}_{3}=\begin{bmatrix}1\\
1\\
-1\end{bmatrix}\}.
\end{align*}
@col
Then
\begin{align*}
\displaystyle\mathbf{v}_{1}&=\mathbf{v}^{\prime}_{1}+\mathbf{v}^{\prime}_{3} \\
\displaystyle\mathbf{v}_{2}&=2\mathbf{v}^{\prime}_{1}+\mathbf{v}^{\prime}_{3} \\
\displaystyle\mathbf{v}_{3}&=6\mathbf{v}^{\prime}_{1}-\mathbf{v}^{\prime}_{2}+2\mathbf{v}^{\prime}_{3}.
\end{align*}
@col
Then
\begin{align*}
\displaystyle C_{{B},{B^{\prime}}}=\begin{bmatrix}1&2&6\\
0&0&-1\\
1&1&2\end{bmatrix}
\end{align*}
@col
So
\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{u})=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{u})=\begin{bmatrix}1&2&6\\
0&0&-1\\
1&1&2\end{bmatrix}\begin{bmatrix}2\\
0\\
1\end{bmatrix}=\begin{bmatrix}8\\
-1\\
4\end{bmatrix}.
\end{align*}
@col
Verification:
\begin{align*}
\displaystyle \mathbf{u}=8\mathbf{v}^{\prime}_{1}-\mathbf{v}^{\prime}_{2}+4\mathbf{v}^{\prime}_{3},
\end{align*}
i.e.
\begin{align*}
\displaystyle \begin{bmatrix}3\\
4\\
3\end{bmatrix}=8\begin{bmatrix}0\\
0\\
1\end{bmatrix}-\begin{bmatrix}1\\
0\\
1\end{bmatrix}+4\begin{bmatrix}1\\
1\\
-1\end{bmatrix}.
\end{align*}
@endcol
@end
@section{Similar matrices}

In this section, we will give the motivation of <b>similar matrices</b>.
Let $V={\mathbb{R}}^{n}$, $B=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\}$ be a basis for $V$.
Let $B^{\prime}=\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}$. Let $Q=C_{{B},{B^{\prime}}}$.
Then for any $\mathbf{v}\in V$,
\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{v})=C_{{B},{B^{\prime}}}\rho_{B}(\mathbf{v})
\end{align*}
\begin{align*}
\displaystyle \mathbf{v}=Q\rho_{B}(\mathbf{v}).
\end{align*}
@newcol
Let $\mathbf{w}=L_{A}(\mathbf{v})=A\mathbf{v}$. Then
\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{w})=\rho_{B^{\prime}}(A\mathbf{v})
\end{align*}
\begin{align*}
\displaystyle \rho_{B^{\prime}}(\mathbf{w})=A\mathbf{v}=A\rho_{B^{\prime}}(\mathbf{v})
\end{align*}
\begin{align*}
\displaystyle Q\rho_{B}(\mathbf{w})=AQ\rho_{B}(\mathbf{v}).
\end{align*}
\begin{align*}
\displaystyle \rho_{B}(\mathbf{w})=Q^{-1}AQ\rho_{B}(\mathbf{v}).
\end{align*}
@col
So we say that two matrices $A_{1},A_{2}$ are similar if there exists an invertible matrix $Q$ such that $Q^{-1}A_{1}Q=A_{2}$.
More details will be given in lecture 19.

@endcol
@setchapter{17}
@chapter{Inverse}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Section MISLE, Section MINM (print version p149 - p161)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at http://linear.ups.edu/download/fcla-3.50-solution-manual.pdfSection MISLE (p60-64), all. Section MINM C20, C40, M10, M11, M15, M80, T25.

@section{Solution Inverse}
@label{SI}
The inverse of a square matrix, and solutions to linear systems with square coefficient matrices, are intimately connected.
@slide
@eg
\begin{align*}
\displaystyle-7x_{1}-6x_{2}-12x_{3}&=-33 \\
\displaystyle 5x_{1}+5x_{2}+7x_{3}&=24 \\
\displaystyle x_{1}+4x_{3}&=5
\end{align*}
@newcol
We can represent this system of equations as
\begin{align*}
\displaystyle A\mathbf{x}=\mathbf{b}
\end{align*}
where
\begin{align*}
\displaystyle A&=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}
&
\displaystyle\mathbf{x}&=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}&\mathbf{b}&=\begin{bmatrix}-33\\
24\\
5\end{bmatrix}
\end{align*}
@col
Now, entirely unmotivated, we define the $3\times 3$ matrix $B$,
\begin{align*}
\displaystyle \begin{bmatrix}-10&-12&-9\\
\frac{13}{2}&8&\frac{11}{2}\\
\frac{5}{2}&3&\frac{5}{2}\end{bmatrix}
\end{align*}
and note the remarkable fact that
\begin{align*}
\displaystyle BA=\begin{bmatrix}1&0&0\\
0&1&0\\
0&0&1\end{bmatrix}
\end{align*}
@col
Now apply this computation to the problem of solving the system of equations,
\begin{align*}
\displaystyle\mathbf{x}=I_{3}\mathbf{x}=(BA)\mathbf{x}=B(A\mathbf{x})=B\mathbf{b}
\end{align*}
@col
So we have
\begin{align*}
\displaystyle \mathbf{x}=B\mathbf{b}=\begin{bmatrix}-3\\
5\\
2\end{bmatrix}
\end{align*}
@col
So with the help and assistance of $B$ we have been able to determine a solution to the system represented by $A\mathbf{x}=\mathbf{b}$ through judicious use of matrix multiplication. Since the coefficient matrix in this example is nonsingular, there would be a unique solution, no matter what the choice of $\mathbf{b}$. The derivation above amplifies this result, since we were forced to conclude that $\mathbf{x}=B\mathbf{b}$ and the solution could not be anything else. You should notice that this argument would hold for any particular choice of $\mathbf{b}$.
@endcol
@end
@newcol
The matrix $B$ of the previous example is called the inverse of $A$. When $A$ and $B$ are combined via matrix multiplication, the result is the identity matrix, which can be inserted <em>in front</em> of $\mathbf{x}$ as the first step in finding the solution. This is entirely analogous to how we might solve a single linear equation like $3x=12$.
\begin{align*}
\displaystyle x=1x=\left(\frac{1}{3}\left(3\right)\right)x=\frac{1}{3}\left(3x\right)=\frac{1}{3}\left(12\right)=4
\end{align*}
@col
Here we have obtained a solution by employing the <b>multiplicative inverse</b> of $3$, $3^{-1}=\frac{1}{3}$. This works fine for any scalar multiple of $x$, except for zero, since zero does not have a multiplicative inverse. Consider separately the two linear equations,
\begin{align*}
\displaystyle 0x&=12& 0x&=0
\end{align*}
@col
The first has no solutions, while the second has infinitely many solutions. For matrices, it is all just a little more complicated. Some matrices have inverses, some do not. And when a matrix does have an inverse, just how would we compute it? In other words, just where did that matrix $B$ in the last example come from? Are there other matrices that might have worked just as well?

@endcol
@section{Inverse of a Matrix}
@defn
Suppose $A$ and $B$ are square matrices of size $n$ such that $AB=I_{n}$ and $BA=I_{n}$. Then $A$ is <b>invertible</b> and $B$ is the <b>inverse</b> of $A$. In this situation, we write $B=A^{-1}$.
@end
@newcol
Notice that if $B$ is the inverse of $A$, then we can just as easily say $A$ is the inverse of $B$, or $A$ and $B$ are inverses of each other.

@col
Not every square matrix has an inverse.
@endcol
@slide
@eg
<b>A matrix without an inverse</b>

@newcol
Consider the coefficient matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}1&-1&2\\
2&1&1\\
1&1&0\end{bmatrix}
\end{align*}
@col
Suppose that $A$ is invertible and does have an inverse, say $B$. Choose the vector of constants
\begin{align*}
\displaystyle \mathbf{b}=\begin{bmatrix}1\\
3\\
2\end{bmatrix}
\end{align*}
and consider the system of equations $A\mathbf{x}=\mathbf{b}$. Just as in the previous example, this vector equation would have the unique solution $\mathbf{x}=B\mathbf{b}$.

@col
However, the system $A\mathbf{x}=\mathbf{b}$ is inconsistent. Form the augmented matrix $\left[A|\mathbf{b}\right]$ and row-reduce to
\begin{align*}
\displaystyle \begin{bmatrix}\boxed{1}&0&1&0\\
0&\boxed{1}&-1&0\\
0&0&0&\boxed{1}\end{bmatrix}
\end{align*}
which allows us to recognize the inconsistency.

@col
So the assumption of $A$’s inverse leads to a logical inconsistency (the system cannot be both consistent and inconsistent), so our assumption is false. $A$ is not invertible.
@endcol
@end
@newcol
Let us look at one more matrix inverse before we embark on a more systematic study.
@endcol
@slide
@eg
<b>Matrix inverse</b>

@newcol
<strong>1.</strong>
\begin{align*}
\displaystyle A=\left[\begin{array}[]{cc}1&2\\
2&3\\
\end{array}\right],B=\left[\begin{array}[]{cc}-3&2\\
2&-1\\
\end{array}\right],
\end{align*}
@col
Then
\begin{align*}
\displaystyle AB=BA=I_{2}.
\end{align*}
@col
So $B$ is the inverse of $A$.

@col
<strong>2.</strong>
\begin{align*}
\displaystyle A=\left[\begin{array}[]{ccc}1&1&1\\
1&0&-1\\
0&1&1\\
\end{array}\right],B=\left[\begin{array}[]{ccc}1&0&-1\\
-1&1&2\\
1&-1&-1\\
\end{array}\right]
\end{align*}
@col
Then
\begin{align*}
\displaystyle AB=BA=I_{3}.
\end{align*}
@col
So $B$ is the inverse of $A$.

<strong>3.</strong> Consider the matrices,
\begin{align*}
\displaystyle A&=\begin{bmatrix}1&2&1&2&1\\
-2&-3&0&-5&-1\\
1&1&0&2&1\\
-2&-3&-1&-3&-2\\
-1&-3&-1&-3&1\end{bmatrix}& B&=\begin{bmatrix}-3&3&6&-1&-2\\
0&-2&-5&-1&1\\
1&2&4&1&-1\\
1&0&1&1&0\\
1&-1&-2&0&1\end{bmatrix}
\end{align*}
@col
Then
\begin{align*}
\displaystyle AB&=\begin{bmatrix}1&2&1&2&1\\
-2&-3&0&-5&-1\\
1&1&0&2&1\\
-2&-3&-1&-3&-2\\
-1&-3&-1&-3&1\end{bmatrix}\begin{bmatrix}-3&3&6&-1&-2\\
0&-2&-5&-1&1\\
1&2&4&1&-1\\
1&0&1&1&0\\
1&-1&-2&0&1\end{bmatrix}=\begin{bmatrix}1&0&0&0&0\\
0&1&0&0&0\\
0&0&1&0&0\\
0&0&0&1&0\\
0&0&0&0&1\end{bmatrix} \\
\\
\displaystyle BA&=\begin{bmatrix}-3&3&6&-1&-2\\
0&-2&-5&-1&1\\
1&2&4&1&-1\\
1&0&1&1&0\\
1&-1&-2&0&1\end{bmatrix}\begin{bmatrix}1&2&1&2&1\\
-2&-3&0&-5&-1\\
1&1&0&2&1\\
-2&-3&-1&-3&-2\\
-1&-3&-1&-3&1\end{bmatrix}=\begin{bmatrix}1&0&0&0&0\\
0&1&0&0&0\\
0&0&1&0&0\\
0&0&0&1&0\\
0&0&0&0&1\end{bmatrix}
\end{align*}
so by the definition of inverse matrix, we can say that $A$ is invertible and write $B=A^{-1}$.
@endcol
@end
@newcol
We will now concern ourselves less with whether or not an inverse of a matrix exists, but instead with how you can find one when it does exist.
Later we will have some theorems that allow us to more quickly and easily determine just when a matrix is invertible.

@endcol
@section{Computing the Inverse of a Matrix}

How would we compute an inverse? And just when is a matrix invertible, and when is it not? Writing a putative inverse with $n^{2}$ unknowns and solving the resultant $n^{2}$ equations is one approach. Applying this approach to $2\times 2$ matrices can get us somewhere, so just for fun, let us do it.

@slide
@thm
@title{Two-by-Two Matrix Inverse}
@label{TTMI}
Suppose
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
c&d\end{bmatrix}
\end{align*}
@newcol
Then $A$ is invertible if and only if $ad-bc\neq 0$. When $A$ is invertible, then
\begin{align*}
\displaystyle A^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d&-b\\
-c&a\end{bmatrix}
\end{align*}
@endcol
@end
@proof

$\Leftarrow$
@newcol
Assume that $ad-bc\neq 0$. We will use the definition of the inverse of a matrix to establish that $A$ has an inverse. Note that if $ad-bc\neq 0$ then the displayed formula for $A^{-1}$ is legitimate since we are not dividing by zero).
Using this proposed formula for the inverse of $A$, we compute
\begin{align*}
\displaystyle AA^{-1}&=\begin{bmatrix}a&b\\
c&d\end{bmatrix}\left(\frac{1}{ad-bc}\begin{bmatrix}d&-b\\
-c&a\end{bmatrix}\right)=\frac{1}{ad-bc}\begin{bmatrix}ad-bc&0\\
0&ad-bc\end{bmatrix}=\begin{bmatrix}1&0\\
0&1\end{bmatrix} \\
\\
\displaystyle A^{-1}A&=\frac{1}{ad-bc}\begin{bmatrix}d&-b\\
-c&a\end{bmatrix}\begin{bmatrix}a&b\\
c&d\end{bmatrix}=\frac{1}{ad-bc}\begin{bmatrix}ad-bc&0\\
0&ad-bc\end{bmatrix}=\begin{bmatrix}1&0\\
0&1\end{bmatrix}
\end{align*}
@col
This is sufficient to establish that $A$ is invertible, and that the expression for $A^{-1}$ is correct.
@endcol

$\Rightarrow$
@newcol
Assume that $A$ is invertible, and proceed with a proof by contradiction, by assuming also that $ad-bc=0$. This translates to $ad=bc$. Let
\begin{align*}
\displaystyle B=\begin{bmatrix}e&f\\
g&h\end{bmatrix}
\end{align*}
be a putative inverse of $A$.

@col
This means that
\begin{align*}
\displaystyle I_{2}=AB=\begin{bmatrix}a&b\\
c&d\end{bmatrix}\begin{bmatrix}e&f\\
g&h\end{bmatrix}=\begin{bmatrix}ae+bg&af+bh\\
ce+dg&cf+dh\end{bmatrix}
\end{align*}
@col
Working on the matrices on two ends of this equation, we will multiply the top row by $c$ and the bottom row by $a$.
\begin{align*}
\displaystyle \begin{bmatrix}c&0\\
0&a\end{bmatrix}=\begin{bmatrix}ace+bcg&acf+bch\\
ace+adg&acf+adh\end{bmatrix}
\end{align*}
@col
We are assuming that $ad=bc$, so we can replace two occurrences of $ad$ by $bc$ in the bottom row of the right matrix.
\begin{align*}
\displaystyle \begin{bmatrix}c&0\\
0&a\end{bmatrix}=\begin{bmatrix}ace+bcg&acf+bch\\
ace+bcg&acf+bch\end{bmatrix}
\end{align*}
@col
The matrix on the right now has two rows that are identical, and therefore the same must be true of the matrix on the left. Identical rows for the matrix on the left implies that $a=0$ and $c=0$.

@col
With this information, the product $AB$ becomes
\begin{align*}
\displaystyle \begin{bmatrix}1&0\\
0&1\end{bmatrix}=I_{2}=AB=\begin{bmatrix}ae+bg&af+bh\\
ce+dg&cf+dh\end{bmatrix}=\begin{bmatrix}bg&bh\\
dg&dh\end{bmatrix}
\end{align*}
@col
So $bg=dh=1$ and thus $b,g,d,h$ are all nonzero. But then $bh$ and $dg$
(the <strong>other corners</strong>) must also be nonzero, so this is (finally) a contradiction. So our assumption was false and we see that $ad-bc\neq 0$ whenever $A$ has an inverse.
@qed
@endcol
@end
@newcol
There are several ways one could try to prove this theorem, but there is a continual temptation to divide by one of the eight entries involved ($a$ through $f$), but we can never be sure if these numbers are zero or not. This could lead to an analysis by cases, which is messy, messy, messy. Note how the above proof never divides, but always multiplies, and how zero/nonzero considerations are handled. Pay attention to the expression $ad-bc$, as we will see it again in a while.

@col
This theorem is cute, and it is nice to have a formula for the inverse, and a condition that tells us when we can use it. However, this approach becomes impractical for larger matrices, even though it is possible to demonstrate that, in theory, there is a general formula. (Think for a minute about extending this result to just $3\times 3$ matrices. For starters, we need 18 letters!) Instead, we will work column-by-column. Let us first work an example that will motivate the main theorem and remove some of the previous mystery.
@endcol
@slide
@eg
<b>Computing a matrix inverse</b>

@newcol
Consider
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&1&2&1\\
-2&-3&0&-5&-1\\
1&1&0&2&1\\
-2&-3&-1&-3&-2\\
-1&-3&-1&-3&1\end{bmatrix}
\end{align*}
@col
For its inverse, we desire a matrix $B$ so that $AB=I_{5}$. Emphasizing the structure of the columns and employing the definition of matrix multiplication Let $A$ be as defined in Example 3.
\begin{align*}
\displaystyle AB&=I_{5} \\
\displaystyle A[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}|\mathbf{B}_{4}|\mathbf{B}_{5}]&=[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}|\mathbf{e}_{4}|\mathbf{e}_{5}] \\
\displaystyle[A\mathbf{B}_{1}|A\mathbf{B}_{2}|A\mathbf{B}_{3}|A\mathbf{B}_{4}|A\mathbf{B}_{5}]&=[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}|\mathbf{e}_{4}|\mathbf{e}_{5}]
\end{align*}
@col
Equating the matrices column-by-column we have
\begin{align*}
\displaystyle A\mathbf{B}_{1}&=\mathbf{e}_{1}& A\mathbf{B}_{2}&=\mathbf{e}_{2}& A\mathbf{B}_{3}&=\mathbf{e}_{3}& A\mathbf{B}_{4}&=\mathbf{e}_{4}& A\mathbf{B}_{5}&=\mathbf{e}_{5}.
\end{align*}
@col
Since the matrix $B$ is what we are trying to compute, we can view each column, $\mathbf{B}_{i}$, as a column vector of unknowns. Then we have five systems of equations to solve, each with 5 equations in 5 variables. Notice that all 5 of these systems have the same coefficient matrix. We will now solve each system in turn,
\begin{align*}
\\
\displaystyle\begin{bmatrix}1&2&1&2&1&1\\
-2&-3&0&-5&-1&0\\
1&1&0&2&1&0\\
-2&-3&-1&-3&-2&0\\
-1&-3&-1&-3&1&0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&0&-3\\
0&\boxed{1}&0&0&0&0\\
0&0&\boxed{1}&0&0&1\\
0&0&0&\boxed{1}&0&1\\
0&0&0&0&\boxed{1}&1\end{bmatrix};\mathbf{B}_{1}=\begin{bmatrix}-3\\
0\\
1\\
1\\
1\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&2&1&2&1&0\\
-2&-3&0&-5&-1&1\\
1&1&0&2&1&0\\
-2&-3&-1&-3&-2&0\\
-1&-3&-1&-3&1&0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&0&3\\
0&\boxed{1}&0&0&0&-2\\
0&0&\boxed{1}&0&0&2\\
0&0&0&\boxed{1}&0&0\\
0&0&0&0&\boxed{1}&-1\end{bmatrix};\mathbf{B}_{2}=\begin{bmatrix}3\\
-2\\
2\\
0\\
-1\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&2&1&2&1&0\\
-2&-3&0&-5&-1&0\\
1&1&0&2&1&1\\
-2&-3&-1&-3&-2&0\\
-1&-3&-1&-3&1&0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&0&6\\
0&\boxed{1}&0&0&0&-5\\
0&0&\boxed{1}&0&0&4\\
0&0&0&\boxed{1}&0&1\\
0&0&0&0&\boxed{1}&-2\end{bmatrix};\mathbf{B}_{3}=\begin{bmatrix}6\\
-5\\
4\\
1\\
-2\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&2&1&2&1&0\\
-2&-3&0&-5&-1&0\\
1&1&0&2&1&0\\
-2&-3&-1&-3&-2&1\\
-1&-3&-1&-3&1&0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&0&-1\\
0&\boxed{1}&0&0&0&-1\\
0&0&\boxed{1}&0&0&1\\
0&0&0&\boxed{1}&0&1\\
0&0&0&0&\boxed{1}&0\end{bmatrix};\mathbf{B}_{4}=\begin{bmatrix}-1\\
-1\\
1\\
1\\
0\end{bmatrix} \\
\\
\displaystyle\begin{bmatrix}1&2&1&2&1&0\\
-2&-3&0&-5&-1&0\\
1&1&0&2&1&0\\
-2&-3&-1&-3&-2&0\\
-1&-3&-1&-3&1&1\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&0&-2\\
0&\boxed{1}&0&0&0&1\\
0&0&\boxed{1}&0&0&-1\\
0&0&0&\boxed{1}&0&0\\
0&0&0&0&\boxed{1}&1\end{bmatrix};\mathbf{B}_{5}=\begin{bmatrix}-2\\
1\\
-1\\
0\\
1\end{bmatrix}
\end{align*}
@col
We can now collect our 5 solution vectors into the matrix $B$,
\begin{align*}
\displaystyle B=&[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}|\mathbf{B}_{4}|\mathbf{B}_{5}] \\
\displaystyle=&\left[\begin{bmatrix}-3\\
0\\
1\\
1\\
1\end{bmatrix}\left\lvert\begin{bmatrix}3\\
-2\\
2\\
0\\
-1\end{bmatrix}\right.\left\lvert\begin{bmatrix}6\\
-5\\
4\\
1\\
-2\end{bmatrix}\right.\left\lvert\begin{bmatrix}-1\\
-1\\
1\\
1\\
0\end{bmatrix}\right.\left\lvert\begin{bmatrix}-2\\
1\\
-1\\
0\\
1\end{bmatrix}\right.\right] \\
&=\begin{bmatrix}-3&3&6&-1&-2\\
0&-2&-5&-1&1\\
1&2&4&1&-1\\
1&0&1&1&0\\
1&-1&-2&0&1\end{bmatrix}
\end{align*}
@col
By this method, we know that $AB=I_{5}$. Check that $BA=I_{5}$, and then we will know that we have the inverse of $A$.
@endcol
@end
@eg
<b>Computing a matrix inverse</b>

@newcol
Let
\begin{align*}
\displaystyle A=\left[\begin{array}[]{ccc}1&1&1\\
1&0&-1\\
0&1&1\\
\end{array}\right].
\end{align*}
@col
Suppose $B$ is the inverse of $A$ (at this point, we don’t know inverse exists or not).
\begin{align*}
\displaystyle AB=[A\mathbf{B}_{1}|A\mathbf{B}_{2}|A\mathbf{B}_{3}]=I_{3}=[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}]
\end{align*}
@col
So
\begin{align*}
\displaystyle A\mathbf{B}_{i}=\mathbf{e}_{i}.
\end{align*}
@col
Hence $\mathbf{B}_{i}$ is a solution of $A\mathbf{x}=\mathbf{e}_{i}$. The solution can be obtained RREF $[A|\mathbf{e}_{i}]$. To solve for $A\mathbf{x}=\mathbf{e}_{1}$:
\begin{align*}
\displaystyle [A|\mathbf{e}_{1}]=\left[\begin{array}[]{ccc|c}1&1&1&1\\
1&0&-1&0\\
0&1&1&0\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&1&1&1\\
0&-1&-2&-1\\
0&1&1&0\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|c}1&1&1&1\\
0&1&1&0\\
0&-1&-2&-1\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}1&1&1&1\\
0&1&1&0\\
0&0&-1&-1\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}1&0&0&1\\
0&1&1&0\\
0&0&-1&-1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|c}1&0&0&1\\
0&1&0&-1\\
0&0&1&1\\
\end{array}\right]
\end{align*}
@col
We can take $\mathbf{B}_{1}=\begin{bmatrix}1\\
-1\\
1\end{bmatrix}$.

@col
Next we want to find solution of $A\mathbf{x}=\mathbf{e}_{2}$ by row reducing $[A|\mathbf{e}_{2}]$. Note that we can use the
<strong>exact same row operations</strong>.
\begin{align*}
\displaystyle [A|\mathbf{e}_{2}]=\left[\begin{array}[]{ccc|c}1&1&1&0\\
1&0&-1&1\\
0&1&1&0\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&1&1&0\\
0&-1&-2&1\\
0&1&1&0\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|c}1&1&1&0\\
0&1&1&0\\
0&-1&-2&1\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}1&1&1&0\\
0&1&1&0\\
0&0&-1&1\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}1&0&0&0\\
0&1&1&0\\
0&0&-1&1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|c}1&0&0&0\\
0&1&0&1\\
0&0&1&-1\\
\end{array}\right]
\end{align*}
@col
We can take $\mathbf{B}_{2}=\begin{bmatrix}0\\
1\\
-1\end{bmatrix}$.

@col
Next we want to find solution of $A\mathbf{x}=\mathbf{e}_{3}$ by row reducing $[A|\mathbf{e}_{3}]$. Again we can use the <strong>exact same row operations</strong>.
\begin{align*}
\displaystyle [A|\mathbf{e}_{3}]=\left[\begin{array}[]{ccc|c}1&1&1&0\\
1&0&-1&0\\
0&1&1&1\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|c}1&1&1&0\\
0&-1&-2&0\\
0&1&1&1\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|c}1&1&1&0\\
0&1&1&1\\
0&-1&-2&0\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|c}1&1&1&0\\
0&1&1&1\\
0&0&-1&1\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|c}1&0&0&-1\\
0&1&1&1\\
0&0&-1&1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|c}1&0&0&-1\\
0&1&0&2\\
0&0&1&-1\\
\end{array}\right]
\end{align*}
@col
We can take $\mathbf{B}_{1}=\begin{bmatrix}1\\
-1\\
1\end{bmatrix}$.

@col
So
\begin{align*}
\displaystyle B=[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}]=\left[\begin{array}[]{ccc}1&0&-1\\
-1&1&2\\
1&-1&-1\\
\end{array}\right]
\end{align*}
@col
And $AB=I_{3}$. We need to check $BA=I_{3}$ <strong>but later we will show that $BA=I_{3}$ follows from $AB=I_{3}$, so we don’t have to check it</strong>.

We see that we follows the exact same row operations for each case. We can combine all three cases into one.

<strong>Better method for finding inverse</strong>:

@col
\begin{align*}
\displaystyle [A|\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}]=\left[\begin{array}[]{ccc|ccc}1&1&1&1&0&0\\
1&0&-1&0&1&0\\
0&1&1&0&0&1\\
\end{array}\right]\xrightarrow{-1R_{1}+R_{2}}\left[\begin{array}[]{ccc|ccc}1&1&1&1&0&0\\
0&-1&-2&-1&1&0\\
0&1&1&0&0&1\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{R_{2}\leftrightarrow R_{3}}\left[\begin{array}[]{ccc|ccc}1&1&1&1&0&0\\
0&1&1&0&0&1\\
0&-1&-2&-1&1&0\\
\end{array}\right]\xrightarrow{1R_{2}+R_{3}}\left[\begin{array}[]{ccc|ccc}1&1&1&1&0&0\\
0&1&1&0&0&1\\
0&0&-1&-1&1&1\\
\end{array}\right]
\end{align*}
\begin{align*}
\displaystyle \xrightarrow{-1R_{2}+R_{1}}\left[\begin{array}[]{ccc|ccc}1&0&0&1&0&-1\\
0&1&1&0&0&1\\
0&0&-1&-1&1&1\\
\end{array}\right]\xrightarrow{1R_{3}+R_{2},-1R_{3}}\left[\begin{array}[]{ccc|ccc}1&0&0&1&0&-1\\
0&1&0&-1&1&2\\
0&0&1&1&-1&-1\\
\end{array}\right]
\end{align*}
@col
So
\begin{align*}
\displaystyle B=[\mathbf{B}_{1}|\mathbf{B}_{2}|\mathbf{B}_{3}]=\left[\begin{array}[]{ccc}1&0&-1\\
-1&1&2\\
1&-1&-1\\
\end{array}\right]
\end{align*}
@endcol
@end
@slide
@thm
@title{Computing the Inverse of a Nonsingular Matrix}
@label{CINM}
Suppose $A$ is a nonsingular square matrix of size $n$. Create the $n\times 2n$ matrix $M$ by placing the $n\times n$ identity matrix $I_{n}$ to the right of the matrix $A$. Let $N$ be a matrix that is row-equivalent to $M$ and in reduced row-echelon form. Finally, let $J$ be the matrix formed from the final $n$ columns of $N$. Then $AJ=I_{n}$.
@end
@proof
@newcol
$A$ is nonsingular, there is a sequence of row operations that will convert $A$ into $I_{n}$. It is this same sequence of row operations that will convert $M$ into $N$, since having the identity matrix in the first $n$ columns of $N$ is sufficient to guarantee that $N$ is in reduced row-echelon form.

@col
If we consider the systems of linear equations, $A\mathbf{x}=\mathbf{e}_{i}$, $1\leq i\leq n$, we see that the aforementioned sequence of row operations will also bring the augmented matrix of each of these systems into reduced row-echelon form. Furthermore, the unique solution to $A\mathbf{x}=\mathbf{e}_{i}$ appears in column $n+1$ of the row-reduced augmented matrix of the system and is identical to column $n+i$ of $N$. Let $\mathbf{N}_{1},\,\mathbf{N}_{2},\,\mathbf{N}_{3},\,\ldots,\,\mathbf{N}_{2n}$ denote the columns of $N$. So we find,
\begin{align*}
\displaystyle AJ=& A[\mathbf{N}_{n+1}|\mathbf{N}_{n+2}|\mathbf{N}_{n+3}|\ldots|\mathbf{N}_{n+n}] \\
\displaystyle=&[A\mathbf{N}_{n+1}|A\mathbf{N}_{n+2}|A\mathbf{N}_{n+3}|\ldots|A\mathbf{N}_{n+n}] \\
\displaystyle=&[\mathbf{e}_{1}|\mathbf{e}_{2}|\mathbf{e}_{3}|\ldots|\mathbf{e}_{n}] \\
\displaystyle=& I_{n}
\end{align*}
as desired.
@qed
@endcol
@end
@newcol
We have to be just a bit careful here about both what this theorem says and what it does not say. If $A$ is a nonsingular matrix, then we are guaranteed a matrix $B$ such that $AB=I_{n}$, and the proof gives us a process for constructing $B$. However, the definition of the inverse of a matrix requires that $BA=I_{n}$ also. So at this juncture we must compute the matrix product in the <strong>opposite</strong> order before we claim $B$ as the inverse of $A$. However, we will soon see that this is always the case.

@col
What if $A$ is singular? At this point we only know that Theorem   @ref{CINM} cannot be applied. The question of $A$’s inverse is still open.
We will solve it later.
@endcol
@slide
@eg
<b>Computing a matrix inverse</b>

@newcol
\begin{align*}
\displaystyle B=&\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}. \\
\\
\displaystyle M=&\begin{bmatrix}-7&-6&-12&1&0&0\\
5&5&7&0&1&0\\
1&0&4&0&0&1\end{bmatrix}. \\
\\
\displaystyle N=&\begin{bmatrix}1&0&0&-10&-12&-9\\
0&1&0&\frac{13}{2}&8&\frac{11}{2}\\
0&0&1&\frac{5}{2}&3&\frac{5}{2}\end{bmatrix}. \\
\\
\displaystyle B^{-1}=&\begin{bmatrix}-10&-12&-9\\
\frac{13}{2}&8&\frac{11}{2}\\
\frac{5}{2}&3&\frac{5}{2}\end{bmatrix}.
\end{align*}
@endcol
@end
@section{Properties of Matrix Inverses}

The inverse of a matrix enjoys some nice properties. We collect a few here. First, a matrix can have but one inverse.

@slide
@thm
@title{Matrix Inverse is Unique}
@label{MIU}
Suppose the square matrix $A$ has an inverse. Then $A^{-1}$ is unique.
@end
@proof
@newcol
We will assume that $A$ has two inverses. The hypothesis tells there is at least one. Suppose then that $B$ and $C$ are both inverses for $A$.
Then $AB=BA=I_{n}$ and $AC=CA=I_{n}$. Then we have,
\begin{align*}
\displaystyle B&=BI_{n} \\
&=B(AC) \\
&=(BA)C \\
&=I_{n}C \\
&=C
\end{align*}
@col
So we conclude that $B$ and $C$ are the same, and cannot be different. So any matrix that acts like an inverse, must be the inverse.
@qed
@endcol
@end
@newcol
When most of us dress in the morning, we put on our socks first, followed by our shoes. In the evening we must then first remove our shoes, followed by our socks. Try to connect the conclusion of the following theorem with this everyday example.

@endcol
@slide
@thm
@title{Socks and Shoes}
@label{SS}
Suppose $A$ and $B$ are invertible matrices of size $n$. Then $AB$ is an invertible matrix and $(AB)^{-1}=B^{-1}A^{-1}$.
@end
@proof
@newcol
\begin{align*}
\displaystyle(B^{-1}A^{-1})(AB)&=B^{-1}(A^{-1}A)B \\
&=B^{-1}I_{n}B \\
&=B^{-1}B \\
&=I_{n} \\
\\
\displaystyle(AB)(B^{-1}A^{-1})&=A(BB^{-1})A^{-1} \\
&=AI_{n}A^{-1} \\
&=AA^{-1} \\
&=I_{n}
\end{align*}
@col
So the matrix $B^{-1}A^{-1}$ has met all of the requirements to be $AB$’s inverse (date) and with the ensuing marriage proposal we can announce that $(AB)^{-1}=B^{-1}A^{-1}$.
@qed
@endcol
@end
@slide
@thm
@title{Matrix Inverse of a Matrix Inverse}
@label{MIMI}
Suppose $A$ is an invertible matrix. Then $A^{-1}$ is invertible and $(A^{-1})^{-1}=A$.
@end
@proof
@newcol
As with the proof of of the previous example, we examine if $A$ is a suitable inverse for $A^{-1}$ (by definition, the opposite is true).
\begin{align*}
\displaystyle AA^{-1}&=I_{n} \\
\\
\displaystyle A^{-1}A&=I_{n}
\end{align*}
@col
The matrix $A$ has met all the requirements to be the inverse of $A^{-1}$, and so is invertible and we can write $A=(A^{-1})^{-1}$.
@qed
@endcol
@end
@slide
@thm
@title{Matrix Inverse of a Transpose}
@label{MIT}
Suppose $A$ is an invertible matrix. Then $A^{t}$ is invertible and $(A^{t})^{-1}=(A^{-1})^{t}$.
@end
@proof
@newcol
As with the proof of Theorem   @ref{SS}, we see if $(A^{-1})^{t}$ is a suitable inverse for $A^{t}$.
\begin{align*}
\displaystyle(A^{-1})^{t}A^{t}&=(AA^{-1})^{t} \\
&=I_{n}^{t} \\
&=I_{n} \\
\\
\displaystyle A^{t}(A^{-1})^{t}&=(A^{-1}A)^{t} \\
&=I_{n}^{t} \\
&=I_{n}
\end{align*}
@col
The matrix $(A^{-1})^{t}$ has met all the requirements to be the inverse of $A^{t}$, and so is invertible and we can write $(A^{t})^{-1}=(A^{-1})^{t}$.
@qed
@endcol
@end
@slide
@thm
@title{Matrix Inverse of a Scalar Multiple}
@label{MISM}
Suppose $A$ is an invertible matrix and $\alpha$ is a nonzero scalar. Then $\left(\alpha A\right)^{-1}=\frac{1}{\alpha}A^{-1}$ and $\alpha A$ is invertible.
@end
@proof
@newcol
As with the proof of Theorem   @ref{SS}, we see if $\frac{1}{\alpha}A^{-1}$ is a suitable inverse for $\alpha A$.
\begin{align*}
\displaystyle\left(\frac{1}{\alpha}A^{-1}\right)\left(\alpha A\right)&=\left(\frac{1}{\alpha}\alpha\right)\left(A^{-1}A\right) \\
&=1I_{n} \\
&=I_{n} \\
\\
\displaystyle\left(\alpha A\right)\left(\frac{1}{\alpha}A^{-1}\right)&=\left(\alpha\frac{1}{\alpha}\right)\left(AA^{-1}\right) \\
&=1I_{n} \\
&=I_{n}
\end{align*}
@col
The matrix $\frac{1}{\alpha}A^{-1}$ has met all the requirements to be the inverse of $\alpha A$, so we can write $\left(\alpha A\right)^{-1}=\frac{1}{\alpha}A^{-1}$.
@qed
@endcol
@end
@newcol
Notice that there are some likely theorems that are missing here. For example, it would be tempting to think that $(A+B)^{-1}=A^{-1}+B^{-1}$, but this is false. Can you find a counterexample?

@endcol
@section{Nonsingular Matrices are Invertible}
@label{NMI}
For $\alpha,\beta\in{\mathbb{R}}^{\hbox{}}$, then $\alpha\beta\neq 0$ if and only if $\alpha\neq 0$ and $\beta\neq 0$.
We have a similar result for nonsingular matrix

@slide
@thm
@title{Nonsingular Product has Nonsingular Terms}
@label{NPNT}
Suppose that $A$ and $B$ are square matrices of size $n$. The product $AB$ is nonsingular if and only if $A$ and $B$ are both nonsingular.
@end
@proof
@newcol
($\Rightarrow$) For this portion of the proof we will form the logically-equivalent contrapositive and prove that statement using two cases.

<blockquote>
$AB$ is nonsingular implies $A$ and $B$ are both nonsingular.
</blockquote>
becomes

@col
<blockquote>
$A$ or $B$ is singular implies $AB$ is singular.
</blockquote>

@col
Case 1. Suppose $B$ is singular. Then there is a nonzero vector $\mathbf{z}$ that is a solution to $B\mathbf{x}=\mathbf{0}$. So
\begin{align*}
\displaystyle(AB)\mathbf{z}&=A(B\mathbf{z}) \\
&=A\mathbf{0} \\
&=\mathbf{0}
\end{align*}
@col
Then $\mathbf{z}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired.

@col
Case 2. Suppose $A$ is singular, and $B$ is not singular.
Because $A$ is singular, there is a nonzero vector $\mathbf{y}$ that is a solution to $A\mathbf{x}=\mathbf{0}$. Now consider the linear system $B\mathbf{x}=\mathbf{y}$. Since $B$ is nonsingular, the system has a unique solution, which we will denote as $\mathbf{w}$. We first claim $\mathbf{w}$ is not the zero vector either. Assuming the opposite, suppose that $\mathbf{w}=\mathbf{0}$. Then
\begin{align*}
\displaystyle\mathbf{y}&=B\mathbf{w} \\
&=B\mathbf{0} \\
&=\mathbf{0} \\
\\
\displaystyle(AB)\mathbf{w}&=A(B\mathbf{w}) \\
&=A\mathbf{y} \\
&=\mathbf{0}
\end{align*}
@col
So $\mathbf{w}$ is a nonzero solution to $AB\mathbf{x}=\mathbf{0}$. Thus $AB$ is singular as desired. And this conclusion holds for both cases.

($\Leftarrow$) Now assume that both $A$ and $B$ are nonsingular. Suppose that $\mathbf{x}\in{\mathbb{R}}^{n}$ is a solution to $AB\mathbf{x}=\mathbf{0}$. Then
\begin{align*}
\displaystyle\mathbf{0}&=\left(AB\right)\mathbf{x} \\
&=A\left(B\mathbf{x}\right)
\end{align*}
@col
So $B\mathbf{x}$ is a solution to $A\mathbf{x}=\mathbf{0}$, and by the definition of a nonsingular matrix, we conclude that $B\mathbf{x}=\mathbf{0}$. Now, by an entirely similar argument, the nonsingularity of $B$ forces us to conclude that $\mathbf{x}=\mathbf{0}$. So the only solution to $AB\mathbf{x}=\mathbf{0}$ is the zero vector and we conclude that $AB$ is nonsingular.
@qed
@endcol
@end
@newcol
The contrapositive of this entire result is equally interesting. It says that $A$ or $B$ (or both) is a singular matrix if and only if the product $AB$ is singular.

@endcol
@slide
@thm
@title{One-Sided Inverse is Sufficient}
@label{OSIS}
Suppose $A$ and $B$ are square matrices of size $n$ such that $AB=I_{n}$. Then $BA=I_{n}$.
@end
@proof
@newcol
The matrix $I_{n}$ is nonsingular. So $A$ and $B$ are nonsingular by, so in particular $B$ is nonsingular. We can therefore apply Theorem   @ref{CINM} to assert the existence of a matrix $C$ so that $BC=I_{n}$.
$B$ is nonsingular, so there must be a <b>right-inverse</b> for $B$, and we are calling it $C$.

@col
Now
\begin{align*}
\displaystyle BA&=(BA)I_{n} \\
&=(BA)(BC) \\
&=B(AB)C \\
&=BI_{n}C \\
&=BC \\
&=I_{n}
\end{align*}
which is the desired conclusion.

@qed
@endcol
@end
@newcol
So above theorem tells us that if $A$ is nonsingular, then the matrix $B$ guaranteed by Theorem   @ref{CINM} will be both a <b>right-inverse</b> and a <b>left-inverse</b> for $A$, so $A$ is invertible and $A^{-1}=B$.

@col
So if you have a nonsingular matrix, $A$, you can use the procedure described in Theorem   @ref{CINM} to find an inverse for $A$. If $A$ is singular, then the procedure in Theorem   @ref{CINM} will fail as the first $n$ columns of $M$ will not row-reduce to the identity matrix. However, we can say a bit more. When $A$ is singular, then $A$ does not have an inverse (which is very different from saying that the procedure in Theorem   @ref{CINM} fails to find an inverse).
This may feel like we are splitting hairs, but it is important that we do not make unfounded assumptions. These observations motivate the next theorem.

@endcol
@slide
@thm
@title{Nonsingularity is Invertibility}
@label{NI}
Suppose that $A$ is a square matrix. Then $A$ is nonsingular if and only if $A$ is invertible.
@end
@proof
@newcol
($\Leftarrow$) Since $A$ is invertible, we can write $I_{n}=AA^{-1}$. Notice that $I_{n}$ is nonsingular, so Theorem   @ref{NPNT} implies that $A$ (and $A^{-1}$) is nonsingular.

($\Rightarrow$) Suppose now that $A$ is nonsingular. By Theorem   @ref{CINM} we find $B$ so that $AB=I_{n}$. Then Theorem   @ref{OSIS} tells us that $BA=I_{n}$. So $B$ is $A$’s inverse, and by construction, $A$ is invertible.

@qed
@endcol
@end
@newcol
So for a square matrix, the properties of having an inverse and of having a trivial null space are one and the same. Cannot have one without the other.

@endcol
@slide
@thm
@title{Nonsingular Matrix Equivalences, Round 3}
@label{NME3}
Suppose that $A$ is a square matrix of size $n$. The following are equivalent.

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is nonsingular.
</li>
<li class="ltx_item">
$A$ row-reduces to the identity matrix.
</li>
<li class="ltx_item">
The null space of $A$ contains only the zero vector, ${\mathcal{N}}\!\left(A\right)=\{\mathbf{0}\}$.
</li>
<li class="ltx_item">
The linear system $A\mathbf{x}=\mathbf{b}$ has a unique solution for every possible choice of $\mathbf{b}$.
</li>
<li class="ltx_item">
The columns of $A$ are a linearly independent set.
</li>
<li class="ltx_item">
$A$ is invertible.
</li>

</ol>
@end
@newcol
In the case that $A$ is a nonsingular coefficient matrix of a system of equations, the inverse allows us to very quickly compute the unique solution, for any vector of constants.

@endcol
@slide
@thm
@title{Solution with Nonsingular Coefficient Matrix}
Suppose that $A$ is nonsingular. Then the unique solution to $A\mathbf{x}=\mathbf{b}$ is $A^{-1}\mathbf{b}$.
@end
@proof
@newcol
We can show this by simply plug $A^{-1}\mathbf{b}$ in the solution.
\begin{align*}
\displaystyle A\left(A^{-1}\mathbf{b}\right)&=\left(AA^{-1}\right)\mathbf{b} \\
&=I_{n}\mathbf{b} \\
&=\mathbf{b}
\end{align*}
@col
Since $A\mathbf{x}=\mathbf{b}$ is true when we substitute $A^{-1}\mathbf{b}$ for $\mathbf{x}$, $A^{-1}\mathbf{b}$ is a (the!) solution to $A\mathbf{x}=\mathbf{b}$.
@qed
@endcol
@end
@slide
@eg
Using the previous theorem, solve
\begin{align*}
\displaystyle x_{1}+x_{2}-x_{3}+4x_{4}&=1 \\
\displaystyle x_{1}-x_{2}+2x_{3}+3x_{4}&=2 \\
\displaystyle 2x_{1}+x_{2}+x_{3}+x_{4}&=0 \\
\displaystyle 2x_{1}+2x_{2}+2x_{3}-9x_{4}&=-1
\end{align*}
@newcol
The matrix coefficient is
\begin{align*}
\displaystyle A=\left[\begin{array}[]{cccc}1&1&-1&4\\
1&-1&2&3\\
2&1&1&1\\
2&2&2&-9\\
\end{array}\right].
\end{align*}
@col
After some computations,
\begin{align*}
\displaystyle A^{-1}=\left[\begin{array}[]{cccc}-33&-22&45&-17\\
35&23&-47&18\\
25&17&-34&13\\
6&4&-8&3\\
\end{array}\right].
\end{align*}
@col
Then the solution of the system of linear equations is
\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\end{bmatrix}=A^{-1}\begin{bmatrix}1\\
2\\
0\\
-1\end{bmatrix}=\begin{bmatrix}-60\\
63\\
46\\
11\end{bmatrix}.
\end{align*}
@endcol
@end
@setchapter{18}
@chapter{Determinant}
<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Chapter D (print version p261-282)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at
@href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}
(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section DM p.98 - 101 all, Section PDM p.101-102 M30, T10, T15, T20

@section{Definition of the determinant}

The <b>determinant</b> is a function that take a square matrix as an input and
produces a scalar as an output.

@newcol
Suppose $A$ is an $m\times n$ matrix. Then the submatrix $A(i|j)$ is the $(m-1)\times(n-1)$
matrix obtained from $A$ by removing row $i$ and column $j$.
@endcol
@slide
@eg
Suppose
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&3&4\\
5&6&7&8\\
9&10&11&12\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle A(2|3)=\begin{bmatrix}1&2&4\\
9&10&12\end{bmatrix}\qquad A(3|1)=\begin{bmatrix}2&3&4\\
6&7&8\end{bmatrix}
\end{align*}
@endcol
@end
@eg
\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&a_{12}&a_{13}&a_{14}\\
a_{21}&a_{22}&a_{23}&a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}\\
a_{41}&a_{42}&a_{43}&a_{44}\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle A(3|2)=\begin{bmatrix}a_{11}&a_{13}&a_{14}\\
a_{21}&a_{23}&a_{24}\\
a_{41}&a_{42}&a_{44}\end{bmatrix}\qquad A(4|1)=\begin{bmatrix}a_{12}&a_{13}&a_{14}\\
a_{22}&a_{23}&a_{24}\\
a_{32}&a_{33}&a_{34}\end{bmatrix}
\end{align*}
@endcol
@end
@slide
@defn
Suppose $A$ is a square matrix. Then its <b>determinant</b>, $\det\left(A\right)$ (or denoted by $|A|$), is an element of ${\mathbb{R}}^{\hbox{}}$ defined recursively by:

<ol class="ltx_enumerate">
<li class="ltx_item">
If $A$ is a $1\times 1$ matrix, then $\det\left(A\right)=\left[A\right]_{11}$.
</li>
<li class="ltx_item">
If $A$ is a matrix of size $n$ with $n\geq 2$, then
\begin{align*}
\displaystyle\det\left(A\right)&=\left[A\right]_{11}\det\left(A\left(1|1\right)\right)-\left[A\right]_{12}\det\left(A\left(1|2\right)\right)+\left[A\right]_{13}\det\left(A\left(1|3\right)\right)- \\
&\quad\left[A\right]_{14}\det\left(A\left(1|4\right)\right)+\cdots+(-1)^{n+1}\left[A\right]_{1n}\det\left(A\left(1|n\right)\right) \\
&\quad=\sum_{i=1}^{n}(-1)^{i+1}\left[A\right]_{1i}\det\left(A\left(1|i\right)\right)
\end{align*}
</li>

</ol>
@end
@newcol
So to compute the determinant of a $5\times 5$ matrix we must build 5 submatrices, each of size $4$. To compute the determinants of each the $4\times 4$ matrices we need to create 4 submatrices each, these now of size $3$ and so on. To compute the determinant of a $10\times 10$ matrix would require computing the determinant of $10!=10\times 9\times 8\times 7\times 6\times 5\times 4\times 3\times 2=3,628,800$
$1\times 1$ matrices. Fortunately there are <b>better ways</b>).

@col
Let us compute the determinant of a reasonably sized matrix by hand.
@endcol
@slide
@eg
Suppose that we have the $3\times 3$ matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}3&2&-1\\
4&1&6\\
-3&-1&2\end{bmatrix}
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle\det\left(A\right)=|A|&=\begin{vmatrix}3&2&-1\\
4&1&6\\
-3&-1&2\end{vmatrix} \\
&=3\begin{vmatrix}1&6\\
-1&2\end{vmatrix}-2\begin{vmatrix}4&6\\
-3&2\end{vmatrix}+(-1)\begin{vmatrix}4&1\\
-3&-1\end{vmatrix} \\
&=3\left(1\begin{vmatrix}2\\
\end{vmatrix}-6\begin{vmatrix}-1\end{vmatrix}\right)-2\left(4\begin{vmatrix}2\end{vmatrix}-6\begin{vmatrix}-3\end{vmatrix}\right)-\left(4\begin{vmatrix}-1\end{vmatrix}-1\begin{vmatrix}-3\end{vmatrix}\right) \\
&=3\left(1(2)-6(-1)\right)-2\left(4(2)-6(-3)\right)-\left(4(-1)-1(-3)\right) \\
&=24-52+1 \\
&=-27
\end{align*}
@endcol
@end
@slide
@thm
Suppose
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b\\
c&d\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle \det\left(A\right)=ad-bc.
\end{align*}
@endcol
@end
@proof
@newcol
\begin{align*}
\displaystyle \begin{vmatrix}a&b\\
c&d\end{vmatrix}=a\begin{vmatrix}d\end{vmatrix}-b\begin{vmatrix}c\end{vmatrix}=ad-bc
\end{align*}
@qed
@endcol
@end
@slide
@thm
Suppose
\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle \det\left(A\right)=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align*}
@endcol
@end
@proof
@newcol
\begin{align*}
\displaystyle\det\left(A\right)&=a_{11}|A\left(1|1\right)|-a_{12}|A\left(1|2\right)|+a_{13}|A\left(1|3\right)| \\
&=a_{11}\begin{vmatrix}a_{22}&a_{23}\\
a_{32}&a_{33}\end{vmatrix}-a_{12}\begin{vmatrix}a_{21}&a_{23}\\
a_{31}&a_{33}\end{vmatrix}+a_{13}\begin{vmatrix}a_{21}&a_{22}\\
a_{31}&a_{32}\end{vmatrix} \\
&=a_{11}(a_{22}a_{33}-a_{23}a_{32})-a_{12}(a_{21}a_{33}-a_{23}a_{31})+a_{13}(a_{21}a_{32}-a_{22}a_{31}) \\
&=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align*}
@qed
@endcol
@end
@section{Computing Determinants}
@thm
@title{Determinant Expansion about Rows}
Suppose that $A$ is a square matrix of size $n$. Then for $1\leq i\leq n$
\begin{align*}
\displaystyle\det\left(A\right)&=(-1)^{i+1}\left[A\right]_{i1}\det\left(A\left(i|1\right)\right)+(-1)^{i+2}\left[A\right]_{i2}\det\left(A\left(i|2\right)\right) \\
&\quad+(-1)^{i+3}\left[A\right]_{i3}\det\left(A\left(i|3\right)\right)+\cdots+(-1)^{i+n}\left[A\right]_{in}\det\left(A\left(i|n\right)\right)
\end{align*}
which is known as <b>expansion</b> about row $i$.
@end
@proof
@newcol
Skip the proof. If you are interested, see Beezer, p.266.
@qed
@endcol
@end
@slide
@thm
@title{Determinant of the Transpose}
Suppose that $A$ is a square matrix. Then $\det\left(A^{t}\right)=\det\left(A\right)$.
@end
@proof
@newcol
Skip the proof.
If you are interested, see Beezer, p.267.
@qed
@endcol
@end
@slide
@thm
@title{Determinant Expansion about Columns}
Suppose that $A$ is a square matrix of size $n$. Then for $1\leq j\leq n$
\begin{align*}
\displaystyle\det\left(A\right)&=(-1)^{1+j}\left[A\right]_{1j}\det\left(A\left(1|j\right)\right)+(-1)^{2+j}\left[A\right]_{2j}\det\left(A\left(2|j\right)\right) \\
&\quad+(-1)^{3+j}\left[A\right]_{3j}\det\left(A\left(3|j\right)\right)+\cdots+(-1)^{n+j}\left[A\right]_{nj}\det\left(A\left(n|j\right)\right)
\end{align*}
which is known as <b>expansion</b> about column $j$.
@end
@proof
@newcol
Skip the proof.
If you are interested, see Beezer, p.268.
@qed
@endcol
@end
@slide
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}-2&3&0&1\\
9&-2&0&1\\
1&3&-2&-1\\
4&1&2&6\end{bmatrix}
\end{align*}
@newcol
Then expanding about the fourth row yields,
\begin{align*}
\displaystyle|A|&=(4)(-1)^{4+1}\begin{vmatrix}3&0&1\\
-2&0&1\\
3&-2&-1\end{vmatrix}+(1)(-1)^{4+2}\begin{vmatrix}-2&0&1\\
9&0&1\\
1&-2&-1\end{vmatrix} \\
&\quad\quad+(2)(-1)^{4+3}\begin{vmatrix}-2&3&1\\
9&-2&1\\
1&3&-1\end{vmatrix}+(6)(-1)^{4+4}\begin{vmatrix}-2&3&0\\
9&-2&0\\
1&3&-2\end{vmatrix} \\
&=(-4)(10)+(1)(-22)+(-2)(61)+6(46)=92
\end{align*}
@col
Expanding about column 3 gives
\begin{align*}
\displaystyle|A|&=(0)(-1)^{1+3}\begin{vmatrix}9&-2&1\\
1&3&-1\\
4&1&6\end{vmatrix}+(0)(-1)^{2+3}\begin{vmatrix}-2&3&1\\
1&3&-1\\
4&1&6\end{vmatrix}+ \\
&\quad\quad(-2)(-1)^{3+3}\begin{vmatrix}-2&3&1\\
9&-2&1\\
4&1&6\end{vmatrix}+(2)(-1)^{4+3}\begin{vmatrix}-2&3&1\\
9&-2&1\\
1&3&-1\end{vmatrix} \\
&=0+0+(-2)(-107)+(-2)(61)=92
\end{align*}
@col
Notice how much easier the second computation was. By choosing to expand about the third column, we have two entries that are zero, so two $3\times 3$ determinants need not be computed at all!
@endcol
@end
@newcol
When a matrix has all zeros above (or below) the diagonal, exploiting the zeros by expanding about the proper row or column makes computing a determinant insanely easy.
@endcol
@slide
@eg
Suppose that
\begin{align*}
\displaystyle T=\begin{bmatrix}2&3&-1&3&3\\
0&-1&5&2&-1\\
0&0&3&9&2\\
0&0&0&-1&3\\
0&0&0&0&5\end{bmatrix}
\end{align*}
@newcol
We will compute the determinant of this $5\times 5$ matrix by consistently expanding about the first column for each submatrix that arises and does not have a zero entry multiplying it.
\begin{align*}
\displaystyle\det\left(T\right)&=\begin{vmatrix}2&3&-1&3&3\\
0&-1&5&2&-1\\
0&0&3&9&2\\
0&0&0&-1&3\\
0&0&0&0&5\end{vmatrix} \\
&=2(-1)^{1+1}\begin{vmatrix}-1&5&2&-1\\
0&3&9&2\\
0&0&-1&3\\
0&0&0&5\end{vmatrix} \\
&=2(-1)(-1)^{1+1}\begin{vmatrix}3&9&2\\
0&-1&3\\
0&0&5\end{vmatrix} \\
&=2(-1)(3)(-1)^{1+1}\begin{vmatrix}-1&3\\
0&5\end{vmatrix} \\
&=2(-1)(3)(-1)(-1)^{1+1}\begin{vmatrix}5\end{vmatrix} \\
&=2(-1)(3)(-1)(5)=30
\end{align*}
@endcol
@end
@slide
@thm
Suppose $A$ is upper triangular matrix, i.e.
\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&a_{12}&a_{13}&\cdots&a_{1n}\\
0&a_{22}&a_{23}&\cdots&a_{2n}\\
0&0&a_{33}&\cdots&a_{3n}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&a_{nn}\end{bmatrix}
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle \det\left(A\right)=a_{11}a_{22}\cdots a_{nn}.
\end{align*}
@endcol
@end
@proof
@newcol
\begin{align*}
\displaystyle\det\left(A\right)&=a_{11}\det\left(\begin{bmatrix}a_{22}&a_{23}&\cdots&a_{2n}\\
0&a_{33}&\cdots&a_{3n}\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&a_{nn}\end{bmatrix}\right)&xpand along the column  \\
&=a_{11}a_{22}\det\left(\begin{bmatrix}a_{33}&\cdots&a_{3n}\\
\vdots&\ddots&\vdots\\
0&\cdots&a_{nn}\end{bmatrix}\right)&xpand along the column  \\
&\cdots \\
&=a_{11}a_{22}\cdots a_{nn}
\end{align*}
@qed
@endcol
@end

@thm
@col
Suppose $A$ is lower triangular matrix, i.e.
\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&0&0&\cdots&0\\
a_{21}&a_{22}&0&\cdots&0\\
a_{31}&a_{32}&a_{33}&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
a_{n1}&a_{n2}&a_{n3}&\cdots&a_{nn}\end{bmatrix}
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle \det\left(A\right)=a_{11}a_{22}\cdots a_{nn}.
\end{align*}
@endcol
@end
@newcol
When you consult other texts in your study of determinants, you may run into the terms <b>minor</b> and <b>cofactor</b>, especially in a discussion centered on expansion about rows and columns. We have chosen not to make these definitions formally since we have been able to get along without them. However, informally, a <b>minor</b> is a determinant of a submatrix, specifically $\det\left(A\left(i|j\right)\right)$ and is usually referenced as the minor of $\left[A\right]_{ij}$. A <b>cofactor</b> is a signed minor, specifically the cofactor of $\left[A\right]_{ij}$ is $(-1)^{i+j}\det\left(A\left(i|j\right)\right)$.

@endcol
@section{Properties of Determinants of Matrces}
@thm
@title{Determinant with Zero Row or Column}
@label{DZRC}
Suppose that $A$ is a square matrix with a row where every entry is zero, or a column where every entry is zero. Then $\det\left(A\right)=0$.
@end
@proof
@newcol
Suppose that $A$ is a square matrix of size $n$ and row $i$ has every entry equal to zero. We compute $\det\left(A\right)$ via expansion about row $i$.
\begin{align*}
\displaystyle\det\left(A\right)&=\sum_{j=1}^{n}(-1)^{i+j}\left[A\right]_{ij}\det\left(A\left(i|j\right)\right) \\
&=\sum_{j=1}^{n}(-1)^{i+j}\,0\,\det\left(A\left(i|j\right)\right)&\text{row $i$ is zero.} \\
&=\sum_{j=1}^{n}0=0
\end{align*}
@col
The proof for the case of a zero column is entirely similar, or could be derived by the fact that
$\det\left(A\right)=\det\left(A^{t}\right)$.
@qed
@endcol
@end
@slide
@thm
@title{Determinant for Row or Column Swap}
@label{DRCS}
Suppose that $A$ is a square matrix. Let $B$ be the square matrix obtained from $A$ by interchanging the location of two rows, or interchanging the location of two columns. Then $\det\left(B\right)=-\det\left(A\right)$.
@end
@proof
@newcol
Skip the proof. If you are interested, see Beezer p.273.
@qed
@endcol
@end
@slide
@thm
@title{Determinant for Row or Column Multiples}
@label{DRCM}
Suppose that $A$ is a square matrix. Let $B$ be the square matrix obtained from $A$ by multiplying a single row (say, row $i$) by the scalar $\alpha$, or by multiplying a single column by the scalar $\alpha$. Then $\det\left(B\right)=\alpha\det\left(A\right)$.
@end
@proof
@newcol
Expand along row $i$, then
\begin{align*}
\displaystyle \det\left(B\right)=(-1)^{i+1}[B]_{i1}\det\left(B(i|1)\right)+(-1)^{i+1}[B]_{i2}\det\left(B(i|2)\right)+\cdots+(-1)^{i+n}[B]_{in}\det\left(B(i|n)\right)
\end{align*}
\begin{align*}
\displaystyle =\alpha((-1)^{i+1}[A]_{i1}\det\left(A(i|1)\right)+(-1)^{i+1}[A]_{i2}\det\left(A(i|2)\right)+\cdots+(-1)^{i+n}[A]_{in}\det\left(A(i|n)\right))=\det\left(A\right).
\end{align*}
@qed
@endcol
@end

@thm
@title{Determinant with Equal Rows or Columns}
@label{DERC}
@col
Suppose that $A$ is a square matrix with two equal rows, or two equal columns. Then $\det\left(A\right)=0$.
@end
@proof
@newcol
Skip the proof. If you are interested, see Beezer p.274.
@qed
@endcol
@end
@slide
@thm
@title{Determinant for Row or Column Multiples and Addition}
@label{DRCMA}
Suppose that $A$ is a square matrix. Let $B$ be the square matrix obtained from $A$ by multiplying a row by the scalar $\alpha$ and then adding it to another row, or by multiplying a column by the scalar $\alpha$ and then adding it to another column. Then $\det\left(B\right)=\det\left(A\right)$.
@end
@proof
@newcol
Suppose the row operation is $\alpha R_{i}+R_{j}$, expand along row $j$. For details, see Beezer p.275.
@qed
@endcol
@end
@slide
@thm
\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{i-1,1}&a_{i-1,2}&\cdots&a_{i-1,n}\\
b_{1}+c_{1}&b_{2}+c_{2}&\cdots&b_{n}+c_{n}\\
a_{i+1,1}&a_{i+1,2}&\cdots&a_{i+1,n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}\\
\end{vmatrix}=\begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{i-1,1}&a_{i-1,2}&\cdots&a_{i-1,n}\\
b_{1}&b_{2}&\cdots&b_{n}\\
a_{i+1,1}&a_{i+1,2}&\cdots&a_{i+1,n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}\\
\end{vmatrix}+\begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{i-1,1}&a_{i-1,2}&\cdots&a_{i-1,n}\\
c_{1}&c_{2}&\cdots&c_{n}\\
a_{i+1,1}&a_{i+1,2}&\cdots&a_{i+1,n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}\\
\end{vmatrix}
\end{align*}
@newcol
Similarly
\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&\cdots&a_{1,i-1}&b_{1}+c_{1}&a_{1,i+1}&\cdots&a_{1n}\\
a_{21}&\cdots&a_{2,i-1}&b_{2}+c_{2}&a_{2,i+1}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{n1}&\cdots&a_{n,i-1}&b_{n}+c_{n}&a_{n,i+1}&\cdots&a_{nn}\end{vmatrix}
\end{align*}
\begin{align*}
\displaystyle =\begin{vmatrix}a_{11}&\cdots&a_{1,i-1}&b_{1}&a_{1,i+1}&\cdots&a_{1n}\\
a_{21}&\cdots&a_{2,i-1}&b_{2}&a_{2,i+1}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{n1}&\cdots&a_{n,i-1}&b_{n}&a_{n,i+1}&\cdots&a_{nn}\end{vmatrix}+\begin{vmatrix}a_{11}&\cdots&a_{1,i-1}&c_{1}&a_{1,i+1}&\cdots&a_{1n}\\
a_{21}&\cdots&a_{2,i-1}&c_{2}&a_{2,i+1}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{n1}&\cdots&a_{n,i-1}&c_{n}&a_{n,i+1}&\cdots&a_{nn}\end{vmatrix}
\end{align*}
@endcol
@end
@proof
@newcol
Expand along row $i$ (or column $i$).
@qed
@endcol
@end
@slide
@eg
Suppose we desire the determinant of the $4\times 4$ matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}2&0&2&3\\
1&3&-1&1\\
-1&1&-1&2\\
3&5&4&0\end{bmatrix}
\end{align*}
@newcol
We will perform a sequence of row operations on this matrix, shooting for an upper triangular matrix, whose determinant will be simply the product of its diagonal entries. For each row operation, we will track the effect on the determinant via Theorem   @ref{DRCS} Theorem   @ref{DRCM} and Theorem   @ref{DRCMA}.
\begin{align*}
\displaystyle \begin{vmatrix}2&0&2&3\\
1&3&-1&1\\
-1&1&-1&2\\
3&5&4&0\end{vmatrix}
\end{align*}
\begin{align*}
\displaystyle =-\begin{vmatrix}1&3&-1&1\\
2&0&2&3\\
-1&1&-1&2\\
3&5&4&0\end{vmatrix}\qquad(R_{1}\leftrightarrow R_{2})
\end{align*}
\begin{align*}
\displaystyle =-\begin{vmatrix}1&3&-1&1\\
0&-6&4&1\\
-1&1&-1&2\\
3&5&4&0\end{vmatrix}\qquad(-2R_{1}+R_{2})
\end{align*}
\begin{align*}
\displaystyle =-\begin{vmatrix}1&3&-1&1\\
0&-6&4&1\\
0&4&-2&3\\
3&5&4&0\end{vmatrix}\qquad(1R_{1}+R_{3})
\end{align*}
\begin{align*}
\displaystyle =-\begin{vmatrix}1&3&-1&1\\
0&-6&4&1\\
0&4&-2&3\\
0&-4&7&-3\end{vmatrix}\qquad(-3R_{1}+R_{4})
\end{align*}
\begin{align*}
\displaystyle =-\begin{vmatrix}1&3&-1&1\\
0&-2&2&4\\
0&4&-2&3\\
0&-4&7&-3\end{vmatrix}\qquad(1R_{3}+R_{2})
\end{align*}
\begin{align*}
\displaystyle =2\begin{vmatrix}1&3&-1&1\\
0&1&-1&-2\\
0&4&-2&3\\
0&-4&7&-3\end{vmatrix}\qquad(-\frac{1}{2}R_{2})
\end{align*}
\begin{align*}
\displaystyle =2\begin{vmatrix}1&3&-1&1\\
0&1&-1&-2\\
0&0&2&11\\
0&-4&7&-3\end{vmatrix}\qquad(-4R_{2}+R_{3})
\end{align*}
\begin{align*}
\displaystyle =2\begin{vmatrix}1&3&-1&1\\
0&1&-1&-2\\
0&0&2&11\\
0&0&3&-11\end{vmatrix}\qquad(-4R_{2}+R_{4})
\end{align*}
\begin{align*}
\displaystyle =2\begin{vmatrix}1&3&-1&1\\
0&1&-1&-2\\
0&0&2&11\\
0&0&1&-22\end{vmatrix}\qquad(-1R_{3}+R_{4})
\end{align*}
\begin{align*}
\displaystyle =2\begin{vmatrix}1&3&-1&1\\
0&1&-1&-2\\
0&0&0&55\\
0&0&1&-22\end{vmatrix}\qquad(-2R_{4}+R_{3})
\end{align*}
\begin{align*}
\displaystyle =-2\begin{vmatrix}1&3&-1&1\\
0&1&-1&-2\\
0&0&1&-22\\
0&0&0&55\end{vmatrix}\qquad(R_{3}\leftrightarrow R_{4})
\end{align*}
\begin{align*}
\displaystyle =-2\times 1\times 1\times 1\times 55=-110.
\end{align*}
@endcol
@end
@section{Examples}
@eg
Compute
\begin{align*}
\displaystyle \begin{vmatrix}1&a_{1}&a_{2}&a_{3}\\
1&a_{1}+b_{1}&a_{2}&a_{3}\\
1&a_{1}&a_{2}+b_{2}&a_{3}\\
1&a_{1}&a_{2}&a_{3}+b_{3}.\end{vmatrix}
\end{align*}
@newcol
The above is
\begin{align*}
\displaystyle \begin{vmatrix}1&a_{1}&a_{2}&a_{3}\\
0&b_{1}&0&0\\
0&0&b_{2}&0\\
0&0&0&b_{3}\end{vmatrix}\qquad(-1R_{1}+R_{2},-1R_{1}+R_{3},-1R_{1}+R_{4})
\end{align*}
\begin{align*}
\displaystyle =b_{1}b_{2}b_{3}\qquad(\text{upper triangular matrix})
\end{align*}
@endcol
@end
@eg
Compute
\begin{align*}
\displaystyle \begin{vmatrix}1&1&1\\
a&b&c\\
a^{2}&b^{2}&c^{2}\end{vmatrix}.
\end{align*}
\begin{align*}
\displaystyle =\begin{vmatrix}1&1&1\\
a&b&c\\
0&b(b-a)&c(c-a)\end{vmatrix}\qquad(-aR_{2}+R_{3})
\end{align*}
\begin{align*}
\displaystyle =\begin{vmatrix}1&1&1\\
0&b-a&c-a\\
0&b(b-a)&c(c-a)\end{vmatrix}\qquad(-aR_{1}+R_{2})
\end{align*}
\begin{align*}
\displaystyle =\begin{vmatrix}b-a&c-a\\
b(b-a)&c(c-a)\end{vmatrix}\qquad(\text{expand along the first column})
\end{align*}
\begin{align*}
\displaystyle =(b-a)\begin{vmatrix}1&c-a\\
b&c(c-a)\end{vmatrix}\qquad(\text{pull out $b-a$ from column 1})
\end{align*}
\begin{align*}
\displaystyle =(b-a)(c-a)\begin{vmatrix}1&1\\
b&c\end{vmatrix}\qquad(\text{pull out $c-a$ from column 2})
\end{align*}
\begin{align*}
\displaystyle =(b-a)(c-a)(c-b).
\end{align*}
@end
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}0&0&0&a_{14}\\
0&0&a_{23}&a_{24}\\
0&a_{32}&a_{33}&a_{34}\\
a_{41}&a_{42}&a_{43}&a_{44}\end{bmatrix}
\end{align*}
@newcol
Find $\det\left(A\right)$. $\det\left(A\right)$
\begin{align*}
\displaystyle =-\begin{vmatrix}a_{41}&a_{42}&a_{43}&a_{44}\\
0&0&a_{23}&a_{24}\\
0&a_{32}&a_{33}&a_{34}\\
0&0&0&a_{14}\end{vmatrix}\qquad(R_{1}\leftrightarrow R_{4})
\end{align*}
\begin{align*}
\displaystyle =\begin{vmatrix}a_{41}&a_{42}&a_{43}&a_{44}\\
0&a_{32}&a_{33}&a_{34}\\
0&0&a_{23}&a_{24}\\
0&0&0&a_{14}\end{vmatrix}\qquad(R_{2}\leftrightarrow R_{3})
\end{align*}
\begin{align*}
\displaystyle =a_{41}a_{32}a_{23}a_{14}.
\end{align*}
@endcol
@end
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}\end{bmatrix}
\end{align*}
@newcol
Given $\det\left(A\right)=1$.
Find
\begin{align*}
\displaystyle \begin{vmatrix}2a_{11}&3a_{12}&4a_{13}\\
2a_{21}&3a_{22}&4a_{23}\\
2a_{31}&3a_{32}&4a_{33}\end{vmatrix}.
\end{align*}
@col
The above is
\begin{align*}
\displaystyle =2\begin{vmatrix}a_{11}&3a_{12}&4a_{13}\\
a_{21}&3a_{22}&4a_{23}\\
a_{31}&3a_{32}&4a_{33}\end{vmatrix}\qquad(\frac{1}{2}C_{1})
\end{align*}
\begin{align*}
\displaystyle =2\times 3\begin{vmatrix}a_{11}&a_{12}&4a_{13}\\
a_{21}&a_{22}&4a_{23}\\
a_{31}&a_{32}&4a_{33}\end{vmatrix}\qquad(\frac{1}{3}C_{2})
\end{align*}
\begin{align*}
\displaystyle =2\times 3\times 4\begin{vmatrix}a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}\end{vmatrix}\qquad(\frac{1}{4}C_{3})
\end{align*}
\begin{align*}
\displaystyle =24.
\end{align*}
@endcol
@end
@eg
Compute
\begin{align*}
\displaystyle \det\left(A\right)=\begin{vmatrix}1&1&1&1&1\\
2&2&1&2&2\\
1&2&1&2&3\\
1&1&1&3&2\\
1&1&1&1&4\end{vmatrix}
\end{align*}
@newcol
By $-1R_{1}+R_{2}$, $-1R_{1}+R_{3}$, $-1R_{1}+R_{4}$, $-1R_{1}+R_{5}$,
the above is
\begin{align*}
\displaystyle \begin{vmatrix}1&1&1&1&1\\
1&1&0&1&1\\
0&1&0&1&2\\
0&0&0&2&1\\
0&0&0&0&3\end{vmatrix}.
\end{align*}
@col
Expand along column 3,
the above is
\begin{align*}
\displaystyle (-1)^{1+3}\times 1\times\begin{vmatrix}1&1&1&1\\
0&1&1&2\\
0&0&2&1\\
0&0&0&3\end{vmatrix}=1\times 1\times 2\times 3=6.
\end{align*}
@endcol
@end
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle B=\begin{bmatrix}b_{11}&b_{12}\\
b_{21}&b_{22}\end{bmatrix}.
\end{align*}
@newcol
Let
\begin{align*}
\displaystyle C=\begin{bmatrix}A&{\cal O}_{32}\\
{\cal O}_{23}&B\end{bmatrix}=\begin{bmatrix}a_{11}&a_{12}&a_{13}&0&0\\
a_{21}&a_{22}&a_{23}&0&0\\
a_{31}&a_{32}&a_{33}&0&0\\
0&0&0&b_{11}&b_{12}\\
0&0&0&b_{21}&b_{22}\end{bmatrix}
\end{align*}
@col
Show that
\begin{align*}
\displaystyle \det\left(C\right)=\det\left(A\right)\det\left(B\right).
\end{align*}
@col
Expand $C$ along the last row, $\det\left(C\right)=$
\begin{align*}
\displaystyle (-1)^{5+4}b_{21}\begin{vmatrix}a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&a_{23}&0\\
a_{31}&a_{32}&a_{33}&0\\
0&0&0&b_{12}\\
\end{vmatrix}+(-1)^{5+5}b_{22}\begin{vmatrix}a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&a_{23}&0\\
a_{31}&a_{32}&a_{33}&0\\
0&0&0&b_{11}\\
\end{vmatrix}.
\end{align*}
@col
For each $4\times 4$ submatrix, expand along the last row, the above is
\begin{align*}
\displaystyle (-1)^{5+4}b_{21}(-1)^{4+4}b_{12}\begin{vmatrix}a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}\\
\end{vmatrix}+(-1)^{5+5}b_{22}(-1)^{4+4}b_{11}\begin{vmatrix}a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}\\
\end{vmatrix}
\end{align*}
\begin{align*}
\displaystyle =(b_{11}b_{22}-b_{21}b_{12})\det\left(A\right)=\det\left(A\right)\det\left(B\right).
\end{align*}
<b>Remark</b>: The result is also valid when $A$ is a square matrix of size $n$ and $B$ is a square matrix of size $m$.
@endcol
@end
@eg
Compute
\begin{align*}
\displaystyle \det\left(A\right)=\begin{vmatrix}a&1&1&1\\
1&a&1&1\\
1&1&a&1\\
1&1&1&a.\end{vmatrix}
\end{align*}
@newcol
By $-1R_{1}+R_{2}$, $-1R_{1}+R_{3}$, $-1R_{1}+R_{4}$, the above is
\begin{align*}
\displaystyle \begin{vmatrix}a&1&1&1\\
1-a&a-1&0&0\\
1-a&0&a-1&0\\
1-a&0&0&a-1\end{vmatrix}
\end{align*}
@col
Take out the common factor $a-1$ of row 2, row 3 and row 4, the above is
\begin{align*}
\displaystyle (a-1)^{3}\begin{vmatrix}a&1&1&1\\
-1&1&0&0\\
-1&0&1&0\\
-1&0&0&1\end{vmatrix}
\end{align*}
\begin{align*}
\displaystyle =(a-1)^{3}\begin{vmatrix}a+3&0&0&0\\
-1&1&0&0\\
-1&0&1&0\\
-1&0&0&1\end{vmatrix}\qquad(-1R_{4}+R_{1},-1R_{3}+R_{1},-1R_{2}+R_{1})
\end{align*}
\begin{align*}
\displaystyle =(a+3)(a-1)^{3}.
\end{align*}
@endcol
@end
@section{More examples}
@eg
Let $A_{n}$ be a $n\times n$ matrix
\begin{align*}
\displaystyle \left.\left[\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}x&1&1&\cdots&1\\
1&x&1&\cdots&1\\
1&1&x&\cdots&1\\
\vdots&\vdots&\vdots&\vdots&1\\
1&1&1&\cdots&x\end{array}}_{n}}\right]\right\}\,n
\end{align*}
Find $\det(A_{n})$.

@col
Add columns $C_{2},C_{3},\ldots,C_{n}$ to $C_{1}$:
\[
\displaystyle \det(A_{n})
=
\left|
%% \vphantom{\begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}}
%% \smash{\underbrace{
\begin{array}[]{ccccc}x+(n-1)&1&1&\cdots&1\\
x+(n-1)&x&1&\cdots&1\\
x+(n-1)&1&x&\cdots&1\\
\vdots&\vdots&\vdots&\ddots&1\\
x+(n-1)&1&1&\cdots&x\end{array}
%%}_{n}}
\right|
\]
\[
=
(x+(n-1))\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}1&1&1&\cdots&1\\
1&x&1&\cdots&1\\
1&1&x&\cdots&1\\
\vdots&\vdots&\vdots&\ddots&1\\
1&1&1&\cdots&x\end{array}}_{n}}\right|
\]
@col
Performing the following sequence of column operations:
\[
-C_{1}+C_{2},\quad-C_{1}+C_{3},\ldots,-C_{1}+C_{n},
\]
we conclude that the determinant is equal to:
\begin{align*}
\displaystyle \left.\left|\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{ccccc}1&0&0&\cdots&0\\
1&x-1&0&\cdots&0\\
1&0&x-1&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
1&0&0&\cdots&x-1\end{array}}_{n}}\right|\right\}\,n=(x+n-1)(x-1)^{n-1}.
\end{align*}
@col
The last step follows by the fact that the matrix on the left hand side is the lower triangular matrix.
@endcol
@end
@slide
@eg
Let $B_{n}$ be a $n\times n$ matrix in the form
\begin{align*}
\displaystyle \begin{bmatrix}1-a_{1}&a_{2}&0&\cdots&0&0\\
-1&1-a_{2}&a_{3}&\cdots&0&0\\
0&-1&1-a_{3}&\cdots&0&0\\
\vdots&\vdots&\vdots&&\vdots&\vdots\\
0&0&0&\cdots&1-a_{n-1}&a_{n}\\
0&0&0&\cdots&-1&1-a_{n}\end{bmatrix}
\end{align*}
<ol class="ltx_enumerate">
<li class="ltx_item">
Show that $\det(B_{n})=\det(B_{n-1})+(-1)^{n}(a_{1}a_{2}\cdots a_{n})$.
</li>
<li class="ltx_item">
Hence show $\det(B_{n})=1+\sum_{i=1}^{n}(-1)^{i}(a_{1}a_{2}\cdots a_{i})$.
</li>
</ol>
@end
@sol
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
@col
Adding rows $R_{1},\ldots,R_{n-1}$ to $R_{n}$, we have:
@col
$\det(B_{n})=$
\begin{align*}
\displaystyle \begin{vmatrix}1-a_{1}&a_{2}&0&\cdots&0&0\\
-1&1-a_{2}&a_{3}&\cdots&0&0\\
0&-1&1-a_{3}&\cdots&0&0\\
\vdots&\vdots&\vdots&&\vdots&\vdots\\
0&0&0&\cdots&1-a_{n-1}&a_{n}\\
-a_{1}&0&0&\cdots&0&1\end{vmatrix}
\end{align*}
@newcol
Expand along the last row, the determinant above is equal to:
\begin{align*}
\displaystyle (-1)^{n+1}(-a_{1})\begin{vmatrix}a_{2}&0&\cdots&0&0\\
1-a_{2}&a_{3}&\cdots&0&0\\
-1&1-a_{3}&\cdots&0&0\\
\vdots&\vdots&\vdots&&\vdots\\
0&0&\cdots&1-a_{n-1}&a_{n}\end{vmatrix}+
\end{align*}
\begin{align*}
\displaystyle +(-1)^{n+n}\begin{vmatrix}1-a_{1}&a_{2}&0&\cdots&0\\
-1&1-a_{2}&a_{3}&\cdots&0\\
0&-1&1-a_{3}&\cdots&0\\
\vdots&\vdots&\vdots&&\vdots\\
0&0&0&\cdots&1-a_{n-1}\\
\end{vmatrix}
\end{align*}
@col
The first matrix is an lower triangular matrix, so the determinant is the product of the diagonal entries, the second matrix is $B_{n-1}$.
\begin{align*}
\displaystyle =(-1)^{n}(a_{1}\cdots a_{n})+\det(B_{n-1}).
\end{align*}
@endcol</li>
<li class="ltx_item">
@col
We prove the result by <strong>mathematical induction</strong>:

<strong>Step 1</strong>:
@newcol
The formula is valid for $n=1$: $\det(B_{1})=1-a_{1}$.
@endcol

<strong>Step 2</strong>:
@newcol
Suppose the formula is true for $n=k$, we want to show that the formula is true for $n=k+1$:
\begin{align*}
\displaystyle B_{k+1}=(-1)^{k+1}(a_{1}\cdots a_{k+1})+\det(B_{k})
\end{align*}
\begin{align*}
\displaystyle =1+\sum_{i=1}^{k}(-1)^{i}(a_{1}a_{2}\cdots a_{i})+(-1)^{k+1}(a_{1}\cdots a_{k+1})
\end{align*}
\begin{align*}
\displaystyle =1+\sum_{i=1}^{k+1}(-1)^{i}(a_{1}\cdots a_{i})
\end{align*}
\begin{align*}
\displaystyle =1+\sum_{i=1}^{n}(-1)^{i}(a_{1}\cdots a_{i}).
\end{align*}
@col
The formula is true for $n=k+1$.
@endcol

<strong>Step 3</strong>:
@newcol
By mathematical induction, the formula is valid for all positive integer. <b>Explanation</b>: the formula is true for $k=1$, then it is true for $k+1=2$, so true for $k+1=3$, etc. Hence true for all integers.
@endcol
</li>
</ol>
@qed
@end
@slide
@eg
Let $C_{n}$ be a $n\times n$ matrix given by
\begin{align*}
\displaystyle C_{n}=\left.\left[\vphantom{\begin{array}[]{c}1\\
1\\
1\\
1\\
1\\
1\end{array}}\smash{\underbrace{\begin{array}[]{cccccc}x&a&a&\cdots&a&a\\
-a&x&a&\cdots&a&a\\
-a&-a&x&\cdots&a&a\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
-a&-a&-a&\cdots&-a&x\end{array}}_{n}}\right]\right\}\,n
\end{align*}
<ol class="ltx_enumerate">
<li class="ltx_item">
Show that $\det(C_{n})=a(x+a)^{n-1}+(x-a)\det(C_{n-1})$.
</li>
<li class="ltx_item">
Show that $\det(C_{n})=\frac{1}{2}((x+a)^{n}+(x-a)^{n})$.
</li>

</ol>
@end
@sol
@col
<ol class="ltx_enumerate">
<li class="ltx_item">
@col
The last column can be written as
\begin{align*}
\displaystyle \begin{bmatrix}a\\
a\\
a\\
\vdots\\
x\end{bmatrix}=\begin{bmatrix}a\\
a\\
a\\
\vdots\\
a\end{bmatrix}+\begin{bmatrix}0\\
0\\
0\\
\vdots\\
x-a\end{bmatrix}.
\end{align*}
@newcol
Then $\det(C_{n})=$
\begin{align*}
\displaystyle
\left|
%% \vphantom{\begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}}\smash{\underbrace{
%%
\begin{array}[]{cccccc}x&a&a&\cdots&a&a\\
-a&x&a&\cdots&a&a\\
-a&-a&x&\cdots&a&a\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
-a&-a&-a&\cdots&-a&a\end{array}
%% }_{n}}
%%
\right| +
\left|
%% \vphantom{\begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}}\smash{\underbrace{
\begin{array}[]{cccccc}x&a&a&\cdots&a&0\\
-a&x&a&\cdots&a&0\\
-a&-a&x&\cdots&a&0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
-a&-a&-a&\cdots&-a&x-a\end{array}
%% }_{n}}
\right|
\end{align*}
@col
For the first determinant, pulling out $a$ from the last column,
it is equal to:
\begin{align*}
\displaystyle a\left|
%% \vphantom{\begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}}\smash{\underbrace{
\begin{array}[]{cccccc}x&a&a&\cdots&a&1\\
-a&x&a&\cdots&a&1\\
-a&-a&x&\cdots&a&1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
-a&-a&-a&\cdots&-a&1\end{array}
%% }_{n}}
\right|
\end{align*}
Then, performing the row operations:
\[
-1R_{n}+R_{1},\ldots,-1R_{n-1}+R_{n-1},
\]
the determinant above is equal to:
\begin{align*}
\displaystyle a\left|
%% \vphantom{\begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}}\smash{\underbrace{
\begin{array}[]{cccccc}x+a&2a&2a&\cdots&2a&0\\
0&x+a&2a&\cdots&2a&0\\
0&0&x+a&\cdots&2a&0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
-a&-a&-a&\cdots&-a&1\end{array}
%% }_{n}}
\right|
\end{align*}
\begin{align*}
\displaystyle =(-1)^{n+n}a
\left|
%% \vphantom{\begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}
%% }\smash{\underbrace{
\begin{array}[]{cccccc}x+a&2a&2a&\cdots&2a\\
0&x+a&2a&\cdots&2a\\
0&0&x+a&\cdots&2a\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
0&0&0&\cdots&x+a\end{array}
%% }_{n-1}}
\right|
%% \right\}\,n-1
\end{align*}
(Expand along the last column.)
\begin{align*}
\displaystyle =a(x+a)^{n-1}
\end{align*}
(Determinant of upper triangular matrix.)

@col
For the second determinant,
\begin{align*}
\displaystyle \left|
%% \vphantom{
%% \begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}}\smash{\underbrace{
\begin{array}[]{cccccc}x&a&a&\cdots&a&0\\
-a&x&a&\cdots&a&0\\
-a&-a&x&\cdots&a&0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
-a&-a&-a&\cdots&-a&x-a\end{array}
%% }_{n}}
\right|
%%\right\}\,n
\end{align*}
\begin{align*}
\displaystyle =(x-a)\left|
%% \vphantom{\begin{array}[]{c}1\\
%% 1\\
%% 1\\
%% 1\\
%% 1\end{array}}\smash{\underbrace{
\begin{array}[]{ccccc}x&a&a&\cdots&a\\
-a&x&a&\cdots&a\\
-a&-a&x&\cdots&a\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
-a&-a&-a&\cdots&x-a\end{array}
%% }_{n-1}}
\right|
%% \right\}\,n-1\,\,\,(\text{expand along the last column})
\end{align*}
(Expand along the last column.)
\begin{align*}
\displaystyle =(x-a)\det(C_{n-1}).
\end{align*}
@col
Adding the results
\begin{align*}
\displaystyle \det(C_{n})=a(x+a)^{n-1}+(x-a)\det(C_{n-1}).
\end{align*}
@endcol
</li>
<li class="ltx_item">
@col
We will prove the formula by induction.

<strong>Step 1</strong>:
@newcol
When $n=1$, $C_{1}=[x]$, $\det(C_{1})=x=\frac{1}{2}((x+a)+(x-a))$. So the formula is valid for $n=1$.
@endcol

<strong>Step 2</strong>:
@newcol
Suppose the formula is valid for $n=k$, i.e.
\begin{align*}
\displaystyle \det(C_{k})=\frac{1}{2}((x+a)^{k}+(x-a)^{k}).
\end{align*}
@col
Then for $n=k+1$,
\begin{align*}
\displaystyle \det(C_{k+1}) &=a(x+a)^{k}+(x-a)\det(C_{k})\\
\displaystyle &=a(x+a)^{k}+(x-a)\frac{1}{2}((x+a)^{k}+(x-a)^{k})\\
\displaystyle &=\frac{1}{2}(x+a)^{k}(2a+x-a)+\frac{1}{2}(x-a)^{k+1}\\
\displaystyle &=\frac{1}{2}((x+a)^{k+1}+(x-a)^{k+1})\\
\displaystyle &=\frac{1}{2}((x+a)^{n}+(x-a)^{n}).
\end{align*}
@col
So the formula is valid for $n=k+1$.
@endcol

<strong>Step 3</strong>:
@newcol
By mathematical induction, the formula is valid for all integers $n\geq 1$.
@endcol
@qed
</li>
</ol>
@end
@slide
@eg
@title{Vandermonde Determinant}
This is the most important example of determinant. Let
\begin{align*}
\displaystyle V_{n}=\begin{bmatrix}1&1&1&\cdots&1\\
a_{1}&a_{2}&a_{3}&\cdots&a_{n}\\
a_{1}^{2}&a_{2}^{2}&a_{3}^{2}&\cdots&a_{n}^{2}\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{1}^{n-2}&a_{2}^{n-2}&a_{3}^{n-2}&\cdots&a_{n}^{n-2}\\
a_{1}^{n-1}&a_{2}^{n-1}&a_{3}^{n-1}&\cdots&a_{n}^{n-1}\end{bmatrix}
\end{align*}
<ol class="ltx_enumerate">
<li class="ltx_item">
$\det(V_{n})=\det(V_{n-1})\prod_{i=1}^{n-1}(a_{n}-a_{i})$.
</li>
<li class="ltx_item">
$\det(V_{n})=\prod_{1\leq i < j\leq n}(a_{j}-a_{i})$ for $n\geq 2$.
</li>
</ol>
@end
@sol
<ol class="ltx_enumerate">
<li class="ltx_item">
@col
Performing the row operations:
\[
-a_{n}R_{n-1}+R_{n}, -a_{n}R_{n-2}+R_{n-1}, \ldots, -a_{n}R_{1}+R_{2},
\]
we have:
$\det V_{n}=$
\begin{align*}
\displaystyle =\begin{vmatrix}1&1&1&\cdots&1\\
a_{1}-a_{n}&a_{2}-a_{n}&a_{3}-a_{n}&\cdots&0\\
a_{1}^{2}-a_{1}a_{n}&a_{2}^{2}-a_{2}a_{n}&a_{3}^{2}-a_{3}a_{n}&\cdots&0\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{1}^{n-2}-a_{1}^{n-3}a_{n}&a_{2}^{n-2}-a_{2}^{n-3}a_{n}&a_{3}^{n-2}-a_{3}^{n-3}a_{n}&\cdots&0\\
a_{1}^{n-1}-a_{1}^{n-2}a_{n}&a_{2}^{n-1}-a_{2}^{n-2}a_{n}&a_{3}^{n-1}-a_{3}^{n-2}a_{n}&\cdots&0\end{vmatrix}
\end{align*}
(expanding along the last column)
\begin{align*}
\displaystyle =(-1)^{1+n}\begin{vmatrix}a_{1}-a_{n}&a_{2}-a_{n}&a_{3}-a_{n}&\cdots&a_{n-1}-a_{n}\\
a_{1}(a_{1}-a_{n})&a_{2}(a_{2}-a_{n})&a_{3}(a_{3}-a_{n})&\cdots&a_{n-1}(a_{n-1}-a_{n})\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{1}^{n-3}(a_{1}-a_{n})&a_{2}^{n-3}(a_{2}-a_{n})&a_{3}^{n-3}(a_{3}-a_{n})&\cdots&a_{n-1}^{n-3}(a_{n-1}-a_{n})\\
a_{1}^{n-2}(a_{1}-a_{n})&a_{2}^{n-2}(a_{2}-a_{n})&a_{3}^{n-2}(a_{3}-a_{n})&\cdots&a_{n-1}^{n-2}(a_{n-1}-a_{n})\\
\end{vmatrix}
\end{align*}
(pull out factor $a_{1}-a_{n}$ from column 1, $a_{2}-a_{n}$ from column 2, …., $a_{n-1}-a_{n}$ from column $n-1$)
\begin{align*}
\displaystyle =(-1)^{n-1}(a_{1}-a_{n})(a_{2}-a_{n})\cdots(a_{n-1}-a_{n})\begin{vmatrix}1&1&1&\cdots&1\\
a_{1}&a_{2}&a_{3}&\cdots&a_{n-1}\\
a_{1}^{2}&a_{2}^{2}&a_{3}^{2}&\cdots&a_{n-1}^{2}\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{1}^{n-2}&a_{2}^{n-2}&a_{3}^{n-2}&\cdots&a_{n-1}^{n-2}\\
\end{vmatrix}
\end{align*}
\begin{align*}
\displaystyle =(a_{n}-a_{1})\cdots(a_{n}-a_{n-1})\det(V_{n-1})=\det(V_{n-1})\prod_{i=1}^{n-1}(a_{n}-a_{i}).
\end{align*}
</li>
<li class="ltx_item">
@col
Again by mathematical induction:

<strong>Step 1</strong>
@newcol
When $n=2$,
\begin{align*}
\displaystyle \begin{vmatrix}1&1\\
a_{1}&a_{2}\end{vmatrix}=a_{2}-a_{1}.
\end{align*}
@col
So the formula is valid for $n=2$.
@endcol

<strong>Step 2</strong>
@newcol
Suppose the statement is true for $n=k$, i.e.
\begin{align*}
\displaystyle \det(V_{k})=\prod_{1\leq i<j\leq k}(a_{j}-a_{i}).
\end{align*}
@col
Then for $n=k+1$
\begin{align*}
\displaystyle \det(V_{k+1})=\det(V_{k})\prod_{i=1}^{k}(a_{k+1}-a_{i})
\end{align*}
\begin{align*}
\displaystyle =\prod_{1\leq i<j\leq k}(a_{j}-a_{i})\prod_{i=1}^{k}(a_{k+1}-a_{i})
\end{align*}
\begin{align*}
\displaystyle =\prod_{1\leq i<j\leq k+1}(a_{j}-a_{i})
\end{align*}
\begin{align*}
\displaystyle =\prod_{1\leq 1<j\leq n}(a_{j}-a_{i}).
\end{align*}
@col
The formula is valid for $n=k+1$.
@endcol

<strong>Step 3</strong>
@newcol
By mathematical induction, the formula is valid for all $n\geq 2$. Or without mathematical induction, you can simple repeat the steps again and again until $n=2$.
@endcol
</li>
</ol>
@qed
@end

@section{More properties of determinants}
@cor
<ol class="ltx_enumerate">
<li class="ltx_item">
Let $I_{n}\xrightarrow{R_{i}\leftrightarrow R_{j}}J$, then $\det\left(J\right)=-1$.
</li>
<li class="ltx_item">
Let $I_{n}\xrightarrow{\alpha R_{i}}J$, then $\det\left(J\right)=\alpha$.
</li>
<li class="ltx_item">
Let $I_{n}\xrightarrow{\alpha R_{i}+R_{j}}J$, then $\det\left(J\right)=1$.
</li>

</ol>
@end
@slide
@cor
@label{cor:detBJA}
Let $A$ be a square matrix, apply row operation on $A$ and obtain a new matrix $B$.
Let $J$ be obtained by applying the same row operation on $I_{n}$. By lecture 13, $B=JA$.
Then $\det\left(B\right)=\det\left(JA\right)=\det\left(J\right)\det\left(A\right)$.
@end
@slide
Recall that an $n\times n$ square matrix $A$ is nonsingular if and only if $A$ is row equivalent to $I_n$.

@col
By @ref{thm:ROEM},
this is in turn equivalent to the existence of a sequence of
"elementary matrices" $J_i$, corresponding to row operations,
such that:
\[
A = J_1 J_2 \cdots J_k I_n.
\]
@col
Applying the previous corollary repeatedly,
we have:
\[
\det(A) = \det(J_1) \det(J_2) \cdots \det(J_k) \det(I_n).
\]
@col
Since, each $\det(J_i) \neq 0$, we have:
@endcol
@thm
@label{thm:nonsingulardet}
@col
$A$ is nonsingular if and only if $\det\left(A\right)\neq 0$.
@end
<!-- @proof
@newcol
Suppose $A$ is nonsingular, then $A$ is invertible, so
\begin{align*}
\displaystyle AA^{-1}=I_{n}.
\end{align*}
\begin{align*}
\displaystyle \det\left(A\right)\det\left(A^{-1}\right)=\det\left(I_{n}\right)=1.&
\end{align*}
@col
So $\det\left(A\right)$ is nonzero. Next suppose $\det\left(A\right)\neq 0$, we want to show that $A$ is nonsingular.
If $A$ is singular, then by lecture 13, there exists an invertible matrix $J$, such that $JA=B$,
where $B$ is of RREF with rank $< n$. Hence $B$ has a zero row. Therefore $\det\left(B\right)=0$.
Because $J$ is invertible, $\det\left(J\right)\neq 0$. So $\det\left(A\right)=0$. Contradiction.
@qed
@endcol
@end -->
@slide
@thm
@label{thm:DETMULT}
If $A$ and $B$ are square matrices. Then
\begin{align*}
\displaystyle \det\left(AB\right)=\det\left(A\right)\det\left(B\right).
\end{align*}
@end
@proof
@col
Suppose $A$ or $B$ is singular.
Then, accordingly $\det(A)$ or $\det(B)$ is equal to zero.
By @ref{NPNT} the matrix $AB$ is also singular, hence:
\[
\det(AB) = 0 = \det(A)\det(B).
\]

@col
If both $A$ and $B$ are nonsingular,
then there are elementary matrices, $J_i$ and $K_i$,
coresponding to row operations, such that:
\[
A = J_1 J_2\cdots J_k I_n,
\]
\[
B = K_1 K_2\cdots K_l I_n.
\]
This implies that:
\[
AB = (J_1 J_2\cdots J_k)(K_1 K_2\cdots K_l) I_n
\]
@col
By @ref{cor:detBJA}, we have:
\begin{align*}
\det(A) &= \det(J_1)\det(J_2)\cdots\det(J_k),\\
\det(B) &= \det(K_1)\det(K_2)\cdots\det(K_l),\\
\det(AB) &= \det(J_1)\det(J_2)\cdots\det(J_k)\\
&\quad\quad\cdot \det(K_1)\det(K_2)\cdots\det(K_l).
\end{align*}
Hence, $\det(AB) = \det(A)\det(B)$.
@qed
@end
@slide
@eg
Find $x$ such that
\begin{align*}
\displaystyle A=\begin{bmatrix}2&1&0&1\\
0&1&1&1\\
1&0&0&x\\
0&2&3&1\\
\end{bmatrix}
\end{align*}
is singular.Expand along the first column
\begin{align*}
\displaystyle \det\left(A\right)=2\begin{vmatrix}1&1&1\\
0&0&x\\
2&3&1\end{vmatrix}+\begin{vmatrix}1&0&1\\
1&1&1\\
2&3&1\end{vmatrix}.
\end{align*}
@newcol
Expand along the second row
\begin{align*}
\displaystyle \begin{vmatrix}1&1&1\\
0&0&x\\
2&3&1\end{vmatrix}=-x\begin{vmatrix}1&1\\
2&3\end{vmatrix}=-x.
\end{align*}
@col
Finally
\begin{align*}
\displaystyle \begin{vmatrix}1&0&1\\
1&1&1\\
2&3&1\end{vmatrix}=-1.
\end{align*}
@col
Hence
\begin{align*}
\displaystyle \det\left(A\right)=-2x-1.
\end{align*}
@col
It is singular if and only if $\det\left(A\right)=0$ if and only if $x=-\frac{1}{2}$.
@endcol
@end
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}a&b&c&d\\
e&0&0&0\\
f&0&0&0\\
g&0&0&0\end{bmatrix}
\end{align*}
find $\det\left(A\right)$.

<strong>Method 1</strong>
@newcol
\begin{align*}
\displaystyle a\begin{vmatrix}0&0&0\\
0&0&0\\
0&0&0\end{vmatrix}-b\begin{vmatrix}e&0&0\\
f&0&0\\
g&0&0\end{vmatrix}+c\begin{vmatrix}e&0&0\\
f&0&0\\
g&0&0\end{vmatrix}-d\begin{vmatrix}e&0&0\\
f&0&0\\
g&0&0\end{vmatrix}
\end{align*}
@col
In each of the above matrices, there is one zero columns, so all the determinants of the $3\times 3$ submatrices must be zero.
Therefore the above is
\begin{align*}
\displaystyle a0-b0+c0-d0=0.
\end{align*}
@endcol

<strong>Method 2</strong>
@newcol
If $c=0$, then column 3 is the zero column, so $\det\left(A\right)=0$. Otherwise
\begin{align*}
\displaystyle A\begin{bmatrix}0\\
1\\
-b/c\\
0\end{bmatrix}=\mathbf{0}.
\end{align*}
@col
So $A$ is singular and hence $\det\left(A\right)=0$.
@endcol
@end
@slide
@thm
If $\det\left(A\right)\neq 0$, then $A$ is invertible and
\begin{align}
\label{A-1}
\displaystyle \det\left(A^{-1}\right)=\frac{1}{\det\left(A\right)}.
\end{align}
@end
@proof
@newcol
It follows from @ref{thm:nonsingulardet} that $A^{-1}$ exists.
The identity $\eqref{A-1}$ then follows from:
\[
AA^{-1} = I_n
\]
and @ref{thm:DETMULT}.
@qed
@endcol
@end
@slide
@thm
@title{Cramer’s rule}
Let $A$ be a invertible square matrix of size $n$. Let $\mathbf{b}\in{\mathbb{R}}^{n}$.
Let $M_{k}$ be the square matrix by replacing the $k$-th column of $A$ by $\mathbf{b}$.
If
\begin{align*}
\displaystyle \mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}\end{bmatrix}
\end{align*}
is a solution of $A\mathbf{x}=\mathbf{b}$, then
\begin{align*}
\displaystyle x_{k}=\frac{\det\left(M_{k}\right)}{\det\left(A\right)}
\end{align*}
where $k=1,\ldots,n$.
@end
@proof
@newcol
Because $A$ is invertible, $A\mathbf{x}=\mathbf{b}$ has a unique solution. Let $X_{k}$ be matrix obtained from the identity matrix $I_{n}$ by replacing column $k$ with $\mathbf{x}$.
Then
\begin{align*}
\displaystyle A\mathbf{e}_{i}=\mathbf{A}_{i}\text{ if $i\neq k$}\qquad A\mathbf{x}=\mathbf{b}\text{ if $i=k$.}
\end{align*}
@col
Hence
\begin{align*}
\displaystyle AX_{k}=M_{k}.
\end{align*}
@col
Expanding $X_{k}$ along the row $k$, we have
\begin{align*}
\displaystyle \det\left(X_{k}\right)=x_{k}\det\left(I_{n-1}\right)=x_{k}.
\end{align*}
@col
So
\begin{align*}
\displaystyle \det\left(M_{k}\right)=\det\left(AX_{k}\right)=\det\left(A\right)\det\left(X_{k}\right)=\det\left(A\right)x_{k}.
\end{align*}
@col
Therefore
\begin{align*}
\displaystyle x_{k}=\frac{\det\left(M_{k}\right)}{\det\left(A\right)}.
\end{align*}
@qed
@endcol
@end
@slide
@eg
Using Carmaer’s rule to solve the following system of linear equation.
\begin{align*}
\displaystyle x_{1}+2x_{2}+3x_{3}=2 \\
\displaystyle x_{1}\qquad+x_{3}=3 \\
\displaystyle x_{1}+x_{2}-x_{3}=1
\end{align*}
@newcol
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&3\\
1&0&1\\
1&1&-1\end{bmatrix}\qquad\mathbf{b}=\begin{bmatrix}2\\
3\\
1\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle \det\left(A\right)=6.
\end{align*}
\begin{align*}
\displaystyle M_{1}=\begin{bmatrix}2&2&3\\
3&0&1\\
1&1&-1\end{bmatrix},\det\left(M_{1}\right)=15.
\end{align*}
\begin{align*}
\displaystyle x_{1}=\frac{\det\left(M_{1}\right)}{\det\left(A\right)}=\frac{15}{6}=\frac{5}{2}.
\end{align*}
\begin{align*}
\displaystyle M_{2}=\begin{bmatrix}1&2&3\\
1&3&1\\
1&1&-1\end{bmatrix},\det\left(M_{2}\right)=-6.
\end{align*}
\begin{align*}
\displaystyle x_{2}=\frac{\det\left(M_{2}\right)}{\det\left(A\right)}=\frac{-6}{6}=-1.
\end{align*}
\begin{align*}
\displaystyle M_{3}=\begin{bmatrix}1&2&2\\
1&0&3\\
1&1&1\end{bmatrix},\det\left(M_{3}\right)=3.
\end{align*}
\begin{align*}
\displaystyle x_{3}=\frac{\det\left(M_{3}\right)}{\det\left(A\right)}=\frac{3}{6}=\frac{1}{2}.
\end{align*}
@col
Thus
\begin{align*}
\displaystyle \begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}\end{bmatrix}=\begin{bmatrix}\frac{5}{2}\\
-1\\
\frac{1}{2}\end{bmatrix}
\end{align*}
is a solution.
@endcol
@end
@slide
@thm
@title{Formula for inverse}
Suppose $A$ is an invertible matrix. Then
\begin{align*}
\displaystyle [A^{-1}]_{ji}=\frac{(-1)^{i+j}\det\left(A(i|j)\right)}{\det\left(A\right)}.
\end{align*}
@newcol
Pay attention to the order of the indexes $i$ and $j$!
@endcol
@end
@proof
@newcol
Let $B=A^{-1}$.
Let $\mathbf{B}_{i}$ be the $i$-th column of $B$. Then
\begin{align*}
\displaystyle A\mathbf{B}_{i}=\mathbf{e}_{i}.
\end{align*}
@col
The vector $\mathbf{B}_{i}$ is a solution of $A\mathbf{x}=\mathbf{e}_{i}$.
We can use the previous theorem to find $\mathbf{B}_{i}$.
Let $M_{j}$ be the square matrix by replacing the $j$-th column of $A$ by $\mathbf{e}_{i}$.
Expand along the $j$-th column of $M_{j}$, we have
\begin{align*}
\displaystyle \det\left(M_{j}\right)=(-1)^{i+j}\det\left(M_{j}(i|j)\right)=(-1)^{i+j}\det\left(A(i|j)\right).
\end{align*}
@col
Then the $j$-th coordinate of $\mathbf{B}_{i}$ is given by
\begin{align*}
\displaystyle B_{ji}=[\mathbf{B}_{i}]_{j}=\frac{\det\left(M_{j}\right)}{{\det\left(A\right)}}=\frac{(-1)^{i+j}\det\left(A(i|j)\right)}{\det\left(A\right)}.
\end{align*}
@qed
@endcol
@end
@slide
@eg
By the above formula, find the inverse of
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2&3\\
1&0&1\\
1&1&-1\end{bmatrix},
\end{align*}
\begin{align*}
\displaystyle \det\left(A\right)=6.
\end{align*}
\begin{align*}
\displaystyle A(1|1)=\begin{bmatrix}0&1\\
1&-1\end{bmatrix},\qquad\det\left(A(1|1)\right)=-1,
\end{align*}
\begin{align*}
\displaystyle A(1|2)=\begin{bmatrix}1&1\\
1&-1\end{bmatrix},\qquad\det\left(A(1|2)\right)=-2,
\end{align*}
\begin{align*}
\displaystyle A(1|3)=\begin{bmatrix}1&0\\
1&1\end{bmatrix},\qquad\det\left(A(1|3)\right)=1,
\end{align*}
\begin{align*}
\displaystyle A(2|1)=\begin{bmatrix}2&3\\
1&-1\end{bmatrix},\qquad\det\left(A(2|1)\right)=-5,
\end{align*}
\begin{align*}
\displaystyle A(2|2)=\begin{bmatrix}1&3\\
1&-1\end{bmatrix},\qquad\det\left(A(2|2)\right)=-4,
\end{align*}
\begin{align*}
\displaystyle A(2|3)=\begin{bmatrix}1&2\\
1&1\end{bmatrix},\qquad\det\left(A(2|3)\right)=-1,
\end{align*}
\begin{align*}
\displaystyle A(3|1)=\begin{bmatrix}2&3\\
0&1\end{bmatrix},\qquad\det\left(A(3|1)\right)=2,
\end{align*}
\begin{align*}
\displaystyle A(3|2)=\begin{bmatrix}1&3\\
1&1\end{bmatrix},\qquad\det\left(A(3|2)\right)=-2,
\end{align*}
\begin{align*}
\displaystyle A(3|3)=\begin{bmatrix}1&2\\
1&0\end{bmatrix},\qquad\det\left(A(3|3)\right)=-2.
\end{align*}
@end
\begin{align*}
\displaystyle A^{-1}=\frac{1}{\det\left(A\right)}\begin{bmatrix}\det\left(A(1|1)\right)&-\det\left(A(2|1)\right)&\det\left(A(3|1)\right)\\
-\det\left((A(1|2)\right)&\det\left(A(2|2)\right)&-\det\left(A(3|2)\right)\\
\det\left(A(1|3)\right)&-\det\left(A(2|3)\right)&\det\left(A(3|3)\right)\end{bmatrix}=\begin{bmatrix}-\frac{1}{6}&\frac{5}{6}&\frac{1}{3}\\
\frac{1}{3}&-\frac{2}{3}&\frac{1}{3}\\
\frac{1}{6}&\frac{1}{6}&-\frac{1}{3}\end{bmatrix}
\end{align*}
@section{Properties of Determinant (summary)}

Let $A$ be a square matrix with size $n$.

<ol class="ltx_enumerate">
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{vmatrix}a&b\\
c&d\end{vmatrix}=ad-bc.
\end{align*}
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle A=\begin{vmatrix}a_{11}&a_{12}&a_{13}\\
a_{21}&a_{22}&a_{23}\\
a_{31}&a_{32}&a_{33}\end{vmatrix}=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align*}
</li>
<li class="ltx_item">
Expand along row $i$
\begin{align*}
\displaystyle\det\left(A\right)&=(-1)^{i+1}\left[A\right]_{i1}\det\left(A\left(i|1\right)\right)+(-1)^{i+2}\left[A\right]_{i2}\det\left(A\left(i|2\right)\right) \\
&\quad+(-1)^{i+3}\left[A\right]_{i3}\det\left(A\left(i|3\right)\right)+\cdots+(-1)^{i+n}\left[A\right]_{in}\det\left(A\left(i|n\right)\right)
\end{align*}
</li>
<li class="ltx_item">
Expand along column $j$
\begin{align*}
\displaystyle\det\left(A\right)&=(-1)^{1+j}\left[A\right]_{1j}\det\left(A\left(1|j\right)\right)+(-1)^{2+j}\left[A\right]_{2j}\det\left(A\left(2|j\right)\right) \\
&\quad+(-1)^{3+j}\left[A\right]_{3j}\det\left(A\left(3|j\right)\right)+\cdots+(-1)^{n+j}\left[A\right]_{nj}\det\left(A\left(n|j\right)\right)
\end{align*}
</li>
<li class="ltx_item">
$\det\left(A^{t}\right)=\det\left(A\right)$
</li>
<li class="ltx_item">
Determinant of upper/lower triangular matrix.
\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&a_{12}&a_{13}&\cdots&a_{1n}\\
0&a_{22}&a_{23}&\cdots&a_{2n}\\
0&0&a_{33}&\cdots&a_{3n}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&a_{nn}\end{vmatrix}=a_{11}a_{22}\cdots a_{nn}.
\end{align*}
\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&0&0&\cdots&0\\
a_{21}&a_{22}&0&\cdots&0\\
a_{31}&a_{32}&a_{33}&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
a_{n1}&a_{n2}&a_{n3}&\cdots&a_{nn}\end{vmatrix}=a_{11}a_{22}\cdots a_{nn}.
\end{align*}
</li>
<li class="ltx_item">
Suppose that $A$ is a square matrix with a row where every entry is zero, or a column where every entry is zero. Then $\det\left(A\right)=0$.
</li>
<li class="ltx_item">
Suppose that $A$ is a square matrix with two equal rows, or two equal columns,
i.e., $R_{i}=R_{j}$ or $C_{i}=C_{j}$ for $i\neq j$. Then $\det\left(A\right)=0$.
</li>
<li class="ltx_item">
Let $B$ be the square matrix obtained from $A$ by interchanging the location of two rows, or interchanging the location of two columns, i.e., $R_{i}\leftrightarrow R_{j}$ or $C_{i}\leftrightarrow C_{j}$, $i\neq j$. Then $\det\left(B\right)=-\det\left(A\right)$.
</li>
<li class="ltx_item">
Let $B$ be the square matrix obtained from $A$ by multiplying a single row (say, row $i$) by the scalar $\alpha$, or by multiplying a single column by the scalar $\alpha$,
i.e., $\alpha R_{i}$ or $\alpha C_{i}$. Then $\det\left(B\right)=\alpha\det\left(A\right)$.
</li>
<li class="ltx_item">
Let $B$ be the square matrix obtained from $A$ by multiplying a row by the scalar $\alpha$ and then adding it to another row, or by multiplying a column by the scalar $\alpha$ and then adding it to another column, i.e., $\alpha R_{i}+R_{j}$ or $\alpha C_{i}+C_{j}$ for $i\neq j$. Then $\det\left(B\right)=\det\left(A\right)$.
</li>
<li class="ltx_item">
\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{i-1,1}&a_{i-1,2}&\cdots&a_{i-1,n}\\
b_{1}+c_{1}&b_{2}+c_{2}&\cdots&b_{n}+c_{n}\\
a_{i+1,1}&a_{i+1,2}&\cdots&a_{i+1,n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}\\
\end{vmatrix}=\begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{i-1,1}&a_{i-1,2}&\cdots&a_{i-1,n}\\
b_{1}&b_{2}&\cdots&b_{n}\\
a_{i+1,1}&a_{i+1,2}&\cdots&a_{i+1,n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}\\
\end{vmatrix}+\begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{i-1,1}&a_{i-1,2}&\cdots&a_{i-1,n}\\
c_{1}&c_{2}&\cdots&c_{n}\\
a_{i+1,1}&a_{i+1,2}&\cdots&a_{i+1,n}\\
\vdots&\vdots&\cdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}\\
\end{vmatrix}
\end{align*}
@newcol
Similarly
\begin{align*}
\displaystyle \begin{vmatrix}a_{11}&\cdots&a_{1,i-1}&b_{1}+c_{1}&a_{1,i+1}&\cdots&a_{1n}\\
a_{21}&\cdots&a_{2,i-1}&b_{2}+c_{2}&a_{2,i+1}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{n1}&\cdots&a_{n,i-1}&b_{n}+c_{n}&a_{n,i+1}&\cdots&a_{nn}\end{vmatrix}
\end{align*}
\begin{align*}
\displaystyle =\begin{vmatrix}a_{11}&\cdots&a_{1,i-1}&b_{1}&a_{1,i+1}&\cdots&a_{1n}\\
a_{21}&\cdots&a_{2,i-1}&b_{2}&a_{2,i+1}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{n1}&\cdots&a_{n,i-1}&b_{n}&a_{n,i+1}&\cdots&a_{nn}\end{vmatrix}+\begin{vmatrix}a_{11}&\cdots&a_{1,i-1}&c_{1}&a_{1,i+1}&\cdots&a_{1n}\\
a_{21}&\cdots&a_{2,i-1}&c_{2}&a_{2,i+1}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
a_{n1}&\cdots&a_{n,i-1}&c_{n}&a_{n,i+1}&\cdots&a_{nn}\end{vmatrix}
\end{align*}
@endcol</li>
<li class="ltx_item">
If $A$ and $B$ are square matrices, then
\begin{align*}
\displaystyle \det\left(AB\right)=\det\left(A\right)\det\left(B\right).
\end{align*}
</li>

</ol>
@setchapter{19}
@chapter{Eigenvalues and Eigenvectors}

<h5 class="notkw">Reference.</h5>
Beezer, Ver 3.5 Subsection EEM (print version p283-285), Subsection CEE and ECEE (print version p289-297)

<h5 class="notkw">Exercise.</h5>
Exercises with solutions can be downloaded at @href{http://linear.ups.edu/download/fcla-3.50-solution-manual.pdf}

(Replace $\mathbb{C}$ by ${\mathbb{R}}^{\hbox{}}$)Section EE, p103-108, all except C22, M60. Note that for the questions regarding diagonalizability, use our method instead of the method in the solution manual.

@section{Eigenvalues and Eigenvectors of a Matrix}
@defn
@title{Eigenvalues and Eigenvectors of a Matrix}
@label{EEM}
Suppose that $A$ is a square matrix of size $n$, $\mathbf{x}$ a non-zero vector in ${\mathbb{R}}^{n}$, and $\lambda$ a scalar in ${\mathbb{R}}^{\hbox{}}$. We say $\mathbf{x}$ is an <b>eigenvector</b> of $A$ with <b>eigenvalue</b> $\lambda$ if
\begin{align*}
\displaystyle A\mathbf{x}=\lambda\mathbf{x}.
\end{align*}
@end
@slide
@eg
Let
\begin{align*}
\displaystyle A=\begin{bmatrix}2&1&1\\
1&2&1\\
1&1&2\end{bmatrix}
\end{align*}
and let
\begin{align*}
\displaystyle \mathbf{u}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\,\,\mathbf{v}=\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\,\,\mathbf{w}=\begin{bmatrix}1\\
0\\
-1\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle A\mathbf{u}=\begin{bmatrix}4\\
4\\
4\end{bmatrix}=4\mathbf{u},\,\,A\mathbf{v}=\begin{bmatrix}1\\
-1\\
0\end{bmatrix}=1\mathbf{v},\,\,A\mathbf{w}=\begin{bmatrix}1\\
0\\
-1\end{bmatrix}=1\mathbf{w}.
\end{align*}
@col
So $\mathbf{u}$ is an eigenvector of $A$ with eigenvalue $4$,
$\mathbf{v}$ is an eigenvector of $A$ with eigenvalue $1$, and
$\mathbf{w}$ is an eigenvector $A$ with eigenvalue $1$. Now let $\mathbf{x}=100\mathbf{u}$. Then
\begin{align*}
\displaystyle A\mathbf{x}=100A\mathbf{u}=400\mathbf{u}=4\mathbf{x}.
\end{align*}
@col
So $\mathbf{x}$ is an eigenvector of $A$ with eigenvalue $4$.Next let $\mathbf{y}=\mathbf{v}+\mathbf{w}$, then
\begin{align*}
\displaystyle A\mathbf{y}=A\mathbf{v}+A\mathbf{w}=\mathbf{v}+\mathbf{w}=1\mathbf{y}.
\end{align*}
@col
So $\mathbf{y}$ is an eigenvector of $A$ with eigenvalue $1$. Finally, let $\mathbf{z}=\mathbf{u}+\mathbf{v}=\begin{bmatrix}2\\
0\\
1\end{bmatrix}$. Then
\begin{align*}
\displaystyle A\mathbf{z}=4\mathbf{u}+\mathbf{v}=\begin{bmatrix}5\\
3\\
4\end{bmatrix}
\end{align*}
is not a scalar multiple of $\mathbf{z}$. So $\mathbf{z}$ is not an eigenvector.
This shows that sum of eigenvectors need not be an eigenvector.
@endcol
@end
@eg
@label{eg:example3}
Consider the matrix
\begin{align*}
\displaystyle A=\begin{bmatrix}204&98&-26&-10\\
-280&-134&36&14\\
716&348&-90&-36\\
-472&-232&60&28\end{bmatrix}
\end{align*}
and the vectors
\begin{align*}
\displaystyle\mathbf{x}=\begin{bmatrix}1\\
-1\\
2\\
5\end{bmatrix}&\mathbf{y}=\begin{bmatrix}-3\\
4\\
-10\\
4\end{bmatrix}&\mathbf{z}=\begin{bmatrix}-3\\
7\\
0\\
8\end{bmatrix}&\mathbf{w}=\begin{bmatrix}1\\
-1\\
4\\
0\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle A\mathbf{x}=\begin{bmatrix}204&98&-26&-10\\
-280&-134&36&14\\
716&348&-90&-36\\
-472&-232&60&28\end{bmatrix}\begin{bmatrix}1\\
-1\\
2\\
5\end{bmatrix}=\begin{bmatrix}4\\
-4\\
8\\
20\end{bmatrix}=4\begin{bmatrix}1\\
-1\\
2\\
5\end{bmatrix}=4\mathbf{x}
\end{align*}
so that $\mathbf{x}$ is an eigenvector of $A$ with eigenvalue $\lambda=4$.

@col
Also,
\begin{align*}
\displaystyle A\mathbf{y}=\begin{bmatrix}204&98&-26&-10\\
-280&-134&36&14\\
716&348&-90&-36\\
-472&-232&60&28\end{bmatrix}\begin{bmatrix}-3\\
4\\
-10\\
4\end{bmatrix}=\begin{bmatrix}0\\
0\\
0\\
0\end{bmatrix}=0\begin{bmatrix}-3\\
4\\
-10\\
4\end{bmatrix}=0\mathbf{y}
\end{align*}
so that $\mathbf{y}$ is an eigenvector of $A$ with eigenvalue $\lambda=0$.

@col
Also,
\begin{align*}
\displaystyle A\mathbf{z}=\begin{bmatrix}204&98&-26&-10\\
-280&-134&36&14\\
716&348&-90&-36\\
-472&-232&60&28\end{bmatrix}\begin{bmatrix}-3\\
7\\
0\\
8\end{bmatrix}=\begin{bmatrix}-6\\
14\\
0\\
16\end{bmatrix}=2\begin{bmatrix}-3\\
7\\
0\\
8\end{bmatrix}=2\mathbf{z}
\end{align*}
so that $\mathbf{z}$ is an eigenvector of $A$ with eigenvalue $\lambda=2$.

@col
Finally,
\begin{align*}
\displaystyle A\mathbf{w}=\begin{bmatrix}204&98&-26&-10\\
-280&-134&36&14\\
716&348&-90&-36\\
-472&-232&60&28\end{bmatrix}\begin{bmatrix}1\\
-1\\
4\\
0\end{bmatrix}=\begin{bmatrix}2\\
-2\\
8\\
0\end{bmatrix}=2\begin{bmatrix}1\\
-1\\
4\\
0\end{bmatrix}=2\mathbf{w}
\end{align*}
so that $\mathbf{w}$ is an eigenvector of $A$ with eigenvalue $\lambda=2$.

@col
We have demonstrated four eigenvectors of $A$. Are there more? Yes, any nonzero scalar multiple of an eigenvector is again an eigenvector. In this example, setting $\mathbf{u}=30\mathbf{x}$, we have
\begin{align*}
\displaystyle A\mathbf{u}=A(30\mathbf{x})=30A\mathbf{x}=30(4\mathbf{x})=4(30\mathbf{x})=4\mathbf{u}
\end{align*}
so that $\mathbf{u}$ is also an eigenvector of $A$ with the same eigenvalue, $\lambda=4$.

@col
The vectors $\mathbf{z}$ and $\mathbf{w}$ are both eigenvectors of $A$ for the same eigenvalue $\lambda=2$, yet this is not as simple as the two vectors just being scalar multiples of each other (they are not). Look what happens when we add them together, forming $\mathbf{v}=\mathbf{z}+\mathbf{w}$, which we then multiply by $A$:
\begin{align*}
\displaystyle A\mathbf{v}=A(\mathbf{z}+\mathbf{w})=A\mathbf{z}+A\mathbf{w}
\end{align*}
\begin{align*}
\displaystyle =2\mathbf{z}+2\mathbf{w}=2(\mathbf{z}+\mathbf{w})=2\mathbf{v}.
\end{align*}
@col
Hence, $\mathbf{v}$ is also an eigenvector of $A$ with eigenvalue $\lambda=2$. It would appear that the set of eigenvectors that are associated with a fixed eigenvalue is closed under the vector space operations of ${\mathbb{R}}^{n}$.

@col
The vector $\mathbf{y}$ is an eigenvector of $A$ for the eigenvalue $\lambda=0$,
so $A\mathbf{y}=0\mathbf{y}=\mathbf{0}$. But this also means that $\mathbf{y}\in{\mathcal{N}}\!\left(A\right)$. There would appear to be a connection here also.
@endcol
@end
@section{Existence of Eigenvalues and Eigenvectors}

Observe that
\begin{align*}
\displaystyle A\mathbf{x}=\lambda\mathbf{x}\iff A\mathbf{x}-\lambda I_{n}\mathbf{x}=\mathbf{0}\iff\left(A-\lambda I_{n}\right)\mathbf{x}=\mathbf{0}.
\end{align*}
@newcol
So, any eigenvector $x$ with eigenvalue $\lambda$ is a nonzero element of the null space of $A-\lambda I_{n}$. In particular, the matrix $A-\lambda I_{n}$ is necessarily singular, therefore having zero determinant. These ideas motivate the following definition and example.

@endcol
@slide
@defn
@title{Characteristic Polynomial}
Suppose that $A$ is a square matrix of size $n$. Then the <b>characteristic polynomial</b> of $A$ is the polynomial $p_{A}\left(x\right)$ defined by
\begin{align*}
\displaystyle p_{A}\left(x\right)=\det\left(A-xI_{n}\right)
\end{align*}
@end
@slide
@eg
Consider
\begin{align*}
\displaystyle F=\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}.
\end{align*}
@newcol
We compute
\begin{align*}
\displaystyle p_{F}\left(x\right)&=\det\left(F-xI_{3}\right) \\
&=\begin{vmatrix}-13-x&-8&-4\\
12&7-x&4\\
24&16&7-x\end{vmatrix} \\
&=(-13-x)\begin{vmatrix}7-x&4\\
16&7-x\end{vmatrix}+(-8)(-1)\begin{vmatrix}12&4\\
24&7-x\end{vmatrix} \\
&\quad\quad+(-4)\begin{vmatrix}12&7-x\\
24&16\end{vmatrix} \\
&=(-13-x)((7-x)(7-x)-4(16)) \\
&\quad\quad+(-8)(-1)(12(7-x)-4(24)) \\
&\quad\quad+(-4)(12(16)-(7-x)(24)) \\
&=3+5x+x^{2}-x^{3} \\
&=-(x-3)(x+1)^{2}.
\end{align*}
@endcol
@end
@newcol
The characteristic polynomial is our main computational tool for finding eigenvalues, and will sometimes be used to aid us in determining the properties of eigenvalues.

@endcol
@slide
@thm
@title{Eigenvalues of a Matrix are Roots of Characteristic Polynomials}
Suppose that $A$ is a square matrix.
Then $\lambda$ is an eigenvalue of $A$ if and only if $p_{A}\left(\lambda\right)=0$.
@end
@proof
@newcol
We have the following equivalences:

$\lambda$ is an eigenvalue of $A$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $A\mathbf{x}=\lambda\mathbf{x}$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $A\mathbf{x}-\lambda\mathbf{x}=\mathbf{0}$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $A\mathbf{x}-\lambda I_{n}\mathbf{x}=\mathbf{0}$ $\iff$ there exists $\mathbf{x}\neq\mathbf{0}$ so that $(A-\lambda I_{n})\mathbf{x}=\mathbf{0}$ $\iff$ $A-\lambda I_{n}$ is singular $\iff$ $p_{A}\left(\lambda\right)=\det\left(A-\lambda I_{n}\right)=0$.
@qed
@endcol
@end
@slide
@thm
@title{Degree of the Characteristic Polynomial}
@label{DCP}
Suppose that $A$ is a square matrix of size $n$. Then the characteristic polynomial $p_{A}\left(x\right)$ has degree $n$.
@end
@proof
@newcol
You can skip the proof. The following briefly explains why the theorem is true. It is not a rigorous proof.We have
\begin{align*}
\displaystyle p_{A}\left(x\right)=\begin{vmatrix}a_{11}-x&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}-x&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{n1}&a_{n2}&\cdots&a_{nn}-x\end{vmatrix}.
\end{align*}
@col
The determinant is a sum of products of entries of $A-xI_{n}$, and all such products have degree at most $n-1$, except the product of the diagonal entries,
\begin{align*}
\displaystyle (a_{11}-x)(a_{22}-x)\cdots(a_{nn}-x),
\end{align*}
which has degree $n$. <b>Remark</b>: We can also see that the leading coefficient is $(-1)^{n}$.
@qed
@endcol
@end
@slide
@eg
@label{eg:example4}
In @ref{eg:example3}, we found the characteristic polynomial of
\begin{align*}
\displaystyle F=\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}
\end{align*}
to be $p_{F}\left(x\right)=-(x-3)(x+1)^{2}$. Being written in factored form, we can simply read off its roots; they are $x=3$ and $x=-1$. By the previous theorem, $\lambda=3$ and $\lambda=-1$ are both eigenvalues of $F$. Moreover, these are the only eigenvalues of $F$.
@end
@slide
@defn
@title{Eigenspace of a Matrix}
Suppose that $A$ is a square matrix and $\lambda$ is an eigenvalue of $A$.
Then the <b>eigenspace</b> of $A$ for $\lambda$, denoted by ${\mathcal{E}}_{A}\left(\lambda\right)$, is the set of all eigenvectors of $A$ with eigenvalue $\lambda$, together with the zero vector.
@end
@slide
@thm
@title{Eigenspace of a Matrix is a Null Space}
Suppose that $A$ is a square matrix of size $n$ and $\lambda$ is an eigenvalue of $A$. Then
\begin{align*}
\displaystyle {\mathcal{E}}_{A}\left(\lambda\right)={\mathcal{N}}\!\left(A-\lambda I_{n}\right).
\end{align*}
@end
@proof
@newcol
First, notice that $\mathbf{0}\in{\mathcal{E}}_{A}\left(\lambda\right)$ (by definition) and $\mathbf{0}\in{\mathcal{N}}\!\left(A-\lambda I_{n}\right)$.
Now consider any nonzero vector $\mathbf{x}\in{\mathbb{R}}^{n}$,$\mathbf{x}\in{\mathcal{E}}_{A}\left(\lambda\right)\iff A\mathbf{x}=\lambda\mathbf{x}$$\iff A\mathbf{x}-\lambda\mathbf{x}=\mathbf{0}$ $\iff A\mathbf{x}-\lambda I_{n}\mathbf{x}=\mathbf{0}$$\iff\left(A-\lambda I_{n}\right)\mathbf{x}=\mathbf{0}$$\iff\mathbf{x}\in{\mathcal{N}}\!\left(A-\lambda I_{n}\right)$.

@col
This completes the proof.
@qed
@endcol
@end
@slide
@thm
@title{Eigenspace for a Matrix is a Subspace}
Suppose that $A$ is a square matrix of size $n$ and $\lambda$ is an eigenvalue of $A$.
Then the eigenspace ${\mathcal{E}}_{A}\left(\lambda\right)$ is a subspace of the vector space ${\mathbb{R}}^{n}$.
@end
@proof
@newcol
This holds because:
\[
{\mathcal{E}}_{A}\left(\lambda\right)={\mathcal{N}}\!\left(A-\lambda I_{n}\right),
\]
and the null space of any $m \times n$ matrix is a subspace of $\mathbb{R}^n$.
@qed
@endcol
@end
@slide
@eg
@ref{eg:example3} and @ref{eg:example4}
describe the characteristic polynomial and eigenvalues of the $3\times 3$ matrix
\begin{align*}
\displaystyle F=\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}.
\end{align*}
@newcol
We will now take each eigenvalue in turn and compute its eigenspace. To do this, we row-reduce the matrix
$F-\lambda I_{3}$ in order to find all solutions to the homogeneous system $F-\lambda I_{3}\mathbf{x}=\mathbf{0}$. We then express the eigenspace ${\mathcal{E}}_{F}\left(\lambda\right)$ as the nullspace of $F-\lambda I_{3}$. Then we can write the nullspace as the span of a basis.

@col
\begin{align*}
\displaystyle\lambda&=3:& F-3I_{3}&=\begin{bmatrix}-16&-8&-4\\
12&4&4\\
24&16&4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&\frac{1}{2}\\
0&\boxed{1}&-\frac{1}{2}\\
0&0&0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle{\mathcal{E}}_{F}\left(3\right)&={\mathcal{N}}\!\left(F-3I_{3}\right)=\left< \left\{\begin{bmatrix}-\frac{1}{2}\\
\frac{1}{2}\\
1\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}-1\\
1\\
2\end{bmatrix}\right\}\right>
\end{align*}
@col
\begin{align*}
\displaystyle\lambda&=-1:& F+1I_{3}&=\begin{bmatrix}-12&-8&-4\\
12&8&4\\
24&16&8\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&\frac{2}{3}&\frac{1}{3}\\
0&0&0\\
0&0&0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle{\mathcal{E}}_{F}\left(-1\right)&={\mathcal{N}}\!\left(F+1I_{3}\right)=\left< \left\{\begin{bmatrix}-\frac{2}{3}\\
1\\
0\end{bmatrix},\,\begin{bmatrix}-\frac{1}{3}\\
0\\
1\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}-2\\
3\\
0\end{bmatrix},\,\begin{bmatrix}-1\\
0\\
3\end{bmatrix}\right\}\right>
\end{align*}
@col
Eigenspaces in hand, we can easily compute eigenvectors by forming nontrivial linear combinations of the basis vectors describing each eigenspace. In particular, notice that we can pretty up our basis vectors by using scalar multiples to clear out fractions.
@endcol
@end
@section{Examples of Computing Eigenvalues and Eigenvectors}
@eg
@label{eg:B}
@label{eg:example6}
Consider the matrix
\begin{align*}
\displaystyle B=\begin{bmatrix}-2&1&-2&-4\\
12&1&4&9\\
6&5&-2&-4\\
3&-4&5&10\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle p_{B}\left(x\right)=8-20x+18x^{2}-7x^{3}+x^{4}=(x-1)(x-2)^{3}.
\end{align*}
@col
So the eigenvalues are $\lambda=1,\,2$.
Computing eigenvectors, we find
\begin{align*}
\displaystyle\lambda&=1:& B-1I_{4}&=\begin{bmatrix}-3&1&-2&-4\\
12&0&4&9\\
6&5&-3&-4\\
3&-4&5&9\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&\frac{1}{3}&0\\
0&\boxed{1}&-1&0\\
0&0&0&\boxed{1}\\
0&0&0&0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle{\mathcal{E}}_{B}\left(1\right)&={\mathcal{N}}\!\left(B-1I_{4}\right)=\left< \left\{\begin{bmatrix}-\frac{1}{3}\\
1\\
1\\
0\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}-1\\
3\\
3\\
0\end{bmatrix}\right\}\right>
\end{align*}
\begin{align*}
\displaystyle\lambda&=2:& B-2I_{4}&=\begin{bmatrix}-4&1&-2&-4\\
12&-1&4&9\\
6&5&-4&-4\\
3&-4&5&8\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&1/2\\
0&\boxed{1}&0&-1\\
0&0&\boxed{1}&1/2\\
0&0&0&0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle{\mathcal{E}}_{B}\left(2\right)&={\mathcal{N}}\!\left(B-2I_{4}\right)=\left< \left\{\begin{bmatrix}-\frac{1}{2}\\
1\\
-\frac{1}{2}\\
1\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}-1\\
2\\
-1\\
2\end{bmatrix}\right\}\right>
\end{align*}
@endcol
@end
@eg
@label{C}
@label{eg:example7}
Consider the matrix
\begin{align*}
\displaystyle C=\begin{bmatrix}1&0&1&1\\
0&1&1&1\\
1&1&1&0\\
1&1&0&1\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle p_{C}\left(x\right)=-3+4x+2x^{2}-4x^{3}+x^{4}=(x-3)(x-1)^{2}(x+1).
\end{align*}
@col
So the eigenvalues are $\lambda=3,\,1,\,-1$.
Computing eigenvectors, we find
\begin{align*}
\displaystyle\lambda&=3:& C-3I_{4}&=\begin{bmatrix}-2&0&1&1\\
0&-2&1&1\\
1&1&-2&0\\
1&1&0&-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&-1\\
0&\boxed{1}&0&-1\\
0&0&\boxed{1}&-1\\
0&0&0&0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle{\mathcal{E}}_{C}\left(3\right)&={\mathcal{N}}\!\left(C-3I_{4}\right)=\left< \left\{\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix}\right\}\right>
\end{align*}
\begin{align*}
\displaystyle\lambda&=1:& C-1I_{4}&=\begin{bmatrix}0&0&1&1\\
0&0&1&1\\
1&1&0&0\\
1&1&0&0\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&1&0&0\\
0&0&\boxed{1}&1\\
0&0&0&0\\
0&0&0&0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle{\mathcal{E}}_{C}\left(1\right)&={\mathcal{N}}\!\left(C-1I_{4}\right)=\left< \left\{\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\,\begin{bmatrix}0\\
0\\
-1\\
1\end{bmatrix}\right\}\right>
\end{align*}
\begin{align*}
\displaystyle\lambda&=-1:& C+1I_{4}&=\begin{bmatrix}2&0&1&1\\
0&2&1&1\\
1&1&2&0\\
1&1&0&2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&1\\
0&\boxed{1}&0&1\\
0&0&\boxed{1}&-1\\
0&0&0&0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle{\mathcal{E}}_{C}\left(-1\right)&={\mathcal{N}}\!\left(C+1I_{4}\right)=\left< \left\{\begin{bmatrix}-1\\
-1\\
1\\
1\end{bmatrix}\right\}\right>
\end{align*}
@endcol
@end
@eg
@label{E}
@label{eg:example8}
Consider the matrix
\begin{align*}
\displaystyle E=\begin{bmatrix}29&14&2&6&-9\\
-47&-22&-1&-11&13\\
19&10&5&4&-8\\
-19&-10&-3&-2&8\\
7&4&3&1&-3\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle p_{E}\left(x\right)=-16+16x+8x^{2}-16x^{3}+7x^{4}-x^{5}=-(x-2)^{4}(x+1).
\end{align*}
@col
So the eigenvalues are $\lambda=2,\,-1$. Computing eigenvectors, we find
\begin{align*}
\displaystyle\lambda&=2: \\
\displaystyle E-2I_{5}&=\begin{bmatrix}27&14&2&6&-9\\
-47&-24&-1&-11&13\\
19&10&3&4&-8\\
-19&-10&-3&-4&8\\
7&4&3&1&-5\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&1&0\\
0&\boxed{1}&0&-\frac{3}{2}&-\frac{1}{2}\\
0&0&\boxed{1}&0&-1\\
0&0&0&0&0\\
0&0&0&0&0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{E}\left(2\right)&={\mathcal{N}}\!\left(E-2I_{5}\right)=\left< \left\{\begin{bmatrix}-1\\
\frac{3}{2}\\
0\\
1\\
0\end{bmatrix},\,\begin{bmatrix}0\\
\frac{1}{2}\\
1\\
0\\
1\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}-2\\
3\\
0\\
2\\
0\end{bmatrix},\,\begin{bmatrix}0\\
1\\
2\\
0\\
2\end{bmatrix}\right\}\right> \\
\displaystyle\lambda&=-1: \\
\displaystyle E+1I_{5}&=\begin{bmatrix}30&14&2&6&-9\\
-47&-21&-1&-11&13\\
19&10&6&4&-8\\
-19&-10&-3&-1&8\\
7&4&3&1&-2\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&2&0\\
0&\boxed{1}&0&-4&0\\
0&0&\boxed{1}&1&0\\
0&0&0&0&\boxed{1}\\
0&0&0&0&0\end{bmatrix} \\
\displaystyle{\mathcal{E}}_{E}\left(-1\right)&={\mathcal{N}}\!\left(E+1I_{5}\right)=\left< \left\{\begin{bmatrix}-2\\
4\\
-1\\
1\\
0\end{bmatrix}\right\}\right>
\end{align*}
@endcol
@end
@eg
@label{H}
Consider the matrix
\begin{align*}
\displaystyle H=\begin{bmatrix}15&18&-8&6&-5\\
5&3&1&-1&-3\\
0&-4&5&-4&-2\\
-43&-46&17&-14&15\\
26&30&-12&8&-10\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle p_{H}\left(x\right)=-6x+x^{2}+7x^{3}-x^{4}-x^{5}=x(x-2)(x-1)(x+1)(x+3).
\end{align*}
@col
So the eigenvalues are $\lambda=2,\,1,\,0,\,-1,\,-3$.

@col
Computing eigenvectors, we find
\begin{align*}
\displaystyle\lambda&=2: \\
& H-2I_{5}=\begin{bmatrix}13&18&-8&6&-5\\
5&1&1&-1&-3\\
0&-4&3&-4&-2\\
-43&-46&17&-16&15\\
26&30&-12&8&-12\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&-1\\
0&\boxed{1}&0&0&1\\
0&0&\boxed{1}&0&2\\
0&0&0&\boxed{1}&1\\
0&0&0&0&0\end{bmatrix} \\
&{\mathcal{E}}_{H}\left(2\right)={\mathcal{N}}\!\left(H-2I_{5}\right)=\left< \left\{\begin{bmatrix}1\\
-1\\
-2\\
-1\\
1\end{bmatrix}\right\}\right>
\end{align*}
\begin{align*}
\displaystyle\lambda&=1: \\
& H-1I_{5}=\begin{bmatrix}14&18&-8&6&-5\\
5&2&1&-1&-3\\
0&-4&4&-4&-2\\
-43&-46&17&-15&15\\
26&30&-12&8&-11\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&-\frac{1}{2}\\
0&\boxed{1}&0&0&0\\
0&0&\boxed{1}&0&\frac{1}{2}\\
0&0&0&\boxed{1}&1\\
0&0&0&0&0\end{bmatrix} \\
&{\mathcal{E}}_{H}\left(1\right)={\mathcal{N}}\!\left(H-1I_{5}\right)=\left< \left\{\begin{bmatrix}\frac{1}{2}\\
0\\
-\frac{1}{2}\\
-1\\
1\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}1\\
0\\
-1\\
-2\\
2\end{bmatrix}\right\}\right>
\end{align*}
\begin{align*}
\displaystyle\lambda&=0: \\
& H-0I_{5}=\begin{bmatrix}15&18&-8&6&-5\\
5&3&1&-1&-3\\
0&-4&5&-4&-2\\
-43&-46&17&-14&15\\
26&30&-12&8&-10\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&1\\
0&\boxed{1}&0&0&-2\\
0&0&\boxed{1}&0&-2\\
0&0&0&\boxed{1}&0\\
0&0&0&0&0\end{bmatrix} \\
&{\mathcal{E}}_{H}\left(0\right)={\mathcal{N}}\!\left(H-0I_{5}\right)=\left< \left\{\begin{bmatrix}-1\\
2\\
2\\
0\\
1\end{bmatrix}\right\}\right>
\end{align*}
\begin{align*}
\displaystyle\lambda&=-1: \\
& H+1I_{5}=\begin{bmatrix}16&18&-8&6&-5\\
5&4&1&-1&-3\\
0&-4&6&-4&-2\\
-43&-46&17&-13&15\\
26&30&-12&8&-9\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&-1/2\\
0&\boxed{1}&0&0&0\\
0&0&\boxed{1}&0&0\\
0&0&0&\boxed{1}&1/2\\
0&0&0&0&0\end{bmatrix} \\
&{\mathcal{E}}_{H}\left(-1\right)={\mathcal{N}}\!\left(H+1I_{5}\right)=\left< \left\{\begin{bmatrix}\frac{1}{2}\\
0\\
0\\
-\frac{1}{2}\\
1\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}1\\
0\\
0\\
-1\\
2\end{bmatrix}\right\}\right>
\end{align*}
\begin{align*}
\displaystyle\lambda&=-3: \\
& H+3I_{5}=\begin{bmatrix}18&18&-8&6&-5\\
5&6&1&-1&-3\\
0&-4&8&-4&-2\\
-43&-46&17&-11&15\\
26&30&-12&8&-7\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}\boxed{1}&0&0&0&-1\\
0&\boxed{1}&0&0&\frac{1}{2}\\
0&0&\boxed{1}&0&1\\
0&0&0&\boxed{1}&2\\
0&0&0&0&0\end{bmatrix} \\
&{\mathcal{E}}_{H}\left(-3\right)={\mathcal{N}}\!\left(H+3I_{5}\right)=\left< \left\{\begin{bmatrix}1\\
-\frac{1}{2}\\
-1\\
-2\\
1\end{bmatrix}\right\}\right>=\left< \left\{\begin{bmatrix}-2\\
1\\
2\\
4\\
-2\end{bmatrix}\right\}\right>
\end{align*}
@endcol
@end
@section{Similar Matrices}
@defn
@title{Similar Matrices}
@label{SIM}
Suppose that $A$ and $B$ are square matrices of size $n$. Then $A$ and $B$ are <b>similar</b> if there exists a nonsingular matrix of size $n$, $S$, such that $A=S^{-1}BS$.
We will also say $A$ is similar to $B$ via $S$.
Finally, we will refer to $S^{-1}BS$ as a <b>similarity transformation</b> when we want to emphasize the way that $S$ changes $B$.
@end
@slide
@eg
Define
\begin{align*}
\displaystyle B&=\begin{bmatrix}-5&-7\\
4&6\end{bmatrix}\\
\displaystyle S&=\begin{bmatrix}1&2\\
2&3\end{bmatrix}.
\end{align*}
@newcol
Check that $S$ is nonsingular and then compute
\begin{align*}
\displaystyle A=S^{-1}BS \\
\displaystyle=\begin{bmatrix}-3&2\\
2&-1\\
\end{bmatrix}\begin{bmatrix}-5&-7\\
4&6\end{bmatrix}\begin{bmatrix}1&2\\
2&3\end{bmatrix} \\
\displaystyle=\begin{bmatrix}89&145\\
-54&-88\\
\end{bmatrix}.
\end{align*}
@col
It follows that $A$ and $B$ are similar.
@endcol
@end
@eg
Define
\begin{align*}
\displaystyle B&=\begin{bmatrix}-4&1&-3&-2&2\\
1&2&-1&3&-2\\
-4&1&3&2&2\\
-3&4&-2&-1&-3\\
3&1&-1&1&-4\end{bmatrix}\\
\displaystyle S&=\begin{bmatrix}1&2&-1&1&1\\
0&1&-1&-2&-1\\
1&3&-1&1&1\\
-2&-3&3&1&-2\\
1&3&-1&2&1\\
\end{bmatrix}.
\end{align*}
@newcol
Check that $S$ is nonsingular and then compute
\begin{align*}
\displaystyle A=S^{-1}BS \\
\displaystyle=\begin{bmatrix}10&1&0&2&-5\\
-1&0&1&0&0\\
3&0&2&1&-3\\
0&0&-1&0&1\\
-4&-1&1&-1&1\end{bmatrix}\begin{bmatrix}-4&1&-3&-2&2\\
1&2&-1&3&-2\\
-4&1&3&2&2\\
-3&4&-2&-1&-3\\
3&1&-1&1&-4\end{bmatrix}\begin{bmatrix}1&2&-1&1&1\\
0&1&-1&-2&-1\\
1&3&-1&1&1\\
-2&-3&3&1&-2\\
1&3&-1&2&1\end{bmatrix} \\
\displaystyle=\begin{bmatrix}-10&-27&-29&-80&-25\\
-2&6&6&10&-2\\
-3&11&-9&-14&-9\\
-1&-13&0&-10&-1\\
11&35&6&49&19\end{bmatrix}.
\end{align*}
@col
This shows that $A$ and $B$ are similar.
@endcol
@end
@eg
Define
\begin{align*}
\displaystyle B=\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}\\
\displaystyle S=\begin{bmatrix}1&1&2\\
-2&-1&-3\\
1&-2&0\end{bmatrix}.
\end{align*}
@newcol
Check that $S$ is nonsingular and then compute
\begin{align*}
\displaystyle A&=S^{-1}BS \\
&=\begin{bmatrix}-6&-4&-1\\
-3&-2&-1\\
5&3&1\end{bmatrix}\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}\begin{bmatrix}1&1&2\\
-2&-1&-3\\
1&-2&0\end{bmatrix} \\
&=\begin{bmatrix}-1&0&0\\
0&3&0\\
0&0&-1\end{bmatrix}.
\end{align*}
@endcol
@end
@slide
@thm
@title{Similarity is an Equivalence Relation}
@label{SER}
Suppose that $A$, $B$ and $C$ are square matrices of size $n$. Then

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is similar to $A$. (Reflexive)
</li>
<li class="ltx_item">
If $A$ is similar to $B$, then $B$ is similar to $A$. (Symmetric)
</li>
<li class="ltx_item">
If $A$ is similar to $B$ and $B$ is similar to $C$, then $A$ is similar to $C$. (Transitive)
</li>

</ol>
@end
@proof
@newcol
To see that $A$ is similar to $A$, we need only demonstrate a nonsingular matrix that effects a similarity transformation of $A$ to $A$. We can take $I_{n}$, which is nonsingular and satisfies $I_{n}^{-1}AI_{n}=I_{n}AI_{n}=A$.

@col
If we assume that $A$ is similar to $B$, then we know there exists is a nonsingular matrix $S$ so that $A=S^{-1}BS$.
But then $S^{-1}$ is invertible and therefore nonsingular. So
\begin{align*}
\displaystyle (S^{-1})^{-1}A(S^{-1})=SAS^{-1}=SS^{-1}BSS^{-1}
\end{align*}
\begin{align*}
\displaystyle =\left(SS^{-1}\right)B\left(SS^{-1}\right)=I_{n}BI_{n}=B
\end{align*}
and we see that $B$ is similar to $A$.

@col
Assume that $A$ is similar to $B$ and that $B$ is similar to $C$. This gives us the existence of nonsingular matrices, $S$ and $R$, such that $A=S^{-1}BS$ and $B=R^{-1}CR$. Since $S$ and $R$ are invertible, so too is $RS$, which has inverse $S^{-1}R^{-1}$. Then we compute
\begin{align*}
\displaystyle (RS)^{-1}C(RS)=S^{-1}R^{-1}CRS=S^{-1}\left(R^{-1}CR\right)S
\end{align*}
\begin{align*}
\displaystyle =S^{-1}BS=A
\end{align*}
so $A$ is similar to $C$ via the nonsingular matrix $RS$.
@qed
@endcol
@end
@slide
@thm
@title{Similar Matrices have Equal Eigenvalues}
@label{SMEE}
Suppose that $A$ and $B$ are similar matrices. Then the characteristic polynomials of $A$ and $B$ are equal, that is, $p_{A}\left(x\right)=p_{B}\left(x\right)$.
@end
@proof
@newcol
Let $n$ denote the size of $A$ and $B$. Since $A$ and $B$ are similar, there exists a nonsingular matrix $S$, such that $A=S^{-1}BS$.
Then
@steps
\begin{align*}
\displaystyle p_{A}\left(x\right)
&= \det\left(A-xI_{n}\right)
\\&
@nstep{=\det\left(S^{-1}BS-xI_{n}\right)}
\\&
@nstep{=\det\left(S^{-1}BS-xS^{-1}I_{n}S\right)}
\\&
@nstep{=\det\left(S^{-1}BS-S^{-1}xI_{n}S\right)}
\\&
@nstep{=\det\left(S^{-1}\left(B-xI_{n}\right)S\right)}
\\&
@nstep{=\det\left(S^{-1}\right)\det\left(B-xI_{n}\right)\det\left(S\right)}
\\&
@nstep{=\det\left(S^{-1}\right)\det\left(S\right)\det\left(B-xI_{n}\right)}
\\&
@nstep{=\det\left(S^{-1}S\right)\det\left(B xI_{n}\right)}
\\&
@nstep{=\det\left(I_{n}\right)\det\left(B-xI_{n}\right)}
\\&
@nstep{=1\det\left(B-xI_{n}\right)}
\\&
@nstep{=p_{B}\left(x\right).}
\end{align*}
@endsteps
@qed
@endcol
@end
@slide
@eg
We claim that the matrices
\begin{align*}
\displaystyle A=\begin{bmatrix}1&2\\
3&4\end{bmatrix},\,\,B=\begin{bmatrix}1&2\\
0&4\end{bmatrix}
\end{align*}
are not similar.

@col
To show this, we compute
\begin{align*}
\displaystyle p_{A}\left(x\right)=\begin{vmatrix}1-x&2\\
3&4-x\end{vmatrix}=(1-x)(4-x)-6=x^{2}-5x-2
\end{align*}
and
\begin{align*}
\displaystyle p_{B}\left(x\right)=\begin{vmatrix}1-x&2\\
0&4-x\end{vmatrix}=(1-x)(4-x)=x^{2}-5x+4.
\end{align*}
@newcol
Because $p_{A}\left(x\right)\neq p_{B}\left(x\right)$, we conclude that $A$ and $B$ are not similar.
@endcol
@end
@eg
<b>Same characteristic polynomial, but not similar</b>

@newcol
Define
\begin{align*}
\displaystyle A=\begin{bmatrix}1&1\\
0&1\end{bmatrix}\qquad B=\begin{bmatrix}1&0\\
0&1\end{bmatrix}.
\end{align*}
@col
We have
\begin{align*}
\displaystyle p_{A}\left(x\right)=p_{B}\left(x\right)=1-2x+x^{2}=(x-1)^{2},
\end{align*}
so that $A$ and $B$ have equal characteristic polynomials. If the converse of the above theorem were true, then $A$ and $B$ would be similar. Suppose this is the case. More precisely, suppose there exists is a nonsingular matrix $S$ so that $A=S^{-1}BS$. Then
\begin{align*}
\displaystyle A=S^{-1}BS=S^{-1}I_{2}S=S^{-1}S=I_{2}
\end{align*}
@col
Clearly $A\neq I_{2}$. This contradiction tells us that the converse of the above theorem is false.
@endcol
@end
@section{Diagonalizability}

Good things happen when a matrix is similar to a diagonal matrix. For example, the eigenvalues of the matrix are the entries on the diagonal of the diagonal matrix. It is also much simpler matter to compute high powers of the matrix. Diagonalizable matrices are also of interest in more abstract settings. Here are the relevant definitions, then our main theorem for this section.

@slide
@defn
@title{Diagonal Matrix}
@label{DIM}
Suppose that $A$ is a square matrix ofh size $n$. Then $A$ is a <b>diagonal matrix</b> if $\left[A\right]_{ij}=0$ whenever $i\neq j$, i.e.
\begin{align*}
\displaystyle A=\begin{bmatrix}\lambda_{1}&0&0&\cdots&0\\
0&\lambda_{2}&0&\cdots&0\\
0&0&\lambda_{3}&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
0&0&0&\cdots&\lambda_{n}\end{bmatrix}.
\end{align*}
@newcol
We will often denote such a matrix $A$ by $\operatorname{diag}(\lambda_{1},\lambda_{2},\ldots,\lambda_{n})$.
@endcol
@end
@slide
@defn
@title{Diagonalizable Matrix}
@label{DZM}
Suppose that $A$ is a square matrix. Then $A$ is <b>diagonalizable</b> if $A$ is similar to a diagonal matrix, i.e, there exists an invertible matrix
$S$ and real numbers $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ such that
\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}).
\end{align*}
@end
@slide
@eg
Let
\begin{align*}
\displaystyle B=\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}.
\end{align*}
@newcol
This matrix is similar to a diagonal matrix, as can be seen by the following computation with the nonsingular matrix $S$:
\begin{align*}
\displaystyle S^{-1}BS=\begin{bmatrix}-5&-3&-2\\
3&2&1\\
1&1&1\end{bmatrix}^{-1}\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}\begin{bmatrix}-5&-3&-2\\
3&2&1\\
1&1&1\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle =\begin{bmatrix}-1&-1&-1\\
2&3&1\\
-1&-2&1\end{bmatrix}\begin{bmatrix}-7&-6&-12\\
5&5&7\\
1&0&4\end{bmatrix}\begin{bmatrix}-5&-3&-2\\
3&2&1\\
1&1&1\end{bmatrix}=\begin{bmatrix}-1&0&0\\
0&1&0\\
0&0&2\end{bmatrix}.
\end{align*}
@endcol
@end
@slide
@thm
@title{Diagonalization Characterization}
@label{DC}
Suppose that $A$ is a square matrix of size $n$. Then $A$ is diagonalizable if and only if there exists a linearly independent set $T$ that contains $n$ eigenvectors of $A$.
@end
@proof

($\Rightarrow$)
@newcol
Suppose that $A$ is diagonalizable. Then there exists an invertible matrix $S$ and real numbers $\lambda_{1},\ldots,\lambda_{n}$ such that
\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n}).
\end{align*}
@col
Let $\mathbf{S}_{i}$ be column $i$ of $S$.
Let $T$ be the columns of $S$. Because $S$ is invertible (nonsingular), the columns of $S$ are linearly independent.
Also
\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n}).
\end{align*}
@col
So
\begin{align*}
\displaystyle AS=S\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n})
\end{align*}
or equivalently
\begin{align*}
\displaystyle [A\mathbf{S}_{1}|A\mathbf{S}_{2}|\cdots|A\mathbf{S}_{n}]=[\lambda_{1}\mathbf{S}_{1}|\lambda_{2}\mathbf{S}_{2}|\cdots|\lambda_{n}\mathbf{S}_{n}].
\end{align*}
@col
Hence, for $1\leq i\leq n$, we have:
\begin{align*}
\displaystyle A\mathbf{S}_{i}=\lambda_{i}\mathbf{S}_{i}.
\end{align*}
@col
Obviously $\mathbf{S}_{i}\neq\mathbf{0}$, because $S$ is nonsingular. So $\mathbf{S}_{i}$ is an eigenvector with eigenvalue $\lambda_{i}$.
Hence $T$ is a linearly independent set consisting of eigenvectors of $A$.
@endcol

($\Leftarrow$)
@newcol
Suppose that $T=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}$ is a linearly independent set consisting of eigenvectors of $A$
with eigenvalues $\lambda_{1},\ldots,\lambda_{n}$, i.e. $A\mathbf{v}_{i}=\lambda_{i}\mathbf{v}_{i}$ for $i=1,\ldots,n$.
Let $D=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n})$.
Let
\begin{align*}
\displaystyle S=[\mathbf{v}_{1}|\mathbf{v}_{2}|\cdots|\mathbf{v}_{n}].
\end{align*}
@col
Because $T$ is linearly independent, $S$ is invertible. Similarly to the above computation, we compute:
\begin{align*}
\displaystyle AS=[A\mathbf{v}_{1}|A\mathbf{v}_{2}|\cdots|A\mathbf{v}_{n}]=[\lambda_{1}\mathbf{v}_{1}|\lambda_{2}\mathbf{v}_{2}|\cdots|\lambda_{n}\mathbf{v}_{n}]=SD.
\end{align*}
@col
So
\begin{align*}
\displaystyle S^{-1}AS=S^{-1}SD=D.
\end{align*}
@col
Therefore $A$ is diagonalizable.
@endcol
@qed
@end
@remark
@col
Notice that the proof is constructive. To diagonalize a matrix, we need only locate $n$ linearly independent eigenvectors. Then we can construct a nonsingular matrix $S$, using the eigenvectors as columns, with the property that $S^{-1}AS$ is a diagonal matrix ($D$). The entries on the diagonal of $D$ will be the eigenvalues of the eigenvectors used to create $S$, in the same order as the eigenvectors appear in $S$. We illustrate this by <b>diagonalizing</b> some matrices.
@end
@slide
@eg
Consider the matrix
\begin{align*}
\displaystyle F=\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}
\end{align*}
from previous examples. The eigenvalues and eigenspaces of $F$’s are
\begin{align*}
\displaystyle\lambda&=3&{\mathcal{E}}_{F}\left(3\right)&=\left< \left\{\begin{bmatrix}-\frac{1}{2}\\
\frac{1}{2}\\
1\end{bmatrix}\right\}\right> \\
\displaystyle\lambda&=-1&{\mathcal{E}}_{F}\left(-1\right)&=\left< \left\{\begin{bmatrix}-\frac{2}{3}\\
1\\
0\end{bmatrix},\,\begin{bmatrix}-\frac{1}{3}\\
0\\
1\end{bmatrix}\right\}\right>
\end{align*}
@newcol
Define the matrix $S$ to be the $3\times 3$ matrix whose columns are the three basis vectors in the eigenspaces for $F$:
\begin{align*}
\displaystyle S=\begin{bmatrix}-\frac{1}{2}&-\frac{2}{3}&-\frac{1}{3}\\
\frac{1}{2}&1&0\\
1&0&1\end{bmatrix}
\end{align*}
@col
Check that $S$ is nonsingular (row-reduces to the identity matrix, or has a nonzero determinant).

<strong>Remark</strong>: After we introduce Theorem @ref{diagcond}, you don’t need to check that $S$ is nonsingular. See examples below).

@col
The three columns of $S$ are a linearly independent set. By Theorem @ref{DC} we now know that $F$ is diagonalizable. Furthermore, the construction in the proof of Theorem @ref{DC}
tells us that $S^{-1}FS=\operatorname{diag}(3,-1,-1)$. Let us check this directly:
\begin{align*}
\displaystyle S^{-1}FS&=\begin{bmatrix}-\frac{1}{2}&-\frac{2}{3}&-\frac{1}{3}\\
\frac{1}{2}&1&0\\
1&0&1\end{bmatrix}^{-1}\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}\begin{bmatrix}-\frac{1}{2}&-\frac{2}{3}&-\frac{1}{3}\\
\frac{1}{2}&1&0\\
1&0&1\end{bmatrix} \\
&=\begin{bmatrix}6&4&2\\
-3&-1&-1\\
-6&-4&-1\end{bmatrix}\begin{bmatrix}-13&-8&-4\\
12&7&4\\
24&16&7\end{bmatrix}\begin{bmatrix}-\frac{1}{2}&-\frac{2}{3}&-\frac{1}{3}\\
\frac{1}{2}&1&0\\
1&0&1\end{bmatrix} \\
&=\begin{bmatrix}3&0&0\\
0&-1&0\\
0&0&-1\end{bmatrix}.
\end{align*}
@endcol
@end
@slide
@thm
@label{diagcond}
Suppose that $A$ is a square matrix of size $n$. Suppose that $\lambda_{1},\ldots,\lambda_{k}$ are all of the distinct eigenvalues of $A$. Then $A$ is diagonalizable if and only if
\begin{align}
\label{diagconde}
\displaystyle \sum_{i=1}^{k}\dim{\mathcal{E}}_{A}\left(\lambda_{i}\right)=\dim{\mathcal{E}}_{A}\left(\lambda_{1}\right)+\cdots+\dim{\mathcal{E}}_{A}\left(\lambda_{k}\right)=n.&
\end{align}
@newcol
Suppose that the above condition is satisfies by $A$ and let $T_{i}=\left\{\mathbf{v}_{i1},\,\mathbf{v}_{i2},\,\mathbf{v}_{i3},\,\ldots,\,\mathbf{v}_{id_{i}}\right\}$ be a basis for the eigenspace of $\lambda_{i}$, ${\mathcal{E}}_{A}\left(\lambda_{i}\right)$, for each $1\leq i\leq k$
and let $d_{i}=\dim{\mathcal{E}}_{A}\left(\lambda_{i}\right)$. Then
\begin{align*}
\displaystyle T=T_{1}\cup T_{2}\cup T_{3}\cup\cdots\cup T_{k}
\end{align*}
is a set of linearly independent eigenvectors for $A$ with size $n$. By Theorem @ref{DC}, let $S$ be a square matrix whose $i$-th column is the $i$-th vector of the set $T$, i.e.
\begin{align*}
\displaystyle S=[\mathbf{v}_{11}|\cdots|\mathbf{v}_{1d_{1}}|\mathbf{v}_{21}|\cdots|\mathbf{v}_{2d_{2}}|\cdots|\mathbf{v}_{k1}|\cdots|\mathbf{v}_{kd_{k}}]
\end{align*}
@col
Then
\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(\underbrace{\lambda_{1},\ldots,\lambda_{1}}_{d_{1}},\underbrace{\lambda_{2},\ldots,\lambda_{2}}_{d_{2}},\ldots,\underbrace{\lambda_{k},\ldots,\lambda_{k}}_{d_{k}}).
\end{align*}
@endcol
@end
@proof
@newcol
See the textbook
@qed
@endcol
@end
@remark
@col
Equation $\eqref{diagconde}$ may be rewritten as:
\begin{align*}
\displaystyle \sum_{i=1}^{k}n\!\left(A-\lambda_{i}I_{n}\right)=n\!\left(A-\lambda_{1}I_{n}\right)+\cdots+n\!\left(A-\lambda_{k}I_{n}\right)=n,
\end{align*}
where on the left-hand side $n(M)$ denotes the nullity of a matrix $M$.
or
\begin{align*}
\displaystyle \sum_{i=1}^{k}\left(n-r\left(A-\lambda_{i}I_{n}\right)\right)=(n-r\left(A-\lambda_{1}I_{n}\right))+\cdots+(n-r\left(A-\lambda_{k}I_{n}\right))=n,
\end{align*}
where $r(M)$ denotes the rank of a matrix $M$.
@end
@slide
@thm
@title{Distinct Eigenvalues implies Diagonalizable}
@label{DED}
Suppose that $A$ is a square matrix of size $n$ with $n$ distinct eigenvalues.
Then $A$ is diagonalizable.
@end
@proof
@newcol
You can skip the proof. See the textbook.
@qed
@endcol
@end
@slide
@eg
Determine if the matrix $B$ in @ref{eg:B}
is diagonalizable. The characteristic polynomial is
\begin{align*}
\displaystyle p_{B}\left(x\right)=\det\left(B-xI_{4}\right)=(x-1)(x-2)^{3}.
\end{align*}
@newcol
We conclude that $\lambda_{1}=1$ and $\lambda_{2}=2$ are all of the distinct eigenvalues of $B$.

@col
In @ref{eg:example6}, we compute the RREF of $B-I_{4}$ and $B-2I_{4}$. By the RREFs, we have $r\left(B-I_{4}\right)=3$, $r\left(B-2I_{4}\right)=3$.
Therefore
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{B}\left(1\right)=n\left(B-I_{4}\right)=4-r\left(B-I_{4}\right)=1
\end{align*}
and
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{B}\left(2\right)=n\left(B-2I_{4}\right)=4-r\left(B-2I_{4}\right)=4-3=1.
\end{align*}
@col
Now
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{B}\left(1\right)+\dim{\mathcal{E}}_{B}\left(2\right)=1+1=2\neq 4.
\end{align*}
@col
By Theorem @ref{diagcond}, $B$ is not diagonalizable.
@endcol
@end
@eg
Determine if the matrix $C$
in @ref{eg:example7} is diagonalizable.Because
\begin{align*}
\displaystyle p_{C}\left(x\right)=-3+4x+2x^{2}-4x^{3}+x^{4}=(x-3)(x-1)^{2}(x+1),
\end{align*}
all the distinct eigenvalues are $\lambda_{1}=3$, $\lambda_{2}=1$ and $\lambda_{3}=-1$. We have
\begin{align*}
\displaystyle \sum_{i=1}^{3}\dim{\mathcal{E}}_{C}\left(\lambda_{i}\right)=\sum_{i=1}^{3}(4-r\left(C-\lambda_{i}I_{4}\right))
\end{align*}
\begin{align*}
\displaystyle =(4-3)+(4-2)+(4-3)=4.
\end{align*}
@newcol
By Theorem @ref{diagcond}, $C$ is diagonalizable.
@endcol
@end
@eg
Determine if the matrix $E$ in @ref{eg:example8} is diagonalizable.The characteristic polynomial is $p_{E}\left(x\right)=-(x-2)^{4}(x+1)$.
The eigenvalues are $\lambda_{1}=2$ and $\lambda_{2}=-1$ and we have
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{E}\left(\lambda_{1}\right)+\dim{\mathcal{E}}_{E}\left(\lambda_{2}\right)=(5-r\left(E-2I_{4}\right))+(5-r\left(E+I_{5}\right))
\end{align*}
\begin{align*}
\displaystyle =(5-3)+(5-4)=2+1=3\neq 5.
\end{align*}
@newcol
So $E$ is not diagonalizable.
@endcol
@end
@eg
Determine if the matrix $H$ in Example 9 is diagonalizable.Because $p_{H}\left(x\right)=x(x-2)(x-1)(x+1)(x+3)$, has $5$ distinct eigenvalues, Theorem @ref{DED} implies that $H$ is diagonalizable.
@end
@eg
Diagonalize $C$ in @ref{eg:example7} (see also Example 18). By the computation in Example 18, $C$ is diagonalizable. By the computation in @ref{eg:example7}, we have that$\left\{\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix}\right\}$ is a basis for ${\mathcal{E}}_{C}\left(3\right)$, $\left\{\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\,\begin{bmatrix}0\\
0\\
-1\\
1\end{bmatrix}\right\}$ is a basis for ${\mathcal{E}}_{C}\left(1\right)$, $\left\{\begin{bmatrix}-1\\
-1\\
1\\
1\end{bmatrix}\right\}$ is a basis for ${\mathcal{E}}_{C}\left(-1\right)$. By Theorem @ref{diagcond},
\begin{align*}
\displaystyle S=\begin{bmatrix}1&-1&0&-1\\
1&1&0&-1\\
1&0&-1&1\\
1&0&1&1\end{bmatrix}.
\end{align*}
@newcol
Then $S^{-1}CS=\operatorname{diag}(3,1,1,-1)$.

<strong>Remark</strong>:
Note that the invertibility of $S$ is guaranteed by Theorem @ref{diagcond}.
@endcol
@end
@eg
Diagonalize $H$ in Example 9 (see also Example 21).By the discussion of Example 21, $H$ is diagonalizable. By the computation in Example 9,
$\begin{bmatrix}1\\
-1\\
-2\\
-1\\
1\end{bmatrix}$, $\begin{bmatrix}1\\
0\\
-1\\
-2\\
2\end{bmatrix}$, $\begin{bmatrix}-1\\
2\\
2\\
0\\
1\end{bmatrix}$, $\begin{bmatrix}1\\
0\\
0\\
-1\\
2\end{bmatrix}$
and $\begin{bmatrix}-2\\
1\\
2\\
4\\
-2\end{bmatrix}$ are bases for ${\mathcal{E}}_{H}\left(2\right),{\mathcal{E}}_{H}\left(1\right),{\mathcal{E}}_{H}\left(0\right),{\mathcal{E}}_{H}\left(-1\right)$ and ${\mathcal{E}}_{H}\left(-3\right)$ respectively.
By Theorem @ref{diagcond}, let
\begin{align*}
\displaystyle S=\begin{bmatrix}1&1&-1&1&-2\\
-1&0&2&0&1\\
-2&-1&2&0&2\\
-1&-2&0&-1&4\\
1&2&1&2&-2\\
\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle S^{-1}HS=\operatorname{diag}(2,1,0,-1,-3).
\end{align*}
@endcol
@end
@eg
Determine if
\begin{align*}
\displaystyle J=\begin{bmatrix}2&1&1\\
1&2&1\\
1&1&2\end{bmatrix}
\end{align*}
is diagonalizable. If it is diagonalizable, find $S$ such that $S^{-1}JS$ is diagonal.

@col
<strong>Step 1</strong>:
@newcol
$p_{J}\left(x\right)=-x^{3}+6x^{2}-9x+4=-(x-4)(x-1)^{2}$. All the distict eigenvalues of $J$ is $\lambda_{1}=4$, $\lambda_{2}=1$.
@endcol

<strong>Step 2</strong>:
@newcol
\begin{align*}
\displaystyle J-4I_{3}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&-1\\
0&1&-1\\
0&0&0\\
\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{J}\left(4\right)=3-r\left(J-4I_{3}\right)=3-2=1.
\end{align*}
\begin{align*}
\displaystyle J-I_{3}\xrightarrow{\text{RREF}}\begin{bmatrix}1&1&1\\
0&0&0\\
0&0&0\\
\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{J}\left(1\right)=3-r\left(J-I_{3}\right)=3-1=2.
\end{align*}
col
Now
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{J}\left(4\right)+\dim{\mathcal{E}}_{J}\left(1\right)=1+2=3.
\end{align*}
@col
By Theorem @ref{diagcond}, $J$ is diagonalizable.
@endcol

<strong>Step 3</strong>:
@newcol
$\left\{\begin{bmatrix}1\\
1\\
1\end{bmatrix}\right\}$ is a basis for ${\mathcal{E}}_{J}\left(4\right)$. $\left\{\begin{bmatrix}-1\\
1\\
0\end{bmatrix},\begin{bmatrix}-1\\
0\\
1\end{bmatrix}\right\}$ is a basis for ${\mathcal{E}}_{J}\left(1\right)$. By Theorem @ref{diagcond}, we can take
\begin{align*}
\displaystyle S=\begin{bmatrix}1&-1&-1\\
1&1&0\\
1&0&1\end{bmatrix}.
\end{align*}
@col
Then
\begin{align*}
\displaystyle S^{-1}JS=\operatorname{diag}(4,1,1).
\end{align*}
@endcol
@endcol
@end
@eg
Determine if
\begin{align*}
\displaystyle K=\begin{bmatrix}-4&-4&0&-11&-2&-5\\
138&87&-6&248&44&122\\
-24&-16&2&-44&-8&-20\\
-62&-39&2&-110&-20&-54\\
-63&-39&3&-114&-19&-57\\
56&35&-2&101&18&51\\
\end{bmatrix}
\end{align*}
is diagonalizable and if it is diagonalizable, find $S$ such that $S^{-1}KS$ is diagonal.

@col
<strong>Step 1</strong>:
@newcol
The characteristic polynomial is
\begin{align*}
\displaystyle p_{K}\left(x\right)=\det\left(K-xI_{6}\right)=(x+1)(x-1)^{2}(x-2)^{3}.
\end{align*}
@col
The eigenvalues are $\lambda_{1}=-1,\lambda_{2}=1$ and $\lambda_{3}=2$.
@endcol

<strong>Step 2</strong>:
@newcol
\begin{align*}
\displaystyle K+I_{4}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&0&0&0&\frac{1}{9}\\
0&1&0&0&0&-\frac{22}{9}\\
0&0&1&0&0&\frac{4}{9}\\
0&0&0&1&0&\frac{10}{9}\\
0&0&0&0&1&\frac{10}{9}\\
0&0&0&0&0&0\\
\end{bmatrix},
\end{align*}
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(-1\right)=n\left(K+I_{4}\right)=6-r\left(K+I_{4}\right)=6-5=1.
\end{align*}
\begin{align*}
\displaystyle A-I_{6}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&0&0&\frac{1}{18}&\frac{1}{6}\\
0&1&0&0&\frac{5}{18}&-\frac{13}{6}\\
0&0&1&0&\frac{2}{9}&\frac{2}{3}\\
0&0&0&1&\frac{1}{18}&\frac{7}{6}\\
0&0&0&0&0&0\\
0&0&0&0&0&0\\
\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(1\right)=n\left(K-I_{6}\right)=6-r\left(K-I_{6}\right)=6-4=2.
\end{align*}
\begin{align*}
\displaystyle K-2I_{6}\xrightarrow{\text{RREF}}\begin{bmatrix}1&0&0&\frac{1}{2}&-1&-\frac{1}{2}\\
0&1&0&2&2&2\\
0&0&1&-\frac{3}{2}&-2&-\frac{7}{2}\\
0&0&0&0&0&0\\
0&0&0&0&0&0\\
0&0&0&0&0&0\\
\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(2\right)=n\left(K-2I_{6}\right)=6-r\left(K-2I_{6}\right)=6-3=3.
\end{align*}
@col
Because
\begin{align*}
\displaystyle \dim{\mathcal{E}}_{K}\left(-1\right)+\dim{\mathcal{E}}_{K}\left(1\right)+\dim{\mathcal{E}}_{K}\left(2\right)=1+2+3=6.
\end{align*}
@col
By Theorem @ref{diagcond}, $K$ is diagonalizable.
@endcol

<strong>Step 3</strong>:
@newcol
A basis for ${\mathcal{E}}_{K}\left(-1\right)={\mathcal{N}}\!\left(K+I_{6}\right)$ is
\begin{align*}
\displaystyle \left\{\begin{bmatrix}-1\\
22\\
-4\\
-10\\
-10\\
9\end{bmatrix}\right\}
\end{align*}
(we use the method in Lecture 8 @ref{eg:example6} and multiply the result by $9$ to clear the denominator.) A basis for ${\mathcal{E}}_{K}\left(1\right)={\mathcal{N}}\!\left(K-I_{6}\right)$ is
\begin{align*}
\displaystyle \left\{\begin{bmatrix}-1\\
-5\\
-4\\
-1\\
18\\
0\end{bmatrix},\begin{bmatrix}-1\\
13\\
-4\\
-7\\
0\\
6\end{bmatrix}\right\}
\end{align*}
(Again, we use the method in Lecture 8 @ref{eg:example6} and multiply the first vector by $18$ and the second vector by $6$ to clear the denominators.)
\begin{align*}
\displaystyle \left\{\begin{bmatrix}-1\\
-4\\
3\\
2\\
0\\
0\end{bmatrix},\begin{bmatrix}1\\
-2\\
2\\
0\\
1\\
0\end{bmatrix},\begin{bmatrix}1\\
-4\\
7\\
0\\
0\\
2\end{bmatrix}\right\}.
\end{align*}
(Again, we use the method in Lecture 8 @ref{eg:example6} and multiply the first and the third vector by $2$ to clear the denominators.) So we can take
\begin{align*}
\displaystyle S=\begin{bmatrix}-1&-1&-1&1&1&-1\\
22&13&-5&-4&-2&-4\\
-4&-4&-4&7&2&3\\
-10&-7&-1&0&0&2\\
-10&0&18&0&1&0\\
9&6&0&2&0&0\\
\end{bmatrix}.
\end{align*}
@col
Then
\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(-1,1,1,2,2,2).
\end{align*}
@endcol
@end
@eg
Determine if
\begin{align*}
\displaystyle J=\begin{bmatrix}2&1&1\\
1&2&1\\
1&1&2\end{bmatrix},\,\,L=\begin{bmatrix}-8&6&6\\
-9&7&6\\
-9&6&7\\
\end{bmatrix}
\end{align*}
are similar. If they are similar, Find $R$ such that $R^{-1}JR=L$. We know that $J$ is

@newcol
The characteristic polynomials
\begin{align*}
\displaystyle p_{J}\left(x\right)=-(-4+x)(-1+x)^{2}=p_{L}\left(x\right).
\end{align*}
(If the characteristic polynomials are different, $J$ and $L$ are not similar, end of the story.) In example 23, we know that $J$ is diagonalizable. Follow similar method, we can show that $L$ is diagonalizable (fill the detail).
\begin{align*}
\displaystyle Q=\begin{bmatrix}1&2&2\\
1&3&0\\
1&0&3\\
\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle Q^{-1}LQ=\operatorname{diag}(4,1,1).
\end{align*}
@col
So $J$ is similar to $\operatorname{diag}(4,1,1)$ which in turns similar to $L$. So $J$ is similar to $L$ (Theorem @ref{SER}). In fact
\begin{align*}
\displaystyle S^{-1}JS=Q^{-1}LQ
\end{align*}
\begin{align*}
\displaystyle (SQ^{-1})^{-1}J(SQ^{-1})=L.
\end{align*}
@col
So we can take
\begin{align*}
\displaystyle R=SQ^{-1}=\begin{bmatrix}-5&3&3\\
-2&\frac{5}{3}&\frac{4}{3}\\
-2&\frac{4}{3}&\frac{5}{3}\\
\end{bmatrix}.
\end{align*}
@endcol
@end
@section{Powers of Matrices}

Suppose $s$ is a positive integer. Recall the notation
\begin{align*}
\displaystyle A^{s}=\underbrace{A\cdots A}_{s}
\end{align*}
@newcol
Powers of a diagonal matrix are easy to compute. The case of a diagonalizable matrix is only slightly more difficult. Suppose that $A$ is similar to a diagonal matrix $D=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n})$. Let $S$ be an invertible matrix such that
\begin{align*}
\displaystyle S^{-1}AS=D.
\end{align*}
@col
Then
\begin{align*}
\displaystyle A=SDS^{-1}.
\end{align*}
\begin{align*}
\displaystyle A^{s}=\underbrace{SDS^{-1}SDS^{-1}\cdots SDS^{-1}}_{s}=S\underbrace{D\cdots D}_{s}S^{-1}=SD^{s}S^{-1}
\end{align*}
\begin{align*}
\displaystyle =S\operatorname{diag}(\lambda_{1}^{s},\ldots,\lambda_{n}^{s})S^{-1}.
\end{align*}
@endcol
@slide
@eg
Let $s$ be a positive integer and
\begin{align*}
\displaystyle A=\begin{bmatrix}1&3\\
4&2\end{bmatrix}.
\end{align*}
@newcol
We want to find a closed formula for $A^{s}$. The characteristic polynomial of $A$ is
\begin{align*}
\displaystyle p_{A}\left(x\right)=\det\left(A-xI_{2}\right)=(1-x)(2-x)-12=x^{2}-3x-10=(x+2)(x-5).
\end{align*}
@col
For $\lambda=-2$
\begin{align*}
\displaystyle A+2I_{2}=\begin{bmatrix}3&3\\
4&4\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&1\\
0&0\end{bmatrix}.
\end{align*}
@col
So
\begin{align*}
\displaystyle \left\{\begin{bmatrix}1\\
-1\end{bmatrix}\right\}
\end{align*}
is a basis for ${\mathcal{E}}_{A}\left(-2\right)$. For $\lambda=5$
\begin{align*}
\displaystyle A-5I_{2}=\begin{bmatrix}-4&3\\
4&-3\end{bmatrix}\xrightarrow{\text{RREF}}\begin{bmatrix}1&-\frac{3}{4}\\
0&0\end{bmatrix}.
\end{align*}
@col
So
\begin{align*}
\displaystyle \left\{\begin{bmatrix}3\\
4\end{bmatrix}\right\}
\end{align*}
is a basis for ${\mathcal{E}}_{A}\left(5\right)$. Let
\begin{align*}
\displaystyle S=\begin{bmatrix}1&3\\
-1&4\end{bmatrix}.
\end{align*}
@col
Then
\begin{align*}
\displaystyle S^{-1}AS=\operatorname{diag}(-2,5)
\end{align*}
so that
\begin{align*}
\displaystyle A^{s}=S\operatorname{diag}((-2)^{s},5^{s})S^{-1}
\end{align*}
\begin{align*}
\displaystyle =\begin{bmatrix}1&3\\
-1&4\end{bmatrix}\begin{bmatrix}(-2)^{s}&0\\
0&5^{s}\end{bmatrix}\begin{bmatrix}\frac{4}{7}&-\frac{3}{7}\\
\frac{1}{7}&\frac{1}{7}\\
\end{bmatrix}=\begin{bmatrix}\frac{1}{7}(-1)^{s}2^{s+2}+\frac{3\times 5^{s}}{7}&\frac{1}{7}(-3)(-2)^{s}+\frac{3\times 5^{s}}{7}\\
-\frac{1}{7}(-1)^{s}2^{s+2}+\frac{4\times 5^{s}}{7}&\frac{3(-2)^{s}}{7}+\frac{4\times 5^{s}}{7}\\
\end{bmatrix}.
\end{align*}
@endcol
@end
@eg
<b>High power of a diagonalizable matrix</b>

@newcol
Suppose that
\begin{align*}
\displaystyle A=\begin{bmatrix}19&0&6&13\\
-33&-1&-9&-21\\
21&-4&12&21\\
-36&2&-14&-28\end{bmatrix}.
\end{align*}
@col
We wish to compute $A^{20}$. Normally this would require 19 matrix multiplications. But since $A$ is diagonalizable, we can simplify the computations substantially.

@col
First, we diagonalize $A$. With
\begin{align*}
\displaystyle S=\begin{bmatrix}1&-1&2&-1\\
-2&3&-3&3\\
1&1&3&3\\
-2&1&-4&0\end{bmatrix},
\end{align*}
we find
\begin{align*}
\displaystyle D&=S^{-1}AS \\
&=\begin{bmatrix}-6&1&-3&-6\\
0&2&-2&-3\\
3&0&1&2\\
-1&-1&1&1\end{bmatrix}\begin{bmatrix}19&0&6&13\\
-33&-1&-9&-21\\
21&-4&12&21\\
-36&2&-14&-28\end{bmatrix}\begin{bmatrix}1&-1&2&-1\\
-2&3&-3&3\\
1&1&3&3\\
-2&1&-4&0\end{bmatrix} \\
&=\begin{bmatrix}-1&0&0&0\\
0&0&0&0\\
0&0&2&0\\
0&0&0&1\end{bmatrix}.
\end{align*}
@col
Using this, we compute
\begin{align*}
\displaystyle A^{20}=SD^{20}S^{-1}&=S\begin{bmatrix}-1&0&0&0\\
0&0&0&0\\
0&0&2&0\\
0&0&0&1\end{bmatrix}^{20}S^{-1} \\
&=S\begin{bmatrix}(-1)^{20}&0&0&0\\
0&(0)^{20}&0&0\\
0&0&(2)^{20}&0\\
0&0&0&(1)^{20}\end{bmatrix}S^{-1} \\
&=\begin{bmatrix}1&-1&2&-1\\
-2&3&-3&3\\
1&1&3&3\\
-2&1&-4&0\end{bmatrix}\begin{bmatrix}1&0&0&0\\
0&0&0&0\\
0&0&1048576&0\\
0&0&0&1\end{bmatrix}\begin{bmatrix}-6&1&-3&-6\\
0&2&-2&-3\\
3&0&1&2\\
-1&-1&1&1\end{bmatrix} \\
&=\begin{bmatrix}6291451&2&2097148&4194297\\
-9437175&-5&-3145719&-6291441\\
9437175&-2&3145728&6291453\\
-12582900&-2&-4194298&-8388596\end{bmatrix}.
\end{align*}
@col
Generally
\begin{align*}
\displaystyle A^{s}=SD^{s}S^{-1}=\begin{bmatrix}1-6(-1)^{s}+3\ 2^{s+1}&1+(-1)^{s}&-1-3(-1)^{s}+2^{s+1}&-1-6(-1)^{s}+2^{s+2}\\
-3+12(-1)^{s}-9\ 2^{s}&-3-2(-1)^{s}&3+6(-1)^{s}-3\ 2^{s}&3+12(-1)^{s}-3\ 2^{s+1}\\
-3-6(-1)^{s}+9\ 2^{s}&-3+(-1)^{s}&3-3(-1)^{s}+3\ 2^{s}&3-6(-1)^{s}+3\ 2^{s+1}\\
12(-1)^{s}-3\ 2^{s+2}&-2(-1)^{s}&6(-1)^{s}-2^{s+2}&12(-1)^{s}-2^{s+3}\\
\end{bmatrix}.
\end{align*}
@endcol
@end
@section{Summary}

In below $A$ always denote a square matrix of size $n$

<ol class="ltx_enumerate">
<li class="ltx_item">
If $\mathbf{x}\neq 0$ and $A\mathbf{x}=\lambda x$, then $\mathbf{x}$ is called
an <b>eigenvector</b> of $A$ with eigenvalue $\lambda$.
</li>
<li class="ltx_item">
$p_{A}(x)=\det(A-xI_{n})$ is called the <b>characteristic function</b> of $A$.

<ol class="ltx_enumerate">
<li class="ltx_item">
It is a polynomial of degree $n$ with leading coefficient $(-1)^{n}$.
</li>
<li class="ltx_item">
$\lambda$ is an eigenvalue if and only if $p_{A}(\lambda)=0$, i.e., $\lambda$ is a root of $p_{A}(x)$.
</li>

</ol>
</li>
<li class="ltx_item">
${\mathcal{E}}_{A}\left(\lambda\right)$: <b>eigenspace</b> of $A$ for an eigenvalue $\lambda$.

<ol class="ltx_enumerate">
<li class="ltx_item">
It is the set of of all eigenvectors of $A$ for $\lambda$, together with the zero vector,
i.e.
\begin{align*}
\displaystyle {\mathcal{E}}_{\lambda}\left(A\right)=\left\{\mathbf{x}\in{\mathbb{R}}^{n}\,|\,A\mathbf{x}=\lambda\mathbf{x}\right\}.
\end{align*}
</li>
<li class="ltx_item">
${\mathcal{E}}_{\lambda}\left(A\right)={\mathcal{N}}\!\left(A-\lambda I_{n}\right)$.
</li>
<li class="ltx_item">
${\mathcal{E}}_{\lambda}\left(A\right)$ is a subspace of ${\mathbb{R}}^{n}$.
</li>

</ol>
</li>
<li class="ltx_item">
$\alpha_{A}\left(\lambda\right)$: <b>algebraic multiplicity</b>. The power of $(x-\lambda)$ in the factorization of $p_{A}(x)$.
</li>
<li class="ltx_item">
$\gamma_{A}\left(\lambda\right)$: <b>geometric multiplicity</b>. It is $\dim{\mathcal{E}}_{A}\left(\lambda\right)=n\left(A-\lambda I_{n}\right)$.
</li>
<li class="ltx_item">
Basic properties. Suppose $\lambda$ is an eigenvalue of $A$ and $\mathbf{x}$ is an eigenvector of $A$ with eigenvalue $\lambda$.

<ol class="ltx_enumerate">
<li class="ltx_item">
$A$ is invertible if and only if $\lambda=0$ is an eigenvalue.
</li>
<li class="ltx_item">
For positive integer $s$, $\mathbf{x}$ is an eigenvector of $A^{s}$ with eigenvalue $\lambda^{s}$.
</li>
<li class="ltx_item">
If $\lambda\neq 0$, $\mathbf{x}$ is an eigenvector of $A^{-1}$ with eigenvalue $\lambda^{-1}$.
</li>
<li class="ltx_item">
$\lambda$ is an eigenvector of $A^{t}$ (but $\mathbf{x}$ may <b>not</b> be an eigenvector of $A$)
</li>

</ol>
</li>
<li class="ltx_item">
Computational questions

<ol class="ltx_enumerate">
<li class="ltx_item">
Find all the eigenvalues of $A$: find all the roots of $p_{A}(x)$.
</li>
<li class="ltx_item">
Find ${\mathcal{E}}_{A}\left(\lambda\right)$: find ${\mathcal{N}}\!\left(A-\lambda I_{n}\right)$, this can be done by finding the RREF of $A-\lambda I_{n}$.

</li>
<li class="ltx_item">
Find $\alpha_{A}\left(\lambda\right)$: Find the power of $x-\lambda$ in the factorization of $p_{A}(x)$.
</li>
<li class="ltx_item">
Find a basis for ${\mathcal{E}}_{A}\left(\lambda\right)$: again, this is same as finding basis of ${\mathcal{N}}\!\left(A-\lambda I_{n}\right)$. This can be done by $A-\lambda I_{n}\xrightarrow{\text{RREF}}B$ and use the standard method
in finding basis (see Lecture 8 Theorem 4, @ref{eg:example6}).
</li>
<li class="ltx_item">
Find $\gamma_{A}\left(\lambda\right)$: same as finding $n\left(A-\lambda I_{n}\right)=n-r\left(A-\lambda I_{n}\right)$. Suppose $A-\lambda I_{n}\xrightarrow{\text{RREF}}B$.
Then $\gamma_{A}\left(\lambda\right)=n-r\left(B\right)=n-\text{ number of pivot columns of $B$}$.
</li>

</ol>
</li>

</ol>
@setchapter{20}
@chapter{Inner Product}
@section{Basic properties of inner products}
@defn
Given two vectors $\mathbf{v}$ and $\mathbf{w}$ in ${\mathbb{R}}^{m}$, we define
\begin{align}
\label{ipdefe1}
\displaystyle \left< \mathbf{v},\mathbf{w}\right>=\sum_{i=1}^{m}[\mathbf{v}]_{i}[\mathbf{w}]_{i}=[\mathbf{v}]_{1}[\mathbf{w}]_{1}+\cdots+[\mathbf{v}]_{m}[\mathbf{w}]_{m}.&
\end{align}
@newcol
It is called the inner product of ${\mathbb{R}}^{m}$. The vector space ${\mathbb{R}}^{m}$ together with the operation $\langle-,-\rangle$ is called an inner product space.
If we regard $\mathbf{v}$ and $\mathbf{w}$ as $m\times 1$ matrice, then we can write
\begin{align}
\label{ipdefe2}
\displaystyle \left< \mathbf{v},\mathbf{w}\right>=\mathbf{v}^{t}\mathbf{w}.&
\end{align}
@endcol
@end
@slide
@eg
We have
\begin{align*}
\displaystyle \left< \begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}3\\
4\end{bmatrix}\right>=1\times 3+2\times 4=11
\end{align*}
and
\begin{align*}
\displaystyle \left< \begin{bmatrix}1\\
2\\
3\end{bmatrix},\begin{bmatrix}4\\
5\\
6\end{bmatrix}\right>=1\times 4+2\times 5+3\times 6=32.
\end{align*}
@end
@slide
@prop
@label{ipdef}
For any $\mathbf{v},\mathbf{w},\mathbf{u}\in{\mathbb{R}}^{m}$ and $\alpha\in{\mathbb{R}}^{\hbox{}}$. We have

<ol class="ltx_enumerate">
<li class="ltx_item">
$\left< \mathbf{v}+\mathbf{w},\mathbf{u}\right>=\left< \mathbf{v},\mathbf{u}\right>+\left< \mathbf{w},\mathbf{u}\right>$.
</li>
<li class="ltx_item">
$\left< \alpha\mathbf{v},\mathbf{w}\right>=\alpha\left< \mathbf{v},\mathbf{w}\right>$.
</li>
<li class="ltx_item">
$\left< \mathbf{v},\mathbf{w}\right>=\left< \mathbf{w},\mathbf{v}\right>$.
</li>
<li class="ltx_item">
$\left< \mathbf{v},\mathbf{v}\right>>0$ for $\mathbf{v}\neq\mathbf{0}$.
</li>

</ol>
@end
@proof
@newcol
<ol class="ltx_enumerate">
<li class="ltx_item">
We compute
\begin{align*}
\displaystyle\left< \mathbf{v}+\mathbf{w},\mathbf{u}\right>&=[\mathbf{v}+\mathbf{w}]_{1}[\mathbf{u}]_{1}+[\mathbf{v}+\mathbf{w}]_{2}[\mathbf{u}]_{2}+\cdots+[\mathbf{v}+\mathbf{w}]_{m}[\mathbf{u}]_{m} \\
&=([\mathbf{v}]_{1}+[\mathbf{w}]_{1})[\mathbf{u}]_{1}+([\mathbf{v}]_{2}+[\mathbf{w}]_{2})[\mathbf{u}]_{2}+\cdots+([\mathbf{v}]_{n}+[\mathbf{w}]_{m})[\mathbf{u}]_{m} \\
&=[\mathbf{v}]_{1}[\mathbf{u}]_{1}+[\mathbf{w}]_{1}[\mathbf{u}]_{1}+[\mathbf{v}]_{2}[\mathbf{u}]_{2}+[\mathbf{w}]_{2}[\mathbf{u}]_{2}+\cdots+[\mathbf{v}]_{m}[\mathbf{u}]_{m}+[\mathbf{w}]_{m}[\mathbf{u}]_{m} \\
&=[\mathbf{v}]_{1}[\mathbf{u}]_{1}+\cdots+[\mathbf{v}]_{m}[\mathbf{u}]_{m}+[\mathbf{w}]_{1}[\mathbf{u}]_{1}+\cdots+[\mathbf{w}]_{m}[\mathbf{u}]_{m} \\
&=\left< \mathbf{v},\mathbf{u}\right>+\left< \mathbf{w},\mathbf{u}\right>.
\end{align*}
@col
Or we can use $\eqref{ipdefe2}$:
\begin{align*}
\displaystyle \left< \mathbf{v}+\mathbf{w},\mathbf{u}\right>=(\mathbf{v}+\mathbf{w})^{t}\mathbf{u}=(\mathbf{v}^{t}+\mathbf{w}^{t})\mathbf{u}
\end{align*}
\begin{align*}
\displaystyle =\mathbf{v}^{t}\mathbf{u}+\mathbf{w}^{t}\mathbf{u}=\left< \mathbf{v},\mathbf{u}\right>+\left< \mathbf{w},\mathbf{u}\right>.
\end{align*}
</li>
<li class="ltx_item">
We compute
\begin{align*}
\displaystyle\left< \alpha\mathbf{v},\mathbf{w}\right>&=[\alpha\mathbf{v}]_{1}[\mathbf{w}]_{1}+[\alpha\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+[\alpha\mathbf{v}]_{m}[\mathbf{w}]_{m} \\
&=\alpha[\mathbf{v}]_{1}[\mathbf{w}]_{1}+\alpha[\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+\alpha[\mathbf{v}]_{m}[\mathbf{w}]_{m} \\
&=\alpha([\mathbf{v}]_{1}[\mathbf{w}]_{1}+[\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+[\mathbf{v}]_{m}[\mathbf{w}]_{m}) \\
&=\alpha\left< \mathbf{v},\mathbf{w}\right>.
\end{align*}
@col
Or we can use $\eqref{ipdefe2}$:
\begin{align*}
\displaystyle \left< \alpha\mathbf{v},\mathbf{w}\right>=(\alpha\mathbf{v})^{t}\mathbf{w}=\alpha\mathbf{v}^{t}\mathbf{w}=\alpha\left< \mathbf{v},\mathbf{w}\right>.
\end{align*}
</li>
<li class="ltx_item">
We compute
\begin{align*}
\displaystyle\left< \mathbf{v},\mathbf{w}\right>&=[\mathbf{v}]_{1}[\mathbf{w}]_{1}+[\mathbf{v}]_{2}[\mathbf{w}]_{2}+\cdots+[\mathbf{v}]_{m}[\mathbf{w}]_{m}. \\
&=[\mathbf{w}]_{1}[\mathbf{v}]_{1}+[\mathbf{w}]_{2}[\mathbf{v}]_{2}+\cdots+[\mathbf{w}]_{m}[\mathbf{v}]_{m}. \\
&=\left< \mathbf{w},\mathbf{v}\right>.
\end{align*}
</li>
<li class="ltx_item">
We compute
\begin{align*}
\displaystyle \left< \mathbf{v},\mathbf{v}\right>=[\mathbf{v}]_{1}^{2}+[\mathbf{v}]_{2}^{2}+\cdots+[\mathbf{v}]_{m}^{2}\geq 0.
\end{align*}
@col
Noting that $\left< \mathbf{v},\mathbf{v}\right>=0$ if and only if $[\mathbf{v}]_{i}=0$ for all $1\leq i\leq n$, we see that $\mathbf{v}=\mathbf{0}$
</li>

</ol>

@qed
@endcol
@end
@slide
@prop
Let $\alpha,\beta\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{v},\mathbf{w},\mathbf{u}\in{\mathbb{R}}^{m}$. We have

<ol class="ltx_enumerate">
<li class="ltx_item">
$\left< \alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right>=\alpha\left< \mathbf{v},\mathbf{u}\right>+\beta\left< \mathbf{w},\mathbf{u}\right>$.
</li>
<li class="ltx_item">
$\left< \mathbf{u},\alpha\mathbf{v}+\beta\mathbf{w}\right>=\alpha\left< \mathbf{u},\mathbf{v}\right>+\beta\left< \mathbf{u},\mathbf{w}\right>$.
</li>
<li class="ltx_item">
$\left< \mathbf{0},\mathbf{v}\right>=\left< \mathbf{v},\mathbf{0}\right>=0$.
</li>
<li class="ltx_item">
If $\left< \mathbf{v},\mathbf{x}\right>=0$ for all $\mathbf{x}\in{\mathbb{R}}^{m}$, then $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
If $\left< \mathbf{v},\mathbf{x}\right>=\left< \mathbf{w},\mathbf{x}\right>$ for all $x\in{\mathbb{R}}^{m}$, then $\mathbf{v}=\mathbf{w}$.
</li>

</ol>
@end
@proof
@newcol
<ol class="ltx_enumerate">
<li class="ltx_item">
By @ref{ipdef} item 1, we have:
\begin{align*}
\displaystyle \left< \alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right>=\left< \alpha\mathbf{v},\mathbf{u}\right>+\left< \beta\mathbf{w},\mathbf{u}\right>.
\end{align*}
By @ref{ipdef} item 2,
\begin{align*}
\displaystyle
\left< \alpha\mathbf{v},\mathbf{u}\right>+\left< \beta\mathbf{w},\mathbf{u}\right>
=
\alpha\left< \mathbf{v},\mathbf{u}\right>+\beta\left< \mathbf{w},\mathbf{u}\right>.
\end{align*}
@col
Or we can also use $\eqref{ipdefe2}$:
\begin{align*}
\displaystyle \left< \alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right>=(\alpha\mathbf{v}+\beta\mathbf{w})^{t}\mathbf{u}
\end{align*}
\begin{align*}
\displaystyle =\alpha(\mathbf{v})^{t}\mathbf{u}+\beta(\mathbf{w})^{t}\mathbf{u}=\alpha\left< \mathbf{v},\mathbf{u}\right>+\beta\left< \mathbf{w},\mathbf{u}\right>.
\end{align*}
</li>
<li class="ltx_item">
By the previous part and Proposition   @ref{ipdef} item 3, we have
\begin{align*}
\displaystyle \left< \mathbf{u},\alpha\mathbf{v}+\beta\mathbf{w}\right>=\left< \alpha\mathbf{v}+\beta\mathbf{w},\mathbf{u}\right>
\end{align*}
\begin{align*}
\displaystyle =\alpha\left< \mathbf{v},\mathbf{u}\right>+\beta\left< \mathbf{w},\mathbf{u}\right>=\alpha\left< \mathbf{u},\mathbf{v}\right>+\beta\left< \mathbf{u},\mathbf{w}\right>.
\end{align*}
@col
Or we can also use $\eqref{ipdefe2}$ (fill the detail).
</li>
<li class="ltx_item">
We compute
\begin{align*}
\displaystyle \left< \mathbf{0},\mathbf{v}\right>=0[\mathbf{v}]_{1}+\cdots+0[\mathbf{v}]_{m}=0
\end{align*}
and
\begin{align*}
\displaystyle \left< \mathbf{v},\mathbf{0}\right>=[\mathbf{v}]_{1}0+\cdots+[\mathbf{v}]_{m}0=0.
\end{align*}
</li>
<li class="ltx_item">
Suppose $\left< \mathbf{v},\mathbf{x}\right>=0$ for all $\mathbf{x}\in{\mathbb{R}}^{m}$. Let $\mathbf{x}=\mathbf{v}$. Then $\left< \mathbf{v},\mathbf{v}\right>=0$.
By Proposition   @ref{ipdef} item 4, $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
Suppose $\left< \mathbf{v},\mathbf{x}\right>=\left< \mathbf{w},\mathbf{x}\right>$, then $0=\left< \mathbf{v},\mathbf{x}\right>-\left< \mathbf{w},\mathbf{x}\right>=\left< \mathbf{v}-\mathbf{w},\mathbf{x}\right>$
for all $\mathbf{x}\in V$. By the previous part $\mathbf{v}-\mathbf{w}=\mathbf{0}$. So $\mathbf{v}=\mathbf{w}$.
</li>

</ol>

@qed
@endcol
@end
@slide
@defn
@title{Norm}
The <b>norm</b> (or <b>length</b>) of $\mathbf{v}\in{\mathbb{R}}^{n}$ is defined to be $\|\mathbf{v}\|=\sqrt{\left< \mathbf{v},\mathbf{v}\right>}$. Note that $\left< \mathbf{v},\mathbf{v}\right>\geq 0$. So the symbol $\sqrt{\left< \mathbf{v},\mathbf{v}\right>}$ is meaningful.
@end
@slide
@eg
Let $V={\mathbb{R}}^{3}$ with the standard inner product. Let
\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}1\\
2\\
3\end{bmatrix},\mathbf{w}=\begin{bmatrix}1\\
0\\
1\end{bmatrix}.
\end{align*}
@newcol
Then
\begin{align*}
\displaystyle \|\mathbf{v}\|=\sqrt{\left< \mathbf{v},\mathbf{v}\right>}=\sqrt{1^{2}+2^{2}+3^{2}}=\sqrt{14}
\end{align*}
and
\begin{align*}
\displaystyle \|\mathbf{w}\|=\sqrt{\left< \mathbf{w},\mathbf{w}\right>}=\sqrt{1^{2}+0^{2}+1^{2}}=\sqrt{2}.
\end{align*}
@endcol
@end
@slide
@prop
Let $\alpha\in{\mathbb{R}}^{\hbox{}}$ and $\mathbf{v}\in{\mathbb{R}}^{m}$.

<ol class="ltx_enumerate">
<li class="ltx_item">
$\|\mathbf{v}\|=0$ if and only if $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
$\|\alpha\mathbf{v}\|=|\alpha|\|\mathbf{v}\|$.
</li>
<li class="ltx_item">
Suppose that $\mathbf{v}\neq\mathbf{0}$ and let $\alpha=\frac{1}{\|\mathbf{v}\|}$. Then $\|\alpha\mathbf{v}\|=1$.
</li>

</ol>
@end
@proof
@newcol
<ol class="ltx_enumerate">
<li class="ltx_item">
$\|\mathbf{v}\|=0\iff 0=\|\mathbf{v}\|^{2}=\left< \mathbf{v},\mathbf{v}\right>$.
By Proposition (  @ref{ipdef}), item 4, the above is true if and only if $\mathbf{v}=\mathbf{0}$.
</li>
<li class="ltx_item">
$\|\alpha\mathbf{v}\|=\sqrt{\left< \alpha\mathbf{v},\alpha\mathbf{v}\right>}$
$=\sqrt{\alpha\left< \mathbf{v},\alpha\mathbf{v}\right>}=\sqrt{\alpha^{2}\left< \mathbf{v},\mathbf{v}\right>}$
$=|\alpha|\sqrt{\left< \mathbf{v},\mathbf{v}\right>}=|\alpha|\|\mathbf{v}\|$.
</li>
<li class="ltx_item">
By the previous part
\begin{align*}
\displaystyle \|\alpha\mathbf{v}\|=|\alpha|\|\mathbf{v}\|=\frac{1}{\|\mathbf{v}\|}\|\mathbf{v}\|=1.
\end{align*}
</li>

</ol>

@qed
@endcol
@end
@slide
@defn
@title{unit vector}
A vector $\mathbf{v}\in{\mathbb{R}}^{m}$ is said to be a <b>unit vector</b> if $\|\mathbf{v}\|=1$.A non-zero vector $\mathbf{v}$ can be <b>normalized</b> to a unit vector $\frac{\mathbf{v}}{\|\mathbf{v}\|}$ (see the previous proposition item 3).
@end
@slide
@eg
In Example 4, the vectors $\mathbf{v}$ and $\mathbf{w}$ can be normalized to
\begin{align*}
\displaystyle \frac{\mathbf{v}}{\|\mathbf{v}\|}=\frac{\mathbf{v}}{\sqrt{14}}=\begin{bmatrix}\frac{1}{\sqrt{14}}\\
\frac{2}{\sqrt{14}}\\
\frac{3}{\sqrt{14}}\end{bmatrix}
\end{align*}
and
\begin{align*}
\displaystyle \frac{\mathbf{w}}{\|\mathbf{w}\|}=\frac{\mathbf{w}}{\sqrt{2}}=\begin{bmatrix}\frac{1}{\sqrt{2}}\\
0\\
\frac{1}{\sqrt{2}}\end{bmatrix}
\end{align*}
respectively.
@end
@section{Orthogonal sets}
@defn
Two vectors $\mathbf{v}$ and $\mathbf{w}$ in $\mathbb{R}^{n}$ are said <b>orthogonal</b> or <b>perpendicular</b> if $\left< \mathbf{v},\mathbf{w}\right>=0$. In this case we write $\mathbf{v}\perp\mathbf{w}$.
@end
@slide
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
Let $V={\mathbb{R}}^{3}$. Then
\begin{align*}
\displaystyle \begin{bmatrix}1\\
2\\
3\end{bmatrix}\perp\begin{bmatrix}-1\\
-1\\
1\end{bmatrix},
\end{align*}
as
\begin{align*}
\displaystyle \left< \begin{bmatrix}1\\
2\\
3\end{bmatrix},\begin{bmatrix}-1\\
-1\\
1\end{bmatrix}\right>=1\times(-1)+2\times(-1)+3\times 1=0.
\end{align*}
</li>
<li class="ltx_item">
Let $V={\mathbb{R}}^{m}$. Then $\mathbf{e}_{i}\perp\mathbf{e}_{j}$ if $i\neq j$.
</li>

</ol>
@end
@slide
@defn
A subset $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ of ${\mathbb{R}}^{m}$ is said to be <b>orthogonal</b> if the following conditions hold:

<ol class="ltx_enumerate">
<li class="ltx_item">
$\mathbf{0}\notin S$, i.e. $\mathbf{v}_{i}\neq\mathbf{0}$ for $i=1,\ldots,k$.
</li>
<li class="ltx_item">
$\mathbf{v}_{i}\perp\mathbf{v}_{j}$ for $i\neq j$, i.e., $\left< \mathbf{v}_{i},\mathbf{v}_{j}\right>=0$ for $i\neq j$.
</li>

</ol>
@end
@slide
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
$S=\left\{\begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}-2\\
1\end{bmatrix}\right\}$ is orthogonal.
</li>
<li class="ltx_item">
$S=\left\{\begin{bmatrix}1\\
1\\
1\end{bmatrix},\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\right\}$ is orthogonal.
</li>
<li class="ltx_item">
For any $k\leq m$, the set $S=\left\{\mathbf{e}_{1},\mathbf{e}_{2},\ldots,\mathbf{e}_{k}\right\}\subset\mathbb{R}^{m}$ is orthogonal.
</li>

</ol>
@end
@slide
@prop
Let $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{k}\right\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$.
Let
\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},
\end{align*}
\begin{align*}
\displaystyle \mathbf{w}=\beta_{1}\mathbf{v}_{1}+\cdots+\beta_{k}\mathbf{v}_{k}.
\end{align*}
@newcol
Then, for $\alpha_{i},\beta_{i}\in{\mathbb{R}}^{\hbox{}}$, $i=1,\ldots k$, we have
\begin{align*}
\displaystyle \left< \mathbf{v},\mathbf{w}\right>=\alpha_{1}\beta_{1}\|\mathbf{v}_{1}\|^{2}+\cdots+\alpha_{k}\beta_{k}\|\mathbf{v}_{k}\|^{2}.
\end{align*}
@endcol
@end
@proof
@newcol
First for $1\leq i\leq k$, we compute
\begin{align*}
\displaystyle\left< \mathbf{v},\mathbf{v}_{i}\right>&=\left< \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right> \\
&=\alpha_{1}\left< \mathbf{v}_{1},\mathbf{v}_{i}\right>+\cdots+\alpha_{k}\left< \mathbf{v}_{k},\mathbf{v}_{i}\right> \\
&=\alpha_{i}\left< \mathbf{v}_{i},\mathbf{v}_{i}\right>=\alpha_{i}\|\mathbf{v}_{i}\|^{2}.
\end{align*}
@col
The last step follows from the fact that $\left< \mathbf{v}_{j},\mathbf{v}_{i}\right>=0$ for $j\neq i$. But then
\begin{align*}
\displaystyle\left< \mathbf{v},\mathbf{w}\right>&=\left< \mathbf{v},\beta_{1}\mathbf{v}_{1}+\cdots+\beta_{k}\mathbf{v}_{k}\right> \\
&=\beta_{1}\left< \mathbf{v},\mathbf{v}_{1}\right>+\cdots+\beta_{k}\left< \mathbf{v},\mathbf{v}_{k}\right> \\
&=\alpha_{1}\beta_{1}\|\mathbf{v}_{1}\|^{2}+\cdots+\alpha_{k}\beta_{k}\|\mathbf{v}_{k}\|^{2}.
\end{align*}
@qed
@endcol
@end
@slide
@thm
@label{orthind}
Let $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{k}\right\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$. Then $S$ is linearly independent.
@end
@proof
@newcol
Suppose that we have a relation of linear dependence:
\begin{align*}
\displaystyle \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}=\mathbf{0}.
\end{align*}
@col
For $1\leq i\leq k$ we have
\begin{align*}
\displaystyle \left< \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right>=\left< \mathbf{0},\mathbf{v}_{i}\right>=0.
\end{align*}
i.e. for $1\leq i\leq k$,
\begin{align*}
\displaystyle\left< \mathbf{v},\mathbf{v}_{i}\right>&=\left< \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right> \\
&=\alpha_{1}\left< \mathbf{v}_{1},\mathbf{v}_{i}\right>+\cdots+\alpha_{k}\left< \mathbf{v}_{k},\mathbf{v}_{i}\right> \\
&=\alpha_{i}\|\mathbf{v}_{i}\|^{2}=0.
\end{align*}
@col
So for $1\leq i\leq k$ we have
\begin{align*}
\displaystyle \alpha_{i}=0.
\end{align*}
@col
Therefore the relation of linear dependence is trivial. Hence $S$ is linearly independent.
@qed
@endcol
@end
@slide
@thm
@label{o}
Let $S=\left\{\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{k}\right\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$.
Suppose that $\mathbf{v}\in\left< S\right>$. Write
\begin{align*}
\displaystyle \mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},
\end{align*}
for some $\alpha_{i}\in{\mathbb{R}}^{\hbox{}}$, $i=1,\ldots,k$.
Then
\begin{align*}
\displaystyle \alpha_{i}=\frac{\left< \mathbf{v},\mathbf{v}_{i}\right>}{\|\mathbf{v}_{i}\|^{2}},
\end{align*}
i.e.
\begin{align*}
\displaystyle \mathbf{v}=\frac{\left< \mathbf{v},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}+\cdots+\frac{\left< \mathbf{v},\mathbf{v}_{k}\right>}{\|\mathbf{v}_{k}\|^{2}}\mathbf{v}_{k}.
\end{align*}
@end
@proof
@newcol
Suppose that $\mathbf{v}=\alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k}$. Then, for $1\leq i\leq k$, we compute
\begin{align*}
\displaystyle\left< \mathbf{v},\mathbf{v}_{i}\right>&=\left< \alpha_{1}\mathbf{v}_{1}+\cdots+\alpha_{k}\mathbf{v}_{k},\mathbf{v}_{i}\right> \\
&=\alpha_{1}\left< \mathbf{v}_{1},\mathbf{v}_{i}\right>+\cdots+\alpha_{k}\left< \mathbf{v}_{k},\mathbf{v}_{i}\right> \\
&=\alpha_{i}\left< \mathbf{v}_{i},\mathbf{v}_{i}\right>=\alpha_{i}\|\mathbf{v}_{i}\|^{2}.
\end{align*}
@col
Hence
\begin{align*}
\displaystyle \alpha_{i}=\frac{\left< \mathbf{v},\mathbf{v}_{i}\right>}{\|\mathbf{v}_{i}\|^{2}}.
\end{align*}
@qed
@endcol
@end
@remark

@col
The advantage of using the above method is that we don’t have to solve linear equations to find the linear combination.
@endcol

@col
In order to use the theorem, we need to ensure that $\mathbf{v}\in\left< S\right>$.
@endcol
@end
@slide
@eg
We use Example 4, item 2. Let $S=\left\{\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
1\end{bmatrix},\mathbf{v}_{2}=\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\mathbf{v}_{3}=\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\right\}$.
Given that
\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}1\\
2\\
3\end{bmatrix}
\end{align*}
is in $\left< S\right>$, we find the following linear combinations:
\begin{align*}
\displaystyle \alpha_{1}=\frac{\left< \mathbf{v},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}=\frac{6}{3}=2.
\end{align*}
\begin{align*}
\displaystyle \alpha_{2}=\frac{\left< \mathbf{v},\mathbf{v}_{2}\right>}{\|\mathbf{v}_{2}\|^{2}}=\frac{-1}{2}=-\frac{1}{2}.
\end{align*}
\begin{align*}
\displaystyle \alpha_{3}=\frac{\left< \mathbf{v},\mathbf{v}_{3}\right>}{\|\mathbf{v}_{3}\|^{2}}=\frac{-3}{6}=-\frac{1}{2}.
\end{align*}
@newcol
Hence
\begin{align*}
\displaystyle \mathbf{v}=2\mathbf{v}_{1}-\frac{1}{2}\mathbf{v}_{2}-\frac{1}{2}\mathbf{v}_{3}.
\end{align*}
@endcol
@end
@slide
@defn
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
A subset $S$ of $V$ is said to be an <b>orthogonal basis</b> for $V$ if $S$ is a basis of $V$ and $S$ is orthogonal.
@end
@newcol
If $S$ is an orthogonal subset of $V$, then by Theorem   @ref{orthind}, it is automatically linearly independent.
So in order to check if $S$ is an orthogonal basis, we need only check that $\left< S\right>=V$. So we have the following result.

@endcol
@slide
@thm
@label{checkob}
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
Suppose that $S$ is an orthogonal subset of $V$. Then $S$ is an orthogonal basis if and only if $\left< S\right>=V$.
@end
@slide
@cor
Suppose that $S$ is an orthogonal subset of ${\mathbb{R}}^{m}$. Then $S$ is a basis of $\left< S\right>$.
@end
@slide
@cor
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
Suppose that $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\right\}$ is an orthogonal basis of $V$. Then for any $\mathbf{v}\in V$, we have
\begin{align*}
\displaystyle \mathbf{v}=\frac{\left< \mathbf{v},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}+\cdots+\frac{\left< \mathbf{v},\mathbf{v}_{n}\right>}{\|\mathbf{v}_{n}\|^{2}}\mathbf{v}_{n}
\end{align*}
@end
@proof
@newcol
This follows from Theorem   @ref{o}.
@qed
@endcol
@end
@slide
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
The set $S=\left\{\begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}-2\\
1\end{bmatrix}\right\}$
an orthogonal basis of ${\mathbb{R}}^{m}$.
</li>
<li class="ltx_item">
The set $S=\left\{\begin{bmatrix}1\\
1\\
1\end{bmatrix},\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\right\}$ is an orthogonal basis of ${\mathbb{R}}^{3}$.
Indeed, $\dim V=3$ and $S$, with 3 vectors, is linearly independent.
</li>
<li class="ltx_item">
The set $S=\left\{\mathbf{e}_{1},\mathbf{e}_{2},\ldots,\mathbf{e}_{m}\right\}$ is an orthogonal basis of ${\mathbb{R}}^{m}$.
It is called <b>the standard basis</b> for $V$.
</li>

</ol>
@end
@slide
@defn
A subset $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ of ${\mathbb{R}}^{m}$ is said to be <b>orthonormal</b>
if it is orthogonal and every vector in $S$ is a unit vector, i.e.
\begin{align*}
\displaystyle \left< \mathbf{v}_{i},\mathbf{v}_{j}\right>=\begin{cases}1&\text{if $i=j$},\\
0&\text{if $i\neq j$}.\end{cases}
\end{align*}
@newcol
Let $V$ be a subspace of ${\mathbb{R}}^{m}$.
The subset $S$ is said to be an <b>orthonormal basis</b> for $V$ if it is orthonormal and is a basis of $V$.
@endcol
@end
@newcol
Because an orthonormal set $S$ is orthogonal, the above theorems regarding orthogonal sets are also true for orthonormal sets.
In particular we have the following result.

@endcol
@slide
@thm
Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ be an orthonormal subset of ${\mathbb{R}}^{m}$ and let $\mathbf{v}\in\left< S\right>$. Then
\begin{align*}
\displaystyle \mathbf{v}=\left< \mathbf{v},\mathbf{v}_{1}\right>\mathbf{v}_{1}+\cdots+\left< \mathbf{v},\mathbf{v}_{k}\right>\mathbf{v}_{k}.
\end{align*}
@end
@proof
@newcol
By Theorem   @ref{o} and $\|\mathbf{v}_{i}\|=1$ for $i=1,\ldots,k$.
@qed
@endcol
@end
@newcol
If $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ is an orthogonal subset of ${\mathbb{R}}^{m}$, then
$\left\{\frac{\mathbf{v}_{1}}{\|\mathbf{v}_{1}\|},\ldots,\frac{\mathbf{v}_{k}}{\|\mathbf{v}_{k}\|}\right\}$
is an orthonormal subset. The process is called <b>normalization</b>.
@endcol
@slide
@eg
<ol class="ltx_enumerate">
<li class="ltx_item">
The set $S=\left\{\begin{bmatrix}1\\
2\end{bmatrix},\begin{bmatrix}-2\\
1\end{bmatrix}\right\}$ is
an orthogonal basis of ${\mathbb{R}}^{2}$. Normalizing it, we obtain an orthonormal basis
\begin{align*}
\displaystyle S^{\prime}=\left\{\frac{1}{\sqrt{5}}\begin{bmatrix}1\\
2\end{bmatrix},\frac{1}{\sqrt{5}}\begin{bmatrix}-2\\
1\end{bmatrix}\right\}.
\end{align*}
</li>
<li class="ltx_item">
The set $S=\left\{\begin{bmatrix}1\\
1\\
1\end{bmatrix},\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\right\}$ is an orthogonal basis of ${\mathbb{R}}^{3}$.
Normalizing it, we obtain an orthonormal basis
\begin{align*}
\displaystyle S^{\prime}=\left\{\frac{1}{\sqrt{3}}\begin{bmatrix}1\\
1\\
1\end{bmatrix},\frac{1}{\sqrt{2}}\begin{bmatrix}1\\
-1\\
0\end{bmatrix},\frac{1}{\sqrt{6}}\begin{bmatrix}1\\
1\\
-2\end{bmatrix}\right\}.
\end{align*}
</li>

</ol>
@end
@section{Gram-Schmidt Orthogonalization process}

Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$. If $\mathbf{w}\in\left< S\right>$, then
\begin{align*}
\displaystyle \mathbf{w}=\frac{\left< \mathbf{w},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}+\cdots+\frac{\left< \mathbf{w},\mathbf{v}_{k}\right>}{\|\mathbf{v}_{k}\|^{2}}\mathbf{v}_{k}.
\end{align*}
@newcol
But what if $\mathbf{w}$ is not in $\left< S\right>$? Let’s compare the difference. We have the following theorem.

@endcol
@slide
@thm
@label{perp}
Let $S=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ be an orthogonal subset of ${\mathbb{R}}^{m}$ and let $\mathbf{w}\in{\mathbb{R}}^{m}$. Then, for each $i=1,\ldots,k$, the vector
\begin{align*}
\displaystyle \mathbf{v}=\mathbf{w}-\frac{\left< \mathbf{w},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\cdots-\frac{\left< \mathbf{w},\mathbf{v}_{k}\right>}{\|\mathbf{v}_{k}\|^{2}}\mathbf{v}_{k}
\end{align*}
is perpendicular to $\mathbf{v}_{i}$.
@end
@proof
@newcol
For $1\leq i\leq k$, we compute
\begin{align*}
\displaystyle \left< \mathbf{v},\mathbf{v}_{i}\right>=\left< \mathbf{w},\mathbf{v}_{i}\right>-\frac{\left< \mathbf{w},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\left< \mathbf{v}_{1},\mathbf{v}_{i}\right>-\cdots-\frac{\left< \mathbf{w},\mathbf{v}_{k}\right>}{\|\mathbf{v}_{k}\|^{2}}\left< \mathbf{v}_{k},\mathbf{v}_{i}\right>.
\end{align*}
@col
Because $\left< \mathbf{v}_{j},\mathbf{v}_{i}\right>$ is $0$ unless $j=i$, the above becomes
\begin{align*}
\displaystyle \left< \mathbf{w},\mathbf{v}_{i}\right>-\frac{\left< \mathbf{w},\mathbf{v}_{i}\right>}{\|\mathbf{v}_{i}\|^{2}}\left< \mathbf{v}_{i},\mathbf{v}_{i}\right>=\left< \mathbf{w},\mathbf{v}_{i}\right>-\left< \mathbf{v},\mathbf{v}_{i}\right>=0.
\end{align*}
@col
Hence $\mathbf{v}\perp\mathbf{v}_{i}$ for $i=1,\ldots,k$.
@qed
@endcol
@end
@slide
@thm
@title{Gram-Schmidt Orthogonalization Process}
@label{GSO}
Let $S=\left\{\mathbf{w}_{1},\mathbf{w}_{2},\ldots,\mathbf{w}_{k}\right\}$ be a linearly independent subset of $V$.
Let $\mathbf{v}_{1}=\mathbf{w}_{1}$ and set
\begin{align*}
\displaystyle \mathbf{v}_{\ell}=\mathbf{w}_{\ell}-\frac{\left< \mathbf{w}_{\ell},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\cdots-\frac{\left< \mathbf{w}_{\ell},\mathbf{v}_{\ell-1}\right>}{\|\mathbf{v}_{\ell-1}\|^{2}}\mathbf{v}_{\ell-1}\,\,\text{ for $2\leq\ell\leq k$}.
\end{align*}
@newcol
Then $S^{\prime}=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$ is an orthogonal set.Moreover, $\left< \left\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell}\right\}\right>=\left< \left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell}\right\}\right>$ for $\ell=1,\ldots,k$.
In particular $\left< S\right>=\left< S^{\prime}\right>$. The process of obtaining $S^{\prime}$ by the above procedure is called the <b>Gram-Schmidt Orthogonalization process</b>.
@endcol
@end
@proof
@newcol
We have $\left< \left\{\mathbf{w}_{1}\right\}\right>=\left< \left\{\mathbf{v}_{1}\right\}\right>$.
We are going to add one vector at a time. Suppose that $\left< \left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell-1}\right\}\right>=\left< \left\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell-1}\right\}\right>$
and that the set $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell-1}\right\}$ is orthogonal. Thus $\left< \left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell}\right\}\right>=\left< \left\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell-1},\mathbf{v}_{\ell}\right\}\right>$
$=\left< \left\{\mathbf{w}_{1},\ldots,\mathbf{w}_{\ell-1},\mathbf{w}_{\ell}\right\}\right>$. By Theorem   @ref{perp}, $\mathbf{v}_{\ell}\perp\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell-1}$. Thus $\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{\ell}\right\}$ is orthogonal.
We repeat the process by increasing $\ell$ until $\ell=k$.
@qed
@endcol
@end
@slide
@cor
Suppose that $V$ is a subspace of ${\mathbb{R}}^{m}$. Then there exists an orthogonal (orthonormal basis) of $V$.
@end
@proof
@newcol
By Lecture 18 Theorem 8, there exists a basis $S=\left\{\mathbf{w}_{1},\ldots,\mathbf{w}_{k}\right\}$ for $V$.
Applying Gram-Schmidt orthogonalization process to $S$, we obtain an orthogonal set $S^{\prime}=\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{k}\right\}$.
By Theorem   @ref{GSO}, $\left< S^{\prime}\right>=\left< S\right>=V$. By Theorem   @ref{checkob}, $S^{\prime}$ is an orthogonal basis. Normalizing $S^{\prime}$, we can also obtain an orthonormal basis.
@qed
@endcol
@end
@newcol
The above proof actually describes a method to find orthogonal (orthonormal) basis of $V$.
@endcol
@slide
@eg
Let $V={\mathbb{R}}^{4}$ with the standard inner product. Let
\begin{align*}
\displaystyle \mathbf{w}_{1}=\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix},\mathbf{w}_{2}=\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix},\mathbf{w}_{3}=\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}.
\end{align*}
@newcol
Then $\left\{\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3}\right\}$ is linearly independent.
We can apply Gram-Schmidt orthogonalization process to this set of vectors. Take $\mathbf{v}_{1}=\mathbf{w}_{1}=\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix}$. Then
\begin{align*}
\displaystyle \mathbf{v}_{2}=\mathbf{w}_{2}-\frac{\left< \mathbf{w}_{2},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}=\begin{bmatrix}1\\
1\\
1\\
1\end{bmatrix}-\frac{2}{2}\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix}=\begin{bmatrix}0\\
1\\
0\\
1\end{bmatrix}.
\end{align*}
@col
Also
\begin{align*}
\displaystyle \mathbf{v}_{3}=\mathbf{w}_{3}-\frac{\left< \mathbf{w}_{3},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\frac{\left< \mathbf{w}_{3},\mathbf{v}_{2}\right>}{\|\mathbf{v}_{2}\|^{2}}\mathbf{v}_{2}=\begin{bmatrix}0\\
1\\
2\\
1\end{bmatrix}-\frac{2}{2}\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix}-\frac{2}{2}\begin{bmatrix}0\\
1\\
0\\
1\end{bmatrix}=\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix}.
\end{align*}
@col
The set $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\mathbf{v}_{3}\right\}$ is an orthogonal basis of $\left< \left\{\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3}\right\}\right>$.
To obtain an orthonormal basis of $\left< \left\{\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3}\right\}\right>$, we can normalized the vectors
\begin{align*}
\displaystyle \mathbf{u}_{1}=\frac{\mathbf{v}_{1}}{\|\mathbf{v}_{1}\|}=\frac{1}{\sqrt{2}}\begin{bmatrix}1\\
0\\
1\\
0\end{bmatrix},\mathbf{u}_{2}=\frac{\mathbf{v}_{2}}{\|\mathbf{v}_{2}\|}=\frac{1}{\sqrt{2}}\begin{bmatrix}0\\
1\\
0\\
1\end{bmatrix},\mathbf{u}_{3}=\frac{\mathbf{v}_{3}}{\|\mathbf{v}_{3}\|}=\frac{1}{\sqrt{2}}\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix}.
\end{align*}
@endcol
@end
@eg
Let $V={\mathcal{N}}\!\left(\begin{bmatrix}1&1&1&1\end{bmatrix}\right)$. Find an orthonormal basis of $V$. The set
\begin{align*}
\displaystyle S=\left\{\mathbf{w}_{1}=\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\mathbf{w}_{2}=\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix},\mathbf{w}_{3}=\begin{bmatrix}-1\\
0\\
0\\
1\end{bmatrix}\right\}
\end{align*}
is a basis of $V$. We apply Gram-Schmidt orthogonalization process to the set $S$:
\begin{align*}
\displaystyle \mathbf{v}_{1}=\mathbf{w}_{1}=\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix}
\end{align*}
\begin{align*}
\displaystyle \mathbf{v}_{2}=\mathbf{w}_{2}-\frac{\left< \mathbf{w}_{2},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}=\begin{bmatrix}-1\\
0\\
1\\
0\end{bmatrix}-\frac{1}{2}\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix}=\begin{bmatrix}-1/2\\
-1/2\\
1\\
0\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle \mathbf{v}_{3}=\mathbf{w}_{3}-\frac{\left< \mathbf{w}_{3},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\frac{\left< \mathbf{w}_{3},\mathbf{v}_{2}\right>}{\|\mathbf{v}_{2}\|^{2}}\mathbf{v}_{2}=\begin{bmatrix}-1\\
0\\
0\\
1\end{bmatrix}-\frac{1}{2}\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix}-\frac{1}{3}\begin{bmatrix}-1/2\\
-1/2\\
1\\
0\end{bmatrix}=\begin{bmatrix}-1/3\\
-1/3\\
-1/3\\
1\end{bmatrix}.
\end{align*}
@newcol
So
\begin{align*}
\displaystyle \left\{\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\begin{bmatrix}-1/2\\
-1/2\\
1\\
0\end{bmatrix},\begin{bmatrix}-1/3\\
-1/3\\
-1/3\\
1\end{bmatrix}\right\}.
\end{align*}
is an orthogonal basis of $V$. Normalizing it, we can obtain an orthonormal basis of $V$:
\begin{align*}
\displaystyle \left\{\frac{1}{\sqrt{2}}\begin{bmatrix}-1\\
1\\
0\\
0\end{bmatrix},\frac{1}{\sqrt{6}}\begin{bmatrix}-1\\
-1\\
2\\
0\end{bmatrix},\frac{1}{\sqrt{12}}\begin{bmatrix}-1\\
-1\\
-1\\
3\end{bmatrix}\right\}.
\end{align*}
@col
The above process will be easier if we start with another basis:
\begin{align*}
\displaystyle S=\left\{\mathbf{w}_{1}=\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix},\mathbf{w}_{2}=\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix},\mathbf{w}_{3}=\begin{bmatrix}0\\
1\\
-1\\
0\end{bmatrix}\right\}
\end{align*}
@col
Now the first two vectors are perpendicular. Apply Gram-Schmidt orthogonalization process to it:
\begin{align*}
\displaystyle \mathbf{v}_{1}=\mathbf{w}_{1}=\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle \mathbf{v}_{2}=\mathbf{w}_{2}-\frac{\left< \mathbf{w}_{2},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}=\mathbf{w}_{2}-0\mathbf{v}_{1}=\mathbf{w}_{2}=\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix}.
\end{align*}
\begin{align*}
\displaystyle \mathbf{v}_{3}=\mathbf{w}_{3}-\frac{\left< \mathbf{w}_{3},\mathbf{v}_{1}\right>}{\|\mathbf{v}_{1}\|^{2}}\mathbf{v}_{1}-\frac{\left< \mathbf{w}_{3},\mathbf{v}_{2}\right>}{\|\mathbf{v}_{2}\|^{2}}\mathbf{v}_{2}=\begin{bmatrix}1/2\\
1/2\\
-1/2\\
-1/2\end{bmatrix}.
\end{align*}
@col
So
\begin{align*}
\displaystyle \left\{\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix},\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix},\begin{bmatrix}1/2\\
1/2\\
-1/2\\
-1/2\end{bmatrix}\right\}
\end{align*}
is an orthogonal basis of $V$. Normalizing it, we obtain an orthonormal basis
\begin{align*}
\displaystyle \left\{\frac{1}{\sqrt{2}}\begin{bmatrix}1\\
-1\\
0\\
0\end{bmatrix},\frac{1}{\sqrt{2}}\begin{bmatrix}0\\
0\\
1\\
-1\end{bmatrix},\begin{bmatrix}1/2\\
1/2\\
-1/2\\
-1/2\end{bmatrix}\right\}.
\end{align*}
@endcol
@end
@section{Cauchy-Schwarz Inequality}

<b>Can be skipped, will not appear in final exam</b>

@slide
@thm
@title{Cauchy-Schwarz Inequality}
For $\mathbf{v},\mathbf{w}\in{\mathbb{R}}^{m}$,
\begin{align*}
\displaystyle |\left< \mathbf{v},\mathbf{w}\right>|\leq\|\mathbf{v}\|\|\mathbf{w}\|.
\end{align*}
@end
@proof
@newcol
The statement is trivial if $\mathbf{w}=\mathbf{0}$. Suppose $\mathbf{w}\neq\mathbf{0}$. Let $t\in{\mathbb{R}}^{\hbox{}}$, then
\begin{align*}
\displaystyle 0\leq\|\mathbf{v}-t\mathbf{w}\|^{2}=\left< \mathbf{v}-t\mathbf{w},\mathbf{v}-t\mathbf{w}\right>=\left< \mathbf{v},\mathbf{v}-t\mathbf{w}\right>-t\left< \mathbf{w},\mathbf{v}-t\mathbf{w}\right>
\end{align*}
\begin{align*}
\displaystyle =\left< \mathbf{v},\mathbf{v}\right>-t\left< \mathbf{v},\mathbf{w}\right>-t\left< \mathbf{w},\mathbf{v}\right>+t^{2}\left< \mathbf{w},\mathbf{w}\right>=\left< \mathbf{v},\mathbf{v}\right>-2t\left< \mathbf{v},\mathbf{w}\right>+t^{2}\left< \mathbf{w},\mathbf{w}\right>
\end{align*}
@col
Substituting
\begin{align*}
\displaystyle t=\frac{\left< \mathbf{v},\mathbf{w}\right>}{\left< \mathbf{w},\mathbf{w}\right>}
\end{align*}
into the above, we obtain
\begin{align*}
\displaystyle 0\leq\left< \mathbf{v},\mathbf{v}\right>-\frac{|\left< \mathbf{v},\mathbf{w}\right>|^{2}}{\left< \mathbf{w},\mathbf{w}\right>}=\|\mathbf{v}\|^{2}-\frac{|\left< \mathbf{v},\mathbf{w}\right>|^{2}}{\|\mathbf{w}\|^{2}}.
\end{align*}
@col
Hence
\begin{align*}
\displaystyle |\left< \mathbf{v},\mathbf{w}\right>|\leq\sqrt{\|\mathbf{v}\|^{2}\|\mathbf{w}\|^{2}}=\|\mathbf{v}\|\|\mathbf{w}\|.
\end{align*}
@qed
@endcol
@end
@remark

@col
The $t$ above is obtained by minimizing the quadratic equation
$\left< \mathbf{v},\mathbf{v}\right>-2t\left< \mathbf{v},\mathbf{w}\right>+t^{2}\left< \mathbf{w},\mathbf{w}\right>$.
@endcol

@col
Following the proof, the equality occurs if (i) $\mathbf{v}=\mathbf{0}$ or (ii) $\mathbf{w}=\mathbf{0}$
or (iii) $\mathbf{v}-t\mathbf{w}=0$ $\Leftrightarrow$ $\mathbf{v}$ and $\mathbf{w}$ are parallel,
i.e., $\mathbf{v}=\alpha\mathbf{w}$ for some scalar $\alpha\in{\mathbb{R}}^{\hbox{}}$.
@endcol
@end
@slide
@thm
@title{Triangle Inequality}
\begin{align*}
\displaystyle \|\mathbf{v}+\mathbf{w}\|\leq\|\mathbf{v}\|+\|\mathbf{w}\|.
\end{align*}
@end
@proof
@newcol
\begin{align*}
\displaystyle \|\mathbf{v}+\mathbf{w}\|^{2}=\left< \mathbf{v},\mathbf{v}\right>+\left< \mathbf{v},\mathbf{w}\right>+\left< \mathbf{w},\mathbf{v}\right>+\left< \mathbf{w},\mathbf{w}\right>=\|\mathbf{v}\|^{2}+2\left< \mathbf{v},\mathbf{w}\right>+\|\mathbf{w}\|^{2}.
\end{align*}
@col
By the Cauchy-Schwarz inequality
\begin{align*}
\displaystyle |\left< \mathbf{v},\mathbf{w}\right>|\leq\|\mathbf{v}\|\|\mathbf{w}\|,
\end{align*}
thus
\begin{align*}
\displaystyle \|\mathbf{v}+\mathbf{w}\|^{2}\leq\|\mathbf{v}\|^{2}+2\|\mathbf{v}\|\|\mathbf{w}\|+\|\mathbf{w}\|^{2}=(\|\mathbf{v}\|+\|\mathbf{w}\|)^{2}.
\end{align*}
@col
The result follows by taking square roots on both sides.
@qed
@endcol
@end
@slide
@eg
Let $\mathbf{v},\mathbf{w}\in{\mathbb{R}}^{m}$ with the standard inner product. Let
\begin{align*}
\displaystyle \mathbf{v}=\begin{bmatrix}v_{1}\\
\vdots\\
v_{m}\end{bmatrix}\qquad\mathbf{w}=\begin{bmatrix}w_{1}\\
\vdots\\
w_{m}\end{bmatrix}
\end{align*}
@newcol
Cauchy-Schwarz inequality:
\begin{align*}
\displaystyle |v_{1}w_{1}+\cdots+v_{m}w_{m}|\leq\sqrt{v_{1}^{2}+\cdots+v_{m}^{2}}\sqrt{w_{1}^{2}+\cdots+w_{m}^{2}}.
\end{align*}
@col
Triangle inequality:
\begin{align*}
\displaystyle \sqrt{(v_{1}+w_{1})^{2}+\cdots+(v_{m}+w_{m})^{2}}\leq\sqrt{v_{1}^{2}+\cdots+v_{m}^{2}}+\sqrt{w_{1}^{2}+\cdots+w_{m}^{2}}.
\end{align*}
@endcol
@end
